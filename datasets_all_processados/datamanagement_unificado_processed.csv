description,storypoint,issuekey,dataset_name,treated_description_ml,treated_description_bert
"Finalize DM mission statement The proposed mission statement for the LSST Software Stack development: {quote} Enabling LSST science by creating a well documented, state-of-the-art, high-performance, scalable, multi-camera, open source, O/IR survey data processing and analysis system {quote} with the following rationale:  * well documented -- as otherwise it will be impossible to maintain or be usable for Level 3 * state-of-the-art -- because the quality of the instrument and needs of the science to be systematic limited require us to develop new algorithms an break new ground in a number of areas * high-performance -- because of the massive amount of data we need to process for a reasonable budget * scalable -- because we need it to run from between a single core (for developers, or Level 3 users) to tens of thousands of cores (for Data Release production) * multi-camera -- because we need to process precursor data for development purposes, and because our as-delivered camera won't be ideal. * open source -- because we want the community and future surveys to benefit from our efforts. ",2,DM-8,datamanagement,finalize dm mission statement propose mission statement lsst software stack development quote enabling lsst science create document state art high performance scalable multi camera open source ir survey datum processing analysis system quote following rationale document impossible maintain usable level state art quality instrument need science systematic limited require develop new algorithm break new ground number area high performance massive datum need process reasonable budget scalable need run single core developer level user ten thousand core data release production multi camera need process precursor datum development purpose deliver camera will ideal open source want community future survey benefit effort,"Finalize DM mission statement The proposed mission statement for the LSST Software Stack development: {quote} Enabling LSST science by creating a well documented, state-of-the-art, high-performance, scalable, multi-camera, open source, O/IR survey data processing and analysis system {quote} with the following rationale: * well documented -- as otherwise it will be impossible to maintain or be usable for Level 3 * state-of-the-art -- because the quality of the instrument and needs of the science to be systematic limited require us to develop new algorithms an break new ground in a number of areas * high-performance -- because of the massive amount of data we need to process for a reasonable budget * scalable -- because we need it to run from between a single core (for developers, or Level 3 users) to tens of thousands of cores (for Data Release production) * multi-camera -- because we need to process precursor data for development purposes, and because our as-delivered camera won't be ideal. * open source -- because we want the community and future surveys to benefit from our efforts."
"Open up LSST software mailing lists We all benefit from making LSST software development as open as possible and conducive to outside volunteer contributions (*). One way to increase community involvement is to open up our development mailing lists to the public, analogous to the way other open source projects do. For example, we could have:  * software-devel@lsstcorp.org: the development mailing list, equivalent to current lsst-data * software-users@lsstcorp.org: the users mailing list, equivalent to current lsst-dm-stack-users mailing list (but it could possibly be replaced by StackOverflow/Confluence Questions) * lsst-dm@lsstcorp.org: internal, DM-staff only mailing list, for the *rare* discussions/notices that should go out to staff only.  (*) Though we don't rely on them for meeting the project specs (legally required disclaimer :) ).",1,DM-9,datamanagement,open lsst software mailing list benefit make lsst software development open possible conducive outside volunteer contribution way increase community involvement open development mailing list public analogous way open source project example software-devel@lsstcorp.org development mailing list equivalent current lsst datum software-users@lsstcorp.org user mailing list equivalent current lsst dm stack user mailing list possibly replace stackoverflow confluence questions lsst-dm@lsstcorp.org internal dm staff mailing list rare discussion notice staff rely meet project spec legally require disclaimer :),"Open up LSST software mailing lists We all benefit from making LSST software development as open as possible and conducive to outside volunteer contributions (*). One way to increase community involvement is to open up our development mailing lists to the public, analogous to the way other open source projects do. For example, we could have: * software-devel@lsstcorp.org: the development mailing list, equivalent to current lsst-data * software-users@lsstcorp.org: the users mailing list, equivalent to current lsst-dm-stack-users mailing list (but it could possibly be replaced by StackOverflow/Confluence Questions) * lsst-dm@lsstcorp.org: internal, DM-staff only mailing list, for the *rare* discussions/notices that should go out to staff only. (*) Though we don't rely on them for meeting the project specs (legally required disclaimer :) )."
Release EUPS 1.3.0 Release EUPS 1.3.0 in RHL's github repository.,1,DM-20,datamanagement,release eups 1.3.0 release eups 1.3.0 rhl github repository,Release EUPS 1.3.0 Release EUPS 1.3.0 in RHL's github repository.
Determine final URL/location for W'14 stack Need to know where W'14 stack files are going to be housed.,1,DM-21,datamanagement,determine final url location w'14 stack need know w'14 stack file go house,Determine final URL/location for W'14 stack Need to know where W'14 stack files are going to be housed.
Build Winter'14 release Run lsst-build scripts and 'eups distrib create' to build the Winter'14 release.,1,DM-25,datamanagement,build winter'14 release run lsst build script eup distrib create build winter'14 release,Build Winter'14 release Run lsst-build scripts and 'eups distrib create' to build the Winter'14 release.
Update installation instructions Update Confluence instructions on how to install the Winter'14 stack.  The instructions are at: https://confluence.lsstcorp.org/display/LSWUG/LSST+Software+User+Guide,1,DM-26,datamanagement,update installation instruction update confluence instruction install winter'14 stack instruction https://confluence.lsstcorp.org/display/lswug/lsst+software+user+guide,Update installation instructions Update Confluence instructions on how to install the Winter'14 stack. The instructions are at: https://confluence.lsstcorp.org/display/LSWUG/LSST+Software+User+Guide
"Modify gitolite permissions to allow issue/DM-NNNN branches Use issue/DM-NNNN branches for issues tracked in JIRA, to differentiate them from tickets/NNNN branches that are still in Trac.",1,DM-28,datamanagement,modify gitolite permission allow issue dm nnnn branch use issue dm nnnn branch issue track jira differentiate ticket nnnn branch trac,"Modify gitolite permissions to allow issue/DM-NNNN branches Use issue/DM-NNNN branches for issues tracked in JIRA, to differentiate them from tickets/NNNN branches that are still in Trac."
Qserv configuration - detailed design Detailed design covering how all Qserv components will be configured for runtime. ,3,DM-52,datamanagement,qserv configuration detail design detailed design cover qserv component configure runtime,Qserv configuration - detailed design Detailed design covering how all Qserv components will be configured for runtime.
"Node configuration and bootstrapping - detailed design Design covering how all Qserv components will be configured for runtime.  Design how new Qserv nodes will be bootstrapped when we add them to the cluster, and how already added nodes will get updated after they were offline (crashed, turned off for maintenance etc) ",6,DM-53,datamanagement,node configuration bootstrappe detailed design design cover qserv component configure runtime design new qserv node bootstrappe add cluster add node update offline crash turn maintenance etc,"Node configuration and bootstrapping - detailed design Design covering how all Qserv components will be configured for runtime. Design how new Qserv nodes will be bootstrapped when we add them to the cluster, and how already added nodes will get updated after they were offline (crashed, turned off for maintenance etc)"
"Data Distribution Design v1 Need to come up with detailed design covering how we will deal with data distribution: managing multiple replicas, recovering from faults, adding new nodes to the cluster, registering new data from L2 ingest and user data (L3).",8,DM-71,datamanagement,data distribution design v1 need come detailed design cover deal data distribution manage multiple replicas recover fault add new node cluster register new datum l2 ingest user datum l3,"Data Distribution Design v1 Need to come up with detailed design covering how we will deal with data distribution: managing multiple replicas, recovering from faults, adding new nodes to the cluster, registering new data from L2 ingest and user data (L3)."
"Modify format of version numbers The versions auto-generated by the new EUPS+buildbot look like this:  {code} ========== $ eups list .... pipe_tasks            7.3.2.0_5_g455c355d0f+070b2c1b35  b61 pyfits                3.1.2+9ef17db9b7  b61 b60 python                0.0.1             b61 b60 scisql                0.3+2b5a2f1b52    b61 b60 scons                 2.1.0+2b5a2f1b52  b61 b60 sconsUtils            6.2.0.0_11_gf38997df3e+759c3944a1         b61 sconsUtils            6.2.0.0_19_g755151c0a5+759c3944a1         b60 shapelet              7.3.1.0_1_g9331ee763c+0c72f294dd  b61 shapelet              7.3.1.0_1_g9331ee763c+e56ee84a68  b60 skymap                7.3.1.0_1_g64b750c066+db36490146  b60 skymap                7.3.1.0_1_ga6cd540cd3+493a438aa2  b61 skypix                6.1.0.0_1_g1157bf09ae+65137c93cd  b61 skypix                6.1.0.0_1_gea33592463+6039c04989  b60 .... ========== {code}  where the part before the plus sign is the output of git describe (slightly mangled), and the part after is the SHA1 of the sorted names+sha1s of the dependencies.  While this has the benefit that any two causally disconnected buildbots with the same inputs will build the same versions, many people have complained that they're plain ugly.  So here's an alternative proposal:  * If a tag exist on a commit, use <tag> as the version. * If there's no tag, use branchname-gSHA1ABBREV, where any illegal characters in branchname get turned into dots * If the package has dependencies, and a build of this package with different dependencies already exists, append a +N to the end. Keep the mapping of +N -> (dependency name, sha1s) in a special git repository. Given the source code and this git repo, two causally disconnected buildbots will again generate the same set of versions.  Example versions: * 7.10.2.1 * 7.10.2.1+5 * master-gdeadbeef * feature.dm-1234-gdeadbeef * feature.dm-1234-gdeadbeef+3 ",1,DM-75,datamanagement,modify format version number version auto generate new eups+buildbot look like code eup list pipe_task 7.3.2.0_5_g455c355d0f+070b2c1b35 b61 pyfit 3.1.2 9ef17db9b7 b61 b60 python 0.0.1 b61 b60 scisql 0.3 2b5a2f1b52 b61 b60 scon 2.1.0 2b5a2f1b52 b61 b60 sconsutils 6.2.0.0_11_gf38997df3e+759c3944a1 b61 sconsutils 6.2.0.0_19_g755151c0a5 759c3944a1 b60 shapelet 7.3.1.0_1_g9331ee763c+0c72f294dd b61 shapelet 7.3.1.0_1_g9331ee763c+e56ee84a68 b60 skymap 7.3.1.0_1_g64b750c066+db36490146 b60 skymap 7.3.1.0_1_ga6cd540cd3 493a438aa2 b61 skypix 6.1.0.0_1_g1157bf09ae+65137c93cd b61 skypix 6.1.0.0_1_gea33592463 6039c04989 b60 code plus sign output git describe slightly mangle sha1 sorted names+sha1s dependency benefit causally disconnect buildbot input build version people complain plain ugly alternative proposal tag exist commit use version tag use branchname gsha1abbrev illegal character branchname turn dot package dependency build package different dependency exist append end mapping dependency sha1s special git repository give source code git repo causally disconnect buildbot generate set version example version 7.10.2.1 7.10.2.1 master gdeadbeef feature.dm-1234 gdeadbeef feature.dm-1234 gdeadbeef+3,"Modify format of version numbers The versions auto-generated by the new EUPS+buildbot look like this: {code} ========== $ eups list .... pipe_tasks 7.3.2.0_5_g455c355d0f+070b2c1b35 b61 pyfits 3.1.2+9ef17db9b7 b61 b60 python 0.0.1 b61 b60 scisql 0.3+2b5a2f1b52 b61 b60 scons 2.1.0+2b5a2f1b52 b61 b60 sconsUtils 6.2.0.0_11_gf38997df3e+759c3944a1 b61 sconsUtils 6.2.0.0_19_g755151c0a5+759c3944a1 b60 shapelet 7.3.1.0_1_g9331ee763c+0c72f294dd b61 shapelet 7.3.1.0_1_g9331ee763c+e56ee84a68 b60 skymap 7.3.1.0_1_g64b750c066+db36490146 b60 skymap 7.3.1.0_1_ga6cd540cd3+493a438aa2 b61 skypix 6.1.0.0_1_g1157bf09ae+65137c93cd b61 skypix 6.1.0.0_1_gea33592463+6039c04989 b60 .... ========== {code} where the part before the plus sign is the output of git describe (slightly mangled), and the part after is the SHA1 of the sorted names+sha1s of the dependencies. While this has the benefit that any two causally disconnected buildbots with the same inputs will build the same versions, many people have complained that they're plain ugly. So here's an alternative proposal: * If a tag exist on a commit, use  as the version. * If there's no tag, use branchname-gSHA1ABBREV, where any illegal characters in branchname get turned into dots * If the package has dependencies, and a build of this package with different dependencies already exists, append a +N to the end. Keep the mapping of +N -> (dependency name, sha1s) in a special git repository. Given the source code and this git repo, two causally disconnected buildbots will again generate the same set of versions. Example versions: * 7.10.2.1 * 7.10.2.1+5 * master-gdeadbeef * feature.dm-1234-gdeadbeef * feature.dm-1234-gdeadbeef+3"
"Save a git-branch when a forced push is detected Create a gitolite hook that will save a branch when a forced push is detected.  E.g., if we have a ticket: 'tickets/DM-AAAA' and someone rebases it and pushes  with '--force' before applying the update --- then the hook will branch off the old state into (say):  backups/tickets/DM-AAAA/NNNN where NNNN is a monotonically increasing number (per branch).",1,DM-78,datamanagement,save git branch force push detect create gitolite hook save branch force push detect e.g. ticket ticket dm aaaa rebase push apply update hook branch old state backup ticket dm aaaa nnnn nnnn monotonically increase number branch,"Save a git-branch when a forced push is detected Create a gitolite hook that will save a branch when a forced push is detected. E.g., if we have a ticket: 'tickets/DM-AAAA' and someone rebases it and pushes with '--force' before applying the update --- then the hook will branch off the old state into (say): backups/tickets/DM-AAAA/NNNN where NNNN is a monotonically increasing number (per branch)."
"from __future__ import division breaks division of Extent* If one does: {{from __future__ import division}} the division operator on Extent types raises an exception.  How to repeat: I've tried this with v7_3 and master: {code:py} from __future__ import division import lsst.afw.geom as afwGeom npt = afwGeom.Extent2I(10,10)/2 {code} an exception is raised.  Removing the first line succeeds as expected.",4,DM-83,datamanagement,"future import division break division extent future import division division operator extent type raise exception repeat try v7_3 master code py future import division import lsst.afw.geom afwgeom npt afwgeom extent2i(10,10)/2 code exception raise remove line succeed expect","from __future__ import division breaks division of Extent* If one does: {{from __future__ import division}} the division operator on Extent types raises an exception. How to repeat: I've tried this with v7_3 and master: {code:py} from __future__ import division import lsst.afw.geom as afwGeom npt = afwGeom.Extent2I(10,10)/2 {code} an exception is raised. Removing the first line succeeds as expected."
"tests/testPsfDetermination.py has a broken test In meas_algorithms tests/testPsfDetermination.py has a test testRejectBlends which does not operate as expected. When it calls pcaPsfDeterminer it results in no usable psf candidates BEFORE blends are rejected. Formerly this resulted in a numpy array named ""sizes"" containing one uninitialized value, which might raise an unexpected exception or raise the desired exception, depending on whether that value was negative or positive.    On tickets/DM-3117 I pushed a fix for the bug that caused the invalid ""sizes"" array, but the unit test is now reliably broken because no viable psf candidates raises the wrong exception and does not test blend rejection in any case. So on this same ticket I have commented out the bad test for now.",2,DM-92,datamanagement,test testpsfdetermination.py broken test meas_algorithm test testpsfdetermination.py test testrejectblend operate expect call pcapsfdeterminer result usable psf candidate blend reject result numpy array name size contain uninitialized value raise unexpected exception raise desire exception depend value negative positive ticket push fix bug cause invalid size array unit test reliably break viable psf candidate raise wrong exception test blend rejection case ticket comment bad test,"tests/testPsfDetermination.py has a broken test In meas_algorithms tests/testPsfDetermination.py has a test testRejectBlends which does not operate as expected. When it calls pcaPsfDeterminer it results in no usable psf candidates BEFORE blends are rejected. Formerly this resulted in a numpy array named ""sizes"" containing one uninitialized value, which might raise an unexpected exception or raise the desired exception, depending on whether that value was negative or positive. On tickets/DM-3117 I pushed a fix for the bug that caused the invalid ""sizes"" array, but the unit test is now reliably broken because no viable psf candidates raises the wrong exception and does not test blend rejection in any case. So on this same ticket I have commented out the bad test for now."
"Configure transition screens for DM agile workflow Whenever an issue is transitioned on JIRA Agile board to 'Ready for Review', a screen should pop up to ask for a reviewer.  Whenever it's moved out of that state, another screen should ask for a new assignee.",1,DM-94,datamanagement,configure transition screen dm agile workflow issue transition jira agile board ready review screen pop ask reviewer move state screen ask new assignee,"Configure transition screens for DM agile workflow Whenever an issue is transitioned on JIRA Agile board to 'Ready for Review', a screen should pop up to ask for a reviewer. Whenever it's moved out of that state, another screen should ask for a new assignee."
"Make lsst-build reuse buildIDs if nothing's changed All built packages are EUPS-tagged with build IDs (the bNNN EUPS tags). Without this change, new EUPS tags are declared even when nothing changed since the previous build (and EUPS' tags code doesn't scale well at this time).  This will be implemented by comparing the newly built manifest against ones stored in versiondb, and reusing the build IDs if a matching one is found.",1,DM-95,datamanagement,lsst build reuse buildids change build package eups tag build id bnnn eups tag change new eups tag declare change previous build eups tag code scale time implement compare newly build manifest one store versiondb reuse build id match find,"Make lsst-build reuse buildIDs if nothing's changed All built packages are EUPS-tagged with build IDs (the bNNN EUPS tags). Without this change, new EUPS tags are declared even when nothing changed since the previous build (and EUPS' tags code doesn't scale well at this time). This will be implemented by comparing the newly built manifest against ones stored in versiondb, and reusing the build IDs if a matching one is found."
clean up isr utility code There is some commented code in isr.py.  This should be removed or updated so that it works.,2,DM-98,datamanagement,clean isr utility code comment code isr.py remove update work,clean up isr utility code There is some commented code in isr.py. This should be removed or updated so that it works.
"Improve naming of getters in AmpInfoTable The names of the methods to get values from a record on AmpInfoCatalog are potentially confusing.    This is because the convention is to call the getters get[attributename].  We could change the method names in the AmpInfoCatalog, or add methods in the SWIG wrapper.",1,DM-148,datamanagement,improve naming getter ampinfotable name method value record ampinfocatalog potentially confusing convention getter get[attributename change method name ampinfocatalog add method swig wrapper,"Improve naming of getters in AmpInfoTable The names of the methods to get values from a record on AmpInfoCatalog are potentially confusing. This is because the convention is to call the getters get[attributename]. We could change the method names in the AmpInfoCatalog, or add methods in the SWIG wrapper."
"Box2I(bbox.getMin(), bbox.getMax()) fails for an empty bbox Empty Box2I cannot be round tripped: {code:py} from lsst.afw.geom import Box2I b1 = Box2I() b2 = Box2I(b1.getMin(), b1.getMax()) assert b2.isEmpty() {code}  It is confusing and surprising that this round tripping fails.  It is also a trap for the unwary because saving min and max is the logical way to store boxes in afw tables. Records can contain points but not extents and so it saves casting back and forth and simplifies and clarifies the code to save max instead of extent. Thus that is the path most users will take, and the problem can be a time bomb: it could be quite some time before somebody tries to store an empty box and finds that it does not get retrieved correctly.",1,DM-177,datamanagement,box2i(bbox.getmin bbox.getmax fail bbox box2i round trip code py lsst.afw.geom import box2i b1 box2i b2 box2i(b1.getmin b1.getmax assert b2.isempty code confusing surprising round tripping fail trap unwary save min max logical way store box afw table record contain point extent save cast forth simplifie clarify code save max instead extent path user problem time bomb time somebody try store box find retrieve correctly,"Box2I(bbox.getMin(), bbox.getMax()) fails for an empty bbox Empty Box2I cannot be round tripped: {code:py} from lsst.afw.geom import Box2I b1 = Box2I() b2 = Box2I(b1.getMin(), b1.getMax()) assert b2.isEmpty() {code} It is confusing and surprising that this round tripping fails. It is also a trap for the unwary because saving min and max is the logical way to store boxes in afw tables. Records can contain points but not extents and so it saves casting back and forth and simplifies and clarifies the code to save max instead of extent. Thus that is the path most users will take, and the problem can be a time bomb: it could be quite some time before somebody tries to store an empty box and finds that it does not get retrieved correctly."
"Replace PositionFunctor with some flavor of XYTransform afw has a special functor PositionFunctor that acts like an XYTransform. Unless PositionFunctor does not need to be invertible, it makes sense to merge these, likely by replacing PositionFunctor with the transform from afw::image::XYTransformFromWcsPair (as suggested by Jim Bosch on Trac ticket #2214).",1,DM-197,datamanagement,replace positionfunctor flavor xytransform afw special functor positionfunctor act like xytransform positionfunctor need invertible make sense merge likely replace positionfunctor transform afw::image::xytransformfromwcspair suggest jim bosch trac ticket 2214,"Replace PositionFunctor with some flavor of XYTransform afw has a special functor PositionFunctor that acts like an XYTransform. Unless PositionFunctor does not need to be invertible, it makes sense to merge these, likely by replacing PositionFunctor with the transform from afw::image::XYTransformFromWcsPair (as suggested by Jim Bosch on Trac ticket #2214)."
"Qserv: unit testing (query execution)  Design and build toy prototype of a test framework for testing query execution module. This might require a mock framework, as we want to be able to test things in isolation, without testing everything around the query execution module at the same time. This is related to DMTF-16570-21.",8,DM-202,datamanagement,qserv unit testing query execution design build toy prototype test framework testing query execution module require mock framework want able test thing isolation test query execution module time relate dmtf-16570 21,"Qserv: unit testing (query execution) Design and build toy prototype of a test framework for testing query execution module. This might require a mock framework, as we want to be able to test things in isolation, without testing everything around the query execution module at the same time. This is related to DMTF-16570-21."
"Prepare for setting up new cluster at IN2P3 for continuous integration/testing Once the hardware is available, setup the environment where we could easily run integration testing of different Qserv releases, including testing/comparing performance.  Integrate changes implemented in DM-1078  Add install script that exposes individual steps and allows modifications to the config file: newinstall, qserv-configure --prepare, then edit config file (or copy from somewhere), qserv-configure",4,DM-203,datamanagement,prepare set new cluster in2p3 continuous integration testing hardware available setup environment easily run integration testing different qserv release include testing compare performance integrate change implement dm-1078 add install script expose individual step allow modification config file newinstall qserv configure edit config file copy qserv configure,"Prepare for setting up new cluster at IN2P3 for continuous integration/testing Once the hardware is available, setup the environment where we could easily run integration testing of different Qserv releases, including testing/comparing performance. Integrate changes implemented in DM-1078 Add install script that exposes individual steps and allows modifications to the config file: newinstall, qserv-configure --prepare, then edit config file (or copy from somewhere), qserv-configure"
"Setup multi-node testbed It'd be useful to test Qserv using Winter2014 or Summer2014 data set on a multi-node cluster, just to exercise all pieces of the software and double check we are not missing anything.",5,DM-213,datamanagement,setup multi node testbe useful test qserv winter2014 summer2014 datum set multi node cluster exercise piece software double check miss,"Setup multi-node testbed It'd be useful to test Qserv using Winter2014 or Summer2014 data set on a multi-node cluster, just to exercise all pieces of the software and double check we are not missing anything."
"Qserv worker scheduler – code cleanup The qserv worker scheduler code is a bit ugly.  The actual composable scheduler classes (FifoScheduler, ScanScheduler, GroupScheduler, BlendScheduler) might be pretty clean, but the interactions with the rest (wdb, wcontrol) may be harder to understand.  There should be a small amount of low-hanging fruit of code to clean up, but to make things more sensible and understandable may require some new abstractions and shuffling of logic to new/different classes.  Since this issue was opened, some refactoring work has been done as part of the new xrdssi port, so the organization may be somewhat cleaner now. Still, it's worth it to take a fresh look to evaluate the design/interactions to see how much can/should be reorganized.",6,DM-219,datamanagement,qserv worker scheduler code cleanup qserv worker scheduler code bit ugly actual composable scheduler class fifoscheduler scanscheduler groupscheduler blendscheduler pretty clean interaction rest wdb wcontrol hard understand small low hang fruit code clean thing sensible understandable require new abstraction shuffling logic new different class issue open refactoring work new xrdssi port organization somewhat clean worth fresh look evaluate design interaction reorganize,"Qserv worker scheduler code cleanup The qserv worker scheduler code is a bit ugly. The actual composable scheduler classes (FifoScheduler, ScanScheduler, GroupScheduler, BlendScheduler) might be pretty clean, but the interactions with the rest (wdb, wcontrol) may be harder to understand. There should be a small amount of low-hanging fruit of code to clean up, but to make things more sensible and understandable may require some new abstractions and shuffling of logic to new/different classes. Since this issue was opened, some refactoring work has been done as part of the new xrdssi port, so the organization may be somewhat cleaner now. Still, it's worth it to take a fresh look to evaluate the design/interactions to see how much can/should be reorganized."
Switch to MariaDB We should switch Qserv to the MariaDB Foundation based MySQL.,3,DM-224,datamanagement,switch mariadb switch qserv mariadb foundation base mysql,Switch to MariaDB We should switch Qserv to the MariaDB Foundation based MySQL.
"Setup dev test environment Setup whole Qserv environment, including installing data set, and validate it by running some simple queries. Suggest changes/improvements as appropriate.",8,DM-228,datamanagement,setup dev test environment setup qserv environment include instal datum set validate run simple query suggest change improvement appropriate,"Setup dev test environment Setup whole Qserv environment, including installing data set, and validate it by running some simple queries. Suggest changes/improvements as appropriate."
"meas_base plugins for CModel magnitudes Create meas_base Plugins for single-frame and forced measurement that uses the model-fitting primitives in meas_multifit to implement SDSS-style CModel magnitudes, in which we fit an exp and dev model separately and then fit the linear combination with ellipse parameters held fixed.  An old-style plugin has already been implemented on the HSC fork, and should be used as a guide; this issue involves adapting that implementation to meas_base and potentially cleaning it up a bit.  Note that the HSC implementation cannot be transferred directly to the LSST side because the meas_algorithms APIs are slightly different on the two forks.",6,DM-240,datamanagement,meas_base plugin cmodel magnitude create meas_base plugins single frame force measurement use model fitting primitive meas_multifit implement sdss style cmodel magnitude fit exp dev model separately fit linear combination ellipse parameter hold fix old style plugin implement hsc fork guide issue involve adapt implementation meas_base potentially clean bit note hsc implementation transfer directly lsst meas_algorithm api slightly different fork,"meas_base plugins for CModel magnitudes Create meas_base Plugins for single-frame and forced measurement that uses the model-fitting primitives in meas_multifit to implement SDSS-style CModel magnitudes, in which we fit an exp and dev model separately and then fit the linear combination with ellipse parameters held fixed. An old-style plugin has already been implemented on the HSC fork, and should be used as a guide; this issue involves adapting that implementation to meas_base and potentially cleaning it up a bit. Note that the HSC implementation cannot be transferred directly to the LSST side because the meas_algorithms APIs are slightly different on the two forks."
"refactor forced tasks into two tasks After looking at it a bit more, I think we should refactor the current meas_base forced photometry task to separate the CmdLineTask from the Measurement task.  This will allow the forced measurement task to share a common base class with SingleFrameMeasurementTask (allowing us to move the callPlugin free functions into that base class), and give us better parallels with existing tasks:  - ProcesImageForcedTask (my proposed name for the base command-line task) will be more similar to ProcessImageTask.  We'll also have ProcessForcedCcdTask and ProcessForcedCoaddTask.  - ForcedMeasurementTask will be more similar to SingleFrameMeasurementTask.  In short, I think this will both clean up the ugliness in callPlugin and make the whole hierarchy easier for newcomers to understand.",6,DM-241,datamanagement,refactor force task task look bit think refactor current meas_base force photometry task separate cmdlinetask measurement task allow force measurement task share common base class singleframemeasurementtask allow callplugin free function base class well parallel exist task procesimageforcedtask propose base command line task similar processimagetask processforcedccdtask processforcedcoaddtask forcedmeasurementtask similar singleframemeasurementtask short think clean ugliness callplugin hierarchy easy newcomer understand,"refactor forced tasks into two tasks After looking at it a bit more, I think we should refactor the current meas_base forced photometry task to separate the CmdLineTask from the Measurement task. This will allow the forced measurement task to share a common base class with SingleFrameMeasurementTask (allowing us to move the callPlugin free functions into that base class), and give us better parallels with existing tasks: - ProcesImageForcedTask (my proposed name for the base command-line task) will be more similar to ProcessImageTask. We'll also have ProcessForcedCcdTask and ProcessForcedCoaddTask. - ForcedMeasurementTask will be more similar to SingleFrameMeasurementTask. In short, I think this will both clean up the ugliness in callPlugin and make the whole hierarchy easier for newcomers to understand."
"switch from '.' to '_' in afw::table fields We've been mapping '.' to '_' in afw::table I/O, which unnecessarily complicates lots of things.  We'd like to switch to using '_' in the field names themselves, which requires ending this mapping in I/O, but we need to be backwards compatible.  So we'll add a version to the FITS headers, and continue the mapping if the version is not present or is less than some value.  Until we do this, the new field names being used in meas_base won't round-trip.",3,DM-242,datamanagement,switch afw::table field map afw::table unnecessarily complicate lot thing like switch field name require end mapping need backwards compatible add version fit header continue mapping version present value new field name meas_base will round trip,"switch from '.' to '_' in afw::table fields We've been mapping '.' to '_' in afw::table I/O, which unnecessarily complicates lots of things. We'd like to switch to using '_' in the field names themselves, which requires ending this mapping in I/O, but we need to be backwards compatible. So we'll add a version to the FITS headers, and continue the mapping if the version is not present or is less than some value. Until we do this, the new field names being used in meas_base won't round-trip."
"Test and migrate to swig 3.0 -------- Original Message -------- Subject: [LSST-data] Swig 3.0 is out (with C++11 support) Date: Mon, 17 Mar 2014 08:26:05 -0400 From: Robert Lupton the Good <rhl@astro.princeton.edu> To: LSST Data <lsst-data@lsstcorp.org>  I tried a pre-release on os/x 10.7.5 and it failed some tests, but I haven't tried this version.  I had some discussion about this with William, but haven't had time to follow through.  							R   > Date: Sun, 16 Mar 2014 22:44:42 +0000 > From: William S Fulton <wsf@fultondesigns.co.uk> >  > *** ANNOUNCE: SWIG 3.0.0 (16 Mar 2014) *** >  > http://www.swig.org >  > We're pleased to announce SWIG-3.0.0, the latest SWIG release. >  > What is SWIG? > ============= >  > SWIG is a software development tool that reads C/C++ header files and > generates the wrapper code needed to make C and C++ code accessible > from other programming languages including Perl, Python, Tcl, Ruby, > PHP, C#, Go, Java, Lua, Scheme (Guile, MzScheme, CHICKEN), D, Ocaml, > Pike, Modula-3, Octave, R, Common Lisp (CLISP, Allegro CL, CFFI, UFFI). > SWIG can also export its parse tree in the form of XML and Lisp > s-expressions.  Major applications of SWIG include generation of > scripting language extension modules, rapid prototyping, testing, > and user interface development for large C/C++ systems. >  > Availability > ============ > The release is available for download on Sourceforge at >  >      http://prdownloads.sourceforge.net/swig/swig-3.0.0.tar.gz >  > A Windows version is also available at >  >      http://prdownloads.sourceforge.net/swig/swigwin-3.0.0.zip >  > Please report problems with this release to the swig-devel mailing list, > details at http://www.swig.org/mail.html. >  > Release Notes > ============= > SWIG-3.0.0 summary: > - This is a major new release focusing primarily on C++ improvements. > - C++11 support added. Please see documentation for details of supported >   features: http://www.swig.org/Doc3.0/CPlusPlus11.html > - Nested class support added. This has been taken full advantage of in >   Java and C#. Other languages can use the nested classes, but require >   further work for a more natural integration into the target language. >   We urge folk knowledgeable in the other target languages to step >   forward and help with this effort. > - Lua: improved metatables and support for %nspace. > - Go 1.3 support added. > - Python import improvements including relative imports. > - Python 3.3 support completed. > - Perl director support added. > - C# .NET 2 support is now the minimum. Generated using statements are >   replaced by fully qualified names. > - Bug fixes and improvements to the following languages: >   C#, Go, Guile, Java, Lua, Perl, PHP, Python, Octave, R, Ruby, Tcl > - Various other bug fixes and improvements affecting all languages. > - Note that this release contains some backwards incompatible changes >   in some languages. > - Full detailed release notes are in the changes file. ",2,DM-259,datamanagement,test migrate swig 3.0 original message subject lsst data swig 3.0 c++11 support date mon 17 mar 2014 08:26:05 -0400 robert lupton good lsst data try pre release os 10.7.5 fail test try version discussion william time follow date sun 16 mar 2014 22:44:42 +0000 william fulton announce swig 3.0.0 16 mar 2014 http://www.swig.org pleased announce swig-3.0.0 late swig release swig swig software development tool read c++ header file generate wrapper code need c++ code accessible programming language include perl python tcl ruby php java lua scheme guile mzscheme chicken ocaml pike modula-3 octave common lisp clisp allegro cl cffi uffi swig export parse tree form xml lisp expression major application swig include generation script language extension module rapid prototyping testing user interface development large c++ system availability release available download sourceforge http://prdownloads.sourceforge.net/swig/swig-3.0.0.tar.gz windows version available http://prdownloads.sourceforge.net/swig/swigwin-3.0.0.zip report problem release swig devel mailing list detail http://www.swig.org/mail.html release note swig-3.0.0 summary major new release focus primarily c++ improvement c++11 support add documentation detail support feature http://www.swig.org/doc3.0/cplusplus11.html nest class support add take advantage java language use nest class require work natural integration target language urge folk knowledgeable target language step forward help effort lua improve metatable support nspace 1.3 support add python import improvement include relative import python 3.3 support complete perl director support add .net support minimum generate statement replace fully qualified name bug fix improvement follow language guile java lua perl php python octave ruby tcl bug fix improvement affect language note release contain backwards incompatible change language detailed release note change file,"Test and migrate to swig 3.0 -------- Original Message -------- Subject: [LSST-data] Swig 3.0 is out (with C++11 support) Date: Mon, 17 Mar 2014 08:26:05 -0400 From: Robert Lupton the Good  To: LSST Data  I tried a pre-release on os/x 10.7.5 and it failed some tests, but I haven't tried this version. I had some discussion about this with William, but haven't had time to follow through. R > Date: Sun, 16 Mar 2014 22:44:42 +0000 > From: William S Fulton  > > *** ANNOUNCE: SWIG 3.0.0 (16 Mar 2014) *** > > http://www.swig.org > > We're pleased to announce SWIG-3.0.0, the latest SWIG release. > > What is SWIG? > ============= > > SWIG is a software development tool that reads C/C++ header files and > generates the wrapper code needed to make C and C++ code accessible > from other programming languages including Perl, Python, Tcl, Ruby, > PHP, C#, Go, Java, Lua, Scheme (Guile, MzScheme, CHICKEN), D, Ocaml, > Pike, Modula-3, Octave, R, Common Lisp (CLISP, Allegro CL, CFFI, UFFI). > SWIG can also export its parse tree in the form of XML and Lisp > s-expressions. Major applications of SWIG include generation of > scripting language extension modules, rapid prototyping, testing, > and user interface development for large C/C++ systems. > > Availability > ============ > The release is available for download on Sourceforge at > > http://prdownloads.sourceforge.net/swig/swig-3.0.0.tar.gz > > A Windows version is also available at > > http://prdownloads.sourceforge.net/swig/swigwin-3.0.0.zip > > Please report problems with this release to the swig-devel mailing list, > details at http://www.swig.org/mail.html. > > Release Notes > ============= > SWIG-3.0.0 summary: > - This is a major new release focusing primarily on C++ improvements. > - C++11 support added. Please see documentation for details of supported > features: http://www.swig.org/Doc3.0/CPlusPlus11.html > - Nested class support added. This has been taken full advantage of in > Java and C#. Other languages can use the nested classes, but require > further work for a more natural integration into the target language. > We urge folk knowledgeable in the other target languages to step > forward and help with this effort. > - Lua: improved metatables and support for %nspace. > - Go 1.3 support added. > - Python import improvements including relative imports. > - Python 3.3 support completed. > - Perl director support added. > - C# .NET 2 support is now the minimum. Generated using statements are > replaced by fully qualified names. > - Bug fixes and improvements to the following languages: > C#, Go, Guile, Java, Lua, Perl, PHP, Python, Octave, R, Ruby, Tcl > - Various other bug fixes and improvements affecting all languages. > - Note that this release contains some backwards incompatible changes > in some languages. > - Full detailed release notes are in the changes file."
"Board workflow modifications On DM Software Development board, I propose we:  * Change ""Ready to Merge"" to ""Review Complete"". Per K-T:  {quote} That's what I thought ""Review Complete"" would be -- most of the work is done, but some fixups are needed before merging, and a re-review is not necessary.  {quote}  * Remove ""Ready for Review"". Instead, the developer should just drag the issue to ""In Review"" and assign it to a reviewer. If/when we re-instate the ""review master"", we may re-introduce ""Ready for Review""",1,DM-260,datamanagement,board workflow modification dm software development board propose change ready merge review complete quote think review complete work fixup need merge review necessary quote remove ready review instead developer drag issue review assign reviewer instate review master introduce ready review,"Board workflow modifications On DM Software Development board, I propose we: * Change ""Ready to Merge"" to ""Review Complete"". Per K-T: {quote} That's what I thought ""Review Complete"" would be -- most of the work is done, but some fixups are needed before merging, and a re-review is not necessary. {quote} * Remove ""Ready for Review"". Instead, the developer should just drag the issue to ""In Review"" and assign it to a reviewer. If/when we re-instate the ""review master"", we may re-introduce ""Ready for Review"""
"Move TCT-relevant  twiki documentation to Confluence Congregate all the trac TCT-relevant documents (standards, policies, guidelines, meeting history) onto Confluence.",2,DM-272,datamanagement,tct relevant twiki documentation confluence congregate trac tct relevant document standard policy guideline meet history confluence,"Move TCT-relevant twiki documentation to Confluence Congregate all the trac TCT-relevant documents (standards, policies, guidelines, meeting history) onto Confluence."
"Develop and then create the organizational structure for DM Confluence space Before we start populating the DM Confluence space with active pages, we should define an overall organizational structure/taxonomy.",1,DM-273,datamanagement,develop create organizational structure dm confluence space start populate dm confluence space active page define overall organizational structure taxonomy,"Develop and then create the organizational structure for DM Confluence space Before we start populating the DM Confluence space with active pages, we should define an overall organizational structure/taxonomy."
"Adding support for numpy scalar array types as arguments to SWIG wrapped methods Building against the anaconda on lsst-dev causes construction of Point2I (and other point types) with numpy dtypes to fail with the standard exception: {code} NotImplementedError: Wrong number or type of arguments for overloaded function {code} I did not know that was not allowed.  If I am doing it others are as well.  I think this should be addressed in some way before W14 release.  How to repeat on lsst-dev: {code:sh} setenv EUPS_DIR ~lsstsw/stack/ source ~lsstsw/eups/bin/setups.csh setup afw setup anaconda echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code}  Using the old 7_3 stack you can do a similar test: {code:sh} source /lsst/DC3/stacks/default/loadLSST.csh setup -t v7_3 afw echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code}",3,DM-274,datamanagement,"add support numpy scalar array type argument swig wrap method build anaconda lsst dev cause construction point2i point type numpy dtype fail standard exception code notimplementederror wrong number type argument overloaded function code know allow think address way w14 release repeat lsst dev code sh setenv eups_dir ~lsstsw stack/ source ~lsstsw eup bin setups.csh setup afw setup anaconda echo import lsst.afw.geom ag\nimport numpy\nprint ag point2i(5,5)\nval numpy.int64(5)\nprint ag point2i(val val test.py python test.py code old 7_3 stack similar test code sh source /lsst dc3 stack default loadlsst.csh setup v7_3 afw echo import lsst.afw.geom ag\nimport numpy\nprint ag point2i(5,5)\nval numpy.int64(5)\nprint ag point2i(val val test.py python test.py code","Adding support for numpy scalar array types as arguments to SWIG wrapped methods Building against the anaconda on lsst-dev causes construction of Point2I (and other point types) with numpy dtypes to fail with the standard exception: {code} NotImplementedError: Wrong number or type of arguments for overloaded function {code} I did not know that was not allowed. If I am doing it others are as well. I think this should be addressed in some way before W14 release. How to repeat on lsst-dev: {code:sh} setenv EUPS_DIR ~lsstsw/stack/ source ~lsstsw/eups/bin/setups.csh setup afw setup anaconda echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code} Using the old 7_3 stack you can do a similar test: {code:sh} source /lsst/DC3/stacks/default/loadLSST.csh setup -t v7_3 afw echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code}"
"Improve handling errors occuring in AsyncQueryManager AsyncQueryManager is initialized based on configuration file, if the configuration is invalid, an exception should be thrown (eg in _readConfig()) and gracefully handled upstream.",1,DM-278,datamanagement,improve handle error occur asyncquerymanager asyncquerymanager initialize base configuration file configuration invalid exception throw eg readconfig gracefully handle upstream,"Improve handling errors occuring in AsyncQueryManager AsyncQueryManager is initialized based on configuration file, if the configuration is invalid, an exception should be thrown (eg in _readConfig()) and gracefully handled upstream."
"clean up multiple aperture photometry code I've been doing some minor work on the HSC-side ApertureFlux algorithm, and I wanted to record some concerns here (from both me and RHL) that should be addressed in the new meas_base version:  - We should consider merging ApertureFlux and EllipticalApertureFlux into the same algorithm (with a config field to choose whether to use elliptical apertures).  We could still register it twice, with a different default config value, and this should eliminate a lot of code duplication.  We could also consider having them inherit from a common base class (instead of having EllipticalApertureFlux inherit from ApertureFlux, as is done now).  - We should test that the threshold at which we switch from Sinc to naive apertures is obeyed exactly.  - We should create a flag for the failure mode in which an aperture cannot be measured because we go off the edge of the image, and test that it appears at the right point.  If possible, we should set this flag and measure what area we can within that aperture, instead of just bailing out.  - (Somewhat off-topic) We should consider having utility functions on SourceSlotConfig to set all slots to None for use in unit tests.",5,DM-280,datamanagement,clean multiple aperture photometry code minor work hsc apertureflux algorithm want record concern rhl address new meas_base version consider merge apertureflux ellipticalapertureflux algorithm config field choose use elliptical aperture register twice different default config value eliminate lot code duplication consider have inherit common base class instead have ellipticalapertureflux inherit apertureflux test threshold switch sinc naive aperture obey exactly create flag failure mode aperture measure edge image test appear right point possible set flag measure area aperture instead bail somewhat topic consider have utility function sourceslotconfig set slot use unit test,"clean up multiple aperture photometry code I've been doing some minor work on the HSC-side ApertureFlux algorithm, and I wanted to record some concerns here (from both me and RHL) that should be addressed in the new meas_base version: - We should consider merging ApertureFlux and EllipticalApertureFlux into the same algorithm (with a config field to choose whether to use elliptical apertures). We could still register it twice, with a different default config value, and this should eliminate a lot of code duplication. We could also consider having them inherit from a common base class (instead of having EllipticalApertureFlux inherit from ApertureFlux, as is done now). - We should test that the threshold at which we switch from Sinc to naive apertures is obeyed exactly. - We should create a flag for the failure mode in which an aperture cannot be measured because we go off the edge of the image, and test that it appears at the right point. If possible, we should set this flag and measure what area we can within that aperture, instead of just bailing out. - (Somewhat off-topic) We should consider having utility functions on SourceSlotConfig to set all slots to None for use in unit tests."
"More helpful location information for errors in duplicator/partitioner input Currently, if the Qserv duplicator or partitioner encounters erroneous input (such as a formatting error, or missing columns), the error message it outputs does not include a mapping back to the location of the error, making it very hard/annoying indeed to fix that erroneous input.  Fabrice would (quite reasonably) like to see a filename and line number in the error message.  Unfortunately, producing a line number is complicated by the following facts:  - multiple threads may be reading sub-sections of the same input file in parallel  - sub-sections are not guaranteed to be read in order  - lines can have varying length  In other words, the code reading/parsing the input has no idea which line it is working on, and making that information available would involve either deferring error reports or extra synchronization (performance loss).  What we can easily (and should) do is to arrange for processing code to know the file and byte offset of the input text being processed; error messages should include both pieces of information.",2,DM-289,datamanagement,helpful location information error duplicator partitioner input currently qserv duplicator partitioner encounter erroneous input format error miss column error message output include mapping location error make hard annoy fix erroneous input fabrice reasonably like filename line number error message unfortunately produce line number complicate follow fact multiple thread read sub section input file parallel sub section guarantee read order line vary length word code read parse input idea line work make information available involve defer error report extra synchronization performance loss easily arrange process code know file byte offset input text process error message include piece information,"More helpful location information for errors in duplicator/partitioner input Currently, if the Qserv duplicator or partitioner encounters erroneous input (such as a formatting error, or missing columns), the error message it outputs does not include a mapping back to the location of the error, making it very hard/annoying indeed to fix that erroneous input. Fabrice would (quite reasonably) like to see a filename and line number in the error message. Unfortunately, producing a line number is complicated by the following facts: - multiple threads may be reading sub-sections of the same input file in parallel - sub-sections are not guaranteed to be read in order - lines can have varying length In other words, the code reading/parsing the input has no idea which line it is working on, and making that information available would involve either deferring error reports or extra synchronization (performance loss). What we can easily (and should) do is to arrange for processing code to know the file and byte offset of the input text being processed; error messages should include both pieces of information."
"Eliminate dependence of query analysis on parser and antlr I would like to write and compile query analyzer code completely independently of the parser and ANTLR (transitively). This doesn't seem to work right now. This is not currently possible.  This might take any where from a day to a week. (I'm not sure if we can finish anything in half a day, if you include the testing, review, feedback, and revision process, but perhaps unit testing will make that faster).  Updates to follow after the scope is estimated.  Dependencies to be broken: query --> parser, antlr (due to predicate depending on antlr nodes) qana --> parser, antlr ",8,DM-290,datamanagement,eliminate dependence query analysis parser antlr like write compile query analyzer code completely independently parser antlr transitively work right currently possible day week sure finish half day include testing review feedback revision process unit testing fast update follow scope estimate dependency break query parser antlr predicate depend antlr nodes qana parser antlr,"Eliminate dependence of query analysis on parser and antlr I would like to write and compile query analyzer code completely independently of the parser and ANTLR (transitively). This doesn't seem to work right now. This is not currently possible. This might take any where from a day to a week. (I'm not sure if we can finish anything in half a day, if you include the testing, review, feedback, and revision process, but perhaps unit testing will make that faster). Updates to follow after the scope is estimated. Dependencies to be broken: query --> parser, antlr (due to predicate depending on antlr nodes) qana --> parser, antlr"
"xrootd initialization should abort if mysql connection fails Currently xrootd will happily start even if it can't connect to mysql, it will only print a message:  {code} Configration invalid: Unable to connect to MySQL with config: {code}  This can be easily overlooked, plus, it is a fatal error and xrootd initialization should be aborted.  while working on this, I propose to also improve validateMysql(). At the moment it connects to mysql and database in one call. If connection is fine, but the database does not exist, it will fail without telling user why it failed. This can be confusing. It'd better to connect to mysql without connecting to database, then do ""select_db"", and if that fails, inform the user that the database does not exist.  (transferred from trac ticket 3165)",1,DM-295,datamanagement,xrootd initialization abort mysql connection fail currently xrootd happily start connect mysql print message code configration invalid unable connect mysql config code easily overlook plus fatal error xrootd initialization abort work propose improve validatemysql moment connect mysql database connection fine database exist fail tell user fail confusing well connect mysql connect database select_db fail inform user database exist transfer trac ticket 3165,"xrootd initialization should abort if mysql connection fails Currently xrootd will happily start even if it can't connect to mysql, it will only print a message: {code} Configration invalid: Unable to connect to MySQL with config: {code} This can be easily overlooked, plus, it is a fatal error and xrootd initialization should be aborted. while working on this, I propose to also improve validateMysql(). At the moment it connects to mysql and database in one call. If connection is fine, but the database does not exist, it will fail without telling user why it failed. This can be confusing. It'd better to connect to mysql without connecting to database, then do ""select_db"", and if that fails, inform the user that the database does not exist. (transferred from trac ticket 3165)"
"fix namespaces in all Qserv core modules This was suggested by the code review for ticket 1945 (https://dev.lsstcorp.org/trac/wiki/SAT/CodeReviews/1945), pasted below:  common/src/*:  While it's not required by the coding standards, I'm a big proponent of using namespace scopes in .cc files, which usually save you from needing namespace aliases and will certainly save you from having prefix every declaration with qserv::.  At some point I'd recommend changing the header file extension from .hh to .h to match the rest of the LSST DM code, unless it's a big backwards compatibility issue.  (transferred from trac ticket 2528)",5,DM-296,datamanagement,fix namespace qserv core module suggest code review ticket 1945 https://dev.lsstcorp.org/trac/wiki/sat/codereviews/1945 paste common src/ require code standard big proponent namespace scope .cc file usually save need namespace alias certainly save having prefix declaration qserv point recommend change header file extension .hh .h match rest lsst dm code big backwards compatibility issue transfer trac ticket 2528,"fix namespaces in all Qserv core modules This was suggested by the code review for ticket 1945 (https://dev.lsstcorp.org/trac/wiki/SAT/CodeReviews/1945), pasted below: common/src/*: While it's not required by the coding standards, I'm a big proponent of using namespace scopes in .cc files, which usually save you from needing namespace aliases and will certainly save you from having prefix every declaration with qserv::. At some point I'd recommend changing the header file extension from .hh to .h to match the rest of the LSST DM code, unless it's a big backwards compatibility issue. (transferred from trac ticket 2528)"
"Qserv should check for loaded spatial UDFs Qserv should have a way of checking for the existence of spatial UDFs loaded in the worker MySQL instances.  Obviously, the worker must perform the physical check. However, the worker has no knowledge that spatial UDFs even exist, since the master is responsible for translating spatial spec into UDF call.  At the moment, the preferred way of checking would be some sort of administrative command that runs on all nodes. An alternative would be some sanity check that is run on a worker before an admin starts a worker. Or the master could devise a MySQL query to check for things and dispatch to all chunks (although this would not get full coverage when replica exist, while being redundant while workers host more than 1 chunk).  (transferred from trac ticket 1959)",2,DM-297,datamanagement,qserv check load spatial udfs qserv way check existence spatial udfs load worker mysql instance obviously worker perform physical check worker knowledge spatial udfs exist master responsible translate spatial spec udf moment preferred way checking sort administrative command run node alternative sanity check run worker admin start worker master devise mysql query check thing dispatch chunk coverage replica exist redundant worker host chunk transfer trac ticket 1959,"Qserv should check for loaded spatial UDFs Qserv should have a way of checking for the existence of spatial UDFs loaded in the worker MySQL instances. Obviously, the worker must perform the physical check. However, the worker has no knowledge that spatial UDFs even exist, since the master is responsible for translating spatial spec into UDF call. At the moment, the preferred way of checking would be some sort of administrative command that runs on all nodes. An alternative would be some sanity check that is run on a worker before an admin starts a worker. Or the master could devise a MySQL query to check for things and dispatch to all chunks (although this would not get full coverage when replica exist, while being redundant while workers host more than 1 chunk). (transferred from trac ticket 1959)"
"restarting mysqld breaks qserv Restaring mysqld results in unusable qserv (even if the restart happens when qserv is completely idle). The error message is:  ERROR 2013 (HY000): Lost connection to MySQL server during query This happens most likely because qserv caches the connection, which becomes invalid when server is restarted. I am guessing the same will happen when there is a long period of inactivity (the connection times out).  (transferred from trac 2853)",1,DM-298,datamanagement,restart mysqld break qserv restare mysqld result unusable qserv restart happen qserv completely idle error message error 2013 hy000 lose connection mysql server query happen likely qserv cache connection invalid server restart guess happen long period inactivity connection time transfer trac 2853,"restarting mysqld breaks qserv Restaring mysqld results in unusable qserv (even if the restart happens when qserv is completely idle). The error message is: ERROR 2013 (HY000): Lost connection to MySQL server during query This happens most likely because qserv caches the connection, which becomes invalid when server is restarted. I am guessing the same will happen when there is a long period of inactivity (the connection times out). (transferred from trac 2853)"
"Centralize hardcoded constants Some values in qserv need to become constant(e.g. chunkId column names, dirs, filenames). Some of these are configurable, others are hardcoded in non-obvious places in the code. When multiple places need this value, they really need to agree, and unfortunately, qserv doesn't have a well-known place for these constants yet.  Any (constant) value that is needed by different parts of the code needs to be managed in a way that is reasonably obvious to unfamiliar programmers.  List of values: * chunkId, subChunkId column names (master...indexing.py, app.py) * environment variable names * + others. This should actually be fairly simple to implement, once the right (?) design is conceived and worked-out.  (transferred from trac ticket #2405)",6,DM-300,datamanagement,centralize hardcode constant value qserv need constant(e.g chunkid column name dir filename configurable hardcode non obvious place code multiple place need value need agree unfortunately qserv know place constant constant value need different part code need manage way reasonably obvious unfamiliar programmer list value chunkid subchunkid column name master indexing.py app.py environment variable name actually fairly simple implement right design conceive work transfer trac ticket 2405,"Centralize hardcoded constants Some values in qserv need to become constant(e.g. chunkId column names, dirs, filenames). Some of these are configurable, others are hardcoded in non-obvious places in the code. When multiple places need this value, they really need to agree, and unfortunately, qserv doesn't have a well-known place for these constants yet. Any (constant) value that is needed by different parts of the code needs to be managed in a way that is reasonably obvious to unfamiliar programmers. List of values: * chunkId, subChunkId column names (master...indexing.py, app.py) * environment variable names * + others. This should actually be fairly simple to implement, once the right (?) design is conceived and worked-out. (transferred from trac ticket #2405)"
"Jira for Qserv Jira setup for Qserv, includes things like adding new tasks, transferring tasks from trac, epic/story/task division, assigning story points, setting scrum board, just learning things and more...",8,DM-309,datamanagement,jira qserv jira setup qserv include thing like add new task transfer task trac epic story task division assign story point set scrum board learn thing,"Jira for Qserv Jira setup for Qserv, includes things like adding new tasks, transferring tasks from trac, epic/story/task division, assigning story points, setting scrum board, just learning things and more..."
"Come up with a standard to handle C++ Exceptions in Qserv (and the rest of DM?) Currently CSS is using one class and relies on different error codes to differentiate between different type of exceptions, while other parts of Qserv core define an exception for each different error. It'd be good to standardize and use the same approach. ",4,DM-312,datamanagement,come standard handle c++ exception qserv rest dm currently css class rely different error code differentiate different type exception part qserv core define exception different error good standardize use approach,"Come up with a standard to handle C++ Exceptions in Qserv (and the rest of DM?) Currently CSS is using one class and relies on different error codes to differentiate between different type of exceptions, while other parts of Qserv core define an exception for each different error. It'd be good to standardize and use the same approach."
"cleanup includes in Qserv core modules Includes need cleanup: group into standard lib, boots and local, sort as appropriate etc. Also, unify forward declarations.",2,DM-313,datamanagement,cleanup include qserv core module include need cleanup group standard lib boot local sort appropriate etc unify forward declaration,"cleanup includes in Qserv core modules Includes need cleanup: group into standard lib, boots and local, sort as appropriate etc. Also, unify forward declarations."
CSS - surviving mysql and zookeeper glitches CSS should gracefully recover from failures such as lost connection to mysqld or zookeeper. It is not clear if it would survive such glitches right now -- this needs to be tested and the code improved as necessary.,4,DM-318,datamanagement,css survive mysql zookeeper glitch css gracefully recover failure lose connection mysqld zookeeper clear survive glitch right need test code improve necessary,CSS - surviving mysql and zookeeper glitches CSS should gracefully recover from failures such as lost connection to mysqld or zookeeper. It is not clear if it would survive such glitches right now -- this needs to be tested and the code improved as necessary.
Create a board (virtual or otherwise) with pictures and names of everyone in DM I tried compiling a list of everyone who works or has worked on the DM code -- it was nearly impossible. We should have a (public) list both of current staff and our alumni (and where they are now).,1,DM-319,datamanagement,create board virtual picture name dm try compile list work work dm code nearly impossible public list current staff alumnus,Create a board (virtual or otherwise) with pictures and names of everyone in DM I tried compiling a list of everyone who works or has worked on the DM code -- it was nearly impossible. We should have a (public) list both of current staff and our alumni (and where they are now).
Re-think thread.cc and dispatcher.cc python interface The mess of thread.cc and dispatcher.cc need to be re-thought and re-designed so that the interface is smaller and more obvious.  ,2,DM-321,datamanagement,think thread.cc dispatcher.cc python interface mess thread.cc dispatcher.cc need thought design interface small obvious,Re-think thread.cc and dispatcher.cc python interface The mess of thread.cc and dispatcher.cc need to be re-thought and re-designed so that the interface is smaller and more obvious.
"Trim python importing by czar in app.py Clean up the way modules are imported in qserv master, use relative import when appropriate instead of lsst.qserv.master.<package>   (migrated from Trac #2369)",2,DM-322,datamanagement,trim python import czar app.py clean way module import qserv master use relative import appropriate instead lsst.qserv.master migrate trac 2369,"Trim python importing by czar in app.py Clean up the way modules are imported in qserv master, use relative import when appropriate instead of lsst.qserv.master. (migrated from Trac #2369)"
"Libraries being built in lib64 on OpenSUSE, when EUPS tables assume lib A report from Darko Jevremovic <darko@aob.rs>: {quote} Hi Mario,  I managed to build stack v8 on OpenSuse13.1  There were standard problems with lib/lib64 - namely system builds libraries in $PREFIX/lib64 and some programs are hard wired for $PREFIX/lib  if you could  change the last line of  mysqlclient-5.1.65+3/ups/eupspkg.cfg.sh  from  (cd $PREFIX/lib && ln -s mysql/* . )  to  ( cd $PREFIX && if [ ! -f ""lib"" ] ; then  ln -sf lib64 lib; fi &&cd $PREFIX/lib && ln -s mysql/* . )  or something along that line (am not sure whether the syntax would  work).  Also if you could add  in the same manner to ups/eupspkg.cfg.sh  ( cd $PREFIX && if [ ! -f ""lib"" ] ; then  ln -sf lib64 lib; fi)  for the following packages:  minuit2 gsl cfitsio wcslib {quote} ",1,DM-326,datamanagement,library build lib64 opensuse eups table assume lib report darko jevremovic quote hi mario manage build stack v8 opensuse13.1 standard problem lib lib64 system build library prefix lib64 program hard wire prefix lib change line mysqlclient-5.1.65 up eupspkg.cfg.sh cd prefix lib ln -s mysql/ cd prefix -f lib ln -sf lib64 lib fi cd prefix lib ln -s mysql/ line sure syntax work add manner ups eupspkg.cfg.sh cd prefix -f lib ln -sf lib64 lib fi follow package minuit2 gsl cfitsio wcslib quote,"Libraries being built in lib64 on OpenSUSE, when EUPS tables assume lib A report from Darko Jevremovic : {quote} Hi Mario, I managed to build stack v8 on OpenSuse13.1 There were standard problems with lib/lib64 - namely system builds libraries in $PREFIX/lib64 and some programs are hard wired for $PREFIX/lib if you could change the last line of mysqlclient-5.1.65+3/ups/eupspkg.cfg.sh from (cd $PREFIX/lib && ln -s mysql/* . ) to ( cd $PREFIX && if [ ! -f ""lib"" ] ; then ln -sf lib64 lib; fi &&cd $PREFIX/lib && ln -s mysql/* . ) or something along that line (am not sure whether the syntax would work). Also if you could add in the same manner to ups/eupspkg.cfg.sh ( cd $PREFIX && if [ ! -f ""lib"" ] ; then ln -sf lib64 lib; fi) for the following packages: minuit2 gsl cfitsio wcslib {quote}"
"Take RAM into account when computing NCORES to use in installs Darko Jevremovic reported he's had to switch off hyperthreading and manually override NCORES, MAKEFLAGS and SCONFLAGS because his 8-core machine had too little RAM to build afw with -j 8.  To fix this, eupspkg default build routines should take RAM into account when computing the level of build parallelism.  In the meantime, we should document the workaround (contact darko@aob.rs).",1,DM-327,datamanagement,ram account compute ncores use install darko jevremovic report switch hyperthreading manually override ncores makeflags sconflags core machine little ram build afw -j fix eupspkg default build routine ram account compute level build parallelism meantime document workaround contact darko@aob.rs,"Take RAM into account when computing NCORES to use in installs Darko Jevremovic reported he's had to switch off hyperthreading and manually override NCORES, MAKEFLAGS and SCONFLAGS because his 8-core machine had too little RAM to build afw with -j 8. To fix this, eupspkg default build routines should take RAM into account when computing the level of build parallelism. In the meantime, we should document the workaround (contact darko@aob.rs)."
"Local lsst-build invocations should use a different build number prefix Buildbot-invoked lsst-build installs packages in the stack with ""b#"" tags.  These are propagated to the distribution server by {{eups distrib create}}.  Local stacks maintained with lsst-build should use different tags so that they don't conflict with these distributed tags.",1,DM-330,datamanagement,local lsst build invocation use different build number prefix buildbot invoke lsst build install package stack tag propagate distribution server eup distrib create local stack maintain lsst build use different tag conflict distribute tag,"Local lsst-build invocations should use a different build number prefix Buildbot-invoked lsst-build installs packages in the stack with ""b#"" tags. These are propagated to the distribution server by {{eups distrib create}}. Local stacks maintained with lsst-build should use different tags so that they don't conflict with these distributed tags."
"Cut Qserv release It'd be very useful to have fully functioning Qserv release with the latest set of changes (build, packaging, CSS, Daniel's fixes etc) during the Hackathon week.",2,DM-334,datamanagement,cut qserv release useful fully function qserv release late set change build packaging css daniel fix etc hackathon week,"Cut Qserv release It'd be very useful to have fully functioning Qserv release with the latest set of changes (build, packaging, CSS, Daniel's fixes etc) during the Hackathon week."
"Migrate std::lists to std::vectors Suggested by Andy when reviewing DM-296, discussed at Qserv mtg 3/27.  std::list --> std::vector  * why? Default now is vector, iterating over vector is much more efficient than over list  * revisit on case by case bases, do not blindly replace  * preferred solution: typedef, and name it in a way that conveys the intent (e.g., might call it a ""container""), underneath use vector",8,DM-335,datamanagement,migrate std::list std::vector suggest andy review dm-296 discuss qserv mtg 3/27 std::list std::vector default vector iterate vector efficient list revisit case case basis blindly replace preferred solution typedef way convey intent e.g. container underneath use vector,"Migrate std::lists to std::vectors Suggested by Andy when reviewing DM-296, discussed at Qserv mtg 3/27. std::list --> std::vector * why? Default now is vector, iterating over vector is much more efficient than over list * revisit on case by case bases, do not blindly replace * preferred solution: typedef, and name it in a way that conveys the intent (e.g., might call it a ""container""), underneath use vector"
"improve code that initializes shared_ptrs  Reported by Andy when reviewing DM-296. Discussed at Qserv mtg 3/27.   boost::shared_ptr =(new T())"" --> boost::make_shared()",4,DM-336,datamanagement,improve code initialize shared_ptrs report andy review dm-296 discuss qserv mtg 3/27 boost::shared_ptr =( new boost::make_shared,"improve code that initializes shared_ptrs Reported by Andy when reviewing DM-296. Discussed at Qserv mtg 3/27. boost::shared_ptr =(new T())"" --> boost::make_shared()"
removed dead code in stringUtil.h Remove obsolete strToDoubleFunc (and more) in util/stringUtil.h.,1,DM-337,datamanagement,remove dead code stringutil.h remove obsolete strtodoublefunc util stringutil.h,removed dead code in stringUtil.h Remove obsolete strToDoubleFunc (and more) in util/stringUtil.h.
Enable gravatars Could you enable gravatars for all our atlassian products (at least Jira + Agile; Confluence)  https://confluence.atlassian.com/display/AOD/Configuring+Gravatar+support,1,DM-345,datamanagement,enable gravatar enable gravatar atlassian product jira agile confluence https://confluence.atlassian.com/display/aod/configuring+gravatar+support,Enable gravatars Could you enable gravatars for all our atlassian products (at least Jira + Agile; Confluence) https://confluence.atlassian.com/display/AOD/Configuring+Gravatar+support
Add cameraGeom overview to Doxygen documentation The CameraGeom package needs an overview page (part of afw's main.dox) as part of the Doxygen documentation. I think it's up to Simon or me to add this.,2,DM-354,datamanagement,add camerageom overview doxygen documentation camerageom package need overview page afw main.dox doxygen documentation think simon add,Add cameraGeom overview to Doxygen documentation The CameraGeom package needs an overview page (part of afw's main.dox) as part of the Doxygen documentation. I think it's up to Simon or me to add this.
"Simplify Co-add example in Software User Guide The current example in the LSST Software User Guide for co-addition reflects the processes necessary to perform a DR production. While thorough, it only really works on the lsst cluster. The example should be simplified to work on a smaller subset of data, and on single-user machines.  Definition of Done: * Following the documentation, it will be possible for users to identify the SDSS data they need (the subset of files) * There will be instructions on how to download the necessary files to their local machine * There will be instructions on how to build the necessary repositories * There will be instructions on how to run the Co-Add+forced photometry tasks.  * Any issues requiring access rights to LSST machines or databases will be identified and issues created for later.",8,DM-359,datamanagement,simplify co add example software user guide current example lsst software user guide co addition reflect process necessary perform dr production thorough work lsst cluster example simplify work small subset datum single user machine definition follow documentation possible user identify sdss datum need subset file instruction download necessary file local machine instruction build necessary repository instruction run co add+forced photometry task issue require access right lsst machine database identify issue create later,"Simplify Co-add example in Software User Guide The current example in the LSST Software User Guide for co-addition reflects the processes necessary to perform a DR production. While thorough, it only really works on the lsst cluster. The example should be simplified to work on a smaller subset of data, and on single-user machines. Definition of Done: * Following the documentation, it will be possible for users to identify the SDSS data they need (the subset of files) * There will be instructions on how to download the necessary files to their local machine * There will be instructions on how to build the necessary repositories * There will be instructions on how to run the Co-Add+forced photometry tasks. * Any issues requiring access rights to LSST machines or databases will be identified and issues created for later."
"Integration tests dataset should be packaged in eupspkg A qserv-integration-tests package should be created : - it would allow to manage easily, in ups/qserv.table, tests version for a given Qserv version. - it would allow to install Qserv dependencies related to testing, like partition (and other data ingest code which may arrive.",3,DM-365,datamanagement,integration test dataset package eupspkg qserv integration test package create allow manage easily up qserv.table test version give qserv version allow install qserv dependency relate testing like partition datum ingest code arrive,"Integration tests dataset should be packaged in eupspkg A qserv-integration-tests package should be created : - it would allow to manage easily, in ups/qserv.table, tests version for a given Qserv version. - it would allow to install Qserv dependencies related to testing, like partition (and other data ingest code which may arrive."
"CSS performance optimizations (avoiding redundant checks) Facade.cc: it seems worthwhile to think about how to tweak the implementation to reduce redundant calls. e.g., getChunkedTables() calls _cssI->exists() for the database for each contained table. It also calls exists() for each table, which it just retrieved. In reality, it only needs to call exists() once, for the db. So we are doing 1+3t reads rather than 1+t.  (This came up in the review of DM-56, the review comments are captured in DM-225) ",8,DM-367,datamanagement,css performance optimization avoid redundant check facade.cc worthwhile think tweak implementation reduce redundant call e.g. getchunkedtables call cssi->exist database contain table call exist table retrieve reality need exist db read 1+t come review dm-56 review comment capture dm-225,"CSS performance optimizations (avoiding redundant checks) Facade.cc: it seems worthwhile to think about how to tweak the implementation to reduce redundant calls. e.g., getChunkedTables() calls _cssI->exists() for the database for each contained table. It also calls exists() for each table, which it just retrieved. In reality, it only needs to call exists() once, for the db. So we are doing 1+3t reads rather than 1+t. (This came up in the review of DM-56, the review comments are captured in DM-225)"
"Improve how CSS exceptions are handled CSS has one class for all exceptions. The model adopted by Daniel (each type as a separate exception) is better, as we can catch individual exceptions  instead of (a) having to catch all css exceptions, (b) checking type and (c) rethrowing if it is not the type we want. To be able to catch all CSS exceptions, we can just introduce a new base class (that is new comparing to Daniel's version).",6,DM-369,datamanagement,improve css exception handle css class exception model adopt daniel type separate exception well catch individual exception instead have catch css exception check type rethrowe type want able catch css exception introduce new base class new compare daniel version,"Improve how CSS exceptions are handled CSS has one class for all exceptions. The model adopted by Daniel (each type as a separate exception) is better, as we can catch individual exceptions instead of (a) having to catch all css exceptions, (b) checking type and (c) rethrowing if it is not the type we want. To be able to catch all CSS exceptions, we can just introduce a new base class (that is new comparing to Daniel's version)."
"improved how default values for CSS are handled Need to improve how defaults are handled in qserv_admin. There seems to be some desire to warn when values are not set--how about setting defaults and just printing what configuration is being used? If this is something human-created, we should have reasonable defaults and not bother the user, unless no default is viable. I think we should only be strict on machine-generated input, where we would like to catch bugs as soon as possible.   (This came up in the review of DM-56, the review comments are captured in DM-225)",5,DM-370,datamanagement,improve default value css handle need improve default handle qserv_admin desire warn value set set default print configuration human create reasonable default bother user default viable think strict machine generate input like catch bug soon possible come review dm-56 review comment capture dm-225,"improved how default values for CSS are handled Need to improve how defaults are handled in qserv_admin. There seems to be some desire to warn when values are not set--how about setting defaults and just printing what configuration is being used? If this is something human-created, we should have reasonable defaults and not bother the user, unless no default is viable. I think we should only be strict on machine-generated input, where we would like to catch bugs as soon as possible. (This came up in the review of DM-56, the review comments are captured in DM-225)"
fix testQueryAnalysis 5 tests fail in the testCppParser.,2,DM-372,datamanagement,fix testqueryanalysis test fail testcppparser,fix testQueryAnalysis 5 tests fail in the testCppParser.
"Catch AttributeError problems in czar I wonder if we could catch more exceptions in czar to simplify debugging. For example, if I change: {code} --- a/core/modules/czar/lsst/qserv/master/app.py +++ b/core/modules/czar/lsst/qserv/master/app.py @@ -418,7 +418,7 @@ class InbandQueryAction:                     self.constraints.size())          dominantDb = getDominantDb(self.sessionId)          dbStriping = getDbStriping(self.sessionId) -        if (dbStriping.stripes < 1) or (dbStriping.subStripes < 1): +        if (dbStriping.x < 1) or (dbStriping.y < 1): {code}  It will return to client a very cryptic error: {code} Qserv error: Unexpected error: (<type 'exceptions.AttributeError'>, AttributeError('x',), <traceback object at 0x969a02c>) {code}  with no other clues, traceback or information in the log. ",2,DM-373,datamanagement,"catch attributeerror problem czar wonder catch exception czar simplify debugging example change code core module czar lsst qserv master app.py core module czar lsst qserv master app.py +418,7 class inbandqueryaction self.constraints.size dominantdb getdominantdb(self.sessionid dbstripe getdbstriping(self.sessionid dbstriping.stripes dbstriping.substripe dbstriping.x dbstriping.y code return client cryptic error code qserv error unexpected error attributeerror('x code clue traceback information log","Catch AttributeError problems in czar I wonder if we could catch more exceptions in czar to simplify debugging. For example, if I change: {code} --- a/core/modules/czar/lsst/qserv/master/app.py +++ b/core/modules/czar/lsst/qserv/master/app.py @@ -418,7 +418,7 @@ class InbandQueryAction: self.constraints.size()) dominantDb = getDominantDb(self.sessionId) dbStriping = getDbStriping(self.sessionId) - if (dbStriping.stripes < 1) or (dbStriping.subStripes < 1): + if (dbStriping.x < 1) or (dbStriping.y < 1): {code} It will return to client a very cryptic error: {code} Qserv error: Unexpected error: (, AttributeError('x',), ) {code} with no other clues, traceback or information in the log."
"loadLSST bug(s) for csh, ksh A flaw in the v8.0 loadLSST scripts (and/or in eups/bin/setups) causes the following errors:     1) When using ksh:   {code}     $INSTALL_DIR/loadLSST.ksh     ksh: /path/to/INSTALL_DIR/eups/bin/setups.ksh: cannot open [No such file or directory]  {code}  And indeed, there is no eups/bin/setups.ksh file.     2) When attempting to run the installation demo (v7.2.0.0):  {code}     $> printenv SHELL     /bin/tcsh  {code}  [The same issue appears with csh, unsurprisingly.]  {code}     $> source /path/to/install_dir/loadLSST.csh     $> cd /path/to/demo     $> setup obs_sdss     $> ./bin/demo.sh     ./bin/demo.sh: line 7:  /volumes/d0/lsst/stack80/eups/*default*/bin/setups.sh: No such file or directory     ./bin/demo.sh: line 12: setup: command not found  {code}  After hand-editing the demo.sh script to omit the ""/default"" string from the offending line, the demo runs normally to completion.     Note that everything works fine for bash with v8.0, which is what I tested awhile back. ",1,DM-380,datamanagement,loadlsst bug(s csh ksh flaw v8.0 loadlsst script and/or eup bin setup cause follow error ksh code install_dir loadlsst.ksh ksh /path install_dir eup bin setups.ksh open file directory code eup bin setups.ksh file attempt run installation demo v7.2.0.0 code printenv shell /bin tcsh code issue appear csh unsurprisingly code source /path install_dir loadlsst.csh cd /path demo setup obs_sdss ./bin demo.sh ./bin demo.sh line /volumes d0 lsst stack80 eups/*default*/bin setups.sh file directory ./bin demo.sh line 12 setup command find code hand edit demo.sh script omit /default string offend line demo run normally completion note work fine bash v8.0 test awhile,"loadLSST bug(s) for csh, ksh A flaw in the v8.0 loadLSST scripts (and/or in eups/bin/setups) causes the following errors: 1) When using ksh: {code} $INSTALL_DIR/loadLSST.ksh ksh: /path/to/INSTALL_DIR/eups/bin/setups.ksh: cannot open [No such file or directory] {code} And indeed, there is no eups/bin/setups.ksh file. 2) When attempting to run the installation demo (v7.2.0.0): {code} $> printenv SHELL /bin/tcsh {code} [The same issue appears with csh, unsurprisingly.] {code} $> source /path/to/install_dir/loadLSST.csh $> cd /path/to/demo $> setup obs_sdss $> ./bin/demo.sh ./bin/demo.sh: line 7: /volumes/d0/lsst/stack80/eups/*default*/bin/setups.sh: No such file or directory ./bin/demo.sh: line 12: setup: command not found {code} After hand-editing the demo.sh script to omit the ""/default"" string from the offending line, the demo runs normally to completion. Note that everything works fine for bash with v8.0, which is what I tested awhile back."
"Add Versioning to SourceTable in lsst::afw::table Add version to afw::table::SourceTable.  Persist that version number to fits file when the table is saved, and restore when the table is restored.  Tables created and saved to disk prior to this modification will have the version number 0, by default.  Tables created with the S14 version will have the version number 1.    This change is to enable a new version of slots and field naming conventions as needed by the Measurement Framework overhaul, at the same time allowing current clients of SourceTable to continue to function.  The work to define and persist the slots depending on the version will be on a separate issue.  Should not appear as an alterable member of the metadata, but should be saved with the metadata and reloaded when the file is reloaded.  getVersion and setVersion methods will be used to allow clients to alter this number.",1,DM-384,datamanagement,add versioning sourcetable lsst::afw::table add version afw::table::sourcetable persist version number fit file table save restore table restore table create save disk prior modification version number default table create s14 version version number change enable new version slot field naming convention need measurement framework overhaul time allow current client sourcetable continue function work define persist slot depend version separate issue appear alterable member metadata save metadata reload file reload getversion setversion method allow client alter number,"Add Versioning to SourceTable in lsst::afw::table Add version to afw::table::SourceTable. Persist that version number to fits file when the table is saved, and restore when the table is restored. Tables created and saved to disk prior to this modification will have the version number 0, by default. Tables created with the S14 version will have the version number 1. This change is to enable a new version of slots and field naming conventions as needed by the Measurement Framework overhaul, at the same time allowing current clients of SourceTable to continue to function. The work to define and persist the slots depending on the version will be on a separate issue. Should not appear as an alterable member of the metadata, but should be saved with the metadata and reloaded when the file is reloaded. getVersion and setVersion methods will be used to allow clients to alter this number."
"Create Command/Event Sender Simulate the OCS and CCS (via the OCS) sending message to the Base DMCS.  Write a library to send commands via method calls, which will be used by commandable  entities and by the Base DMCS.  The method calls in this library will be used to simulate sending commands from the OCS.  This will initially be developed using DM messages, and later switched to use DDS.  Write a command line tool to send these messages.  Commands with no arguments: init, enable, disable, release, stop, abort, reset.  Commands with arguments: configure - arguments are: Set of computers, software and versions to be executed, parameters used to control that software.  Events with arguments: startIntegration, nextVisit.  Definition of done:  * A library that has method calls to each of these commands/events.  Each method call sends one message to the given Topic. * Command line tool that can send any of these commands/events to a commandable entity subscribed to a Topic. * Unit tests for each of the commands/events",6,DM-386,datamanagement,create command event sender simulate ocs ccs ocs send message base dmcs write library send command method call commandable entity base dmcs method call library simulate send command ocs initially develop dm message later switch use dds write command line tool send message command argument init enable disable release stop abort reset command argument configure argument set computer software version execute parameter control software event argument startintegration nextvisit definition library method call command event method send message give topic command line tool send command event commandable entity subscribe topic unit test command event,"Create Command/Event Sender Simulate the OCS and CCS (via the OCS) sending message to the Base DMCS. Write a library to send commands via method calls, which will be used by commandable entities and by the Base DMCS. The method calls in this library will be used to simulate sending commands from the OCS. This will initially be developed using DM messages, and later switched to use DDS. Write a command line tool to send these messages. Commands with no arguments: init, enable, disable, release, stop, abort, reset. Commands with arguments: configure - arguments are: Set of computers, software and versions to be executed, parameters used to control that software. Events with arguments: startIntegration, nextVisit. Definition of done: * A library that has method calls to each of these commands/events. Each method call sends one message to the given Topic. * Command line tool that can send any of these commands/events to a commandable entity subscribed to a Topic. * Unit tests for each of the commands/events"
"Build Base DMCS communications library Write a library to be used by each commandable entity and the Base DMCS.   Methods in this library receiving commands from the simulated OCS. Specific command actions will be handled by each entity, and events are handled by the Base DMCS.  This will initially be developed using DM messages, and later switched to use the DDS.  The library will include:  * An object with methods for blocking receives, blocking receives with timeouts, and non-blocking receives.  Any commands received by these methods are given to another class to call appropriate action methods. * An abstract class implementing each of the following methods for commands: init, configure, enable, disable, release, stop, abort, reset. * An abstract class with methods for implementing the following methods for events:  startIntegration and nextVisit",6,DM-387,datamanagement,build base dmcs communication library write library commandable entity base dmcs method library receive command simulated ocs specific command action handle entity event handle base dmcs initially develop dm message later switch use dds library include object method block receive block receive timeout non blocking receive command receive method give class appropriate action method abstract class implement follow method command init configure enable disable release stop abort reset abstract class method implement follow method event startintegration nextvisit,"Build Base DMCS communications library Write a library to be used by each commandable entity and the Base DMCS. Methods in this library receiving commands from the simulated OCS. Specific command actions will be handled by each entity, and events are handled by the Base DMCS. This will initially be developed using DM messages, and later switched to use the DDS. The library will include: * An object with methods for blocking receives, blocking receives with timeouts, and non-blocking receives. Any commands received by these methods are given to another class to call appropriate action methods. * An abstract class implementing each of the following methods for commands: init, configure, enable, disable, release, stop, abort, reset. * An abstract class with methods for implementing the following methods for events: startIntegration and nextVisit"
"Write Linux Standard Base - compliant init.d scripts Qserv services init.d scripts have to rely on LSB, in order to work on multiple systems.  Remark : xrootd has to be launched as a background process (i.e. with a & at the end). But this always send of return code equal to 0, even if xrootd fails to start, a shell function wait_for_pid will be implemented in xrootd init.d scritps to solve that (inspired from mysqld init.d script).",5,DM-405,datamanagement,write linux standard base compliant script qserv service init.d script rely lsb order work multiple system remark xrootd launch background process i.e. end send return code equal xrootd fail start shell function wait_for_pid implement xrootd init.d scritp solve inspire mysqld init.d script,"Write Linux Standard Base - compliant init.d scripts Qserv services init.d scripts have to rely on LSB, in order to work on multiple systems. Remark : xrootd has to be launched as a background process (i.e. with a & at the end). But this always send of return code equal to 0, even if xrootd fails to start, a shell function wait_for_pid will be implemented in xrootd init.d scritps to solve that (inspired from mysqld init.d script)."
Simulate computation and production of VOEvents for worker batch job Simulate jobs which do alert processing.  Produce VOEvents based on those results.,6,DM-413,datamanagement,simulate computation production voevent worker batch job simulate job alert processing produce voevents base result,Simulate computation and production of VOEvents for worker batch job Simulate jobs which do alert processing. Produce VOEvents based on those results.
"finish adding aliases to afw::table::Schema This issue picks up the partially-completed Trac Ticket #2351, which adds a string-substitution-based alias mechanism to afw::table::Schema. There are still some issues to sort out w.r.t. constness - in other respects, a Table or Catalog's schema cannot be changed once the Table has been constructed, but we do need to be able to change aliases after table construction.",6,DM-417,datamanagement,finish add alias afw::table::schema issue pick partially complete trac ticket 2351 add string substitution base alia mechanism afw::table::schema issue sort w.r.t constness respect table catalog schema change table construct need able change alias table construction,"finish adding aliases to afw::table::Schema This issue picks up the partially-completed Trac Ticket #2351, which adds a string-substitution-based alias mechanism to afw::table::Schema. There are still some issues to sort out w.r.t. constness - in other respects, a Table or Catalog's schema cannot be changed once the Table has been constructed, but we do need to be able to change aliases after table construction."
"add basic FunctorKeys Add the basic FunctorKeys mechanism, and enough implementations to support Slots.",4,DM-421,datamanagement,add basic functorkeys add basic functorkeys mechanism implementation support slot,"add basic FunctorKeys Add the basic FunctorKeys mechanism, and enough implementations to support Slots."
Add FunctorKeys to replace compound field functionality Add more FunctorKeys to replace the functionality in all current compound field types and meas_base result objects.  May involve moving some meas_base definitions to afw.,4,DM-422,datamanagement,add functorkeys replace compound field functionality add functorkeys replace functionality current compound field type meas_base result object involve move meas_base definition afw,Add FunctorKeys to replace compound field functionality Add more FunctorKeys to replace the functionality in all current compound field types and meas_base result objects. May involve moving some meas_base definitions to afw.
"Add FunctorKeys for common analysis tasks Add FunctorKeys for simple, common, calculated fields, including:  - Magnitudes from fluxes  - Coords from Points, Points from Coords  - Ellipse conversions and radius/ellipticity extraction",4,DM-423,datamanagement,add functorkeys common analysis task add functorkeys simple common calculated field include magnitude flux coords points point coords ellipse conversion radius ellipticity extraction,"Add FunctorKeys for common analysis tasks Add FunctorKeys for simple, common, calculated fields, including: - Magnitudes from fluxes - Coords from Points, Points from Coords - Ellipse conversions and radius/ellipticity extraction"
"add live DS9-based debugging to measurement framework The old measurement framework had a lot of live DS9-based display options.  We should ensure the new one has at least as many, and that it still works.    At some point, we should consider a different mechanism for enabling those displays and possibly other tools for displaying them, but that's out of scope for this issue.",6,DM-428,datamanagement,add live ds9 base debugging measurement framework old measurement framework lot live ds9 base display option ensure new work point consider different mechanism enable display possibly tool display scope issue,"add live DS9-based debugging to measurement framework The old measurement framework had a lot of live DS9-based display options. We should ensure the new one has at least as many, and that it still works. At some point, we should consider a different mechanism for enabling those displays and possibly other tools for displaying them, but that's out of scope for this issue."
"Make NoiseReplacer outputs reproduceable We need a way to get back the noise-replaced Exposure as it was when a particular source was measurement, after the measurement has been run, without having to run noise-replacement on all the previous objects again.  There is already code in afw::math::Random to output its state as a string; I think we should probably just save this string in the output catalog.  This will require some API changes to allow the NoiseReplacer to modify the schema and set a field in the output records.",3,DM-429,datamanagement,noisereplacer output reproduceable need way noise replace exposure particular source measurement measurement run have run noise replacement previous object code afw::math::random output state string think probably save string output catalog require api change allow noisereplacer modify schema set field output record,"Make NoiseReplacer outputs reproduceable We need a way to get back the noise-replaced Exposure as it was when a particular source was measurement, after the measurement has been run, without having to run noise-replacement on all the previous objects again. There is already code in afw::math::Random to output its state as a string; I think we should probably just save this string in the output catalog. This will require some API changes to allow the NoiseReplacer to modify the schema and set a field in the output records."
Control log levels on a per-plugin basis We should be able to control log levels so that certain plugins are run at one level while the rest are run at another (to allow a particular plugin being debugged to be more verbose).,4,DM-430,datamanagement,control log level plugin basis able control log level certain plugin run level rest run allow particular plugin debug verbose,Control log levels on a per-plugin basis We should be able to control log levels so that certain plugins are run at one level while the rest are run at another (to allow a particular plugin being debugged to be more verbose).
"Add slot support for meas_base-style outputs The slot mechanism in afwTable currently uses compound fields to save the 3 slot types:  flux, centroid, and shape.  Since the new measurement framework uses a flattened representation in the SourceTable where these types are saved as multiple scalar fields, the slot mechanism need to be altered to handle this new table type.  1.  An alternative to KeyTuple for storing the keys required by the slot 2.  Fixup get(Centroid, Flux, Shape) in SourceRecord to use correct keys. 3.  Fixup the single value getters (getX, getY, etc) to use the correct keys. 4.  Persist slot info to fits correctly, based on table version.",4,DM-433,datamanagement,add slot support meas_base style output slot mechanism afwtable currently use compound field save slot type flux centroid shape new measurement framework use flatten representation sourcetable type save multiple scalar field slot mechanism need alter handle new table type alternative keytuple store key require slot fixup get(centroid flux shape sourcerecord use correct key fixup single value getter getx gety etc use correct key persist slot info fit correctly base table version,"Add slot support for meas_base-style outputs The slot mechanism in afwTable currently uses compound fields to save the 3 slot types: flux, centroid, and shape. Since the new measurement framework uses a flattened representation in the SourceTable where these types are saved as multiple scalar fields, the slot mechanism need to be altered to handle this new table type. 1. An alternative to KeyTuple for storing the keys required by the slot 2. Fixup get(Centroid, Flux, Shape) in SourceRecord to use correct keys. 3. Fixup the single value getters (getX, getY, etc) to use the correct keys. 4. Persist slot info to fits correctly, based on table version."
"add aperture-correction measurement code to the end of calibrate At the end of CalibrateTask, we'll want to compute the PSF and aperture fluxes of the PSF stars, and send those to the PSF model to be stored and interpolated (using the featured added via DM-434).  We'll also need to run any other flux measurement algorithms that need to be tied to the PSF fluxes on these same stars; because these can be somewhat slow, we probably want to limit these measurements to only the PSF stars, rather than requiring all these algorithms to be run as part of calibrate.measurement.  The relationships between these fluxes and the PSF fluxes will be additional fields to be added to and interpolated by the PSF.  The HSC implementation of this work (as well as that of DM-436) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/jbosch/DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/DM-191",8,DM-435,datamanagement,add aperture correction measurement code end calibrate end calibratetask want compute psf aperture flux psf star send psf model store interpolate feature add dm-434 need run flux measurement algorithm need tie psf flux star somewhat slow probably want limit measurement psf star require algorithm run calibrate.measurement relationship flux psf flux additional field add interpolate psf hsc implementation work dm-436 issue hsc-191 https://hsc-jira.astro.princeton.edu/jira/browse/hsc-191 change package relevant one lsst https://github.com/hypersuprime-cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/hypersuprime-cam/meas_algorithms/compare/hsc-3.0.0...u/jbosch/dm-191 https://github.com/hypersuprime-cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/dm-191,"add aperture-correction measurement code to the end of calibrate At the end of CalibrateTask, we'll want to compute the PSF and aperture fluxes of the PSF stars, and send those to the PSF model to be stored and interpolated (using the featured added via DM-434). We'll also need to run any other flux measurement algorithms that need to be tied to the PSF fluxes on these same stars; because these can be somewhat slow, we probably want to limit these measurements to only the PSF stars, rather than requiring all these algorithms to be run as part of calibrate.measurement. The relationships between these fluxes and the PSF fluxes will be additional fields to be added to and interpolated by the PSF. The HSC implementation of this work (as well as that of DM-436) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/jbosch/DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/DM-191"
"apply aperture corrections in measurement tasks We need to interpolate the aperture correction to the position of every source, and apply this correction to all appropriate fluxes.  The HSC implementation of this work (as well as that of DM-435) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/jbosch/DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/DM-191  Note that on the LSST side, we'll want to apply the aperture corrections either within a new plugin in meas_base or as a new part of BaseMeasurementTask, not as a change to the CorrectFluxes algorithm (which will be removed in the future along with the rest of the old measurement framework in meas_algorithms).",7,DM-436,datamanagement,apply aperture correction measurement task need interpolate aperture correction position source apply correction appropriate flux hsc implementation work dm-435 issue hsc-191 https://hsc-jira.astro.princeton.edu/jira/browse/hsc-191 change package relevant one lsst https://github.com/hypersuprime-cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/hypersuprime-cam/meas_algorithms/compare/hsc-3.0.0...u/jbosch/dm-191 https://github.com/hypersuprime-cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/dm-191 note lsst want apply aperture correction new plugin meas_base new basemeasurementtask change correctfluxes algorithm remove future rest old measurement framework meas_algorithms,"apply aperture corrections in measurement tasks We need to interpolate the aperture correction to the position of every source, and apply this correction to all appropriate fluxes. The HSC implementation of this work (as well as that of DM-435) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/jbosch/DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/DM-191 Note that on the LSST side, we'll want to apply the aperture corrections either within a new plugin in meas_base or as a new part of BaseMeasurementTask, not as a change to the CorrectFluxes algorithm (which will be removed in the future along with the rest of the old measurement framework in meas_algorithms)."
"Setup of four new measurement algorithms for processCcd testing The goal of this story is to take the following algorithms and make them fully operational with processCcd.  PsfFlux,SdssShape,SdssCentroid,SincFlux.  This code was moved to meas_base in w14, but has yet to be used in full operation.  This is the first step in that process.  Each algorithm will at least have one test to confirm that it works, but the unit tests will be very simple.  The actual confirmation of full operability will the to run tests against real data, and compare against the existing meas_algorithms.  The algorithm code will still require cleanup even after this story is completed.  That is because it has intentionally been left the same as what existed in meas_algorithms.    The goal of this ticket is to confirm that the each algorithm can (1) complete its measurement with some reasonable result, (2) responds to its major configuration options, and (3) handles at least some of its defined exceptions (that is, flags) correctly.  Confirmation that the results are identical with meas_algorithms is a subsequent ticket  Cleanup and documentation of this code will be done in a subsequent ticket.",4,DM-441,datamanagement,setup new measurement algorithm processccd test goal story following algorithm fully operational processccd psfflux sdssshape sdsscentroid sincflux code move meas_base w14 operation step process algorithm test confirm work unit test simple actual confirmation operability run test real datum compare exist meas_algorithm algorithm code require cleanup story complete intentionally leave exist meas_algorithm goal ticket confirm algorithm complete measurement reasonable result respond major configuration option handle define exception flag correctly confirmation result identical meas_algorithm subsequent ticket cleanup documentation code subsequent ticket,"Setup of four new measurement algorithms for processCcd testing The goal of this story is to take the following algorithms and make them fully operational with processCcd. PsfFlux,SdssShape,SdssCentroid,SincFlux. This code was moved to meas_base in w14, but has yet to be used in full operation. This is the first step in that process. Each algorithm will at least have one test to confirm that it works, but the unit tests will be very simple. The actual confirmation of full operability will the to run tests against real data, and compare against the existing meas_algorithms. The algorithm code will still require cleanup even after this story is completed. That is because it has intentionally been left the same as what existed in meas_algorithms. The goal of this ticket is to confirm that the each algorithm can (1) complete its measurement with some reasonable result, (2) responds to its major configuration options, and (3) handles at least some of its defined exceptions (that is, flags) correctly. Confirmation that the results are identical with meas_algorithms is a subsequent ticket Cleanup and documentation of this code will be done in a subsequent ticket."
"Approve/Acquire HipChat licenses -------- Original Message -------- Subject: Getting HipChat licenses Date: Wed, 09 Apr 2014 08:51:46 -0700 From: Mario Juric <mjuric@lsst.org> Organization: Large Synoptic Survey Telescope Inc. To: Jeffrey Kantor <JKantor@lsst.org>, Iain Goodenow <IGoodenow@lsst.org>,  Stefan Dimmick <SDimmick@lsst.org>  Jeff et al., 	I'd like to start the process to acquire HipChat licenses (we have 10 days left on our evaluation one). Right now we have 26 users, so our first total would be $52/month. I'm expecting it will eventually grow to ~50 users or so.  	In terms of an account to charge, this should really be considered a project-wide tool, but if that's going to cause unnecessary delays I'd propose we charge it to DM as a sign of our infinite kindness and good will :).  	HipChat wants to simply bill a credit card -- once we get the necessary approvals, Iain, I can add you as an admin, and you can use our corporate one if that's OK. ",1,DM-443,datamanagement,approve acquire hipchat license original message subject get hipchat license date 09 apr 2014 08:51:46 -0700 mario juric organization large synoptic survey telescope inc. jeffrey kantor iain goodenow stefan dimmick jeff et al like start process acquire hipchat license 10 day leave evaluation right 26 user total 52 month expect eventually grow user term account charge consider project wide tool go cause unnecessary delay propose charge dm sign infinite kindness good :) hipchat want simply bill credit card necessary approval iain add admin use corporate ok,"Approve/Acquire HipChat licenses -------- Original Message -------- Subject: Getting HipChat licenses Date: Wed, 09 Apr 2014 08:51:46 -0700 From: Mario Juric  Organization: Large Synoptic Survey Telescope Inc. To: Jeffrey Kantor , Iain Goodenow , Stefan Dimmick  Jeff et al., I'd like to start the process to acquire HipChat licenses (we have 10 days left on our evaluation one). Right now we have 26 users, so our first total would be $52/month. I'm expecting it will eventually grow to ~50 users or so. In terms of an account to charge, this should really be considered a project-wide tool, but if that's going to cause unnecessary delays I'd propose we charge it to DM as a sign of our infinite kindness and good will :). HipChat wants to simply bill a credit card -- once we get the necessary approvals, Iain, I can add you as an admin, and you can use our corporate one if that's OK."
"Test four algorithms for compatibility with original meas_algorithms Do a trial run of a small area of sky (using a single exposure from measMosaicData).  First create a source catalog using the old measurement.py, then run the same test with the measurement task in sfm.py.  Compare the results.  If the code has been ported correctly, they should match.",4,DM-445,datamanagement,test algorithm compatibility original meas_algorithm trial run small area sky single exposure measmosaicdata create source catalog old measurement.py run test measurement task sfm.py compare result code port correctly match,"Test four algorithms for compatibility with original meas_algorithms Do a trial run of a small area of sky (using a single exposure from measMosaicData). First create a source catalog using the old measurement.py, then run the same test with the measurement task in sfm.py. Compare the results. If the code has been ported correctly, they should match."
Setup PeakLikelihoodFlux with new Algorithm Framework Move PeakLikehoodFlux to meas_base framework ,1,DM-446,datamanagement,setup peaklikelihoodflux new algorithm framework peaklikehoodflux meas_base framework,Setup PeakLikelihoodFlux with new Algorithm Framework Move PeakLikehoodFlux to meas_base framework
"Setup Flux algorithms for testing with processCcd Similar to DM-441 for flux algorithms GaussianFlux and NaiveFlux  Unit tests for major config options, just to be sure that they do something reasonable.  Test of at least one exception (flag)",2,DM-447,datamanagement,setup flux algorithm test processccd similar dm-441 flux algorithm gaussianflux naiveflux unit test major config option sure reasonable test exception flag,"Setup Flux algorithms for testing with processCcd Similar to DM-441 for flux algorithms GaussianFlux and NaiveFlux Unit tests for major config options, just to be sure that they do something reasonable. Test of at least one exception (flag)"
"Test Flux algorithms agains meas_algorithms Test for compatibility of NaiveFlux, GaussianFlux, and PsfFlux against meas_algorithms  ",2,DM-449,datamanagement,test flux algorithm again meas_algorithm test compatibility naiveflux gaussianflux psfflux meas_algorithm,"Test Flux algorithms agains meas_algorithms Test for compatibility of NaiveFlux, GaussianFlux, and PsfFlux against meas_algorithms"
"reimplement shapelet PSF approximations The CModel code we want to transfer from HSC in DM-240 currently relies on the old ""multishapelet.psf"" algorithm in meas_extensions_multiShapelet.  That means we either need to convert that PSF-to-shapelets code in meas_extensions_multiShapelet to use the meas_base interface, or we need add new PSF-to-shapelets code in meas_multifit.  I think the latter is the better choice, even if we delay DM-240 as a result; the heavy algorithmic code is already available as primitives in meas_multifit, so it should just be a matter of packing those into a simple driver, creating a config class for it, and testing it on a few real and simulated PSFs to learn reasonable defaults for the configs.",6,DM-454,datamanagement,reimplement shapelet psf approximation cmodel code want transfer hsc dm-240 currently rely old multishapelet.psf algorithm meas_extensions_multishapelet mean need convert psf shapelet code meas_extensions_multishapelet use meas_base interface need add new psf shapelet code meas_multifit think well choice delay dm-240 result heavy algorithmic code available primitive meas_multifit matter pack simple driver create config class test real simulated psf learn reasonable default config,"reimplement shapelet PSF approximations The CModel code we want to transfer from HSC in DM-240 currently relies on the old ""multishapelet.psf"" algorithm in meas_extensions_multiShapelet. That means we either need to convert that PSF-to-shapelets code in meas_extensions_multiShapelet to use the meas_base interface, or we need add new PSF-to-shapelets code in meas_multifit. I think the latter is the better choice, even if we delay DM-240 as a result; the heavy algorithmic code is already available as primitives in meas_multifit, so it should just be a matter of packing those into a simple driver, creating a config class for it, and testing it on a few real and simulated PSFs to learn reasonable defaults for the configs."
"Implement backup/restore for CSS It'd be very useful to have a way to ""dump"" and ""restore"" entire contents of the key/value store in zookeeper. The dump part is pretty much there (can dump to an ascii file) ",4,DM-460,datamanagement,implement backup restore css useful way dump restore entire content key value store zookeeper dump pretty dump ascii file,"Implement backup/restore for CSS It'd be very useful to have a way to ""dump"" and ""restore"" entire contents of the key/value store in zookeeper. The dump part is pretty much there (can dump to an ascii file)"
"Add Classes of MeasurementError Some measurement failures are global for a whole exposure, such as a missing Psf or Wcs.  The framework currently does not distinguish this from a failure in a single measurement.  Should add a new subclass of MeasurementError which can be thrown in these cases.  Should also add configuration option to the measurement framework to determine what should be done with this type of error.  We should also add an exception class and associated how-to-handle config for problems that indicate that something has gone wrong in pre-measurement processing, such as NaNs in the image.",4,DM-461,datamanagement,add classes measurementerror measurement failure global exposure missing psf wcs framework currently distinguish failure single measurement add new subclass measurementerror throw case add configuration option measurement framework determine type error add exception class associate handle config problem indicate go wrong pre measurement processing nans image,"Add Classes of MeasurementError Some measurement failures are global for a whole exposure, such as a missing Psf or Wcs. The framework currently does not distinguish this from a failure in a single measurement. Should add a new subclass of MeasurementError which can be thrown in these cases. Should also add configuration option to the measurement framework to determine what should be done with this type of error. We should also add an exception class and associated how-to-handle config for problems that indicate that something has gone wrong in pre-measurement processing, such as NaNs in the image."
"PixelsFlags, SkyCoordAlgorithm, and Classification SkyCoord was moved to DM-441 - done in Python  Classification is also simple if done in Python  Pixel Keys will be done in C++  Some work left from SdssShape will be done with this ticket  Also, since these are out only two Python algorithms in the base set, I will add the exception handling and base fail() methods at this time. ",2,DM-463,datamanagement,pixelsflags skycoordalgorithm classification skycoord move dm-441 python classification simple python pixel keys c++ work leave sdssshape ticket python algorithm base set add exception handling base fail method time,"PixelsFlags, SkyCoordAlgorithm, and Classification SkyCoord was moved to DM-441 - done in Python Classification is also simple if done in Python Pixel Keys will be done in C++ Some work left from SdssShape will be done with this ticket Also, since these are out only two Python algorithms in the base set, I will add the exception handling and base fail() methods at this time."
"add and use ""suspect"" flag in slots and slot-like measurements We need to have both ""fatal"" and ""suspect"" flags to handle different levels of warnings in measurement.  (other tasks previously included on this ticket have been split into other tickets, including DM-461 and DM-984)",4,DM-464,datamanagement,add use suspect flag slot slot like measurement need fatal suspect flag handle different level warning measurement task previously include ticket split ticket include dm-461 dm-984,"add and use ""suspect"" flag in slots and slot-like measurements We need to have both ""fatal"" and ""suspect"" flags to handle different levels of warnings in measurement. (other tasks previously included on this ticket have been split into other tickets, including DM-461 and DM-984)"
"lsst-build updates based on feedback from 1st month of use change lsst-build interface to:   * rename 'prepare' to 'clone'   * use current directory to clone to   * remove the default REPOSITORY_PATTERN, as it isn't complete   * use the current username as default build tag prefix   * don't write the manifest by default; use --manifest=<filename> option instead   * refuse to change/overwrite repositories if they're dirty, use --force to override   * create a separate 'version' verb   * ProductFetcher.fetch doesn't need to return ref, sha1   * read config file within build directory, .btconfig   * rename the binary to bt   * have the versioner use git db by default, but fall back to generating versions from hash by default  See the linked web page for a mock of the command line interface.",6,DM-466,datamanagement,lsst build update base feedback 1st month use change lsst build interface rename prepare clone use current directory clone remove default repository_pattern complete use current username default build tag prefix write manifest default use --manifest= option instead refuse change overwrite repository dirty use override create separate version verb productfetcher.fetch need return ref sha1 read config file build directory .btconfig rename binary bt versioner use git db default fall generate version hash default link web page mock command line interface,"lsst-build updates based on feedback from 1st month of use change lsst-build interface to: * rename 'prepare' to 'clone' * use current directory to clone to * remove the default REPOSITORY_PATTERN, as it isn't complete * use the current username as default build tag prefix * don't write the manifest by default; use --manifest= option instead * refuse to change/overwrite repositories if they're dirty, use --force to override * create a separate 'version' verb * ProductFetcher.fetch doesn't need to return ref, sha1 * read config file within build directory, .btconfig * rename the binary to bt * have the versioner use git db by default, but fall back to generating versions from hash by default See the linked web page for a mock of the command line interface."
"Alias measurement.plugins to measurement.algorithms The config item in the old measurement task, measurement.algorithms was changed to measurement.plugins in meas_base.  The creates a backward compatibility issue for code which refers to this class member.  Jim's suggested fix is to alias plugins with algorithms in the new measurement task.",1,DM-468,datamanagement,alias measurement.plugin measurement.algorithm config item old measurement task measurement.algorithm change measurement.plugin meas_base create backward compatibility issue code refer class member jim suggest fix alia plugin algorithm new measurement task,"Alias measurement.plugins to measurement.algorithms The config item in the old measurement task, measurement.algorithms was changed to measurement.plugins in meas_base. The creates a backward compatibility issue for code which refers to this class member. Jim's suggested fix is to alias plugins with algorithms in the new measurement task."
"Rework exceptions in css (python side) Rework exceptions in css/KwInterface.py: split into key-value related exceptions, possibly moving the rest that deals with db/tables into client.  This came up in the CSS review, see DM-225: ""CssException feels a bit out of place....""",1,DM-470,datamanagement,rework exception css python rework exception css kwinterface.py split key value relate exception possibly move rest deal db table client come css review dm-225 cssexception feel bit place,"Rework exceptions in css (python side) Rework exceptions in css/KwInterface.py: split into key-value related exceptions, possibly moving the rest that deals with db/tables into client. This came up in the CSS review, see DM-225: ""CssException feels a bit out of place...."""
"Create an iPython Notebook visualization of the LSST Demo Data Develop an iPython Notebook to illustrate the processing and results of the LSST Demo. This can be used as both a basic tutorial for the Stack, and a how-to for creating additional visualizations for DM. The visualization should include the source catalog, and possibly also one or more of the processed images. ",4,DM-472,datamanagement,create ipython notebook visualization lsst demo data develop ipython notebook illustrate processing result lsst demo basic tutorial stack create additional visualization dm visualization include source catalog possibly process image,"Create an iPython Notebook visualization of the LSST Demo Data Develop an iPython Notebook to illustrate the processing and results of the LSST Demo. This can be used as both a basic tutorial for the Stack, and a how-to for creating additional visualizations for DM. The visualization should include the source catalog, and possibly also one or more of the processed images."
"Prioritize and define the backlog for Summer 2014 At the DMLT meeting in Seattle, finish the backlog prioritization for Summer 2014 cycle.  Definition of Done: * All epics and major stories defined for Summer 2014. * Rank-ordered in terms of priority. * Leadership teem agreement on their prioritization and backlog. * Teams identified to execute the stories, and stories labeled accordingly. ",2,DM-473,datamanagement,prioritize define backlog summer 2014 dmlt meeting seattle finish backlog prioritization summer 2014 cycle definition epic major story define summer 2014 rank order term priority leadership teem agreement prioritization backlog team identify execute story story label accordingly,"Prioritize and define the backlog for Summer 2014 At the DMLT meeting in Seattle, finish the backlog prioritization for Summer 2014 cycle. Definition of Done: * All epics and major stories defined for Summer 2014. * Rank-ordered in terms of priority. * Leadership teem agreement on their prioritization and backlog. * Teams identified to execute the stories, and stories labeled accordingly."
"Document how to create an astrometry_net_data repository Document how to create astrometry_net_data index files for an arbitrary astronomical dataset, as well as how to create a repository of such data to support processing with the LSST Stack. It must be possible to limit the area of sky coverage in the index files to that appropriate for the images that the user wishes to process. ",6,DM-474,datamanagement,document create astrometry_net_data repository document create astrometry_net_data index file arbitrary astronomical dataset create repository datum support processing lsst stack possible limit area sky coverage index file appropriate image user wish process,"Document how to create an astrometry_net_data repository Document how to create astrometry_net_data index files for an arbitrary astronomical dataset, as well as how to create a repository of such data to support processing with the LSST Stack. It must be possible to limit the area of sky coverage in the index files to that appropriate for the images that the user wishes to process."
"Make JIRA notification e-mail more useful From HipChat/Data Management:  [12:09] Mario Juric: @jbosch @KTL @KSK Could you double-check if any of you got an e-mail from Jira on Saturday (Apr 12th) re issue DM-78 (I made you reviewers, but it looks like you weren't notified)? [12:10] K-T Lim: I don't recall and can't determine now; it would have been deleted (irrevocably). [12:10] Simon Krughoff: I did get an email. [12:10] Jim Bosch: @mjuric, ah, it appears that I actually did.  The fact that I was a reviewer was just buried, and I didn't notice it. [12:10] Simon Krughoff: I must have missed that I was a reviewer. [12:10] Mario Juric: OK, thanks! 		That gives me not one, but two useful data points (#1 -- emails work, #2 -- they're useless :) ). [12:12] Simon Krughoff: I'm not sure why they are useless.  The emails from trac were a very important part of my workflow as far as being notified of review responsibility goes. 		Maybe it's just the volume from Jira. [12:14] Jim Bosch: Yeah, same here.  Though the volume from JIRA hasn't been so bad, so I don't think that's it.  Maybe my brain just has to get used to the new email format. [12:14] K-T Lim: (In my case, I'm mostly paying attention to the RSS feed although the mailbox serves as a backup.) [12:22] Robert Lupton: One of the things that made gnats a good bug tracker was that the emails contained the right amount of information (I did have source code...), and trac was pretty good too when we tuned it;  bugzilla always used to be awful.  I bet we can fiddle with Jira to make its mail more useful;  I don't just mean filtering what it sends, but making sure that each email is self contained, but not too long",1,DM-488,datamanagement,jira notification mail useful hipchat data management 12:09 mario juric @jbosch @ktl @ksk double check get mail jira saturday apr 12th issue dm-78 reviewer look like notify 12:10 lim recall determine delete irrevocably 12:10 simon krughoff email 12:10 jim bosch @mjuric ah appear actually fact reviewer bury notice 12:10 simon krughoff miss reviewer 12:10 mario juric ok thank give useful datum point email work useless 12:12 simon krughoff sure useless email trac important workflow far notify review responsibility go maybe volume jira 12:14 jim bosch yeah volume jira bad think maybe brain new email format 12:14 lim case pay attention rss feed mailbox serve backup 12:22 robert lupton thing gnat good bug tracker email contain right information source code trac pretty good tune bugzilla awful bet fiddle jira mail useful mean filter send make sure email self contain long,"Make JIRA notification e-mail more useful From HipChat/Data Management: [12:09] Mario Juric: @jbosch @KTL @KSK Could you double-check if any of you got an e-mail from Jira on Saturday (Apr 12th) re issue DM-78 (I made you reviewers, but it looks like you weren't notified)? [12:10] K-T Lim: I don't recall and can't determine now; it would have been deleted (irrevocably). [12:10] Simon Krughoff: I did get an email. [12:10] Jim Bosch: @mjuric, ah, it appears that I actually did. The fact that I was a reviewer was just buried, and I didn't notice it. [12:10] Simon Krughoff: I must have missed that I was a reviewer. [12:10] Mario Juric: OK, thanks! That gives me not one, but two useful data points (#1 -- emails work, #2 -- they're useless :) ). [12:12] Simon Krughoff: I'm not sure why they are useless. The emails from trac were a very important part of my workflow as far as being notified of review responsibility goes. Maybe it's just the volume from Jira. [12:14] Jim Bosch: Yeah, same here. Though the volume from JIRA hasn't been so bad, so I don't think that's it. Maybe my brain just has to get used to the new email format. [12:14] K-T Lim: (In my case, I'm mostly paying attention to the RSS feed although the mailbox serves as a backup.) [12:22] Robert Lupton: One of the things that made gnats a good bug tracker was that the emails contained the right amount of information (I did have source code...), and trac was pretty good too when we tuned it; bugzilla always used to be awful. I bet we can fiddle with Jira to make its mail more useful; I don't just mean filtering what it sends, but making sure that each email is self contained, but not too long"
"Build Base DMCS Archiver Command Receiver Base DMCS Archiver  The Archiver receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  prerequisite for configure command - sufficient replicator/distributor pairs are available.  Note that after configure, the Archiver remains disabled.  * enable - Subscribe to the startIntegration event.  * disable - Unsubscribe to the startIntegration event.  Note that this does NOT terminate any replicator jobs which are already executing.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration",6,DM-495,datamanagement,build base dmcs archiver command receiver base dmcs archiver archiver receive command ocs perform follow action init offline mode idle state configure argument set computer use software execute parameter control software verify format accept command change disable mode disable command receive install configuration return success ocs failure illegal configuration instal properly command error failure reason send ocs prerequisite configure command sufficient replicator distributor pair available note configure archiver remain disabled enable subscribe startintegration event disable unsubscribe startintegration event note terminate replicator job execute release unsubscribe startintegration event offline state abort receive configure command cause component error state configuration receive time system transition error state stop progress reset unsubscribe startintegration event idle state configuration,"Build Base DMCS Archiver Command Receiver Base DMCS Archiver The Archiver receives commands from the OCS, and performs the following actions: * init - Move from OFFLINE mode to IDLE state. * configure - (arguments: set of computers to use, the software to be executed, and parameters to control that software). 1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration; 4) return success to the OCS. On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS. prerequisite for configure command - sufficient replicator/distributor pairs are available. Note that after configure, the Archiver remains disabled. * enable - Subscribe to the startIntegration event. * disable - Unsubscribe to the startIntegration event. Note that this does NOT terminate any replicator jobs which are already executing. * release - Unsubscribe to the startIntegration event, and go into OFFLINE state. * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration. If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress. * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration"
"Build Base DMCS Catch-Up Archiver Command Receiver Base DMCS Catch-Up Archiver  The Catch-Up Archiver receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - sufficient catch-up-dedicated replicator/distributor pairs are available.  Note that after configure, the Archiver remains disabled.  * enable - 1) Allows Catch-Up Archiver to scan for unarchived images to be handled; 2) Enables the Orchestration Manager to schedule image archive jobs.  * disable - 1) Stop scanning for unarchived images; 2) Tell the Orchestration Manager to stop scheduling any new image archive jobs.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration",6,DM-502,datamanagement,build base dmcs catch archiver command receiver base dmcs catch archiver catch archiver receive command ocs perform follow action init offline mode idle state configure argument set computer use software execute parameter control software verify format accept command change disable mode disable command receive install configuration return success ocs failure illegal configuration instal properly command error failure reason send ocs prerequisite configure command sufficient catch dedicate replicator distributor pair available note configure archiver remain disabled enable allow catch archiver scan unarchived image handle enable orchestration manager schedule image archive job disable stop scan unarchived image tell orchestration manager stop schedule new image archive job release unsubscribe startintegration event offline state abort receive configure command cause component error state configuration receive time system transition error state stop progress reset unsubscribe startintegration event idle state configuration,"Build Base DMCS Catch-Up Archiver Command Receiver Base DMCS Catch-Up Archiver The Catch-Up Archiver receives commands from the OCS, and performs the following actions: * init - Move from OFFLINE mode to IDLE state. * configure - (arguments: set of computers to use, the software to be executed, and parameters to control that software). 1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration; 4) return success to the OCS. On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS. Prerequisite for configure command - sufficient catch-up-dedicated replicator/distributor pairs are available. Note that after configure, the Archiver remains disabled. * enable - 1) Allows Catch-Up Archiver to scan for unarchived images to be handled; 2) Enables the Orchestration Manager to schedule image archive jobs. * disable - 1) Stop scanning for unarchived images; 2) Tell the Orchestration Manager to stop scheduling any new image archive jobs. * release - Unsubscribe to the startIntegration event, and go into OFFLINE state. * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration. If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress. * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration"
"Build Base DMCS EFD Replicator Command Receiver Base DMCS EFD Replicator  The EFD Replicator receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - communication with the US DAC EFD replica is possible.  Note that after configure, the EFD Replicator remains disabled.  * enable - Causes the Base DMCS to enable the US DAC EFD replica to be a slave to the Chilean DAC EFD replica.  * disable - Causes the Base DMCS to disable the slave operation of the US DAC EFD replica.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration",6,DM-503,datamanagement,build base dmcs efd replicator command receiver base dmcs efd replicator efd replicator receive command ocs perform follow action init offline mode idle state configure argument set computer use software execute parameter control software verify format accept command change disable mode disable command receive install configuration return success ocs failure illegal configuration instal properly command error failure reason send ocs prerequisite configure command communication dac efd replica possible note configure efd replicator remain disabled enable causes base dmcs enable dac efd replica slave chilean dac efd replica disable causes base dmcs disable slave operation dac efd replica release unsubscribe startintegration event offline state abort receive configure command cause component error state configuration receive time system transition error state stop progress reset unsubscribe startintegration event idle state configuration,"Build Base DMCS EFD Replicator Command Receiver Base DMCS EFD Replicator The EFD Replicator receives commands from the OCS, and performs the following actions: * init - Move from OFFLINE mode to IDLE state. * configure - (arguments: set of computers to use, the software to be executed, and parameters to control that software). 1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration; 4) return success to the OCS. On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS. Prerequisite for configure command - communication with the US DAC EFD replica is possible. Note that after configure, the EFD Replicator remains disabled. * enable - Causes the Base DMCS to enable the US DAC EFD replica to be a slave to the Chilean DAC EFD replica. * disable - Causes the Base DMCS to disable the slave operation of the US DAC EFD replica. * release - Unsubscribe to the startIntegration event, and go into OFFLINE state. * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration. If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress. * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration"
"Build Base DMCS Alert Production Cluster Command Receiver Base DMCS Alert Production Cluster  The Alert Production Cluster receives commands from the OCS, and performs the following actions:  * init - Move from OFFLINE mode to IDLE state.  * configure - (arguments:  set of computers to use, the software to be executed, and parameters to control that software).  1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration;  4) return success to the OCS.  On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS.  Prerequisite for configure command - sufficient workers are available.  Note that after configure, the Alter Production Cluster remains disabled.  * enable - Causes the Base DMCS to subscribe to the “nextVisit” topic in normal science mode; another event may be subscribed to in calibration or engineering mode.  * disable - Causes the Base DMCS to unsubscribe from the “nextVisit” topic.  It does NOT terminate any worker jobs already executing.  In particular, the processing for the current visit (not just exposure) will normally complete.  * release - Unsubscribe to the startIntegration event, and go into OFFLINE state.  * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration.  If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress.  * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration",6,DM-504,datamanagement,build base dmcs alert production cluster command receiver base dmcs alert production cluster alert production cluster receive command ocs perform follow action init offline mode idle state configure argument set computer use software execute parameter control software verify format accept command change disable mode disable command receive install configuration return success ocs failure illegal configuration instal properly command error failure reason send ocs prerequisite configure command sufficient worker available note configure alter production cluster remain disabled enable causes base dmcs subscribe nextvisit topic normal science mode event subscribe calibration engineering mode disable causes base dmcs unsubscribe nextvisit topic terminate worker job execute particular processing current visit exposure normally complete release unsubscribe startintegration event offline state abort receive configure command cause component error state configuration receive time system transition error state stop progress reset unsubscribe startintegration event idle state configuration,"Build Base DMCS Alert Production Cluster Command Receiver Base DMCS Alert Production Cluster The Alert Production Cluster receives commands from the OCS, and performs the following actions: * init - Move from OFFLINE mode to IDLE state. * configure - (arguments: set of computers to use, the software to be executed, and parameters to control that software). 1) Verify format and accept command; 2) change to disable mode as if a disable command had been received; 3) install configuration; 4) return success to the OCS. On failure (illegal configuration, cannot be installed properly), a command error with the failure reason is sent to the OCS. Prerequisite for configure command - sufficient workers are available. Note that after configure, the Alter Production Cluster remains disabled. * enable - Causes the Base DMCS to subscribe to the nextVisit topic in normal science mode; another event may be subscribed to in calibration or engineering mode. * disable - Causes the Base DMCS to unsubscribe from the nextVisit topic. It does NOT terminate any worker jobs already executing. In particular, the processing for the current visit (not just exposure) will normally complete. * release - Unsubscribe to the startIntegration event, and go into OFFLINE state. * abort - if received during a configure command, causes this component to go into the ERROR state with no configuration. If received at any other time, the system transitions into an ERROR state, but nothing stops that was already in progress. * reset - Unsubscribe to the startIntegration event, go into IDLE state, with no configuration"
improve initialization of kvMap in testQueryAnalysis Build the kvMap at build-time and embed it into the executable. (this was brought up in DM-225),1,DM-505,datamanagement,improve initialization kvmap testqueryanalysis build kvmap build time embe executable bring dm-225,improve initialization of kvMap in testQueryAnalysis Build the kvMap at build-time and embed it into the executable. (this was brought up in DM-225)
"improve generating kvMap in testFacade.cc Generating the kvmap file, and pasting it into a string inside the test program. (this was brought up in DM-225)",1,DM-506,datamanagement,improve generate kvmap testfacade.cc generate kvmap file paste string inside test program bring dm-225,"improve generating kvMap in testFacade.cc Generating the kvmap file, and pasting it into a string inside the test program. (this was brought up in DM-225)"
shorten internal names in zookeeper rename DATABASE_PARTITIONING to PARTITIONING  rename DATABASES to DBS,2,DM-508,datamanagement,shorten internal name zookeeper rename database_partitioning partition rename database dbs,shorten internal names in zookeeper rename DATABASE_PARTITIONING to PARTITIONING rename DATABASES to DBS
"rename ""dbGroup"" to ""storageClass"" in CSS metadata It is meant to be used to indicate L1, L2, L3... At Qserv design week we decided to rename it (original plan was to remove it all together) ",1,DM-509,datamanagement,rename dbgroup storageclass css metadata mean indicate l1 l2 l3 qserv design week decide rename original plan remove,"rename ""dbGroup"" to ""storageClass"" in CSS metadata It is meant to be used to indicate L1, L2, L3... At Qserv design week we decided to rename it (original plan was to remove it all together)"
"Tweak metadata structure for driving table and secondary index There seem to be confusion about driving table and secondary index. At the moment in zookeeper structure we have {code} /DATABASES/<dbName>/objIdIndex /DATABASES/<dbName>/TABLES/<tableName>/partitioning/secIndexColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/drivingTable /DATABASES/<dbName>/TABLES/<tableName>/partitioning/latColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/lonColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/keyColName {code}  Issues to think about:  * we can't call it objIdIndex, it is too lsst-specific.  * drivingTable and keyColName - perhaps these should be at database level, which means we would only allow one drivingTable and one secondary index per database?  * or, maybe instead of database level, it is a partitioning parameter? Note that two databases might use different name for secondary index or driving table, yet they might be joinable. That argues for introducing a new group, something like /DATABASE/partitioning in addition to /DATABASE_PARTITIONING.  * consider renaming drivingTable to keyTable  * do we really need secIndexColName and keyColName? Can't we get rid of one, and rename to keyColName? ",2,DM-510,datamanagement,tweak metadata structure drive table secondary index confusion drive table secondary index moment zookeeper structure code /databases//objidindex secindexcolname drivingtable latcolname /databases//tables//partitioning loncolname /databases//tables//partitioning keycolname code issue think objidindex lsst specific drivingtable keycolname database level mean allow drivingtable secondary index database maybe instead database level partition parameter note database use different secondary index drive table joinable argue introduce new group like partition addition /database_partitioning consider rename drivingtable keytable need secindexcolname keycolname rid rename keycolname,"Tweak metadata structure for driving table and secondary index There seem to be confusion about driving table and secondary index. At the moment in zookeeper structure we have {code} /DATABASES//objIdIndex /DATABASES//TABLES//partitioning/secIndexColName /DATABASES//TABLES//partitioning/drivingTable /DATABASES//TABLES//partitioning/latColName /DATABASES//TABLES//partitioning/lonColName /DATABASES//TABLES//partitioning/keyColName {code} Issues to think about: * we can't call it objIdIndex, it is too lsst-specific. * drivingTable and keyColName - perhaps these should be at database level, which means we would only allow one drivingTable and one secondary index per database? * or, maybe instead of database level, it is a partitioning parameter? Note that two databases might use different name for secondary index or driving table, yet they might be joinable. That argues for introducing a new group, something like /DATABASE/partitioning in addition to /DATABASE_PARTITIONING. * consider renaming drivingTable to keyTable * do we really need secIndexColName and keyColName? Can't we get rid of one, and rename to keyColName?"
"Generalizing data chunking (n-level chunking rather than stripes/subStripes) It'd be cleaner to use numbering (e.g. 0 for no chunking, 1 for chunks, 2 for subchunks etc) instead of ""chunks"", ""subchunks"" throughout qserv code. This might also be true in partitioner, where flags like -S and -s etc are not entirely obvious.  This came up in the review of CSS, see DM-225.",3,DM-512,datamanagement,generalize datum chunk level chunk stripe substripes clean use numbering e.g. chunking chunk subchunk etc instead chunk subchunk qserv code true partitioner flag like -s -s etc entirely obvious come review css dm-225,"Generalizing data chunking (n-level chunking rather than stripes/subStripes) It'd be cleaner to use numbering (e.g. 0 for no chunking, 1 for chunks, 2 for subchunks etc) instead of ""chunks"", ""subchunks"" throughout qserv code. This might also be true in partitioner, where flags like -S and -s etc are not entirely obvious. This came up in the review of CSS, see DM-225."
"fix threading issues in CSS watcher Fix problems with threads in watcher.py brought up in DM-225 by Serge:  * A thread per database doesn't scale  * There is a thread leak when a database is deleted  * There is another design problem, in that each database thread looks like it is holding on to the same lsst.db.Db instance under the hood. I don't remember any consideration for thread safety from the lsst.db code when I reviewed it. Note for one that it is not safe to use a MySQL connection simultaneously from multiple threads (and I seem to recall that you are caching a connection inside Db instances). In practice, even the Python GIL may not save you, since calls into C code (i.e. the mysql client library) may very well release it.",8,DM-513,datamanagement,fix threading issue css watcher fix problem thread watcher.py bring dm-225 serge thread database scale thread leak database delete design problem database thread look like hold lsst.db db instance hood remember consideration thread safety lsst.db code review note safe use mysql connection simultaneously multiple thread recall cache connection inside db instance practice python gil save call code i.e. mysql client library release,"fix threading issues in CSS watcher Fix problems with threads in watcher.py brought up in DM-225 by Serge: * A thread per database doesn't scale * There is a thread leak when a database is deleted * There is another design problem, in that each database thread looks like it is holding on to the same lsst.db.Db instance under the hood. I don't remember any consideration for thread safety from the lsst.db code when I reviewed it. Note for one that it is not safe to use a MySQL connection simultaneously from multiple threads (and I seem to recall that you are caching a connection inside Db instances). In practice, even the Python GIL may not save you, since calls into C code (i.e. the mysql client library) may very well release it."
"Switch to the ""czar"" name consistently 1) Change lsst.qserv.master to  lsst.qserv.czar in the czar module.  2) Rename masterLib to czarLib  3) Rename startQserv to startCzar ",4,DM-514,datamanagement,switch czar consistently change lsst.qserv.master lsst.qserv.czar czar module rename masterlib czarlib rename startqserv startczar,"Switch to the ""czar"" name consistently 1) Change lsst.qserv.master to lsst.qserv.czar in the czar module. 2) Rename masterLib to czarLib 3) Rename startQserv to startCzar"
"qserv_admin needs to deal with uncommon names/characters qserv/admin/bin/qserv_admin.py needs to deal with strange names (e.g. with embedded spaces or semi-colons), or at minimum, catch and forbid them.",3,DM-517,datamanagement,qserv_admin need deal uncommon name character qserv admin bin qserv_admin.py need deal strange name e.g. embed space semi colon minimum catch forbid,"qserv_admin needs to deal with uncommon names/characters qserv/admin/bin/qserv_admin.py needs to deal with strange names (e.g. with embedded spaces or semi-colons), or at minimum, catch and forbid them."
"Rework exceptions in qserv client There is a bunch of (I think) unnecessary translation from KvException to QservAdmException. Can't you just handle printing KvException in CommandParser.receiveCommands(), and get rid of the CSSERR error code? (This is in /admin/bin/qserv-admin.py)  (this came up in DM-225)",1,DM-518,datamanagement,rework exception qserv client bunch think unnecessary translation kvexception qservadmexception handle print kvexception commandparser.receivecommand rid csserr error code bin qserv admin.py come dm-225,"Rework exceptions in qserv client There is a bunch of (I think) unnecessary translation from KvException to QservAdmException. Can't you just handle printing KvException in CommandParser.receiveCommands(), and get rid of the CSSERR error code? (This is in /admin/bin/qserv-admin.py) (this came up in DM-225)"
"rethink configuration for client _fetchOptionsFromConfigFile in client/qserv_admin.py:: If you are going to use the ConfigParser library, please use SafeConfigParser (or RawConfigParser if you don't need string interpolation). But anyway, I'm not sure the INI file format is the right one to use here, as it forces the use of sections in parameter files, which doesn't make much sense (I notice your code just throws away the section names when reading parameter files). I found myself wishing that the admin client just had syntax for the various options, rather than relying on more or less undocumented parameter files.  (this came up in DM-225)",3,DM-519,datamanagement,rethink configuration client fetchoptionsfromconfigfile client qserv_admin.py go use configparser library use safeconfigpars rawconfigparser need string interpolation sure ini file format right use force use section parameter file sense notice code throw away section name read parameter file find wish admin client syntax option rely undocumented parameter file come dm-225,"rethink configuration for client _fetchOptionsFromConfigFile in client/qserv_admin.py:: If you are going to use the ConfigParser library, please use SafeConfigParser (or RawConfigParser if you don't need string interpolation). But anyway, I'm not sure the INI file format is the right one to use here, as it forces the use of sections in parameter files, which doesn't make much sense (I notice your code just throws away the section names when reading parameter files). I found myself wishing that the admin client just had syntax for the various options, rather than relying on more or less undocumented parameter files. (this came up in DM-225)"
"Remove old partitioner/ loader and duplicator Once Fabrice has migrated the integrated tests towards using the new partitioner and duplicator, we should delete the old partitioner/duplicator (in {{client/examples}}).",1,DM-520,datamanagement,remove old partitioner/ loader duplicator fabrice migrate integrate test new partitioner duplicator delete old partitioner duplicator client example,"Remove old partitioner/ loader and duplicator Once Fabrice has migrated the integrated tests towards using the new partitioner and duplicator, we should delete the old partitioner/duplicator (in {{client/examples}})."
"Confusing error message (non-existing column referenced) A query that references non existing column for non-partitioned table results in a confusing message: ""read failed for chunk(s): 1234567890"".  To repeat, run something like {code} SELECT whatever FROM <existingTable>; {code}  Similar error occurs when we try to reference non-existing table, try something like:  {code} SELECT sce.filterName  FROM StrangeTable AS s,       Science_Ccd_Exposure AS sce  WHERE  (s.scienceCcdExposureId = sce.scienceCcdExposureId); {code} ",5,DM-521,datamanagement,confuse error message non existing column reference query reference non exist column non partitioned table result confusing message read fail chunk(s 1234567890 repeat run like code select code similar error occur try reference non existing table try like code select sce.filtername strangetable science_ccd_exposure sce s.scienceccdexposureid sce.scienceccdexposureid code,"Confusing error message (non-existing column referenced) A query that references non existing column for non-partitioned table results in a confusing message: ""read failed for chunk(s): 1234567890"". To repeat, run something like {code} SELECT whatever FROM ; {code} Similar error occurs when we try to reference non-existing table, try something like: {code} SELECT sce.filterName FROM StrangeTable AS s, Science_Ccd_Exposure AS sce WHERE (s.scienceCcdExposureId = sce.scienceCcdExposureId); {code}"
"gracefully handle misconfigured scons When I try to run scons using the latest master (89aaa6), it fails with  {code} scons: Reading SConscript files ... AttributeError: 'NoneType' object has no attribute 'rfind':   File ""/home/becla/cssProto/qserv_css6/SConstruct"", line 17:     state.init(src_dir)   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 161:     _initEnvironment(src_dir)   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 128:     _initVariables(src_dir)   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 89:     (PathVariable('XROOTD_DIR', 'xrootd install dir', _findPrefix(""XROOTD"", ""xrootd""), PathVariable.PathIsDir)),   File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 53:     (binpath, binname) = os.path.split(binFullPath)   File ""/usr/lib/python2.7/posixpath.py"", line 83:     i = p.rfind('/') + 1 {code}  The scripts should check that requires variables are not set, and print appropriate error (and ideally, suggest how to fix it) ",1,DM-522,datamanagement,"gracefully handle misconfigured scon try run scon late master 89aaa6 fail code scon read sconscript file attributeerror nonetype object attribute rfind file /home becla cssproto qserv_css6 sconstruct line 17 state.init(src_dir file /home becla cssproto qserv_css6 site_scon state.py line 161 initenvironment(src_dir file /home becla cssproto qserv_css6 site_scon state.py line 128 initvariables(src_dir file /home becla cssproto qserv_css6 site_scon state.py line 89 pathvariable('xrootd_dir xrootd install dir findprefix(""xrootd xrootd pathvariable pathisdir file /home becla cssproto qserv_css6 site_scon state.py line 53 binpath binname os.path.split(binfullpath file /usr lib python2.7 posixpath.py line 83 p.rfind('/ code script check require variable set print appropriate error ideally suggest fix","gracefully handle misconfigured scons When I try to run scons using the latest master (89aaa6), it fails with {code} scons: Reading SConscript files ... AttributeError: 'NoneType' object has no attribute 'rfind': File ""/home/becla/cssProto/qserv_css6/SConstruct"", line 17: state.init(src_dir) File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 161: _initEnvironment(src_dir) File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 128: _initVariables(src_dir) File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 89: (PathVariable('XROOTD_DIR', 'xrootd install dir', _findPrefix(""XROOTD"", ""xrootd""), PathVariable.PathIsDir)), File ""/home/becla/cssProto/qserv_css6/site_scons/state.py"", line 53: (binpath, binname) = os.path.split(binFullPath) File ""/usr/lib/python2.7/posixpath.py"", line 83: i = p.rfind('/') + 1 {code} The scripts should check that requires variables are not set, and print appropriate error (and ideally, suggest how to fix it)"
make Image construction robust against integer overflow I just fixed a bug on the HSC side (DM-523) in which integer overflow in the multiplication of width and height in image construction caused problems.  We should backport this fix to LSST.,1,DM-527,datamanagement,image construction robust integer overflow fix bug hsc dm-523 integer overflow multiplication width height image construction cause problem backport fix lsst,make Image construction robust against integer overflow I just fixed a bug on the HSC side (DM-523) in which integer overflow in the multiplication of width and height in image construction caused problems. We should backport this fix to LSST.
"Table column names in new parser Running tests (qserv-testdata.sh) on pre-loaded data I have observed that many test fail for the only reason that the column names in the dumped query results are different between mysql and qserv. Here is an example of query reqult returned from mysql: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +------------+-------+--------+------+ | filterName | field | camcol | run  | +------------+-------+--------+------+ | g          |   670 |      2 | 7202 | +------------+-------+--------+------+ {code} and this is the same query processed by qserv: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +----------+----------+----------+----------+ | QS1_PASS | QS2_PASS | QS3_PASS | QS4_PASS | +----------+----------+----------+----------+ | g        |      670 |        2 |     7202 | +----------+----------+----------+----------+ {code}  We discussed this already with Daniel yesterday and at qserv meeting today, here I just want to collect what we know so far so that we can return to this again later.   As Daniel explained to me this is the result of the new parser assigning aliases to the columns which do not define aliases for themselves. This helps with tracking query proceeding through the processing pipeline. Daniel's observation is that different database engines may assign different names to result columns (or some may not even assign any names), there is no standard in that respect so there is no point in trying to follow what one particular implementation does. Additionally there are issues with conflicting column names and names which are complex expressions.  Difference in column names breaks our tests which dump complete results including table header. The tests could be fixed easily, we could just ignore table headers when dumping the data. More interesting issue is that there may be use cases for better compatibility between mysql and qserv including result column naming. In particular standard Python mysql interface allows one to use column names to retrieve values from queiry result. If qserv assigns arbitrary aliases to the columns it may confuse this kind of clients.  This issue depends very much on what kind of API qserv is going to provide to clients. If mysql (wire-level) protocol is going to be the main API (which would allow all kinds of mysql clients to talk to qserv directly) then we should probably think more about compatibility with mysql. OTOH if we decide to provide our own API then this may not be an issue at all (but we still need to fix current test setup which is based on mysql).  We probably should discuss API question at our dev meeting.",5,DM-530,datamanagement,table column name new parser running test qserv-testdata.sh pre loaded datum observe test fail reason column name dump query result different mysql qserv example query reqult return mysql code mysql select sce.filtername sce.field sce.camcol sce.run science_ccd_exposure sce sce.filtername sce.field 670 sce.camcol sce.run 7202 filtername field camcol run 670 7202 code query process qserv code mysql select sce.filtername sce.field sce.camcol sce.run science_ccd_exposure sce sce.filtername sce.field 670 sce.camcol sce.run 7202 qs1_pass qs2_pass qs3_pass qs4_pass 670 7202 code discuss daniel yesterday qserv meeting today want collect know far return later daniel explain result new parser assign alias column define alias help track query proceed processing pipeline daniel observation different database engine assign different name result column assign name standard respect point try follow particular implementation additionally issue conflicting column name name complex expression difference column name break test dump complete result include table header test fix easily ignore table header dump datum interesting issue use case well compatibility mysql qserv include result column naming particular standard python mysql interface allow use column name retrieve value queiry result qserv assign arbitrary alias column confuse kind client issue depend kind api qserv go provide client mysql wire level protocol go main api allow kind mysql client talk qserv directly probably think compatibility mysql otoh decide provide api issue need fix current test setup base mysql probably discuss api question dev meeting,"Table column names in new parser Running tests (qserv-testdata.sh) on pre-loaded data I have observed that many test fail for the only reason that the column names in the dumped query results are different between mysql and qserv. Here is an example of query reqult returned from mysql: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +------------+-------+--------+------+ | filterName | field | camcol | run | +------------+-------+--------+------+ | g | 670 | 2 | 7202 | +------------+-------+--------+------+ {code} and this is the same query processed by qserv: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +----------+----------+----------+----------+ | QS1_PASS | QS2_PASS | QS3_PASS | QS4_PASS | +----------+----------+----------+----------+ | g | 670 | 2 | 7202 | +----------+----------+----------+----------+ {code} We discussed this already with Daniel yesterday and at qserv meeting today, here I just want to collect what we know so far so that we can return to this again later. As Daniel explained to me this is the result of the new parser assigning aliases to the columns which do not define aliases for themselves. This helps with tracking query proceeding through the processing pipeline. Daniel's observation is that different database engines may assign different names to result columns (or some may not even assign any names), there is no standard in that respect so there is no point in trying to follow what one particular implementation does. Additionally there are issues with conflicting column names and names which are complex expressions. Difference in column names breaks our tests which dump complete results including table header. The tests could be fixed easily, we could just ignore table headers when dumping the data. More interesting issue is that there may be use cases for better compatibility between mysql and qserv including result column naming. In particular standard Python mysql interface allows one to use column names to retrieve values from queiry result. If qserv assigns arbitrary aliases to the columns it may confuse this kind of clients. This issue depends very much on what kind of API qserv is going to provide to clients. If mysql (wire-level) protocol is going to be the main API (which would allow all kinds of mysql clients to talk to qserv directly) then we should probably think more about compatibility with mysql. OTOH if we decide to provide our own API then this may not be an issue at all (but we still need to fix current test setup which is based on mysql). We probably should discuss API question at our dev meeting."
"Qserv returns error table instead of error code Running the tests on pre-loaded data I noticed that for some queries qserv returns result which does not look like it is related in any way to the query - result column names are different from the columns in the query (different in a different way from DM-530). Here is an example of query and result produced: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM   Science_Ccd_Exposure AS sce WHERE  sce.filterName like '%'    AND sce.field = 535    AND sce.camcol like '%'    AND sce.run = 94; +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ | chunkId | code | message                                                                                                                                                                       | timeStamp   | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ |      -1 |    0 | NULL                                                                                                                                                                          | 1.39779e+09 | |      -1 |  100 | Dispatch Query.                                                                                                                                                               | 1.39779e+09 | |   32767 | 1200 | Query Added: url=xroot://qsmaster@127.0.0.1:1094//q/LSST/1234567890, savePath=/dev/shm/qserv-salnikov-80df10ee4ed55693702f55021486cd45647c3a58ce549e7c826d9626/7_1234567890_0 | 1.39779e+09 | |   32767 | 1300 | Query Written.                                                                                                                                                                | 1.39779e+09 | |   32767 | 1400 | Results Read.                                                                                                                                                                 | 1.39779e+09 | |   32767 | 1500 | Results Merged.                                                                                                                                                               | 1.39779e+09 | |   32767 | 1600 | Query Resources Erased.                                                                                                                                                       | 1.39779e+09 | |   32767 | 2000 | Query Finalized.                                                                                                                                                              | 1.39779e+09 | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ {code}  Daniel and Bill explained to me what is happening there - when query results in an error instead of returning mysql error condition (which is an error code plus some text) proxy produces a diagnostic result which is a table (above) containing some info which could be useful to diagnose the issue.   This feature looks potentially useful but it may also be confusing for clients like me who are not aware of this feature. For regular mysql client it may be actually harder to intercept errors because one would need to analyze returned table to understand that error condition happened. It may also be ambiguous in a sense that the legitimate query could produce result with the same column names.  Like in DM-530 this issue is tied to a question what kind of API we want to provide to qserv clients. If mysql wire-level protocol is going to be our main API then we should probably try to be more mysql-compatible. OTOH if we are going to hide everything behind our own API then we may have more freedom in re-defining what kind of result error condition produces.",4,DM-531,datamanagement,qserv return error table instead error code run test pre loaded datum notice query qserv return result look like relate way query result column name different column query different different way dm-530 example query result produce code mysql select sce.filtername sce.field sce.camcol sce.run science_ccd_exposure sce sce.filtername like sce.field 535 sce.camcol like sce.run 94 chunkid code message timestamp -1 null 1.39779e+09 -1 100 dispatch query 1.39779e+09 32767 1200 query add url xroot://qsmaster@127.0.0.1:1094//q lsst/1234567890 savepath=/dev shm qserv salnikov-80df10ee4ed55693702f55021486cd45647c3a58ce549e7c826d9626/7_1234567890_0 1.39779e+09 32767 1300 query written 1.39779e+09 32767 1400 results read 1.39779e+09 32767 1500 results merged 1.39779e+09 32767 1600 query resources erase 1.39779e+09 32767 2000 query finalize 1.39779e+09 ---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ code daniel bill explain happen query result error instead return mysql error condition error code plus text proxy produce diagnostic result table contain info useful diagnose issue feature look potentially useful confusing client like aware feature regular mysql client actually hard intercept error need analyze return table understand error condition happen ambiguous sense legitimate query produce result column name like dm-530 issue tie question kind api want provide qserv client mysql wire level protocol go main api probably try mysql compatible otoh go hide api freedom define kind result error condition produce,"Qserv returns error table instead of error code Running the tests on pre-loaded data I noticed that for some queries qserv returns result which does not look like it is related in any way to the query - result column names are different from the columns in the query (different in a different way from DM-530). Here is an example of query and result produced: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName like '%' AND sce.field = 535 AND sce.camcol like '%' AND sce.run = 94; +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ | chunkId | code | message | timeStamp | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ | -1 | 0 | NULL | 1.39779e+09 | | -1 | 100 | Dispatch Query. | 1.39779e+09 | | 32767 | 1200 | Query Added: url=xroot://qsmaster@127.0.0.1:1094//q/LSST/1234567890, savePath=/dev/shm/qserv-salnikov-80df10ee4ed55693702f55021486cd45647c3a58ce549e7c826d9626/7_1234567890_0 | 1.39779e+09 | | 32767 | 1300 | Query Written. | 1.39779e+09 | | 32767 | 1400 | Results Read. | 1.39779e+09 | | 32767 | 1500 | Results Merged. | 1.39779e+09 | | 32767 | 1600 | Query Resources Erased. | 1.39779e+09 | | 32767 | 2000 | Query Finalized. | 1.39779e+09 | +---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+ {code} Daniel and Bill explained to me what is happening there - when query results in an error instead of returning mysql error condition (which is an error code plus some text) proxy produces a diagnostic result which is a table (above) containing some info which could be useful to diagnose the issue. This feature looks potentially useful but it may also be confusing for clients like me who are not aware of this feature. For regular mysql client it may be actually harder to intercept errors because one would need to analyze returned table to understand that error condition happened. It may also be ambiguous in a sense that the legitimate query could produce result with the same column names. Like in DM-530 this issue is tied to a question what kind of API we want to provide to qserv clients. If mysql wire-level protocol is going to be our main API then we should probably try to be more mysql-compatible. OTOH if we are going to hide everything behind our own API then we may have more freedom in re-defining what kind of result error condition produces."
"transfer multiband processing changes from HSC We have a new multi-band processing scheme for coadds on the HSC side, encompassing changes in afw, meas_deblender, and pipe_tasks.  These should be transferred to the LSST side, with RFCs for any backwards incompatible changes.  Changes to the Footprint classes may only be temporary, as we plan to refactor those classes soon anyway, but they're still worth doing now to support the higher-level changes.",8,DM-533,datamanagement,transfer multiband processing change hsc new multi band processing scheme coadd hsc encompass change afw meas_deblender pipe_task transfer lsst rfc backwards incompatible change change footprint class temporary plan refactor class soon worth support high level change,"transfer multiband processing changes from HSC We have a new multi-band processing scheme for coadds on the HSC side, encompassing changes in afw, meas_deblender, and pipe_tasks. These should be transferred to the LSST side, with RFCs for any backwards incompatible changes. Changes to the Footprint classes may only be temporary, as we plan to refactor those classes soon anyway, but they're still worth doing now to support the higher-level changes."
"scons rebuilds targets without changes I'm seeing something strange when I run scons from current master - running 'scons install' after 'scons build' re-compiles several C++ files even though nothing has changed between these two runs: {code:bash} $ scons build scons: Reading SConscript files ... ... scons: Building targets ... scons: `build' is up to date. scons: done building targets.  $ scons install scons: Reading SConscript files ... ... scons: Building targets ... swig -o build/czar/masterLib_wrap.cc -Ibuild -I/usr/include/python2.6 -python -c++ -Iinclude build/czar/masterLib.i g++ -o build/czar/masterLib_wrap.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/czar/masterLib_wrap.cc g++ -o build/control/AsyncQueryManager.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/AsyncQueryManager.cc g++ -o build/control/dispatcher.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/dispatcher.cc g++ -o build/control/thread.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/thread.cc g++ -o build/merger/TableMerger.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/merger/TableMerger.cc scons: `install' is up to date. scons: done building targets. {code}  This is kind of unexpected, or at least I can't understand now why it happens. Trying to run with --debug=explain shows that some dependencies have disappeared and in some dependencies order is different. No clue yet what that means and how it could happen. Need to study our scons scripts to understand what is going on.",3,DM-546,datamanagement,scon rebuild target change see strange run scon current master run scon install scon build compile c++ file change run code bash scon build scon read sconscript file scon build target scon build date scon building target scon install scon read sconscript file scon build target swig build czar masterlib_wrap.cc -ibuild -i usr include python2.6 -python -iinclude build czar masterlib.i g++ build czar masterlib_wrap.os -c -fpic -d_file_offset_bits=64 -fpic -i u2 salnikov stack stack linux64 protobuf/2.4.1 include -i u2 salnikov stack stack linux64 xrootd qs5 include xrootd -i u2 salnikov stack stack linux64 mysql/5.1.65 include -ibuild -i usr include python2.6 build czar masterlib_wrap.cc g++ build control asyncquerymanager.os -c -fpic -d_file_offset_bits=64 -fpic -i u2 salnikov stack stack linux64 protobuf/2.4.1 include -i u2 salnikov stack stack linux64 xrootd qs5 include xrootd -i u2 salnikov stack stack linux64 mysql/5.1.65 include -ibuild -i usr include python2.6 build control asyncquerymanager.cc g++ build control dispatcher.os -fpic -d_file_offset_bits=64 -fpic -i u2 salnikov stack stack linux64 protobuf/2.4.1 include -i u2 salnikov stack stack linux64 xrootd qs5 include xrootd -i u2 salnikov stack stack linux64 mysql/5.1.65 include -ibuild -i usr include python2.6 build control dispatcher.cc g++ build control thread.os -fpic -d_file_offset_bits=64 -fpic -i u2 salnikov stack stack linux64 protobuf/2.4.1 include -i u2 salnikov stack stack linux64 xrootd qs5 include xrootd -i u2 salnikov stack stack linux64 mysql/5.1.65 include -ibuild -i usr include python2.6 build control thread.cc g++ build merger tablemerger.os -c -g -fpic -d_file_offset_bits=64 -fpic -i u2 salnikov stack stack linux64 protobuf/2.4.1 include -i u2 salnikov stack stack linux64 xrootd qs5 include xrootd -i u2 salnikov stack stack linux64 mysql/5.1.65 include -ibuild -i usr include python2.6 build merger tablemerger.cc scon install date scon building target code kind unexpected understand happen try run explain show dependency disappear dependency order different clue mean happen need study scon script understand go,"scons rebuilds targets without changes I'm seeing something strange when I run scons from current master - running 'scons install' after 'scons build' re-compiles several C++ files even though nothing has changed between these two runs: {code:bash} $ scons build scons: Reading SConscript files ... ... scons: Building targets ... scons: `build' is up to date. scons: done building targets. $ scons install scons: Reading SConscript files ... ... scons: Building targets ... swig -o build/czar/masterLib_wrap.cc -Ibuild -I/usr/include/python2.6 -python -c++ -Iinclude build/czar/masterLib.i g++ -o build/czar/masterLib_wrap.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/czar/masterLib_wrap.cc g++ -o build/control/AsyncQueryManager.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/AsyncQueryManager.cc g++ -o build/control/dispatcher.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/dispatcher.cc g++ -o build/control/thread.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/thread.cc g++ -o build/merger/TableMerger.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/merger/TableMerger.cc scons: `install' is up to date. scons: done building targets. {code} This is kind of unexpected, or at least I can't understand now why it happens. Trying to run with --debug=explain shows that some dependencies have disappeared and in some dependencies order is different. No clue yet what that means and how it could happen. Need to study our scons scripts to understand what is going on."
"clean up include <> --> """" for third party includes According to our coding standard 4.15: https://dev.lsstcorp.org/trac/wiki/C%2B%2BStandard/Files#a4-15.OnlysystemincludefilepathsSHALLbedelimitedwith  we should be using """" for boost, but in quite a few places we do not:  {code} grep 'include <boost' */* |wc      146     314    7916 {code} ",1,DM-559,datamanagement,clean include party include accord code standard 4.15 https://dev.lsstcorp.org/trac/wiki/c%2b%2bstandard/files#a4-15.onlysystemincludefilepathsshallbedelimitedwith boost place code grep include boost |wc 146 314 7916 code,"clean up include <> --> """" for third party includes According to our coding standard 4.15: https://dev.lsstcorp.org/trac/wiki/C%2B%2BStandard/Files#a4-15.OnlysystemincludefilepathsSHALLbedelimitedwith we should be using """" for boost, but in quite a few places we do not: {code} grep 'include <boost' */* |wc 146 314 7916 {code}"
"cleanup includes - add module name Change places like {code}#include ""cssException.h""{code} to {code}#include ""css/cssException.h""{code}   ",1,DM-560,datamanagement,"cleanup include add module change place like code}#include cssexception.h""{code code}#include css cssexception.h""{code","cleanup includes - add module name Change places like {code}#include ""cssException.h""{code} to {code}#include ""css/cssException.h""{code}"
Add distributor to simulator - v2 Add distributor to the simulator  Demonstrate:  Replicator node associated with to distributor node Replicator jobs send job info to distributor node Replicator jobs copy data to distributor node.,6,DM-561,datamanagement,add distributor simulator v2 add distributor simulator demonstrate replicator node associate distributor node replicator job send job info distributor node replicator job copy datum distributor node,Add distributor to simulator - v2 Add distributor to the simulator Demonstrate: Replicator node associated with to distributor node Replicator jobs send job info to distributor node Replicator jobs copy data to distributor node.
"Add Archive DMCS to simulator - v3 Add Archive DMCS  Demonstrate:  Receiving information from duplicator (visit id, exposure sequence number, raft id, and network address).",4,DM-564,datamanagement,add archive dmcs simulator v3 add archive dmcs demonstrate receive information duplicator visit exposure sequence number raft network address,"Add Archive DMCS to simulator - v3 Add Archive DMCS Demonstrate: Receiving information from duplicator (visit id, exposure sequence number, raft id, and network address)."
Fix Doxygen Doc for new meas_base classes The final stage of moving the old algorithms to meas_base will be to generate the doxygen documentation and be sure that it is useful.  We will do this after the  cleanup of the old meas_algorithms code.,3,DM-566,datamanagement,fix doxygen doc new meas_base class final stage move old algorithm meas_base generate doxygen documentation sure useful cleanup old meas_algorithms code,Fix Doxygen Doc for new meas_base classes The final stage of moving the old algorithms to meas_base will be to generate the doxygen documentation and be sure that it is useful. We will do this after the cleanup of the old meas_algorithms code.
Cleanup Source.h.m4 This include file was damaged somewhat by the addition of slot routines to work with the flattened field definitions.  It would be nice to put the Measurement abstraction back in place -- or get rid of it.  We need to decide whether the old slot and compound key mechanisms from SourceTable version 0 are going to be continued for doing this.   ,2,DM-586,datamanagement,cleanup source.h.m4 include file damage somewhat addition slot routine work flatten field definition nice measurement abstraction place rid need decide old slot compound key mechanism sourcetable version go continue,Cleanup Source.h.m4 This include file was damaged somewhat by the addition of slot routines to work with the flattened field definitions. It would be nice to put the Measurement abstraction back in place -- or get rid of it. We need to decide whether the old slot and compound key mechanisms from SourceTable version 0 are going to be continued for doing this.
The way the observation date is translated by obs_cfht breaks the defect registry. The fields of the observation date and time as stored in the CFHT-LS headers are not padded with zeros.  This makes the sqlite DATETIME constructor return a NULL string when the observation time is < 10hrs.  The fix is to force the fields to be padded when the metadata from the input files are ingested.,1,DM-587,datamanagement,way observation date translate obs_cfht break defect registry field observation date time store cfht ls header pad zero make sqlite datetime constructor return null string observation time 10hrs fix force field pad metadata input file ingest,The way the observation date is translated by obs_cfht breaks the defect registry. The fields of the observation date and time as stored in the CFHT-LS headers are not padded with zeros. This makes the sqlite DATETIME constructor return a NULL string when the observation time is < 10hrs. The fix is to force the fields to be padded when the metadata from the input files are ingested.
"load non-LSST FITS tables as version 1 In DM-384 and DM-242, we disabled the periods-to-underscores translations for new tables (""version 1"") but left it in place for old tables (""version 0"").  In addition, when reading a table without a version number, we assumed it was version 0, to maintain backwards compatibility.  I think we should modify this slightly: we should assume a table without a version number is version 0 if and only if it also has the AFW_TYPE key.  Otherwise we should assume it is version 1.  This will allow us to load externally-produced tables without turning any underscores they contain into periods, while still maintaining backwards compatibility with older tables written by afw.",1,DM-590,datamanagement,load non lsst fit table version dm-384 dm-242 disable period underscores translation new table version leave place old table version addition read table version number assume version maintain backwards compatibility think modify slightly assume table version number version afw_type key assume version allow load externally produce table turn underscore contain period maintain backwards compatibility old table write afw,"load non-LSST FITS tables as version 1 In DM-384 and DM-242, we disabled the periods-to-underscores translations for new tables (""version 1"") but left it in place for old tables (""version 0""). In addition, when reading a table without a version number, we assumed it was version 0, to maintain backwards compatibility. I think we should modify this slightly: we should assume a table without a version number is version 0 if and only if it also has the AFW_TYPE key. Otherwise we should assume it is version 1. This will allow us to load externally-produced tables without turning any underscores they contain into periods, while still maintaining backwards compatibility with older tables written by afw."
"Update all DM Software Copyright and License Agreement notices to reflect AURA/LSST The lsstcorp.org/LegalNotices/{LsstLicenseStatement.txt  LsstSourceCopyrightNotice.txt} need to be updated to reference AURA/LSST. The referenced list of LSST partner institutions needs to be either resurrected or the reference deleted.  The git repository for devenv/templates needs the Copyright templates to be  updated.  LsstLicenseStatement.txt needs to be updated to include recent additions of 3rd party tools' Licenses (~10 tools)  to the DM stack  and all the QSERV 3rd party tools' Licenses (~25 tools).  The Copyright banner in all software needs to be updated to reflect the new reality of AURA/LSST in place of LSST Corporation.  Files with no Copyright banner, need to add it.  Update may occur 'the next time' the code file is updated. THis needs to be broadcast to the developers once the Copyright templates and the website versions are updated.",2,DM-593,datamanagement,update dm software copyright license agreement notice reflect aura lsst lsstcorp.org/legalnotices/{lsstlicensestatement.txt lsstsourcecopyrightnotice.txt need update reference aura lsst reference list lsst partner institution need resurrect reference delete git repository devenv template need copyright template update lsstlicensestatement.txt need update include recent addition 3rd party tool licenses ~10 tool dm stack qserv 3rd party tool licenses ~25 tool copyright banner software need update reflect new reality aura lsst place lsst corporation file copyright banner need add update occur time code file update need broadcast developer copyright template website version update,"Update all DM Software Copyright and License Agreement notices to reflect AURA/LSST The lsstcorp.org/LegalNotices/{LsstLicenseStatement.txt LsstSourceCopyrightNotice.txt} need to be updated to reference AURA/LSST. The referenced list of LSST partner institutions needs to be either resurrected or the reference deleted. The git repository for devenv/templates needs the Copyright templates to be updated. LsstLicenseStatement.txt needs to be updated to include recent additions of 3rd party tools' Licenses (~10 tools) to the DM stack and all the QSERV 3rd party tools' Licenses (~25 tools). The Copyright banner in all software needs to be updated to reflect the new reality of AURA/LSST in place of LSST Corporation. Files with no Copyright banner, need to add it. Update may occur 'the next time' the code file is updated. THis needs to be broadcast to the developers once the Copyright templates and the website versions are updated."
"running multiple Qserv installations on the same machine It would be very useful to be able to run multiple installations of Qserv on the same machine (say, 2 developers playing with Qserv on the same machine). I guess we are almost there, we just need to know how to configure all ports so that we are not colliding. Can you test it, tweak whatever is necessary and document what is involved in changing defaults to a unique set of port numbers? ",6,DM-594,datamanagement,run multiple qserv installation machine useful able run multiple installation qserv machine developer play qserv machine guess need know configure port collide test tweak necessary document involve change default unique set port number,"running multiple Qserv installations on the same machine It would be very useful to be able to run multiple installations of Qserv on the same machine (say, 2 developers playing with Qserv on the same machine). I guess we are almost there, we just need to know how to configure all ports so that we are not colliding. Can you test it, tweak whatever is necessary and document what is involved in changing defaults to a unique set of port numbers?"
"Setup multi-node Qserv  We are currently focusing on single-node Qserv. It'd be nice to try setting up multi-node Qserv (say 4 workers and a czar on lsst-dbdev*), and improve installation scripts to simplify the process.",6,DM-595,datamanagement,setup multi node qserv currently focus single node qserv nice try set multi node qserv worker czar lsst dbdev improve installation script simplify process,"Setup multi-node Qserv We are currently focusing on single-node Qserv. It'd be nice to try setting up multi-node Qserv (say 4 workers and a czar on lsst-dbdev*), and improve installation scripts to simplify the process."
"Fix automated tests after css migration After yesterday's merge of DM-58 into master automated tests do not work any more. The part which is broken now is loading of metadata into qserv. We need to replace old script which created metadata with something different that creates metadata using new CSS.   The code which loads metadata in tests is in QservDataLoader class, createQmsDatabase() method (in tests/python/lsst/qserv/test/ directory).",2,DM-596,datamanagement,fix automate test css migration yesterday merge dm-58 master automate test work break load metadata qserv need replace old script create metadata different create metadata new css code load metadata test qservdataloader class createqmsdatabase method test python lsst qserv test/ directory,"Fix automated tests after css migration After yesterday's merge of DM-58 into master automated tests do not work any more. The part which is broken now is loading of metadata into qserv. We need to replace old script which created metadata with something different that creates metadata using new CSS. The code which loads metadata in tests is in QservDataLoader class, createQmsDatabase() method (in tests/python/lsst/qserv/test/ directory)."
"reorganize client module move everything in the client package (qserv_admin*, associated tests and examples) to admin/  move css/bin/watcher.py to admin/  ",1,DM-597,datamanagement,reorganize client module client package qserv_admin associate test example admin/ css bin watcher.py admin/,"reorganize client module move everything in the client package (qserv_admin*, associated tests and examples) to admin/ move css/bin/watcher.py to admin/"
"Look into git-fat Look into the use of git-fat with the LSST DM workflow.  Specifically, how does this work with anonymous access.",4,DM-603,datamanagement,look git fat look use git fat lsst dm workflow specifically work anonymous access,"Look into git-fat Look into the use of git-fat with the LSST DM workflow. Specifically, how does this work with anonymous access."
"Update parse/analysis tests to detect missing css-kvmap early Due to the CSS code merge, the testCppParser test depends on an external kvmap file. If this file doesn't exist, nearly every test will fail. There isn't a way to check whether the cssFacade (constructed from kvmap file) is valid.  This ticket includes, at minimum: * changes to css/ and qproc/ to make the unit tests fail early if the facade could not be constructed.  Optionally, this ticket could include: * renaming testCppParser to testQueryAnalyzer * compile-time linking the kvmap into testCppParser to eliminate the need for an external kvmap file. ",4,DM-604,datamanagement,update parse analysis test detect miss css kvmap early css code merge testcppparser test depend external kvmap file file exist nearly test fail way check cssfacade construct kvmap file valid ticket include minimum change css/ qproc/ unit test fail early facade construct optionally ticket include rename testcppparser testqueryanalyzer compile time link kvmap testcppparser eliminate need external kvmap file,"Update parse/analysis tests to detect missing css-kvmap early Due to the CSS code merge, the testCppParser test depends on an external kvmap file. If this file doesn't exist, nearly every test will fail. There isn't a way to check whether the cssFacade (constructed from kvmap file) is valid. This ticket includes, at minimum: * changes to css/ and qproc/ to make the unit tests fail early if the facade could not be constructed. Optionally, this ticket could include: * renaming testCppParser to testQueryAnalyzer * compile-time linking the kvmap into testCppParser to eliminate the need for an external kvmap file."
"referring to table without database context crashes czar Running a query like ""select * from Object"" if we do not have database context results in  {code} terminate called after throwing an instance of 'lsst::qserv::css::CssException_InternalRunTimeError'   what():  Internal run-time error. (*** css::KvInterfaceImplZoo::exists(). Zookeeper error #-8. (/DATABASES/)) {code}  Need to gracefully catch the zookeeper -8. Need to detect that empty database name is passed in Facade. Need to avoid it higher up. ",3,DM-608,datamanagement,refer table database context crash czar run query like select object database context result code terminate call throw instance lsst::qserv::css::cssexception_internalruntimeerror internal run time error css::kvinterfaceimplzoo::exist zookeeper error -8 /databases/ code need gracefully catch zookeeper -8 need detect database pass facade need avoid high,"referring to table without database context crashes czar Running a query like ""select * from Object"" if we do not have database context results in {code} terminate called after throwing an instance of 'lsst::qserv::css::CssException_InternalRunTimeError' what(): Internal run-time error. (*** css::KvInterfaceImplZoo::exists(). Zookeeper error #-8. (/DATABASES/)) {code} Need to gracefully catch the zookeeper -8. Need to detect that empty database name is passed in Facade. Need to avoid it higher up."
"afw unit tests not built unless afwdata available If afw_data is not available then the afw unit tests are not built. I think it would make more sense to build all of them and run those we can (not all depend on afw_data). One advantage is that a version of afw installed using ""eups distrib install"" would include built tests, so the tests could be run. This is presently not practical because the SConstruct file is not installed (I intend to open a ticket about that, as well).",1,DM-609,datamanagement,afw unit test build afwdata available afw_data available afw unit test build think sense build run depend afw_data advantage version afw instal eup distrib install include build test test run presently practical sconstruct file instal intend open ticket,"afw unit tests not built unless afwdata available If afw_data is not available then the afw unit tests are not built. I think it would make more sense to build all of them and run those we can (not all depend on afw_data). One advantage is that a version of afw installed using ""eups distrib install"" would include built tests, so the tests could be run. This is presently not practical because the SConstruct file is not installed (I intend to open a ticket about that, as well)."
"Switch kazoo version to 2.0b1 or later While we aren't using any of the new features of Kazoo's 2.x series, the removal of zope.interface as a dependency is a worthwhile feature.  The 2.0b1 release seems at least as stable as our own code, so I don't think we'll see any negative effects.  This ticket covers: - Upgrade of the packaged kazoo from 1.3.1 to 2.0b1 (or later). - (optionally) patches for kazoo's setup.py so that it doesn't search for and try to download any dependencies. This can be done in a later ticket, though.  I note that kazoo can be run without installation: you can untar it, cd into the directory, and if you run python from there, you can immediately ""import kazoo"" and use it. Hence we could avoid setup.py completely and just copy the ""kazoo"" subdirectory into some directory in the PYTHONPATH.",2,DM-611,datamanagement,switch kazoo version 2.0b1 later new feature kazoo 2.x series removal zope.interface dependency worthwhile feature 2.0b1 release stable code think negative effect ticket cover upgrade package kazoo 1.3.1 2.0b1 later optionally patch kazoo setup.py search try download dependency later ticket note kazoo run installation untar cd directory run python immediately import kazoo use avoid setup.py completely copy kazoo subdirectory directory pythonpath,"Switch kazoo version to 2.0b1 or later While we aren't using any of the new features of Kazoo's 2.x series, the removal of zope.interface as a dependency is a worthwhile feature. The 2.0b1 release seems at least as stable as our own code, so I don't think we'll see any negative effects. This ticket covers: - Upgrade of the packaged kazoo from 1.3.1 to 2.0b1 (or later). - (optionally) patches for kazoo's setup.py so that it doesn't search for and try to download any dependencies. This can be done in a later ticket, though. I note that kazoo can be run without installation: you can untar it, cd into the directory, and if you run python from there, you can immediately ""import kazoo"" and use it. Hence we could avoid setup.py completely and just copy the ""kazoo"" subdirectory into some directory in the PYTHONPATH."
remove obsolete QMS-related code try running  {code} find core css admin client tests site_scons | xargs grep -i qms {code}  There is a lot of old unused qms related code.,2,DM-612,datamanagement,remove obsolete qms relate code try run code find core css admin client test site_scon xargs grep -i qms code lot old unused qms related code,remove obsolete QMS-related code try running {code} find core css admin client tests site_scons | xargs grep -i qms {code} There is a lot of old unused qms related code.
Automated test differences after CSS migration I'm running automated tests with recent master after merging DM-605 and observe some differences between mysql and qserv. Here I'm going to document all differences (and everything related).,6,DM-613,datamanagement,automate test difference css migration run automate test recent master merge dm-605 observe difference mysql qserv go document difference relate,Automated test differences after CSS migration I'm running automated tests with recent master after merging DM-605 and observe some differences between mysql and qserv. Here I'm going to document all differences (and everything related).
"rename qserv_admin.py to qserv-admin.py We have 6 scripts in qserv/admin/bin that start with ""qserv-"", and one that starts with ""qserv_"", we should rename qserv_admin to qserv-admin.",1,DM-614,datamanagement,rename qserv_admin.py qserv-admin.py script qserv admin bin start qserv- start qserv rename qserv_admin qserv admin,"rename qserv_admin.py to qserv-admin.py We have 6 scripts in qserv/admin/bin that start with ""qserv-"", and one that starts with ""qserv_"", we should rename qserv_admin to qserv-admin."
"commons.config should always be managed as a global variable Qserv configuration object (i.e. commons.config) used in admin and tests should be managed the same way a logger object is (cf. http://hg.python.org/cpython/file/2.7/Lib/logging/__init__.py).  I.e. a function called config.getConfig(""config-type"") (returning an config occurence of a global dictionnary), should be used in order to ease retrieval of all configuration objects related to client, data, (and server?) anywhere in the admin/test python code.",2,DM-616,datamanagement,"commons.config manage global variable qserv configuration object i.e. commons.config admin test manage way logger object cf http://hg.python.org/cpython/file/2.7/lib/logging/__init__.py i.e. function call config.getconfig(""config type return config occurence global dictionnary order ease retrieval configuration object relate client datum server admin test python code","commons.config should always be managed as a global variable Qserv configuration object (i.e. commons.config) used in admin and tests should be managed the same way a logger object is (cf. http://hg.python.org/cpython/file/2.7/Lib/logging/__init__.py). I.e. a function called config.getConfig(""config-type"") (returning an config occurence of a global dictionnary), should be used in order to ease retrieval of all configuration objects related to client, data, (and server?) anywhere in the admin/test python code."
"User friendly single node loading script This entails the creation of an end-to-end loading script that, given database and table param files, a SQL table schema and one or more CSV data files, places appropriate metadata into zookeeper, runs the partitioner, and finally creates and loads chunk tables.",8,DM-621,datamanagement,user friendly single node loading script entail creation end end loading script give database table param file sql table schema csv datum file place appropriate metadata zookeeper run partitioner finally create load chunk table,"User friendly single node loading script This entails the creation of an end-to-end loading script that, given database and table param files, a SQL table schema and one or more CSV data files, places appropriate metadata into zookeeper, runs the partitioner, and finally creates and loads chunk tables."
"Too many connections from czar to zookeeper I have just managed to crash qserv czar by running repeated queries against it. What I see in the logs:  qserv-czar.log: {code} 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response terminate called after throwing an instance of 'lsst::qserv::css::CssException_ConnFailure'   what():  Failed to connect to persistent store. {code}  and zookeeper.log: {code} 2014-05-02 13:21:10,861 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:10,861 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:14,585 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440015, likely client has closed socket         at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)         at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)         at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,585 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49913 which had 2014-05-02 13:21:14,585 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440016, likely client has closed socket         at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)         at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)         at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,586 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49939 which had {code}",1,DM-625,datamanagement,"connection czar zookeeper manage crash qserv czar run repeat query log qserv-czar.log code 2014 05 02 13:21:10,861:22525(0x7f76e37ab700):zoo_error@handle_socket_error_msg@1723 socket 127.0.0.1:12181 zk retcode=-4 errno=104(connection reset peer fail receive server response 2014 05 02 13:21:10,861:22525(0x7f76e37ab700):zoo_error@handle_socket_error_msg@1723 socket 127.0.0.1:12181 zk retcode=-4 errno=104(connection reset peer fail receive server response terminate call throw instance lsst::qserv::css::cssexception_connfailure fail connect persistent store code zookeeper.log code 2014 05 02 13:21:10,861 myid warn nioservercxn factory:0.0.0.0/0.0.0.0:12181 nioservercnxnfactory@193 connection /127.0.0.1 max 60 2014 05 02 13:21:10,861 myid warn nioservercxn factory:0.0.0.0/0.0.0.0:12181 nioservercnxnfactory@193 connection /127.0.0.1 max 60 2014 05 02 13:21:14,585 myid warn nioservercxn factory:0.0.0.0/0.0.0.0:12181 nioservercnxn@357 catch end stream exception endofstreamexception unable read additional datum client sessionid 0x145bdd1b8440015 likely client close socket org.apache.zookeeper.server nioservercnxn.doio(nioservercnxn.java:228 org.apache.zookeeper.server nioservercnxnfactory.run(nioservercnxnfactory.java:208 java.lang thread.run(thread.java:744 2014 05 02 13:21:14,585 myid info nioservercxn factory:0.0.0.0/0.0.0.0:12181 nioservercnxn@1007 close socket connection client /127.0.0.1:49913 2014 05 02 13:21:14,585 myid warn nioservercxn factory:0.0.0.0/0.0.0.0:12181 nioservercnxn@357 catch end stream exception endofstreamexception unable read additional datum client sessionid 0x145bdd1b8440016 likely client close socket org.apache.zookeeper.server nioservercnxn.doio(nioservercnxn.java:228 org.apache.zookeeper.server nioservercnxnfactory.run(nioservercnxnfactory.java:208 java.lang thread.run(thread.java:744 2014 05 02 13:21:14,586 myid info nioservercxn factory:0.0.0.0/0.0.0.0:12181 nioservercnxn@1007 close socket connection client code","Too many connections from czar to zookeeper I have just managed to crash qserv czar by running repeated queries against it. What I see in the logs: qserv-czar.log: {code} 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response terminate called after throwing an instance of 'lsst::qserv::css::CssException_ConnFailure' what(): Failed to connect to persistent store. {code} and zookeeper.log: {code} 2014-05-02 13:21:10,861 [myid:] - WARN [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:10,861 [myid:] - WARN [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:14,585 [myid:] - WARN [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440015, likely client has closed socket at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228) at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208) at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,585 [myid:] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49913 which had 2014-05-02 13:21:14,585 [myid:] - WARN [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440016, likely client has closed socket at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228) at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208) at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,586 [myid:] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49939 which had {code}"
"ORDER BY and DISTINCT do not work reliably in qserv Queries with ORDER BY and DISTINCT are buggy. For example, results do not always come ordered and order changes from one run to another: {code} mysql> SELECT objectId FROM Object   WHERE qserv_areaspec_box(0, 0, 3, 10)  ORDER BY objectId; +-----------------+ | objectId        | +-----------------+ | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | +-----------------+ 9 rows in set (1.27 sec)  mysql> SELECT objectId FROM Object   WHERE qserv_areaspec_box(0, 0, 3, 10)  ORDER BY objectId; +-----------------+ | objectId        | +-----------------+ | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | +-----------------+ 9 rows in set (1.24 sec) {code}  This was done with testdata/case01 data, let me know if you need to load that data.  Also, using case03 data, e.g. {code} SELECT distinct run, field  FROM   Science_Ccd_Exposure WHERE  run = 94 AND field = 535; {code} returns 6 rows in qserv (vs 1 in mysql).  The full list of DISTINCT failures is (all with testdata/case03): - 0002_fetchRunAndFieldById.txt - 0021_selectScienceCCDExposure.txt - 0030_selectScienceCCDExposureByRunField.txt ",1,DM-626,datamanagement,order distinct work reliably qserv queries order distinct buggy example result come ordered order change run code mysql select objectid object qserv_areaspec_box(0 10 order objectid -----------------+ objectid -----------------+ 417857368235490 417861663199589 417865958163688 420949744686724 420954039650823 424042121137958 424046416102057 427134497589192 430226874040426 -----------------+ row set 1.27 sec mysql select objectid object qserv_areaspec_box(0 10 order objectid -----------------+ objectid -----------------+ 424042121137958 424046416102057 427134497589192 430226874040426 417857368235490 417861663199589 417865958163688 420949744686724 420954039650823 -----------------+ row set 1.24 sec code testdata case01 datum let know need load datum case03 datum e.g. code select distinct run field science_ccd_exposure run 94 field 535 code return row qserv vs mysql list distinct failure testdata case03 0002_fetchrunandfieldbyid.txt 0021_selectscienceccdexposure.txt 0030_selectscienceccdexposurebyrunfield.txt,"ORDER BY and DISTINCT do not work reliably in qserv Queries with ORDER BY and DISTINCT are buggy. For example, results do not always come ordered and order changes from one run to another: {code} mysql> SELECT objectId FROM Object WHERE qserv_areaspec_box(0, 0, 3, 10) ORDER BY objectId; +-----------------+ | objectId | +-----------------+ | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | +-----------------+ 9 rows in set (1.27 sec) mysql> SELECT objectId FROM Object WHERE qserv_areaspec_box(0, 0, 3, 10) ORDER BY objectId; +-----------------+ | objectId | +-----------------+ | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | +-----------------+ 9 rows in set (1.24 sec) {code} This was done with testdata/case01 data, let me know if you need to load that data. Also, using case03 data, e.g. {code} SELECT distinct run, field FROM Science_Ccd_Exposure WHERE run = 94 AND field = 535; {code} returns 6 rows in qserv (vs 1 in mysql). The full list of DISTINCT failures is (all with testdata/case03): - 0002_fetchRunAndFieldById.txt - 0021_selectScienceCCDExposure.txt - 0030_selectScienceCCDExposureByRunField.txt"
"Switch to using new partitioner, loader Integrated tests procedure has to rely on new loader",4,DM-627,datamanagement,switch new partitioner loader integrated test procedure rely new loader,"Switch to using new partitioner, loader Integrated tests procedure has to rely on new loader"
"Non-partitioned table query returns duplicated rows Running automated test I noticed that a query on non-partitioned table returns multiple copies of the same row, one copy per chunk. Here is example: {code} mysql> SELECT offset, mjdRef, drift FROM LeapSeconds where offset = 10; +--------+--------+-------+ | offset | mjdRef | drift | +--------+--------+-------+ |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | +--------+--------+-------+ 13 rows in set (5.62 sec) {code}  Czar log file shows that it correctly finds that table is non-chunked but sends query to each chunk anyway: {code} 20140502 16:22:08.745081 0x3172430 INF *** KvInterfaceImplZoo::exist(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning 20140502 16:22:08.745735 0x3172430 INF *** LSST.LeapSeconds is NOT chunked. 20140502 16:22:08.745762 0x3172430 INF *** KvInterfaceImplZoo::get2(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning/subChunks 20140502 16:22:08.746393 0x3172430 INF *** LSST.LeapSeconds is NOT subchunked. 20140502 16:22:08.746409 0x3172430 INF getChunkLevel returns 0 ..... 20140502 16:22:08.757832 0x3172430 INF <py> Using 85 stripes and 12 substripes. 20140502 16:22:08.775586 0x3172430 INF <py> Using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt as default empty chunks file. 20140502 16:22:08.791559 0x3172430 INF <py> empty_LSST.txt not found while loading empty chunks file. 20140502 16:22:08.791592 0x3172430 ERR <py> Couldn't find empty_LSST.txt, using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt. 20140502 16:22:08.891239 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891498 0x7fbb28003660 INF Msg cid=6630 with size=153 20140502 16:22:08.891682 0x7fbb28003660 INF Added query id=6630 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6630_0 20140502 16:22:08.891694 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 20140502 16:22:08.891705 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891882 0x7fbb28003660 INF Msg cid=6631 with size=153 20140502 16:22:08.892077 0x7fbb28003660 INF Added query id=6631 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6631_0 20140502 16:22:08.892087 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 20140502 16:22:08.892097 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.892275 0x7fbb28003660 INF Msg cid=6800 with size=153 20140502 16:22:08.892462 0x7fbb28003660 INF Added query id=6800 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6800 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6800_0 ... {code}  Looking at the code together with Daniel we found that at the Python level (czar/app.py) the code that dispatches query does not check for chunkLevel, this is likely why this happens. The code to look at is in {{InbandQueryAction._applyConstraints()}} method.",5,DM-630,datamanagement,non partitioned table query return duplicate row run automated test notice query non partitioned table return multiple copy row copy chunk example code mysql select offset mjdref drift leapseconds offset 10 --------+--------+-------+ offset mjdref drift --------+--------+-------+ 10 41317 10 41317 10 41317 10 41317 10 41317 10 41317 10 41317 10 41317 10 41317 10 41317 10 41317 10 41317 10 41317 --------+--------+-------+ 13 row set 5.62 sec code czar log file show correctly find table non chunked send query chunk code 20140502 16:22:08.745081 0x3172430 inf kvinterfaceimplzoo::exist key /databases lsst tables leapseconds partition 20140502 16:22:08.745735 0x3172430 inf lsst.leapsecond chunk 20140502 16:22:08.745762 0x3172430 inf kvinterfaceimplzoo::get2 key /databases lsst tables leapseconds partition subchunks 20140502 16:22:08.746393 0x3172430 inf lsst.leapsecond subchunke 20140502 16:22:08.746409 0x3172430 inf getchunklevel return 20140502 16:22:08.757832 0x3172430 inf 85 stripe 12 substripe 20140502 16:22:08.775586 0x3172430 inf /usr local home salnikov dm-613 build dist etc emptychunks.txt default chunk file 20140502 16:22:08.791559 0x3172430 inf empty_lsst.txt find load chunk file 20140502 16:22:08.791592 0x3172430 err find empty_lsst.txt /usr local home salnikov dm-613 build dist etc emptychunks.txt 20140502 16:22:08.891239 0x7fbb28003660 inf querysession::_buildchunkquerie non subchunked 20140502 16:22:08.891498 0x7fbb28003660 inf msg cid=6630 size=153 20140502 16:22:08.891682 0x7fbb28003660 inf add query id=6630 url xroot://qsmaster@127.0.0.1:1094//q lsst/6630 save shm qserv salnikov b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6630_0 20140502 16:22:08.891694 0x7fbb28003660 inf opening xroot://qsmaster@127.0.0.1:1094//q lsst/6630 20140502 16:22:08.891705 0x7fbb28003660 inf querysession::_buildchunkquerie non subchunked 20140502 16:22:08.891882 0x7fbb28003660 inf msg cid=6631 size=153 20140502 16:22:08.892077 0x7fbb28003660 inf add query id=6631 url xroot://qsmaster@127.0.0.1:1094//q lsst/6631 save shm qserv salnikov b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6631_0 20140502 16:22:08.892087 0x7fbb28003660 inf opening xroot://qsmaster@127.0.0.1:1094//q lsst/6631 20140502 16:22:08.892097 0x7fbb28003660 inf querysession::_buildchunkquerie non subchunked 20140502 16:22:08.892275 0x7fbb28003660 inf msg cid=6800 size=153 20140502 16:22:08.892462 0x7fbb28003660 inf add query url xroot://qsmaster@127.0.0.1:1094//q lsst/6800 save shm qserv salnikov b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6800_0 code look code daniel find python level czar app.py code dispatch query check chunklevel likely happen code look inbandqueryaction._applyconstraints method,"Non-partitioned table query returns duplicated rows Running automated test I noticed that a query on non-partitioned table returns multiple copies of the same row, one copy per chunk. Here is example: {code} mysql> SELECT offset, mjdRef, drift FROM LeapSeconds where offset = 10; +--------+--------+-------+ | offset | mjdRef | drift | +--------+--------+-------+ | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | | 10 | 41317 | 0 | +--------+--------+-------+ 13 rows in set (5.62 sec) {code} Czar log file shows that it correctly finds that table is non-chunked but sends query to each chunk anyway: {code} 20140502 16:22:08.745081 0x3172430 INF *** KvInterfaceImplZoo::exist(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning 20140502 16:22:08.745735 0x3172430 INF *** LSST.LeapSeconds is NOT chunked. 20140502 16:22:08.745762 0x3172430 INF *** KvInterfaceImplZoo::get2(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning/subChunks 20140502 16:22:08.746393 0x3172430 INF *** LSST.LeapSeconds is NOT subchunked. 20140502 16:22:08.746409 0x3172430 INF getChunkLevel returns 0 ..... 20140502 16:22:08.757832 0x3172430 INF  Using 85 stripes and 12 substripes. 20140502 16:22:08.775586 0x3172430 INF  Using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt as default empty chunks file. 20140502 16:22:08.791559 0x3172430 INF  empty_LSST.txt not found while loading empty chunks file. 20140502 16:22:08.791592 0x3172430 ERR  Couldn't find empty_LSST.txt, using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt. 20140502 16:22:08.891239 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891498 0x7fbb28003660 INF Msg cid=6630 with size=153 20140502 16:22:08.891682 0x7fbb28003660 INF Added query id=6630 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6630_0 20140502 16:22:08.891694 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 20140502 16:22:08.891705 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891882 0x7fbb28003660 INF Msg cid=6631 with size=153 20140502 16:22:08.892077 0x7fbb28003660 INF Added query id=6631 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6631_0 20140502 16:22:08.892087 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 20140502 16:22:08.892097 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.892275 0x7fbb28003660 INF Msg cid=6800 with size=153 20140502 16:22:08.892462 0x7fbb28003660 INF Added query id=6800 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6800 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6800_0 ... {code} Looking at the code together with Daniel we found that at the Python level (czar/app.py) the code that dispatches query does not check for chunkLevel, this is likely why this happens. The code to look at is in {{InbandQueryAction._applyConstraints()}} method."
"Query sessions are never destroyed Please see DM-625, when I run say 10 ""select count(*) from LSST.Object"" queries, for each query a new AsyncQueryManager is created in dispatcher, but the sessions are never destroyed.",3,DM-633,datamanagement,query session destroy dm-625 run 10 select count lsst.object query query new asyncquerymanager create dispatcher session destroy,"Query sessions are never destroyed Please see DM-625, when I run say 10 ""select count(*) from LSST.Object"" queries, for each query a new AsyncQueryManager is created in dispatcher, but the sessions are never destroyed."
"complexity of eups dependencies relationships  for db package Hello,  I'm currently trying to use the very last version of db package (the one which relies on sconsUtils), but, in order to make it works with Qserv, I had to introduce next update : {code:bash} fjammes@clrlsstwn02-vm:~/src/qserv-packager/dist/dependencies/db (master) $ git diff HEAD~1 diff --git a/ups/db.cfg b/ups/db.cfg index e1ae31b..a469061 100644 --- a/ups/db.cfg +++ b/ups/db.cfg @@ -3,7 +3,7 @@  import lsst.sconsUtils    dependencies = { -    ""required"": [""mysqlclient"", ], +    ""required"": [""mysql"", ],  }    config = lsst.sconsUtils.Configuration( diff --git a/ups/db.table b/ups/db.table index 8c8d831..9e770a3 100644 --- a/ups/db.table +++ b/ups/db.table @@ -1,5 +1,5 @@  setupRequired(python) -setupRequired(mysqlclient) +setupRequired(mysql)  setupRequired(mysqlpython)  setupRequired(sconsUtils) {code}  Is there a solution to describe  in eups that mysqlclient is included in mysql ?  Thanks,  Fabrice",1,DM-637,datamanagement,"complexity eup dependency relationship db package hello currently try use version db package rely sconsutils order work qserv introduce update code bash fjammes@clrlsstwn02 vm:~/src qserv packager dist dependency db master git diff head~1 diff up db.cfg up db.cfg index e1ae31b a469061 100644 up db.cfg up db.cfg +3,7 import lsst.sconsutil dependency require mysqlclient require mysql config lsst.sconsutils configuration diff up db.table up db.table index 8c8d831 9e770a3 100644 up db.table up db.table -1,5 +1,5 setuprequired(python -setuprequired(mysqlclient setuprequired(mysql setuprequired(mysqlpython code solution describe eup mysqlclient include mysql thank fabrice","complexity of eups dependencies relationships for db package Hello, I'm currently trying to use the very last version of db package (the one which relies on sconsUtils), but, in order to make it works with Qserv, I had to introduce next update : {code:bash} fjammes@clrlsstwn02-vm:~/src/qserv-packager/dist/dependencies/db (master) $ git diff HEAD~1 diff --git a/ups/db.cfg b/ups/db.cfg index e1ae31b..a469061 100644 --- a/ups/db.cfg +++ b/ups/db.cfg @@ -3,7 +3,7 @@ import lsst.sconsUtils dependencies = { - ""required"": [""mysqlclient"", ], + ""required"": [""mysql"", ], } config = lsst.sconsUtils.Configuration( diff --git a/ups/db.table b/ups/db.table index 8c8d831..9e770a3 100644 --- a/ups/db.table +++ b/ups/db.table @@ -1,5 +1,5 @@ setupRequired(python) -setupRequired(mysqlclient) +setupRequired(mysql) setupRequired(mysqlpython) setupRequired(sconsUtils) {code} Is there a solution to describe in eups that mysqlclient is included in mysql ? Thanks, Fabrice"
"update overview docs to clarify roles of meas_multifit and shapelet packages From the review of DM-17: {quote} It was not obvious how responsibility is split between {{meas_extensions_multishapelet}}, {{shapelet}}, and {{meas_multifit}}.  Shapelet could use an {{overview.dox}} file. {quote}  {{meas_extensions_multiShapelet}} is on its way out, so we'll wait until that's done and then document the relationship between meas_multifit and shapelet.",2,DM-644,datamanagement,update overview doc clarify role meas_multifit shapelet package review dm-17 quote obvious responsibility split meas_extensions_multishapelet shapelet meas_multifit shapelet use overview.dox file quote meas_extensions_multishapelet way wait document relationship meas_multifit shapelet,"update overview docs to clarify roles of meas_multifit and shapelet packages From the review of DM-17: {quote} It was not obvious how responsibility is split between {{meas_extensions_multishapelet}}, {{shapelet}}, and {{meas_multifit}}. Shapelet could use an {{overview.dox}} file. {quote} {{meas_extensions_multiShapelet}} is on its way out, so we'll wait until that's done and then document the relationship between meas_multifit and shapelet."
"meas_base plugin for sampling-based galaxy fitter In addition to a CModel plugin for galaxy photometry (DM-240), we should create a plugin to do sampling-based galaxy fitting, using the optimizer as a starting point and the existing AdaptiveImportanceSampler class to do most of the work.  A major blocker for this is the fact that the {{SourceTable/SourceRecord}} don't currently provide any way to save multiple samples per object.  This issue may get worked on before it falls into an official sprint, as it's something I want to on my 20% time.",8,DM-645,datamanagement,meas_base plugin sampling base galaxy fitter addition cmodel plugin galaxy photometry dm-240 create plugin sampling base galaxy fitting optimizer starting point exist adaptiveimportancesampler class work major blocker fact sourcetable sourcerecord currently provide way save multiple sample object issue work fall official sprint want 20 time,"meas_base plugin for sampling-based galaxy fitter In addition to a CModel plugin for galaxy photometry (DM-240), we should create a plugin to do sampling-based galaxy fitting, using the optimizer as a starting point and the existing AdaptiveImportanceSampler class to do most of the work. A major blocker for this is the fact that the {{SourceTable/SourceRecord}} don't currently provide any way to save multiple samples per object. This issue may get worked on before it falls into an official sprint, as it's something I want to on my 20% time."
Implement DISTINCT aggregate in qserv It looks like DISTINCT aggregate is not supported yet in qserv. Daniel told me that this should be relatively straightforward to add. Adding this ticket so that we do not forget it.,2,DM-646,datamanagement,implement distinct aggregate qserv look like distinct aggregate support qserv daniel tell relatively straightforward add add ticket forget,Implement DISTINCT aggregate in qserv It looks like DISTINCT aggregate is not supported yet in qserv. Daniel told me that this should be relatively straightforward to add. Adding this ticket so that we do not forget it.
Add support for running unit tests in scons Add code in scons that runs unit tests for Qserv.,5,DM-648,datamanagement,add support run unit test scon add code scon run unit test qserv,Add support for running unit tests in scons Add code in scons that runs unit tests for Qserv.
"framework for documenting ""how to run qserv"" We need to have permanent location for how-to-run-qserv, currently we keep it in https://dev.lsstcorp.org/trac/wiki/db/Qserv/RedesignFY2014/Hackathon2/howToRunQserv. It should be in the code repo. Need to decide on how we format it, and need to expose it on our Qserv trac/confluence pages ",3,DM-649,datamanagement,framework document run qserv need permanent location run qserv currently https://dev.lsstcorp.org/trac/wiki/db/qserv/redesignfy2014/hackathon2/howtorunqserv code repo need decide format need expose qserv trac confluence page,"framework for documenting ""how to run qserv"" We need to have permanent location for how-to-run-qserv, currently we keep it in https://dev.lsstcorp.org/trac/wiki/db/Qserv/RedesignFY2014/Hackathon2/howToRunQserv. It should be in the code repo. Need to decide on how we format it, and need to expose it on our Qserv trac/confluence pages"
"Run baseline HTCondor ClassAds Scenarios Run initial HTCondor ClassAds Scenarios to verify that the implementation for utilizing Rank to place Jobs near data is operating as anticipated.  The main test of the baseline scenarios is to verify that, e.g., a job for a CCD that has calibrations advertised for a particular slot/node  will consistency be executed within that location for appropriate Rank expression. This is to occur even when other open slots are always available (e.g., 4 CCDs, 5 slots). ",4,DM-653,datamanagement,run baseline htcondor classads scenarios run initial htcondor classads scenarios verify implementation utilize rank place jobs near datum operate anticipate main test baseline scenario verify e.g. job ccd calibration advertise particular slot node consistency execute location appropriate rank expression occur open slot available e.g. ccd slot,"Run baseline HTCondor ClassAds Scenarios Run initial HTCondor ClassAds Scenarios to verify that the implementation for utilizing Rank to place Jobs near data is operating as anticipated. The main test of the baseline scenarios is to verify that, e.g., a job for a CCD that has calibrations advertised for a particular slot/node will consistency be executed within that location for appropriate Rank expression. This is to occur even when other open slots are always available (e.g., 4 CCDs, 5 slots)."
"Run ""single slow worker"" HTCondor ClassAds Scenario We run and study a ""single slow worker"" HTCondor ClassAds Scenario. The scenario is a perturbation of the baseline HTCondor ClassAds Scenario. In the baseline, Jobs for a given CCD  are consistently pinned to a slot/node that advertises the presence of associated data files/calibration files for that CCD.  A baseline run may proceed, for example,  with jobs for 4 CCDs repeating executing within same HTCondor slot on a node (even when spare processing slots are readily available.). The baseline is observed to be quite stable, as the pool is empty each time a wave of jobs is submitted. In the ""single slow worker"" scenario, we cause one of the jobs for a chosen CCD to stall (mocking up a slow file transfer, lengthy computation in an algorithm, etc), such that the pool is not empty at the the submission time for a wave of jobs.  We seek to observe how Rank places jobs in this scenario, and work to assign Rank (especially for spare slots) in an optimal way so as to minimize file transfers. ",4,DM-654,datamanagement,run single slow worker htcondor classads scenario run study single slow worker htcondor classads scenario scenario perturbation baseline htcondor classads scenario baseline jobs give ccd consistently pin slot node advertise presence associate datum file calibration file ccd baseline run proceed example job ccd repeat execute htcondor slot node spare processing slot readily available baseline observe stable pool time wave job submit single slow worker scenario cause job choose ccd stall mock slow file transfer lengthy computation algorithm etc pool submission time wave job seek observe rank place job scenario work assign rank especially spare slot optimal way minimize file transfer,"Run ""single slow worker"" HTCondor ClassAds Scenario We run and study a ""single slow worker"" HTCondor ClassAds Scenario. The scenario is a perturbation of the baseline HTCondor ClassAds Scenario. In the baseline, Jobs for a given CCD are consistently pinned to a slot/node that advertises the presence of associated data files/calibration files for that CCD. A baseline run may proceed, for example, with jobs for 4 CCDs repeating executing within same HTCondor slot on a node (even when spare processing slots are readily available.). The baseline is observed to be quite stable, as the pool is empty each time a wave of jobs is submitted. In the ""single slow worker"" scenario, we cause one of the jobs for a chosen CCD to stall (mocking up a slow file transfer, lengthy computation in an algorithm, etc), such that the pool is not empty at the the submission time for a wave of jobs. We seek to observe how Rank places jobs in this scenario, and work to assign Rank (especially for spare slots) in an optimal way so as to minimize file transfers."
"unknown column derails Qserv Running a query that references invalid column hangs Qserv, it looks like the query is never squashed.  For example, I run a query: {code} SELECT distinct run, field  FROM   Science_Ccd_Exposure WHERE  run = 94 AND field = 535; {code} on the pt1.1 data set  Corresponding log from xrootd:  {code} Foreman:>>Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure \ AS QST_1_ WHERE run=94 AND field=535; <<---Error with piece 0 complete (size=1). Foreman:TIMING,q_f99cQueryExecFinish,1399667470 Foreman:Broken! ,q_f99cQueryExec---Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM L\ SST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  (during QueryExec) QueryFragment: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  Foreman:Fail QueryExec phase for q_f99c: Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field \ FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  (during QueryExec) QueryFragment: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535;  Foreman:(FinishFail:0x7f26700055c0) Db = q_f99cc9cd5519d465e673119a84b5570a, dump = /usr/local/home/becla/qserv/1/qserv/build/dist/xrootd-run/result/f99cc9cd5519d465e673119a84b\ 5570a hash=f99cc9cd5519d465e673119a84b5570a Foreman:Finished task Task: msg: session=7 chunk=3598 db=LSST entry time=Fri May  9 15:31:10 2014  frag: q=SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535, sc= rt=r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 ScanSched:ChunkDisk remove for 3598 : SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 BlendSched:Completed: (3598)SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 ScanSched:Completed: (3598)SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 ScanSched:_getNextTasks(32)>->-> ScanSched:ChunkDisk busyness: no ScanSched:_getNextTasks <<<<< BlendSched:Blend trying other sched. GroupSched:_getNextTasks(4)>->-> GroupSched:_getNextTasks <<<<< 140509 15:31:27 13960 cms_Finder: Waiting for cms path /usr/local/home/becla/qserv/1/qserv/build/dist/tmp/worker/.olb/olbd.admin {code}  and the log in czar has {code} 20140509 15:35:40.070654 0x7f4694003660 INF Still 1 in flight. {code}   ",4,DM-655,datamanagement,"unknown column derail qserv run query reference invalid column hang qserv look like query squash example run query code select distinct run field science_ccd_exposure run 94 field 535 code pt1.1 datum set corresponding log xrootd code foreman:>>unknown column run field list unable execute query create table r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 select run field lsst.science_ccd_exposure qst_1 run=94 field=535 ---error piece complete size=1 foreman timing q_f99cqueryexecfinish,1399667470 foreman broken q_f99cqueryexec unknown column run field list unable execute query create table r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 select run field l\ sst.science_ccd_exposure qst_1 run=94 field=535 queryexec queryfragment create table r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 select run field lsst.science_ccd_exposure qst_1 run=94 field=535 foreman fail queryexec phase q_f99c unknown column run field list unable execute query create table r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 select run field lsst.science_ccd_exposure qst_1 run=94 field=535 queryexec queryfragment create table r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 select run field lsst.science_ccd_exposure qst_1 run=94 field=535 foreman:(finishfail:0x7f26700055c0 db q_f99cc9cd5519d465e673119a84b5570a dump /usr local home becla qserv/1 qserv build dist xrootd run result f99cc9cd5519d465e673119a84b\ 5570a hash f99cc9cd5519d465e673119a84b5570a foreman finished task task msg session=7 chunk=3598 db lsst entry time fri 15:31:10 2014 frag select run field lsst.science_ccd_exposure qst_1 run=94 field=535 sc= rt r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 scansched chunkdisk remove 3598 select run field lsst.science_ccd_exposure qst_1 run=94 field=535 blendsched complete 3598)select run field lsst.science_ccd_exposure qst_1 run=94 field=535 scansched complete 3598)select run field lsst.science_ccd_exposure qst_1 run=94 field=535 scansched:_getnexttasks(32)>->- scansched chunkdisk busyness scansched:_getnexttasks blendsche blend try sche groupsched:_getnexttasks(4)>->- groupsched:_getnexttask 140509 15:31:27 13960 cms_finder wait cms path local home becla qserv/1 qserv build dist tmp worker/.olb olbd.admin code log czar code 20140509 15:35:40.070654 0x7f4694003660 inf flight code","unknown column derails Qserv Running a query that references invalid column hangs Qserv, it looks like the query is never squashed. For example, I run a query: {code} SELECT distinct run, field FROM Science_Ccd_Exposure WHERE run = 94 AND field = 535; {code} on the pt1.1 data set Corresponding log from xrootd: {code} Foreman:>>Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure \ AS QST_1_ WHERE run=94 AND field=535; <<---Error with piece 0 complete (size=1). Foreman:TIMING,q_f99cQueryExecFinish,1399667470 Foreman:Broken! ,q_f99cQueryExec---Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM L\ SST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535; (during QueryExec) QueryFragment: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535; Foreman:Fail QueryExec phase for q_f99c: Unknown column 'run' in 'field list' Unable to execute query: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field \ FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535; (during QueryExec) QueryFragment: CREATE TABLE r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535; Foreman:(FinishFail:0x7f26700055c0) Db = q_f99cc9cd5519d465e673119a84b5570a, dump = /usr/local/home/becla/qserv/1/qserv/build/dist/xrootd-run/result/f99cc9cd5519d465e673119a84b\ 5570a hash=f99cc9cd5519d465e673119a84b5570a Foreman:Finished task Task: msg: session=7 chunk=3598 db=LSST entry time=Fri May 9 15:31:10 2014 frag: q=SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535, sc= rt=r_7d8e5a45a6aba94de141cf1d8ab510c89_3598_0 ScanSched:ChunkDisk remove for 3598 : SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 BlendSched:Completed: (3598)SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 ScanSched:Completed: (3598)SELECT run,field FROM LSST.Science_Ccd_Exposure AS QST_1_ WHERE run=94 AND field=535 ScanSched:_getNextTasks(32)>->-> ScanSched:ChunkDisk busyness: no ScanSched:_getNextTasks <<<<< BlendSched:Blend trying other sched. GroupSched:_getNextTasks(4)>->-> GroupSched:_getNextTasks <<<<< 140509 15:31:27 13960 cms_Finder: Waiting for cms path /usr/local/home/becla/qserv/1/qserv/build/dist/tmp/worker/.olb/olbd.admin {code} and the log in czar has {code} 20140509 15:35:40.070654 0x7f4694003660 INF Still 1 in flight. {code}"
"Parser has inverted order for ""limit"" and ""order by"" {code} SELECT run FROM LSST.Science_Ccd_Exposure order by field limit 2 {code}  Works in MySQL, fails in Qserv (ERROR 4120 (Proxy): Error executing query using qserv.)  {code} SELECT run FROM LSST.Science_Ccd_Exposure limit 2 order by field {code}  Works in Qserv, fails in MySQL (limit should be after order by) ",1,DM-661,datamanagement,parser invert order limit order code select run lsst.science_ccd_exposure order field limit code work mysql fail qserv error 4120 proxy error execute query qserv code select run lsst.science_ccd_exposure limit order field code work qserv fail mysql limit order,"Parser has inverted order for ""limit"" and ""order by"" {code} SELECT run FROM LSST.Science_Ccd_Exposure order by field limit 2 {code} Works in MySQL, fails in Qserv (ERROR 4120 (Proxy): Error executing query using qserv.) {code} SELECT run FROM LSST.Science_Ccd_Exposure limit 2 order by field {code} Works in Qserv, fails in MySQL (limit should be after order by)"
"""out of range value"" message when running qserv-testdata (loader.py) Fabrice  I am getting ""out of range value"" when I run the qserv-testdata:  Are you seeing that too?   2014-05-09 18:11:55,975 {/usr/local/home/becla/qserv/1/qserv/build/dist/lib/python/lsst/qserv/admin/commons.py:134} INFO     stderr : /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_gaussian_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_naive_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_centroid_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweightedbad' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweighted' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_shift' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_naive_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_sinc_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_largearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_largearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 2   self.cursor.execute(stmt) ",2,DM-664,datamanagement,"range value message run qserv testdata loader.py fabrice get range value run qserv testdata see 2014 05 09 18:11:55,975 /usr local home becla qserv/1 qserv build dist lib python lsst qserv admin commons.py:134 info stderr /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column calib_detecte row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column calib_psf_candidate row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column calib_psf_use row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_negative row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_badcentroid row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column centroid_sdss_flags row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_edge row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_interpolated_any row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_interpolated_center row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_saturated_any row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_saturated_center row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_cr_any row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_cr_center row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column centroid_gaussian_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column centroid_naive_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column shape_sdss_flags row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column shape_sdss_centroid_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column shape_sdss_flags_unweightedbad row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column shape_sdss_flags_unweighte row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column shape_sdss_flags_shift row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column shape_sdss_flags_maxiter row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flux_psf_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flux_psf_flags_psffactor row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flux_psf_flags_badcorr row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flux_naive_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flux_gaussian_flags row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flux_gaussian_flags_psffactor row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flux_gaussian_flags_badcorr row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flux_sinc_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_psf_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_psf_flags_tinystep row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_psf_flags_constraint_r row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_psf_flags_constraint_q row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_dev_flux_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_dev_flags_psffactor row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_dev_flags_badcorr row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_dev_flags_maxiter row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_dev_flags_tinystep row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_dev_flags_constraint_r row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_dev_flags_constraint_q row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_dev_flags_largearea row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_exp_flux_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_exp_flags_psffactor row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_exp_flags_badcorr row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_exp_flags_maxiter row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_exp_flags_tinystep row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_exp_flags_constraint_r row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_exp_flags_constraint_q row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_exp_flags_largearea row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_combo_flux_flag row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_combo_flags_psffactor row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column multishapelet_combo_flags_badcorr row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column calib_detecte row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column calib_psf_candidate row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column calib_psf_use row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_negative row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_badcentroid row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column centroid_sdss_flags row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_edge row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_interpolated_any row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_interpolated_center row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_saturated_any row self.cursor.execute(stmt /usr local home becla qserv/1 qserv build dist bin loader.py:99 warning range value column flags_pixel_saturated_center row self.cursor.execute(stmt","""out of range value"" message when running qserv-testdata (loader.py) Fabrice I am getting ""out of range value"" when I run the qserv-testdata: Are you seeing that too? 2014-05-09 18:11:55,975 {/usr/local/home/becla/qserv/1/qserv/build/dist/lib/python/lsst/qserv/admin/commons.py:134} INFO stderr : /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_any' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_center' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_gaussian_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_naive_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_centroid_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweightedbad' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweighted' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_shift' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_maxiter' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_psffactor' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_badcorr' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_naive_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_psffactor' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_badcorr' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_sinc_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_maxiter' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_tinystep' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_r' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_q' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flux_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_psffactor' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_badcorr' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_maxiter' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_tinystep' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_r' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_q' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_largearea' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flux_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_psffactor' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_badcorr' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_maxiter' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_tinystep' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_r' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_q' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_largearea' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flux_flags' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_psffactor' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_badcorr' at row 1 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 2 self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 2 self.cursor.execute(stmt)"
"partition package has to detect eups-related boost partition package doesn't detect eups-related boost. This has to be fixed by using sconsUtils, or hand-made procedure.  {code:bash} [fjammes@lsst-dev lsstsw]$ export LD_LIBRARY_PATH=""$LSSTSW/anaconda/lib:$LD_LIBRARY_PATH"" [fjammes@lsst-dev lsstsw]$ setup boost 1.55.0.1+1 [fjammes@lsst-dev lsstsw]$ rebuild partition            partition:  ok (0.5 sec).                boost:  ok (0.3 sec).               python:  ok (0.3 sec).                scons:  ok (0.4 sec). # BUILD ID: b49               python: master-gcbf93ab65b (already installed).                scons: 2.1.0+8 (already installed).                boost: 1.55.0.1+1 (already installed).            partition: master-gf2ef2cf2dc ERROR (1 sec). *** error building product partition. *** exit code = 1 *** log is in /lsst/home/fjammes/src/lsstsw/build/partition/_build.log *** last few lines: :::::  scons: Reading SConscript files ... :::::  Checking for C++ library boost_system-mt... no :::::  Checking for C++ library boost_system... no :::::  Checking for C++ library boost_thread-mt... no :::::  Checking for C++ library boost_thread... no :::::  Checking for C++ library boost_filesystem-mt... no :::::  Checking for C++ library boost_filesystem... no :::::  Checking for C++ library boost_program_options-mt... no :::::  Checking for C++ library boost_program_options... no :::::  Missing required boost library! # BUILD b49 completed. {code}",3,DM-666,datamanagement,"partition package detect eup relate boost partition package detect eup relate boost fix sconsutil hand procedure code bash fjammes@lsst dev lsstsw]$ export ld_library_path=""$lsstsw anaconda lib:$ld_library_path fjammes@lsst dev lsstsw]$ setup boost 1.55.0.1 fjammes@lsst dev lsstsw]$ rebuild partition partition ok 0.5 sec boost ok 0.3 sec python ok 0.3 sec scon ok 0.4 sec build id b49 python master gcbf93ab65b instal scon 2.1.0 instal boost 1.55.0.1 instal partition master gf2ef2cf2dc error sec error building product partition exit code log /lsst home fjamme src lsstsw build partition/_build.log line scon read sconscript file check c++ library boost_system mt check c++ library boost_system check c++ library boost_thread mt check c++ library boost_thread check c++ library boost_filesystem mt check c++ library boost_filesystem check c++ library boost_program_options mt check c++ library boost_program_option miss require boost library build b49 complete code","partition package has to detect eups-related boost partition package doesn't detect eups-related boost. This has to be fixed by using sconsUtils, or hand-made procedure. {code:bash} [fjammes@lsst-dev lsstsw]$ export LD_LIBRARY_PATH=""$LSSTSW/anaconda/lib:$LD_LIBRARY_PATH"" [fjammes@lsst-dev lsstsw]$ setup boost 1.55.0.1+1 [fjammes@lsst-dev lsstsw]$ rebuild partition partition: ok (0.5 sec). boost: ok (0.3 sec). python: ok (0.3 sec). scons: ok (0.4 sec). # BUILD ID: b49 python: master-gcbf93ab65b (already installed). scons: 2.1.0+8 (already installed). boost: 1.55.0.1+1 (already installed). partition: master-gf2ef2cf2dc ERROR (1 sec). *** error building product partition. *** exit code = 1 *** log is in /lsst/home/fjammes/src/lsstsw/build/partition/_build.log *** last few lines: ::::: scons: Reading SConscript files ... ::::: Checking for C++ library boost_system-mt... no ::::: Checking for C++ library boost_system... no ::::: Checking for C++ library boost_thread-mt... no ::::: Checking for C++ library boost_thread... no ::::: Checking for C++ library boost_filesystem-mt... no ::::: Checking for C++ library boost_filesystem... no ::::: Checking for C++ library boost_program_options-mt... no ::::: Checking for C++ library boost_program_options... no ::::: Missing required boost library! # BUILD b49 completed. {code}"
"fix handling of nested control objects Work on the HSC side has revealed some problems with nested control objects being wrapped into config objects.  This is a pull request for those changes (along with writing a unit test for some of them).  Some (but not all of these changes) are part of Trac ticket #3163 (https://dev.lsstcorp.org/trac/ticket/3163), which I'll now close as a duplicate.",2,DM-674,datamanagement,fix handling nest control object work hsc reveal problem nest control object wrap config object pull request change write unit test change trac ticket 3163 https://dev.lsstcorp.org/trac/ticket/3163 close duplicate,"fix handling of nested control objects Work on the HSC side has revealed some problems with nested control objects being wrapped into config objects. This is a pull request for those changes (along with writing a unit test for some of them). Some (but not all of these changes) are part of Trac ticket #3163 (https://dev.lsstcorp.org/trac/ticket/3163), which I'll now close as a duplicate."
"Citizen methods should be private and accessible only through a friend interface The Citizen interface is useful, but it pollutes its derived classes with methods and attributes that can cause confusion later on (I've got a concrete example of that confusion that Perry and I just spent a few days tracking down - Citizen's {{getId()}} was being mistaken for {{SourceRecord.getId()}}).  I think everything Citizen provides should be hidden and only accessible through a friend interface, e.g.: {code} afw::image::Image<float> image(4, 5); daf::base::CitizenAccess::getId(image); {code}  We should also make an effort to ensure that other aspects of Citizen's design don't affect derived classes, perhaps by prefixing an name that could be seen by derived classes with a ""Citizen"" prefix; see https://dev.lsstcorp.org/trac/ticket/2461.",4,DM-675,datamanagement,citizen method private accessible friend interface citizen interface useful pollute derive class method attribute cause confusion later get concrete example confusion perry spend day track citizen getid mistake sourcerecord.getid think citizen provide hide accessible friend interface e.g. code afw::image::image image(4 daf::base::citizenaccess::getid(image code effort ensure aspect citizen design affect derive class prefix see derive class citizen prefix https://dev.lsstcorp.org/trac/ticket/2461,"Citizen methods should be private and accessible only through a friend interface The Citizen interface is useful, but it pollutes its derived classes with methods and attributes that can cause confusion later on (I've got a concrete example of that confusion that Perry and I just spent a few days tracking down - Citizen's {{getId()}} was being mistaken for {{SourceRecord.getId()}}). I think everything Citizen provides should be hidden and only accessible through a friend interface, e.g.: {code} afw::image::Image image(4, 5); daf::base::CitizenAccess::getId(image); {code} We should also make an effort to ensure that other aspects of Citizen's design don't affect derived classes, perhaps by prefixing an name that could be seen by derived classes with a ""Citizen"" prefix; see https://dev.lsstcorp.org/trac/ticket/2461."
"Implement HTCondor dynamic classad solution for Slot based values The HTCondor team will be updating their HOWTO for managing Slot based classads/dynamic classads set by a cron startd process.  We currently have a technique for  dynamic slot based values that is iinefficient from a negotiation perspective, and we will want to update to a more optimal approach that the HTCondor team plans to provide.",2,DM-676,datamanagement,implement htcondor dynamic classad solution slot base value htcondor team update howto manage slot base classad dynamic classad set cron startd process currently technique dynamic slot base value iinefficient negotiation perspective want update optimal approach htcondor team plan provide,"Implement HTCondor dynamic classad solution for Slot based values The HTCondor team will be updating their HOWTO for managing Slot based classads/dynamic classads set by a cron startd process. We currently have a technique for dynamic slot based values that is iinefficient from a negotiation perspective, and we will want to update to a more optimal approach that the HTCondor team plans to provide."
"Develop monitoring for identifying Data processed on a Node/in a Slot To understand the effectiveness with which we are mapping Jobs to Data, it is vital to monitor/record what data has been processed on a given Node, or within a given Slot on a Node.   Under this issue we examine HTCondor monitoring standards like STARTD_HISTORY,  as well as more custom implementation of blackboard type records via  Job Update hooks to be  executed on the execute node (along the lines of OWL.) ",4,DM-677,datamanagement,develop monitoring identify datum process node slot understand effectiveness map job data vital monitor record datum process give node give slot node issue examine htcondor monitoring standard like startd_history custom implementation blackboard type record job update hook execute execute node line owl,"Develop monitoring for identifying Data processed on a Node/in a Slot To understand the effectiveness with which we are mapping Jobs to Data, it is vital to monitor/record what data has been processed on a given Node, or within a given Slot on a Node. Under this issue we examine HTCondor monitoring standards like STARTD_HISTORY, as well as more custom implementation of blackboard type records via Job Update hooks to be executed on the execute node (along the lines of OWL.)"
"Run HTCondor ClassAds Scenarios with heterogeneous data cache The initial series of tests with HTCondor ClassAds work with jobs for individual ccds with a single file representing the data dependency  for the job. In this issue we consider the management of multiple types of data dependencies that may have to be cached for jobs (calibrations, templates, catalogs of sources/objects, etc). ",4,DM-678,datamanagement,run htcondor classads scenario heterogeneous data cache initial series test htcondor classads work job individual ccd single file represent datum dependency job issue consider management multiple type datum dependency cache job calibration template catalog source object etc,"Run HTCondor ClassAds Scenarios with heterogeneous data cache The initial series of tests with HTCondor ClassAds work with jobs for individual ccds with a single file representing the data dependency for the job. In this issue we consider the management of multiple types of data dependencies that may have to be cached for jobs (calibrations, templates, catalogs of sources/objects, etc)."
"Study ORDER BY support We don't have a proper implementation of ORDER BY. Actually, to support ORDER BY properly, we really have to manage all the column names, so we would need to have an in-memory list of columns for the table in question. This is because the general case requires us to ORDER BY a column that may not exist in the select list. In this case, we must add it to the select list if it is not there (or apply *). The easiest solution is to only allow ORDER BY if the sort key column exists explicitly in the select list.",4,DM-680,datamanagement,study order support proper implementation order actually support order properly manage column name need memory list column table question general case require order column exist select list case add select list apply easy solution allow order sort key column exist explicitly select list,"Study ORDER BY support We don't have a proper implementation of ORDER BY. Actually, to support ORDER BY properly, we really have to manage all the column names, so we would need to have an in-memory list of columns for the table in question. This is because the general case requires us to ORDER BY a column that may not exist in the select list. In this case, we must add it to the select list if it is not there (or apply *). The easiest solution is to only allow ORDER BY if the sort key column exists explicitly in the select list."
"Parser ignores syntax after LIMIT Parser stops after the LIMIT condition, believing that it has a complete select statement. It ignores whatever is afterwards.To fix this, we would need to alter the parser to make sure that there isn't garbage afterwards. So we have to make sure that the only thing acceptable afterwards is a semicolon or a comment. The right thing to do is probably to add a grammar rule for this. The easiest thing is to check to see if the entire string was consumed (give-or-take a semicolon), but this would disallow comments.",4,DM-681,datamanagement,parser ignore syntax limit parser stop limit condition believe complete select statement ignore fix need alter parser sure garbage sure thing acceptable semicolon comment right thing probably add grammar rule easy thing check entire string consume semicolon disallow comment,"Parser ignores syntax after LIMIT Parser stops after the LIMIT condition, believing that it has a complete select statement. It ignores whatever is afterwards.To fix this, we would need to alter the parser to make sure that there isn't garbage afterwards. So we have to make sure that the only thing acceptable afterwards is a semicolon or a comment. The right thing to do is probably to add a grammar rule for this. The easiest thing is to check to see if the entire string was consumed (give-or-take a semicolon), but this would disallow comments."
"Estimate expected counts of unassociated sources  We need to have an idea how many sources/forcedSource/diaSources that are not associated with any object we will have to deal with in the database. Can we have a rough estimate? (e.g., per DR).",2,DM-684,datamanagement,estimate expect count unassociated source need idea source forcedsource diasource associate object deal database rough estimate e.g. dr,"Estimate expected counts of unassociated sources We need to have an idea how many sources/forcedSource/diaSources that are not associated with any object we will have to deal with in the database. Can we have a rough estimate? (e.g., per DR)."
"Fine-tune logging messages Fine-tune log messages in Qserv (what messages are printed, what is the error level, etc)",5,DM-685,datamanagement,fine tune log message fine tune log message qserv message print error level etc,"Fine-tune logging messages Fine-tune log messages in Qserv (what messages are printed, what is the error level, etc)"
During scons configure : check if mysql isn't runing Mysqld can't be configured is its running before configuration step.,1,DM-689,datamanagement,scon configure check mysql run mysqld configure run configuration step,During scons configure : check if mysql isn't runing Mysqld can't be configured is its running before configuration step.
"Minor possible enhancements in install procedure Usefull enhancements :  - add swigged target to ""build"" alias (run scons install to see that swigged target are re-builded at install time)  Other possibles enhancements : - manage default (i.e. const.py) for server configuration file ? - state.py : where to save state ? print it to sdtout ? ",7,DM-690,datamanagement,minor possible enhancement install procedure usefull enhancement add swigged target build alia run scon install swigged target build install time possible enhancement manage default i.e. const.py server configuration file state.py save state print sdtout,"Minor possible enhancements in install procedure Usefull enhancements : - add swigged target to ""build"" alias (run scons install to see that swigged target are re-builded at install time) Other possibles enhancements : - manage default (i.e. const.py) for server configuration file ? - state.py : where to save state ? print it to sdtout ?"
"rename git repository qservdata to qserv_testdata eups package have the same name as their related git repos. Renaming git repos would lead to a more understandable name.  Please note that the qserv-testdata may also be cloned from qservdata, and and qservdata be removed.  New repos will also have to be distributed with lsst-sw tool.",1,DM-699,datamanagement,rename git repository qservdata qserv_testdata eup package relate git repos rename git repos lead understandable note qserv testdata clone qservdata qservdata remove new repos distribute lsst sw tool,"rename git repository qservdata to qserv_testdata eups package have the same name as their related git repos. Renaming git repos would lead to a more understandable name. Please note that the qserv-testdata may also be cloned from qservdata, and and qservdata be removed. New repos will also have to be distributed with lsst-sw tool."
"Buildbot CI needs to save manifest file of failed build for later user debug The manifest file created during a build instance is transient and removed as soon as the next build commences.  Due to that volatility, it's important to save the manifest to some well-known location so that the developer responsible for debug and repair can easily setup the failing environment. The location of the manifest file will be provided to the developer(s) in the failure notification.",4,DM-702,datamanagement,buildbot ci need save manifest file fail build late user debug manif file create build instance transient remove soon build commence volatility important save manifest know location developer responsible debug repair easily setup fail environment location manifest file provide developer(s failure notification,"Buildbot CI needs to save manifest file of failed build for later user debug The manifest file created during a build instance is transient and removed as soon as the next build commences. Due to that volatility, it's important to save the manifest to some well-known location so that the developer responsible for debug and repair can easily setup the failing environment. The location of the manifest file will be provided to the developer(s) in the failure notification."
Use of HipChat for Buildbot CI failure notifications should be explored K-T recommended the use of HipChat rather than email when notifying users of a buildbot build failure.  The purpose was twofold: get immediate attention from the developers and help change the culture towards using HipChat more.  This Issue is to explore the feasibility of using HipChat for the notifications.,1,DM-703,datamanagement,use hipchat buildbot ci failure notification explore recommend use hipchat email notify user buildbot build failure purpose twofold immediate attention developer help change culture hipchat issue explore feasibility hipchat notification,Use of HipChat for Buildbot CI failure notifications should be explored K-T recommended the use of HipChat rather than email when notifying users of a buildbot build failure. The purpose was twofold: get immediate attention from the developers and help change the culture towards using HipChat more. This Issue is to explore the feasibility of using HipChat for the notifications.
"Better review notification e-mails Russell writes:  {quote} I think our system for getting code reviewed using JIRA needs some improvements. It seems that people don't always know that they have been assigned to review a ticket. Also, even if I know I have been assigned to review a ticket, I find it hard to find on JIRA.  More concretely, I would like to see these improvements: - Much clearer notification that one has been assigned as a reviewer. Presently the email is quite generic and easy to miss. In fact I find that most JIRA notifications are rather hard to read -- it's not always easy to see what has changed and thus why I should care. The signal to noise ratio is poor.  - By default a user should see which issues they have been assigned as reviewer when they log into JIRA. (If there is a way to reconfigure the dashboard for this, I'd like to know about it, but it really should be the default). One way to fix this, of course, is to reassig the ticket when putting it into review, but we have good reasons to avoid that.  -- Russell {quote}  and I added:  {quote} In fact, you don't know that the ticket has passed into review unless you scroll all the way to the bottom of the comment.  If the comment associated with the change in status is long and you don't scroll all the way down, then you may not know that you were assigned to review.  With Trac, the important information was at the top of the e-mail. {quote}",2,DM-704,datamanagement,well review notification mail russell write quote think system get code review jira need improvement people know assign review ticket know assign review ticket find hard find jira concretely like improvement clear notification assign reviewer presently email generic easy miss fact find jira notification hard read easy change care signal noise ratio poor default user issue assign reviewer log jira way reconfigure dashboard like know default way fix course reassig ticket put review good reason avoid russell quote add quote fact know ticket pass review scroll way comment comment associate change status long scroll way know assign review trac important information mail quote,"Better review notification e-mails Russell writes: {quote} I think our system for getting code reviewed using JIRA needs some improvements. It seems that people don't always know that they have been assigned to review a ticket. Also, even if I know I have been assigned to review a ticket, I find it hard to find on JIRA. More concretely, I would like to see these improvements: - Much clearer notification that one has been assigned as a reviewer. Presently the email is quite generic and easy to miss. In fact I find that most JIRA notifications are rather hard to read -- it's not always easy to see what has changed and thus why I should care. The signal to noise ratio is poor. - By default a user should see which issues they have been assigned as reviewer when they log into JIRA. (If there is a way to reconfigure the dashboard for this, I'd like to know about it, but it really should be the default). One way to fix this, of course, is to reassig the ticket when putting it into review, but we have good reasons to avoid that. -- Russell {quote} and I added: {quote} In fact, you don't know that the ticket has passed into review unless you scroll all the way to the bottom of the comment. If the comment associated with the change in status is long and you don't scroll all the way down, then you may not know that you were assigned to review. With Trac, the important information was at the top of the e-mail. {quote}"
"cleanup extra file names in docstring Reported by Serge in email:  When using doxygen to document C++ source, you can mark a comment block with just:  {code} /** @file   * Blah blah   */ {code}  in which case doxygen assumes you want the comment block tied to the file it appears in. We seem to have lots of ""@file <fileName>” statements all over the place, which is an extra thing we have to remember to change when renaming files. Is there some reason to do it that way that I’m missing?",1,DM-706,datamanagement,cleanup extra file name docstre report serge email doxygen document c++ source mark comment block code @file blah blah code case doxygen assume want comment block tie file appear lot @file statement place extra thing remember change rename file reason way miss,"cleanup extra file names in docstring Reported by Serge in email: When using doxygen to document C++ source, you can mark a comment block with just: {code} /** @file * Blah blah */ {code} in which case doxygen assumes you want the comment block tied to the file it appears in. We seem to have lots of ""@file  statements all over the place, which is an extra thing we have to remember to change when renaming files. Is there some reason to do it that way that I m missing?"
"cleanup exception code in CSS Reported by Serge:  In CssException.h you’ve got:  {code} class CssRunTimeException: public std::runtime_error { … }; class CssException_XXXX : public CssRunTimeException { … }; {code}  This is inconsistent (shouldn’t it be CssRunTimeException_XXX, or maybe even CssRunTimeError?), lengthy, violates the LSST C++ naming conventions, and doesn’t match the KvInterface docs, which all still talk about a CssException class that does not exist. Can we consider changing this to something more like:  {code} class CssError : public std::runtime_error class KeyError : public CssError class NoSuchTable : public KeyError class NoSuchDb : public KeyError class AuthError : public CssError class ConnError : public CssError {code}  ? Then we can succinctly throw and catch css::NoSuchTable, css::AuthError etc…",2,DM-707,datamanagement,cleanup exception code css report serge cssexception.h ve get code class cssruntimeexception public std::runtime_error class cssexception_xxxx public cssruntimeexception code inconsistent shouldn cssruntimeexception_xxx maybe cssruntimeerror lengthy violate lsst c++ naming convention doesn match kvinterface doc talk cssexception class exist consider change like code class csserror public std::runtime_error class keyerror public csserror class nosuchtable public keyerror class nosuchdb public keyerror class autherror public csserror class connerror public csserror code succinctly throw catch css::nosuchtable css::autherror etc,"cleanup exception code in CSS Reported by Serge: In CssException.h you ve got: {code} class CssRunTimeException: public std::runtime_error { }; class CssException_XXXX : public CssRunTimeException { }; {code} This is inconsistent (shouldn t it be CssRunTimeException_XXX, or maybe even CssRunTimeError?), lengthy, violates the LSST C++ naming conventions, and doesn t match the KvInterface docs, which all still talk about a CssException class that does not exist. Can we consider changing this to something more like: {code} class CssError : public std::runtime_error class KeyError : public CssError class NoSuchTable : public KeyError class NoSuchDb : public KeyError class AuthError : public CssError class ConnError : public CssError {code} ? Then we can succinctly throw and catch css::NoSuchTable, css::AuthError etc"
Prepare a fedora64 openstack image which allow to easily build and test Qserv Qserv packaging procedure requires to often rebuild Qserv and relaunch integration tests.  IN2P3 Openstack platform offer next virtual machines :  {code:bash} [fjammes@ccage030 ~]$ nova flavor-list {code} | ID | Name              | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | | 1  | m1.tiny           | 512       | 0    | 0         |      | 1     | 1.0         | True      | | 15 | cc.windows.small  | 4096      | 20   | 0         |      | 2     | 1.0         | True      | | 16 | cc.windows.xlarge | 8192      | 50   | 0         |      | 4     | 1.0         | True      | | 2  | m1.small          | 2048      | 10   | 20        |      | 1     | 1.0         | True      | | 3  | m1.medium         | 4096      | 10   | 40        |      | 2     | 1.0         | True      | | 4  | m1.large          | 8192      | 10   | 80        |      | 4     | 1.0         | True      | | 5  | m1.xlarge         | 16384     | 10   | 160       |      | 8     | 1.0         | True      | | 6  | cc.lsst.medium    | 4096      | 20   | 40        |      | 2     | 1.0         | False     | | 7  | cc.lsst.large     | 16384     | 20   | 160       |      | 8     | 1.0         | False     | | 9  | cc.lsst.xlarge    | 40000     | 20   | 160       |      | 20    | 1.0         | False     |  cc.lsst.xlarge would allow a quick build/test of new Qserv release.,5,DM-709,datamanagement,prepare fedora64 openstack image allow easily build test qserv qserv packaging procedure require rebuild qserv relaunch integration test in2p3 openstack platform offer virtual machine code bash fjammes@ccage030 nova flavor list code id memory_mb disk ephemeral swap vcpu rxtx_factor m1.tiny 512 1.0 true 15 cc.windows.small 4096 20 1.0 true 16 8192 50 1.0 true m1.small 2048 10 20 1.0 true m1.medium 4096 10 40 1.0 true m1.large 8192 10 80 1.0 true m1.xlarge 16384 10 160 1.0 true 4096 20 40 1.0 false cc.lsst.large 16384 20 160 1.0 false 40000 20 160 20 1.0 false allow quick build test new qserv release,Prepare a fedora64 openstack image which allow to easily build and test Qserv Qserv packaging procedure requires to often rebuild Qserv and relaunch integration tests. IN2P3 Openstack platform offer next virtual machines : {code:bash} [fjammes@ccage030 ~]$ nova flavor-list {code} | ID | Name | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | | 1 | m1.tiny | 512 | 0 | 0 | | 1 | 1.0 | True | | 15 | cc.windows.small | 4096 | 20 | 0 | | 2 | 1.0 | True | | 16 | cc.windows.xlarge | 8192 | 50 | 0 | | 4 | 1.0 | True | | 2 | m1.small | 2048 | 10 | 20 | | 1 | 1.0 | True | | 3 | m1.medium | 4096 | 10 | 40 | | 2 | 1.0 | True | | 4 | m1.large | 8192 | 10 | 80 | | 4 | 1.0 | True | | 5 | m1.xlarge | 16384 | 10 | 160 | | 8 | 1.0 | True | | 6 | cc.lsst.medium | 4096 | 20 | 40 | | 2 | 1.0 | False | | 7 | cc.lsst.large | 16384 | 20 | 160 | | 8 | 1.0 | False | | 9 | cc.lsst.xlarge | 40000 | 20 | 160 | | 20 | 1.0 | False | cc.lsst.xlarge would allow a quick build/test of new Qserv release.
"Reduce and comment client configuration file Client configuration file '~/.lsst/qserv.conf) is used by integration test procedure.  Next improvments are required : 1. use templates in it 2. client config file should retrieve templated values from mother config   file""",1,DM-710,datamanagement,reduce comment client configuration file client configuration file ~/.lsst qserv.conf integration test procedure improvment require use template client config file retrieve template value mother config file,"Reduce and comment client configuration file Client configuration file '~/.lsst/qserv.conf) is used by integration test procedure. Next improvments are required : 1. use templates in it 2. client config file should retrieve templated values from mother config file"""
"Rendering an IR node tree should produce properly parenthesized output It appears that rendering a tree of IR nodes doesn't always result in correct generation of parentheses. Consider the following tree: {panel} * OrTerm ** BoolFactor *** NullPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""refObjectId"") ** BoolFactor *** CompPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""flags"") **** Token(""<>"") **** ValueExpr ***** ValueFactor: Const(""2"") {panel}  which corresponds to the SQL for: ""refObjectId IS NULL OR flags<>2"". If one prepends this (via {{WhereClause.prependAndTerm()}}) to the {{WhereClause}} obtained by parsing ""... WHERE foo!=bar AND baz<3.14159;"" and renders the result using {{QueryTemplate}}, one obtains:      {{... WHERE refObjectId IS NULL OR flags<>2 AND foo!=bar AND baz<3.14159}}  This is equivalent to      {{... WHERE refObjectId IS NULL OR (flags<>2 AND foo!=bar AND baz<3.14159)}}  which doesn't match the parse tree - one should obtain:      {{... WHERE (refObjectId IS NULL OR flags<>2) AND foo!=bar AND baz<3.14159}}  This issue involves surveying all IR node classes and making sure that they render parentheses properly. {color:gray}(One way we might test for this is to parse queries containing parenthesized expressions where removal of the parentheses changes the meaning of the query. This would give us some IR that we can render to a string and reparse back into IR. If the rendering logic is correct, one should obtain identical IR trees).{color} Other possibilities that might explain the behavior above is that the input tree is somehow invalid or that {{WhereClause.prependAndTerm}} creates invalid IR.",8,DM-737,datamanagement,"render ir node tree produce properly parenthesize output appear render tree ir node result correct generation parenthesis consider following tree panel orterm boolfactor nullpredicate valueexpr valuefactor columnref(""refobjectid boolfactor comppredicate valueexpr valuefactor columnref(""flag token valueexpr valuefactor const(""2 panel correspond sql refobjectid null flags<>2 prepend whereclause.prependandterm whereclause obtain parse foo!=bar baz<3.14159 render result querytemplate obtain refobjectid null flags<>2 foo!=bar baz<3.14159 equivalent refobjectid null flags<>2 foo!=bar baz<3.14159 match parse tree obtain refobjectid null foo!=bar baz<3.14159 issue involve survey ir node class make sure render parenthese properly color gray}(one way test parse query contain parenthesized expression removal parenthesis change meaning query ir render string reparse ir render logic correct obtain identical ir trees).{color possibility explain behavior input tree invalid whereclause.prependandterm create invalid ir","Rendering an IR node tree should produce properly parenthesized output It appears that rendering a tree of IR nodes doesn't always result in correct generation of parentheses. Consider the following tree: {panel} * OrTerm ** BoolFactor *** NullPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""refObjectId"") ** BoolFactor *** CompPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""flags"") **** Token(""<>"") **** ValueExpr ***** ValueFactor: Const(""2"") {panel} which corresponds to the SQL for: ""refObjectId IS NULL OR flags<>2"". If one prepends this (via {{WhereClause.prependAndTerm()}}) to the {{WhereClause}} obtained by parsing ""... WHERE foo!=bar AND baz<3.14159;"" and renders the result using {{QueryTemplate}}, one obtains: {{... WHERE refObjectId IS NULL OR flags<>2 AND foo!=bar AND baz<3.14159}} This is equivalent to {{... WHERE refObjectId IS NULL OR (flags<>2 AND foo!=bar AND baz<3.14159)}} which doesn't match the parse tree - one should obtain: {{... WHERE (refObjectId IS NULL OR flags<>2) AND foo!=bar AND baz<3.14159}} This issue involves surveying all IR node classes and making sure that they render parentheses properly. {color:gray}(One way we might test for this is to parse queries containing parenthesized expressions where removal of the parentheses changes the meaning of the query. This would give us some IR that we can render to a string and reparse back into IR. If the rendering logic is correct, one should obtain identical IR trees).{color} Other possibilities that might explain the behavior above is that the input tree is somehow invalid or that {{WhereClause.prependAndTerm}} creates invalid IR."
Use geom eups package for installing geometry Use geom eups package instead of downloading geometry.py during Qserv configuration step.,3,DM-742,datamanagement,use geom eup package instal geometry use geom eup package instead download geometry.py qserv configuration step,Use geom eups package for installing geometry Use geom eups package instead of downloading geometry.py during Qserv configuration step.
"Simplify (script) install procedure Install procedure described in READMEs.txt is complex and error prone.  It could be encapsulated in two scripts :  1. the first for installing Qserv current version in the eups stack, following the LSST official install procedure, 2. the second, developer-oriented, for installing Qserv from a git repository to the eups stack installed during 1.  This two scripts logs could be colorized for better ergonomy.  ",3,DM-746,datamanagement,simplify script install procedure install procedure describe readmes.txt complex error prone encapsulate script instal qserv current version eup stack follow lsst official install procedure second developer orient instal qserv git repository eup stack instal script log colorize well ergonomy,"Simplify (script) install procedure Install procedure described in READMEs.txt is complex and error prone. It could be encapsulated in two scripts : 1. the first for installing Qserv current version in the eups stack, following the LSST official install procedure, 2. the second, developer-oriented, for installing Qserv from a git repository to the eups stack installed during 1. This two scripts logs could be colorized for better ergonomy."
"Replacing boost system lib with eups libs breaks scons build While detecting boost, Qserv build system checks for both system lib and then eups lib. This procedure use next code :  {code:python} class BoostChecker:     def __init__(self, env):         self.env = env         self.suffix = None         self.suffixes = [""-gcc41-mt"", ""-gcc34-mt"", ""-mt"", """"]         self.cache = {}         pass      def getLibName(self, libName):         if libName in self.cache:             return self.cache[libName]          r = self._getLibName(libName)         self.cache[libName] = r         return r      def _getLibName(self, libName):         state.log.debug(""BoostChecker._getLibName() LIBPATH : %s, CPPPATH : %s"" % (self.env[""LIBPATH""], self.env[""CPPPATH""]))         if self.suffix == None:             conf = self.env.Configure()              def checkSuffix(sfx):                 return conf.CheckLib(libName + sfx, language=""C++"", autoadd=0) {code}  and this last line run next gcc command :  {code:bash} g++ -o .sconf_temp/conftest_10.o -c -g -pedantic -Wall -Wno-long-long -D_FILE_OFFSET_BITS=64 -fPIC -I/data/fjammes/stack/Linux64/protobuf/master-g832d498170/include -I/data/fjammes/stack/Linux64/boost/1.55.0.1/include -I/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/include -I/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/include -I/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/include -I/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/include/xrootd -Ibuild -I/data/fjammes/stack/Linux64/anaconda/1.8.0/include/python2.7 .sconf_temp/conftest_10.cpp g++ -o .sconf_temp/conftest_10 .sconf_temp/conftest_10.o -L/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/lib -L/data/fjammes/stack/Linux64/protobuf/master-g832d498170/lib -L/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/lib -L/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/lib -L/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/lib -L/data/fjammes/stack/Linux64/boost/1.55.0.1/lib -lboost_regex-mt scons: Configure: yes {code}  As the ""-mt"" suffix is searched before the empty suffix, previous command succeed.In my example boost_regex-mt is a system lib. When launching ""scons build"", then CheckLib only looks for boost in /data/fjammes/stack/Linux64/boost/1.55.0.1/lib, not in /usr/lib/. This behaviour is eups-correct, but prevents to find boost_regex-mt.  In this example, a trivial solution is to reverse self.suffixes in python code, but a better solution would be to prevent g++ to use default search paths (e.g. : /usr/lib and /usr/include) in the second command. Is it possible to to it with scons ?  Mario, did you meet the same problem with sconsUtils ?    Thanks  Fabrice",3,DM-751,datamanagement,"replace boost system lib eup lib break scon build detect boost qserv build system check system lib eup lib procedure use code code python class boostchecker def init__(self env self.env env self.suffix self.suffixe -gcc41 mt -gcc34 mt -mt self.cache pass def getlibname(self libname libname self.cache return self.cache[libname self._getlibname(libname self.cache[libname return def getlibname(self libname state.log.debug(""boostchecker._getlibname libpath cpppath self.env[""libpath self.suffix conf self.env configure def return conf checklib(libname sfx language=""c++ autoadd=0 code line run gcc command code bash g++ .sconf_temp conftest_10.o -c -pedantic -wall -wno long long -d_file_offset_bits=64 -fpic -i datum fjamme stack linux64 protobuf master g832d498170 include -i datum fjamme stack linux64 boost/1.55.0.1 include -i datum fjamme stack linux64 zookeeper master gc48457902f bind include -i datum fjamme stack linux64 mysql master g5d79af2a50 include -i datum fjamme stack linux64 antlr master gc05368a54f include -i datum fjamme stack linux64 xrootd master gfc9bfb2059 include xrootd -ibuild -i datum fjamme stack linux64 anaconda/1.8.0 include python2.7 conftest_10.cpp g++ .sconf_temp conftest_10 .sconf_temp conftest_10.o -l datum fjamme stack linux64 xrootd master gfc9bfb2059 lib -l datum fjamme stack linux64 protobuf master g832d498170 lib -l datum fjamme stack linux64 antlr master gc05368a54f lib -l datum fjamme stack linux64 zookeeper master gc48457902f bind lib -l datum fjamme stack linux64 mysql master g5d79af2a50 lib -l datum fjamme stack linux64 boost/1.55.0.1 lib -lboost_regex mt scon configure yes code -mt suffix search suffix previous command succeed example boost_regex mt system lib launch scon build checklib look boost /data fjamme stack linux64 boost/1.55.0.1 lib lib/. behaviour eup correct prevent find boost_regex mt example trivial solution reverse self.suffixe python code well solution prevent g++ use default search path e.g. /usr lib /usr include second command possible scon mario meet problem sconsutil thank fabrice","Replacing boost system lib with eups libs breaks scons build While detecting boost, Qserv build system checks for both system lib and then eups lib. This procedure use next code : {code:python} class BoostChecker: def __init__(self, env): self.env = env self.suffix = None self.suffixes = [""-gcc41-mt"", ""-gcc34-mt"", ""-mt"", """"] self.cache = {} pass def getLibName(self, libName): if libName in self.cache: return self.cache[libName] r = self._getLibName(libName) self.cache[libName] = r return r def _getLibName(self, libName): state.log.debug(""BoostChecker._getLibName() LIBPATH : %s, CPPPATH : %s"" % (self.env[""LIBPATH""], self.env[""CPPPATH""])) if self.suffix == None: conf = self.env.Configure() def checkSuffix(sfx): return conf.CheckLib(libName + sfx, language=""C++"", autoadd=0) {code} and this last line run next gcc command : {code:bash} g++ -o .sconf_temp/conftest_10.o -c -g -pedantic -Wall -Wno-long-long -D_FILE_OFFSET_BITS=64 -fPIC -I/data/fjammes/stack/Linux64/protobuf/master-g832d498170/include -I/data/fjammes/stack/Linux64/boost/1.55.0.1/include -I/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/include -I/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/include -I/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/include -I/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/include/xrootd -Ibuild -I/data/fjammes/stack/Linux64/anaconda/1.8.0/include/python2.7 .sconf_temp/conftest_10.cpp g++ -o .sconf_temp/conftest_10 .sconf_temp/conftest_10.o -L/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/lib -L/data/fjammes/stack/Linux64/protobuf/master-g832d498170/lib -L/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/lib -L/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/lib -L/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/lib -L/data/fjammes/stack/Linux64/boost/1.55.0.1/lib -lboost_regex-mt scons: Configure: yes {code} As the ""-mt"" suffix is searched before the empty suffix, previous command succeed.In my example boost_regex-mt is a system lib. When launching ""scons build"", then CheckLib only looks for boost in /data/fjammes/stack/Linux64/boost/1.55.0.1/lib, not in /usr/lib/. This behaviour is eups-correct, but prevents to find boost_regex-mt. In this example, a trivial solution is to reverse self.suffixes in python code, but a better solution would be to prevent g++ to use default search paths (e.g. : /usr/lib and /usr/include) in the second command. Is it possible to to it with scons ? Mario, did you meet the same problem with sconsUtils ? Thanks Fabrice"
Update obs_decam for new CameraGeom The obs_decam package worked on by Paul and Andy B. needs to be updated to reflect changes in the camera geometry.,6,DM-754,datamanagement,update obs_decam new camerageom obs_decam package work paul andy b. need update reflect change camera geometry,Update obs_decam for new CameraGeom The obs_decam package worked on by Paul and Andy B. needs to be updated to reflect changes in the camera geometry.
"Generate data for MiniProduction Assuming that the data needed for mini-production are to be simulated, the input files need to be created and simulated.  The input data then need to be put in a repo with appropriate calibrations.",5,DM-761,datamanagement,generate datum miniproduction assume datum need mini production simulate input file need create simulate input datum need repo appropriate calibration,"Generate data for MiniProduction Assuming that the data needed for mini-production are to be simulated, the input files need to be created and simulated. The input data then need to be put in a repo with appropriate calibrations."
Create scripts to run MiniProduction Write command line script to run mini production.  The scripts should be able to be handed directly to the orca layer.,3,DM-762,datamanagement,create script run miniproduction write command line script run mini production script able hand directly orca layer,Create scripts to run MiniProduction Write command line script to run mini production. The scripts should be able to be handed directly to the orca layer.
Exception naming convention The naming convention for exceptions in pex_exceptions is quite redundant.  This issue will make the convention more compact and update all packages that make use of pex_exceptions.,5,DM-764,datamanagement,exception naming convention name convention exception pex_exception redundant issue convention compact update package use pex_exception,Exception naming convention The naming convention for exceptions in pex_exceptions is quite redundant. This issue will make the convention more compact and update all packages that make use of pex_exceptions.
Evaluate moving to C++11 for .cc files Check that {{C\+\+11}} works on .cc files.  Make {{C\+\+11}} the default in SconsUtils.,5,DM-765,datamanagement,evaluate move c++11 .cc file check c\+\+11 work .cc file c\+\+11 default sconsutils,Evaluate moving to C++11 for .cc files Check that {{C\+\+11}} works on .cc files. Make {{C\+\+11}} the default in SconsUtils.
Improve afw::CameraGeom::utils code Some of the utility code in CameraGeom was not completely ported in W13 and documentation is in need of updating.,3,DM-766,datamanagement,improve afw::camerageom::util code utility code camerageom completely port w13 documentation need update,Improve afw::CameraGeom::utils code Some of the utility code in CameraGeom was not completely ported in W13 and documentation is in need of updating.
Determine scope of XY0 convention update It's unclear exactly how much effort will be involved in making a change to how the XY0 is used.  If the parent/child argument is removed completely this change could be quite invasive and wide reaching.,2,DM-767,datamanagement,determine scope xy0 convention update unclear exactly effort involve make change xy0 parent child argument remove completely change invasive wide reach,Determine scope of XY0 convention update It's unclear exactly how much effort will be involved in making a change to how the XY0 is used. If the parent/child argument is removed completely this change could be quite invasive and wide reaching.
Create scripts to assess and report MiniProduction quality. Write a command line script to assess the quality of the mini production run.  This will involve comparing output data to data produced using a standard stack.  The script will provide a report.,7,DM-769,datamanagement,create script assess report miniproduction quality write command line script assess quality mini production run involve compare output datum datum produce standard stack script provide report,Create scripts to assess and report MiniProduction quality. Write a command line script to assess the quality of the mini production run. This will involve comparing output data to data produced using a standard stack. The script will provide a report.
Create script to clean up after a MiniProduction run The run of a mini production will produce an output repository.  It's likely that we will not want to save all output data.  A script to clean up and potentially save parts of the repo is needed.,3,DM-770,datamanagement,create script clean miniproduction run run mini production produce output repository likely want save output datum script clean potentially save part repo need,Create script to clean up after a MiniProduction run The run of a mini production will produce an output repository. It's likely that we will not want to save all output data. A script to clean up and potentially save parts of the repo is needed.
"Package log4cxx Fabrice, can you package log4cxx? I should have asked you earlier, sorry I waited so long, not it becoming urgent! Bill is almost done with his logging prototype and will be turning it into a real package, and we need to have log4cxx packages. Many thanks.  log4cxx version 0.10.0, which was released in 4/3/2008 but is still undergoing ""incubation"" at Apache. ",2,DM-772,datamanagement,package log4cxx fabrice package log4cxx ask early sorry wait long urgent bill log prototype turn real package need log4cxx package thank log4cxx version 0.10.0 release 4/3/2008 undergo incubation apache,"Package log4cxx Fabrice, can you package log4cxx? I should have asked you earlier, sorry I waited so long, not it becoming urgent! Bill is almost done with his logging prototype and will be turning it into a real package, and we need to have log4cxx packages. Many thanks. log4cxx version 0.10.0, which was released in 4/3/2008 but is still undergoing ""incubation"" at Apache."
"XLDB-2015 report Writing the report, most work done by Daniel, with input from Jacek and K-T.",8,DM-775,datamanagement,xldb-2015 report write report work daniel input jacek t.,"XLDB-2015 report Writing the report, most work done by Daniel, with input from Jacek and K-T."
"Restructure and package logging prototype Restructure and package log4cxx-based prototype (currently in branch u/bchick/protolog). It should go into package called ""log""",8,DM-778,datamanagement,restructure package log prototype restructure package log4cxx base prototype currently branch bchick protolog package call log,"Restructure and package logging prototype Restructure and package log4cxx-based prototype (currently in branch u/bchick/protolog). It should go into package called ""log"""
"Access patterns for data store that supports data distribution  Data distribution related data store includes things like. chunk --> node mapping, locations of chunk replicas, runtime information about nodes (and maybe also node configuration?). Need to understand access patterns - who needs to access, how frequently etc. ",5,DM-780,datamanagement,access pattern datum store support data distribution data distribution relate data store include thing like chunk node mapping location chunk replicas runtime information node maybe node configuration need understand access pattern need access frequently etc,"Access patterns for data store that supports data distribution Data distribution related data store includes things like. chunk --> node mapping, locations of chunk replicas, runtime information about nodes (and maybe also node configuration?). Need to understand access patterns - who needs to access, how frequently etc."
research mysql cluster ndb Checkout mysql cluster ndb from the perspective of data distribution - could it be potentially useful to store data related to data distribution?,2,DM-781,datamanagement,research mysql cluster ndb checkout mysql cluster ndb perspective data distribution potentially useful store datum relate datum distribution,research mysql cluster ndb Checkout mysql cluster ndb from the perspective of data distribution - could it be potentially useful to store data related to data distribution?
Automated test should optionally ignore column headers Some types of queries (like COUNT(*)) may return different column headers in qserv and mysql. This differences break our automated tests which dump and compare complete result including headers. It looks like we will not be able to guarantee that qserv can be made to return the same column headers as mysql except for providing aliases in the query itself. Running those queries without aliases is a legitimate use case so it would be nice to have an option in the test runner which ignores headers for some queries.,4,DM-782,datamanagement,automate test optionally ignore column header type query like count return different column header qserv mysql difference break automate test dump compare complete result include header look like able guarantee qserv return column header mysql provide alias query run query alias legitimate use case nice option test runner ignore header query,Automated test should optionally ignore column headers Some types of queries (like COUNT(*)) may return different column headers in qserv and mysql. This differences break our automated tests which dump and compare complete result including headers. It looks like we will not be able to guarantee that qserv can be made to return the same column headers as mysql except for providing aliases in the query itself. Running those queries without aliases is a legitimate use case so it would be nice to have an option in the test runner which ignores headers for some queries.
Disable failing test cases in automated tests There are currently 4 test cases failing in out automated tests. Until we have a fix we want to disable them.,1,DM-783,datamanagement,disable fail test case automate test currently test case fail automate test fix want disable,Disable failing test cases in automated tests There are currently 4 test cases failing in out automated tests. Until we have a fix we want to disable them.
"JOIN queries are broken Running a simple query that does a join:  {code} SELECT s.ra, s.decl, o.raRange, o.declRange FROM   Object o JOIN   Source s USING (objectId) WHERE  o.objectId = 390034570102582 AND    o.latestObsTime = s.taiMidPoint; {code}  results in czar crashing with: {code} 2terminate called after throwing an instance of 'std::logic_error'   what():  Attempted subchunk spec list without subchunks. {code}  This query has been taken from integration tests (case01, 0003_selectMetadataForOneGalaxy.sql) ",3,DM-786,datamanagement,join query break run simple query join code select s.ra s.decl o.rarange o.declrange object join source objectid o.objectid 390034570102582 o.latestobstime s.taimidpoint code result czar crash code 2terminate call throw instance std::logic_error attempt subchunk spec list subchunk code query take integration test case01 0003_selectmetadataforonegalaxy.sql,"JOIN queries are broken Running a simple query that does a join: {code} SELECT s.ra, s.decl, o.raRange, o.declRange FROM Object o JOIN Source s USING (objectId) WHERE o.objectId = 390034570102582 AND o.latestObsTime = s.taiMidPoint; {code} results in czar crashing with: {code} 2terminate called after throwing an instance of 'std::logic_error' what(): Attempted subchunk spec list without subchunks. {code} This query has been taken from integration tests (case01, 0003_selectMetadataForOneGalaxy.sql)"
"SQL injection in czar/proxy.py Running automated tests for some queries I observe python exceptions in czar log which look like this: {code} 20140529 19:47:19.364371 0x7faacc003550 INF <py> Query dispatch (7) toUnhandled exception in thread started by <function waitAndUnlock at 0x18cd8c0> Traceback (most recent call last):   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 78, in waitAndUnlock     lock.unlock()   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 65, in unlock     self._saveQueryMessages()   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 87, in _saveQueryMessages     self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp))   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/db.py"", line 95, in applySql     c.execute(sql)   File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute     self.errorhandler(self, exc, value)   File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler     raise errorclass, errorvalue _mysql_exceptions.ProgrammingError: (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'r' AND sce.tract=0 AND sce.patch='159,3';', 1401410839.000000)' at line 1"") ok 0.000532 seconds {code}  I believe this is due to how query string is being constructed in czar/proxy.py: {code:py} class Lock:      writeTmpl = ""INSERT INTO %s VALUES (%d, %d, '%s', %f);""  # ...................             self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp)) {code}  If {{msg}} happens to contain quotes then resulting query is broken. One should not use Python formatting to construct query strings, instead the parameters should be passed directly to {{cursor.execute()}} method. ",2,DM-794,datamanagement,"sql injection czar proxy.py run automate test query observe python exception czar log look like code 20140529 19:47:19.364371 0x7faacc003550 inf query dispatch tounhandled exception thread start traceback recent file /usr local home salnikov qserv master build dist lib python lsst qserv czar proxy.py line 78 waitandunlock lock.unlock file /usr local home salnikov qserv master build dist lib python lsst qserv czar proxy.py line 65 unlock self._savequerymessages file /usr local home salnikov qserv master build dist lib python lsst qserv czar proxy.py line 87 savequerymessage self.db.applysql(lock.writetmpl self._tablename chunkid code msg timestamp file /usr local home salnikov qserv master build dist lib python lsst qserv czar db.py line 95 applysql c.execute(sql file /u2 salnikov stack linux64 mysqlpython/1.2.3 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb cursors.py line 174 execute self.errorhandler(self exc value file /u2 salnikov stack linux64 mysqlpython/1.2.3 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb connections.py line 36 defaulterrorhandler raise errorclass errorvalue mysql_exception programmingerror 1064 error sql syntax check manual correspond mysql server version right syntax use near sce.tract=0 sce.patch='159,3 1401410839.000000 line ok 0.000532 second code believe query string construct czar proxy.py code py class lock insert values self.db.applysql(lock.writetmpl self._tablename chunkid code msg timestamp code msg happen contain quote result query break use python format construct query string instead parameter pass directly cursor.execute method","SQL injection in czar/proxy.py Running automated tests for some queries I observe python exceptions in czar log which look like this: {code} 20140529 19:47:19.364371 0x7faacc003550 INF  Query dispatch (7) toUnhandled exception in thread started by  Traceback (most recent call last): File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 78, in waitAndUnlock lock.unlock() File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 65, in unlock self._saveQueryMessages() File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 87, in _saveQueryMessages self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp)) File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/db.py"", line 95, in applySql c.execute(sql) File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute self.errorhandler(self, exc, value) File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler raise errorclass, errorvalue _mysql_exceptions.ProgrammingError: (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'r' AND sce.tract=0 AND sce.patch='159,3';', 1401410839.000000)' at line 1"") ok 0.000532 seconds {code} I believe this is due to how query string is being constructed in czar/proxy.py: {code:py} class Lock: writeTmpl = ""INSERT INTO %s VALUES (%d, %d, '%s', %f);"" # ................... self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp)) {code} If {{msg}} happens to contain quotes then resulting query is broken. One should not use Python formatting to construct query strings, instead the parameters should be passed directly to {{cursor.execute()}} method."
"Zookeeper times out I noticed running some queries, leaving system up and them returning few hours later and running more queries can result in:  {code} ZOO_ERROR@handle_socket_error_msg@1723:  Socket [127.0.0.1:12181] zk retcode=-4, errno=112(Host is down):  failed while receiving a server response {code}  It needs to be investigated (if we can reproduce) ",3,DM-800,datamanagement,zookeeper time notice run query leave system return hour later run query result code zoo_error@handle_socket_error_msg@1723 socket 127.0.0.1:12181 zk retcode=-4 errno=112(host fail receive server response code need investigate reproduce,"Zookeeper times out I noticed running some queries, leaving system up and them returning few hours later and running more queries can result in: {code} ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response {code} It needs to be investigated (if we can reproduce)"
"Provide a ""stackfitCalib"" for coadds Coadds have a Psf class that returns the proper sum of input PSFs at a point.  We need the same functionality for the Calib object associated with the Coadd Exposure.  This will require making a Calib a baseclass (it currently doesn't have a virtual dtor (although it does have virtual protected members)) ",4,DM-803,datamanagement,provide stackfitcalib coadd coadds psf class return proper sum input psf point need functionality calib object associate coadd exposure require make calib baseclass currently virtual dtor virtual protect member,"Provide a ""stackfitCalib"" for coadds Coadds have a Psf class that returns the proper sum of input PSFs at a point. We need the same functionality for the Calib object associated with the Coadd Exposure. This will require making a Calib a baseclass (it currently doesn't have a virtual dtor (although it does have virtual protected members))"
"PhotoCalTask doesn't return information about which stars were used in calibration The PhotoCalTask returns numpy arrays of the source and reference fluxes (and errors) of matched ""good"" photometric objects, typically stars.  However, while estimating the zero point, it clips outliers so the actual list of objects used is shorter.  Please add another output to the returned struct, a numpy Bool array ""good"", to indicate which objects are kept. ",1,DM-813,datamanagement,photocaltask return information star calibration photocaltask return numpy array source reference flux error match good photometric object typically star estimate zero point clip outlier actual list object short add output return struct numpy bool array good indicate object keep,"PhotoCalTask doesn't return information about which stars were used in calibration The PhotoCalTask returns numpy arrays of the source and reference fluxes (and errors) of matched ""good"" photometric objects, typically stars. However, while estimating the zero point, it clips outliers so the actual list of objects used is shorter. Please add another output to the returned struct, a numpy Bool array ""good"", to indicate which objects are kept."
Cleanup in core/examples and core/doc - core/examples and core/doc seems to be out of data.  Some cleanup here would be welcome.,1,DM-814,datamanagement,cleanup core example core doc core example core doc datum cleanup welcome,Cleanup in core/examples and core/doc - core/examples and core/doc seems to be out of data. Some cleanup here would be welcome.
"qserv have to use boost from stack To quote Jacek and KT: {code} Andy, re dm-751, KT says never use the system version.  J. {code}  So we need to switch qserv to eups-boost. This should be easy once DM-751 is done, just add boost to qserv.table. Then one can remove conditional part of {{BoostChecker}} which works with system-installed boost. ",1,DM-817,datamanagement,qserv use boost stack quote jacek kt code andy dm-751 kt say use system version j. code need switch qserv eup boost easy dm-751 add boost qserv.table remove conditional boostchecker work system instal boost,"qserv have to use boost from stack To quote Jacek and KT: {code} Andy, re dm-751, KT says never use the system version. J. {code} So we need to switch qserv to eups-boost. This should be easy once DM-751 is done, just add boost to qserv.table. Then one can remove conditional part of {{BoostChecker}} which works with system-installed boost."
"Simplify copying tables while adding columns Currently, if I want to copy a table while adding a few columns (as specified by schema in the example) I need to do something like: {code}         cat = afwTable.SourceCatalog(schema)         cat.table.defineCentroid(srcCat.table.getCentroidDefinition())         cat.table.definePsfFlux(srcCat.table.getPsfFluxDefinition())         # etc.          scm = afwTable.SchemaMapper(srcCat.getSchema(), schema)         for schEl in srcCat.getSchema():             scm.addMapping(schEl.getKey(), True)          cat.extend(srcCat, True, scm) {code}  Please make this easier!  For example  - by adding a flag to the SchemaMapper constructor that automatically does the addMapping (should this be the default?)  - by making it possible to copy all the slots (maybe this'll be the case when the new alias scheme is implemented?).  Maybe we just need a new method: {code} cat = srcCat.extend(schema) {code} that does all the above steps.",4,DM-820,datamanagement,simplify copy table add column currently want copy table add column specify schema example need like code cat afwtable sourcecatalog(schema cat.table.definecentroid(srccat.table.getcentroiddefinition cat.table.definepsfflux(srccat.table.getpsffluxdefinition etc scm afwtable schemamapper(srccat.getschema schema schel srccat.getschema scm.addmapping(schel.getkey true cat.extend(srccat true scm code easy example add flag schemamapper constructor automatically addmapping default make possible copy slot maybe case new alias scheme implement maybe need new method code cat srccat.extend(schema code step,"Simplify copying tables while adding columns Currently, if I want to copy a table while adding a few columns (as specified by schema in the example) I need to do something like: {code} cat = afwTable.SourceCatalog(schema) cat.table.defineCentroid(srcCat.table.getCentroidDefinition()) cat.table.definePsfFlux(srcCat.table.getPsfFluxDefinition()) # etc. scm = afwTable.SchemaMapper(srcCat.getSchema(), schema) for schEl in srcCat.getSchema(): scm.addMapping(schEl.getKey(), True) cat.extend(srcCat, True, scm) {code} Please make this easier! For example - by adding a flag to the SchemaMapper constructor that automatically does the addMapping (should this be the default?) - by making it possible to copy all the slots (maybe this'll be the case when the new alias scheme is implemented?). Maybe we just need a new method: {code} cat = srcCat.extend(schema) {code} that does all the above steps."
"Reimplement C++/Python Exception Translation I'd like to reimplement our Swig bindings for C++ exceptions to replace the ""LsstCppException"" class with a more user-friendly mechanism.  We'd have a Python exception hierarchy that mirrors the C++ hierarchy (generated automatically with the help of a few Swig macros).  These wrapped exceptions could be thrown in Python as if they were pure-Python exceptions, and could be caught in Python in the same language regardless of where they were thrown.  We're doing this as part of a ""Measurement"" sprint because we'd like to define custom exceptions for different kinds of common measurement errors, and we want to be able to raise those exceptions in either language.",8,DM-827,datamanagement,reimplement c++/python exception translation like reimplement swig binding c++ exception replace lsstcppexception class user friendly mechanism python exception hierarchy mirror c++ hierarchy generate automatically help swig macro wrap exception throw python pure python exception catch python language regardless throw measurement sprint like define custom exception different kind common measurement error want able raise exception language,"Reimplement C++/Python Exception Translation I'd like to reimplement our Swig bindings for C++ exceptions to replace the ""LsstCppException"" class with a more user-friendly mechanism. We'd have a Python exception hierarchy that mirrors the C++ hierarchy (generated automatically with the help of a few Swig macros). These wrapped exceptions could be thrown in Python as if they were pure-Python exceptions, and could be caught in Python in the same language regardless of where they were thrown. We're doing this as part of a ""Measurement"" sprint because we'd like to define custom exceptions for different kinds of common measurement errors, and we want to be able to raise those exceptions in either language."
"Design Prototypes for C++ Algorithm API We've never really been happy with the new design for the C++ algorithm API, and Perry and Jim have a few ideas to fix this that need to be fleshed out.  Each of the subtasks of this issue will correspond to a different design idea.  Ideally, for each one, we'll try to do a nearly-complete conversion of the SdssShape algorithm (as a good example of a complicated algorithm) to see how these ideas work in practice.",6,DM-828,datamanagement,design prototypes c++ algorithm api happy new design c++ algorithm api perry jim idea fix need flesh subtask issue correspond different design idea ideally try nearly complete conversion sdssshape algorithm good example complicated algorithm idea work practice,"Design Prototypes for C++ Algorithm API We've never really been happy with the new design for the C++ algorithm API, and Perry and Jim have a few ideas to fix this that need to be fleshed out. Each of the subtasks of this issue will correspond to a different design idea. Ideally, for each one, we'll try to do a nearly-complete conversion of the SdssShape algorithm (as a good example of a complicated algorithm) to see how these ideas work in practice."
"Algorithm API without (or with optional) Result objects In this design prototype, I'll see how much simpler things could be made by making the main algorithm interface one that sets record values directly, instead of going through an intermediate Result object.  Ideally the Result objects would still be an option, but they may not be standardized or reusable.",3,DM-829,datamanagement,algorithm api optional result object design prototype simple thing make main algorithm interface set record value directly instead go intermediate result object ideally result object option standardize reusable,"Algorithm API without (or with optional) Result objects In this design prototype, I'll see how much simpler things could be made by making the main algorithm interface one that sets record values directly, instead of going through an intermediate Result object. Ideally the Result objects would still be an option, but they may not be standardized or reusable."
"add persistable class for aperture corrections We need to create a persistable, map-like container class to hold aperture corrections, with each element of the container being an instance of the class to be added in DM-740.  A prototype has been developed on DM-797 on the HSC side: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-797 and the corresponding code can be found on these changesets: https://github.com/HyperSuprime-Cam/afw/compare/32d7a8e7b75da6f5327fee65515ee59a5b09f6c7...tickets/DM-797",2,DM-832,datamanagement,add persistable class aperture correction need create persistable map like container class hold aperture correction element container instance class add dm-740 prototype develop dm-797 hsc https://hsc-jira.astro.princeton.edu/jira/browse/hsc-797 correspond code find changeset https://github.com/hypersuprime-cam/afw/compare/32d7a8e7b75da6f5327fee65515ee59a5b09f6c7...tickets/dm-797,"add persistable class for aperture corrections We need to create a persistable, map-like container class to hold aperture corrections, with each element of the container being an instance of the class to be added in DM-740. A prototype has been developed on DM-797 on the HSC side: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-797 and the corresponding code can be found on these changesets: https://github.com/HyperSuprime-Cam/afw/compare/32d7a8e7b75da6f5327fee65515ee59a5b09f6c7...tickets/DM-797"
implement coaddition for aperture corrections We need to be able to coadd aperture corrections in much the same way we coadd PSFs.  See the HSC-side HSC-798 and HSC-897 implementation for a prototype: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-798 https://hsc-jira.astro.princeton.edu/jira/browse/HSC-897 with code here: https://github.com/HyperSuprime-Cam/meas_algorithms/compare/d2782da175c...u/jbosch/DM-798 https://github.com/HyperSuprime-Cam/meas_algorithms/compare/c4fcab3251...u/price/HSC-897a https://github.com/HyperSuprime-Cam/pipe_tasks/compare/6eb48e90be12d...u/price/HSC-897a,3,DM-833,datamanagement,implement coaddition aperture correction need able coadd aperture correction way coadd psf hsc hsc-798 hsc-897 implementation prototype https://hsc-jira.astro.princeton.edu/jira/browse/hsc-798 https://hsc-jira.astro.princeton.edu/jira/browse/hsc-897 code https://github.com/hypersuprime-cam/meas_algorithms/compare/d2782da175c...u/jbosch/dm-798 https://github.com/hypersuprime-cam/meas_algorithms/compare/c4fcab3251...u/price/hsc-897a https://github.com/hypersuprime-cam/pipe_tasks/compare/6eb48e90be12d...u/price/hsc-897a,implement coaddition for aperture corrections We need to be able to coadd aperture corrections in much the same way we coadd PSFs. See the HSC-side HSC-798 and HSC-897 implementation for a prototype: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-798 https://hsc-jira.astro.princeton.edu/jira/browse/HSC-897 with code here: https://github.com/HyperSuprime-Cam/meas_algorithms/compare/d2782da175c...u/jbosch/DM-798 https://github.com/HyperSuprime-Cam/meas_algorithms/compare/c4fcab3251...u/price/HSC-897a https://github.com/HyperSuprime-Cam/pipe_tasks/compare/6eb48e90be12d...u/price/HSC-897a
"reduce code- and object-duplication in aperture correction and PSF coaddition The aperture correction code to be added on DM-833 will likely not be as closely integrated with CoaddPsf as it could be, because the original design of CoaddPsf didn't anticipate the addition of other, similar classes.  We should work on allowing these classes to share code.",8,DM-834,datamanagement,reduce code- object duplication aperture correction psf coaddition aperture correction code add dm-833 likely closely integrate coaddpsf original design coaddpsf anticipate addition similar class work allow class share code,"reduce code- and object-duplication in aperture correction and PSF coaddition The aperture correction code to be added on DM-833 will likely not be as closely integrated with CoaddPsf as it could be, because the original design of CoaddPsf didn't anticipate the addition of other, similar classes. We should work on allowing these classes to share code."
"design Array fields for table version 1 While we're trying to eliminate the need for compound fields in afw::table, Arrays present a few problems.  We could use FunctorKeys, the way we plan to use other compound fields, but here we need to guarantee that the per-element keys are contiguous, and we also be able to support views.  We also need to determine the naming scheme.  Finally, we need to make sure these work with aliases and slots.",6,DM-836,datamanagement,design array field table version try eliminate need compound field afw::table arrays present problem use functorkeys way plan use compound field need guarantee element key contiguous able support view need determine naming scheme finally need sure work alias slot,"design Array fields for table version 1 While we're trying to eliminate the need for compound fields in afw::table, Arrays present a few problems. We could use FunctorKeys, the way we plan to use other compound fields, but here we need to guarantee that the per-element keys are contiguous, and we also be able to support views. We also need to determine the naming scheme. Finally, we need to make sure these work with aliases and slots."
"Rewrite multiple-aperture photometry class We've never figured out how to handle wrapping multiple-aperture photometry algorithms.  They can't use the existing Result objects - at least not out of the box.  We should try to write a new multiple-aperture photometry algorithm from the ground up, using the old ones on the HSC branch as a guide, but not trying to transfer the old code over.  The new one should:  - Have the option of using elliptical apertures (as defined by the shape slot) or circular apertures.  - Have a transition radius at which we switch from the sinc photometry algorithm to the naive algorithm (for performance reasons).",2,DM-837,datamanagement,rewrite multiple aperture photometry class figure handle wrap multiple aperture photometry algorithm use exist result object box try write new multiple aperture photometry algorithm ground old one hsc branch guide try transfer old code new option elliptical aperture define shape slot circular aperture transition radius switch sinc photometry algorithm naive algorithm performance reason,"Rewrite multiple-aperture photometry class We've never figured out how to handle wrapping multiple-aperture photometry algorithms. They can't use the existing Result objects - at least not out of the box. We should try to write a new multiple-aperture photometry algorithm from the ground up, using the old ones on the HSC branch as a guide, but not trying to transfer the old code over. The new one should: - Have the option of using elliptical apertures (as defined by the shape slot) or circular apertures. - Have a transition radius at which we switch from the sinc photometry algorithm to the naive algorithm (for performance reasons)."
"Rename methods that return pixel iterators and locators in image-like classes, and change to use parent indexing Image-like classes have a large number of methods that return pixel iterators and locators based on row and/or column. We wish to change these to use parent indexing (making row and column relative to XY0). As the first step in doing this, rename all these methods and modify them to parent indexing. Renaming will help identify and fix all code that uses these methods.",6,DM-839,datamanagement,rename method return pixel iterator locator image like class change use parent indexing image like class large number method return pixel iterator locator base row and/or column wish change use parent indexing make row column relative xy0 step rename method modify parent indexing rename help identify fix code use method,"Rename methods that return pixel iterators and locators in image-like classes, and change to use parent indexing Image-like classes have a large number of methods that return pixel iterators and locators based on row and/or column. We wish to change these to use parent indexing (making row and column relative to XY0). As the first step in doing this, rename all these methods and modify them to parent indexing. Renaming will help identify and fix all code that uses these methods."
"Change code so ImageOrigin must be specified (temporary) Image-like classes have a getBBox method and various constructors that use an ImageOrigin argument which in most or all cases defaults to LOCAL. As the first stage in cleaning this up, try to break code that uses the default as follows: * Remove the default from getBBox(ImageOrigin) so an origin must be specified. * Change the default origin of constructors to a temporary new value UNDEFINED  * Modify code that uses image origin to fail if origin is needed (it is ignored if bbox is empty) and is UNDEFINED.  Note: this is less safe than changing constructors to not have a default value for origin, because the error will be caught at runtime rather than compile time. However, that is messy because then the bounding box will also have to be always specified, and possibly an HDU, so it would be a much more intrusive change.",2,DM-840,datamanagement,change code imageorigin specify temporary image like class getbbox method constructor use imageorigin argument case default local stage clean try break code use default follow remove default getbbox(imageorigin origin specify change default origin constructor temporary new value undefined modify code use image origin fail origin need ignore bbox undefined note safe change constructor default value origin error catch runtime compile time messy bounding box specify possibly hdu intrusive change,"Change code so ImageOrigin must be specified (temporary) Image-like classes have a getBBox method and various constructors that use an ImageOrigin argument which in most or all cases defaults to LOCAL. As the first stage in cleaning this up, try to break code that uses the default as follows: * Remove the default from getBBox(ImageOrigin) so an origin must be specified. * Change the default origin of constructors to a temporary new value UNDEFINED * Modify code that uses image origin to fail if origin is needed (it is ignored if bbox is empty) and is UNDEFINED. Note: this is less safe than changing constructors to not have a default value for origin, because the error will be caught at runtime rather than compile time. However, that is messy because then the bounding box will also have to be always specified, and possibly an HDU, so it would be a much more intrusive change."
"Change data butler I/O of image-like objects to require imageOrigin if bbox specified (temporary) As part of making PARENT the default for image origin, change the data butler to require that imageOrigin be specified if bbox is specified when reading or writing image-like objects.  Note: this ticket turns out to be unnecessary, as all the few necessary change are done as part of DM-840.",2,DM-841,datamanagement,change datum butler image like object require imageorigin bbox specify temporary make parent default image origin change datum butler require imageorigin specify bbox specify read write image like object note ticket turn unnecessary necessary change dm-840,"Change data butler I/O of image-like objects to require imageOrigin if bbox specified (temporary) As part of making PARENT the default for image origin, change the data butler to require that imageOrigin be specified if bbox is specified when reading or writing image-like objects. Note: this ticket turns out to be unnecessary, as all the few necessary change are done as part of DM-840."
Restore names of methods that return pixel iterators and locators Restore the names of methods that return pixel iterators and pixel locators on image-like classes. (This is part of the final stage of eliminating LOCAL pixel indexing).,2,DM-843,datamanagement,restore name method return pixel iterator locator restore name method return pixel iterator pixel locator image like class final stage eliminate local pixel indexing,Restore names of methods that return pixel iterators and locators Restore the names of methods that return pixel iterators and pixel locators on image-like classes. (This is part of the final stage of eliminating LOCAL pixel indexing).
Eliminate ImageOrigin argument Eliminate the ImageOrigin enum and argument from image-like classes.,2,DM-844,datamanagement,eliminate imageorigin argument eliminate imageorigin enum argument image like class,Eliminate ImageOrigin argument Eliminate the ImageOrigin enum and argument from image-like classes.
Eliminate image origin argument from butler for (un)persisting image-like objects Eliminate the image origin argument for butler get and put when dealing with image-like objects.,2,DM-845,datamanagement,eliminate image origin argument butler un)persiste image like object eliminate image origin argument butler deal image like object,Eliminate image origin argument from butler for (un)persisting image-like objects Eliminate the image origin argument for butler get and put when dealing with image-like objects.
"Update code to use restored names for methods that return pixel iterators and locators For all code changed in DM-842, change it again to use the restored names for methods that return pixel iterators and locators.  This task is much simpler than DM-842 because it is merely renaming methods. Nonetheless, it touches many files in many packages.",4,DM-847,datamanagement,update code use restore name method return pixel iterator locator code change dm-842 change use restore name method return pixel iterator locator task simple dm-842 merely rename method nonetheless touch file package,"Update code to use restored names for methods that return pixel iterators and locators For all code changed in DM-842, change it again to use the restored names for methods that return pixel iterators and locators. This task is much simpler than DM-842 because it is merely renaming methods. Nonetheless, it touches many files in many packages."
"Eliminate use of ImageOrigin argument For all code that uses the ImageOrigin argument, eliminate the use of that argument. In other words, update all code to match the changes in DM-844 and DM-845,  This affects all the code affected by DM-848, plus any code that was already using ImageOrigin=PARENT, but is a simpler change.",5,DM-849,datamanagement,eliminate use imageorigin argument code use imageorigin argument eliminate use argument word update code match change dm-844 dm-845 affect code affect dm-848 plus code imageorigin parent simple change,"Eliminate use of ImageOrigin argument For all code that uses the ImageOrigin argument, eliminate the use of that argument. In other words, update all code to match the changes in DM-844 and DM-845, This affects all the code affected by DM-848, plus any code that was already using ImageOrigin=PARENT, but is a simpler change."
"duplicate column name when running near neighbor query Running a simplified version of near neighbor query on test data from case01:  {code} SELECT DISTINCT o1.objectId, o2.objectId FROM   Object o1,         Object o2 WHERE  scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1   AND  o1.objectId <> o2.objectId {code}  Result in an error on the worker:  {code} Foreman:Broken! ,q_38f9QueryExec---Duplicate column name 'objectId' Unable to execute query: CREATE TABLE r_13237cd4cfc9e0fa01497bcf\ 67a91add2_6630_0 SELECT o1.objectId,o2.objectId FROM Subchunks_LSST_6630.Object_6630_0 AS o1,Subchunks_LSST_6630.Object_6630_0 AS o2\  WHERE scisql_angSep(o1.ra_PS,o1.decl_PS,o2.ra_PS,o2.decl_PS)<1 AND o1.objectId<>o2.objectId; {code}  It is fairly obvious what is going on. ""SELECT t1.x, t2.x"" is perfectly valid, but if we add ""INSERT INTO SELECT t1.x, t2.x"", we need to add names, eg. something like ""INSERT INTO SELECT t1.x as x1, t2.x as x2""",8,DM-854,datamanagement,"duplicate column run near neighbor query run simplified version near neighbor query test datum case01 code select distinct o1.objectid o2.objectid object o1 object o2 scisql_angsep(o1.ra_ps o1.decl_ps o2.ra_ps o2.decl_ps o1.objectid o2.objectid code result error worker code foreman broken q_38f9queryexec duplicate column objectid unable execute query create table r_13237cd4cfc9e0fa01497bcf\ 67a91add2_6630_0 select o1.objectid o2.objectid subchunks_lsst_6630.object_6630_0 o1,subchunks_lsst_6630.object_6630_0 o2\ scisql_angsep(o1.ra_ps o1.decl_ps o2.ra_ps o2.decl_ps)<1 o1.objectid<>o2.objectid code fairly obvious go select t1.x t2.x perfectly valid add insert select t1.x t2.x need add name eg like insert select x1 t2.x x2","duplicate column name when running near neighbor query Running a simplified version of near neighbor query on test data from case01: {code} SELECT DISTINCT o1.objectId, o2.objectId FROM Object o1, Object o2 WHERE scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1 AND o1.objectId <> o2.objectId {code} Result in an error on the worker: {code} Foreman:Broken! ,q_38f9QueryExec---Duplicate column name 'objectId' Unable to execute query: CREATE TABLE r_13237cd4cfc9e0fa01497bcf\ 67a91add2_6630_0 SELECT o1.objectId,o2.objectId FROM Subchunks_LSST_6630.Object_6630_0 AS o1,Subchunks_LSST_6630.Object_6630_0 AS o2\ WHERE scisql_angSep(o1.ra_PS,o1.decl_PS,o2.ra_PS,o2.decl_PS)<1 AND o1.objectId<>o2.objectId; {code} It is fairly obvious what is going on. ""SELECT t1.x, t2.x"" is perfectly valid, but if we add ""INSERT INTO SELECT t1.x, t2.x"", we need to add names, eg. something like ""INSERT INTO SELECT t1.x as x1, t2.x as x2"""
"apr and apt_util packages do not install shared library When we installed apr and apr_utils packages (as a dependency of new log4cxx package, see DM-772) we discovered that both these packages only build static libraries but no shared libs are installed. This is problematic if mixed with shared libs and we use shared libs everywhere else. We certainly need to build shared libs for these packages, this ticket is to follow up on this problem.",2,DM-862,datamanagement,apr apt_util package install share library instal apr apr_util package dependency new log4cxx package dm-772 discover package build static library share lib instal problematic mix share lib use share lib certainly need build share lib package ticket follow problem,"apr and apt_util packages do not install shared library When we installed apr and apr_utils packages (as a dependency of new log4cxx package, see DM-772) we discovered that both these packages only build static libraries but no shared libs are installed. This is problematic if mixed with shared libs and we use shared libs everywhere else. We certainly need to build shared libs for these packages, this ticket is to follow up on this problem."
"near neighbor does not return results A query from qserv_testdata (case01/queries/1051_nn.sql) runs through Qserv, but it returns no results, while the same query run on myql does return results.  The exact query for qserv is:   {code} SELECT o1.objectId AS objId  FROM Object o1, Object o2  WHERE qserv_areaspec_box(0, 0, 0.2, 1)  AND scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1 AND o1.objectId <> o2.objectId; {code}",1,DM-863,datamanagement,near neighbor return result query qserv_testdata case01 queries/1051_nn.sql run qserv return result query run myql return result exact query qserv code select o1.objectid objid object o1 object o2 qserv_areaspec_box(0 0.2 scisql_angsep(o1.ra_ps o1.decl_ps o2.ra_ps o2.decl_ps o1.objectid o2.objectid code,"near neighbor does not return results A query from qserv_testdata (case01/queries/1051_nn.sql) runs through Qserv, but it returns no results, while the same query run on myql does return results. The exact query for qserv is: {code} SELECT o1.objectId AS objId FROM Object o1, Object o2 WHERE qserv_areaspec_box(0, 0, 0.2, 1) AND scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1 AND o1.objectId <> o2.objectId; {code}"
"disable extraneous warnings from boost (gcc 4.8) Compiling qserv on ubuntu 14.04 (comes with gcc 4.8.2) results in huge number of warnings coming from boost. We should use the flag ""-Wno-unused-local-typedefs"".",1,DM-869,datamanagement,disable extraneous warning boost gcc 4.8 compile qserv ubuntu 14.04 come gcc 4.8.2 result huge number warning come boost use flag -wno unused local typedef,"disable extraneous warnings from boost (gcc 4.8) Compiling qserv on ubuntu 14.04 (comes with gcc 4.8.2) results in huge number of warnings coming from boost. We should use the flag ""-Wno-unused-local-typedefs""."
"XLDB - strategic positioning Discussions with strategic partners. Improving website and adding new context (community, speakers). 1-pager document",3,DM-873,datamanagement,xldb strategic positioning discussion strategic partner improve website add new context community speaker pager document,"XLDB - strategic positioning Discussions with strategic partners. Improving website and adding new context (community, speakers). 1-pager document"
"W'14 newinstall.sh picks up wrong python? newinstall.sh fails with:  Installing the basic environment ...  Traceback (most recent call last):   File ""/tmp/test_lsst/eups/bin/eups_impl.py"", line 11, in ?     import eups.cmd   File ""/tmp/test_lsst/eups/python/eups/__init__.py"", line 5, in ?     from cmd        import commandCallbacks   File ""/tmp/test_lsst/eups/python/eups/cmd.py"", line 38, in ?     import distrib   File ""/tmp/test_lsst/eups/python/eups/distrib/__init__.py"", line 30, in ?     from Repositories import Repositories   File ""/tmp/test_lsst/eups/python/eups/distrib/Repositories.py"", line 8, in ?     import server   File ""/tmp/test_lsst/eups/python/eups/distrib/server.py"", line 1498     mapping = self._noReinstall if outVersion and outVersion.lower() == ""noreinstall"" else self._mapping                                  ^ SyntaxError: invalid syntax  Perhaps from running the wrong version of python.  Full script/log is attached. ",1,DM-874,datamanagement,w'14 newinstall.sh pick wrong python newinstall.sh fail instal basic environment traceback recent file /tmp test_lsst eup bin eups_impl.py line 11 import eups.cmd file /tmp test_lsst eup python eups/__init__.py line cmd import commandcallback file /tmp test_lsst eup python eup cmd.py line 38 import distrib file /tmp test_lsst eup python eup distrib/__init__.py line 30 repositories import repositories file /tmp test_lsst eup python eup distrib repositories.py line import server file /tmp test_lsst eup python eup distrib server.py line 1498 mapping self._noreinstall outversion outversion.low noreinstall self._mappe syntaxerror invalid syntax run wrong version python script log attach,"W'14 newinstall.sh picks up wrong python? newinstall.sh fails with: Installing the basic environment ... Traceback (most recent call last): File ""/tmp/test_lsst/eups/bin/eups_impl.py"", line 11, in ? import eups.cmd File ""/tmp/test_lsst/eups/python/eups/__init__.py"", line 5, in ? from cmd import commandCallbacks File ""/tmp/test_lsst/eups/python/eups/cmd.py"", line 38, in ? import distrib File ""/tmp/test_lsst/eups/python/eups/distrib/__init__.py"", line 30, in ? from Repositories import Repositories File ""/tmp/test_lsst/eups/python/eups/distrib/Repositories.py"", line 8, in ? import server File ""/tmp/test_lsst/eups/python/eups/distrib/server.py"", line 1498 mapping = self._noReinstall if outVersion and outVersion.lower() == ""noreinstall"" else self._mapping ^ SyntaxError: invalid syntax Perhaps from running the wrong version of python. Full script/log is attached."
"Segmentation fault from writing dotted FITS header keywords Observed on HSC, but I'm not aware of any fix for this on the LSST side either.  {code:sh} pprice@tiger3:~ $ gdb python GNU gdb (GDB) Red Hat Enterprise Linux (7.2-60.el6_4.1) (gdb) r Starting program: /tigress/HSC/products-20130212/Linux64/python/2.7.6/bin/python  >>> from lsst.afw.image import ExposureF >>> exp = ExposureF(1,1) >>> exp.getMetadata().add(""A.B.C.D"", 12345) >>> exp.writeFits(""test.fits"")  Program received signal SIGSEGV, Segmentation fault. lsst::daf::base::PropertySet::combine (this=0x1eda370, source=...)     at src/PropertySet.cc:746 746	        if (dj == _map.end()) { (gdb) w #0  lsst::daf::base::PropertySet::combine (this=0x1eda370, source=...) at src/PropertySet.cc:746 #1  0x00002aaabac068dc in lsst::daf::base::PropertyList::deepCopy (this=0x1ddd4c0) at src/PropertyList.cc:74 #2  0x00002aaab7ba559e in lsst::afw::image::MaskedImage<float, unsigned short, float>::writeFits (this=0x1f1d8f0, fitsfile=..., metadata=<value optimized out>, imageMetadata=..., maskMetadata=..., varianceMetadata=...) at src/image/MaskedImage.cc:569 #3  0x00002aaab7b0a7f0 in lsst::afw::image::Exposure<float, unsigned short, float>::writeFits (this=0x1f1d8d0, fitsfile=...) at src/image/Exposure.cc:251 #4  0x00002aaab7b0a9fc in lsst::afw::image::Exposure<float, unsigned short, float>::writeFits (this=0x1f1d8d0, fileName=""test.fits"") at src/image/Exposure.cc:239 #5  0x00002aaab6b91614 in _wrap_ExposureF_writeFits__SWIG_0 (self=<value optimized out>, args=<value optimized out>) at python/lsst/afw/image/imageLib_wrap.cc:160786 #6  _wrap_ExposureF_writeFits (self=<value optimized out>, args=<value optimized out>) at python/lsst/afw/image/imageLib_wrap.cc:29886 	… {code}  This appears to only affect dotted keywords (which should be supported through use of {{HIERARCH}}, and even if they're not supported, this should never fail with a segfault).",4,DM-882,datamanagement,"segmentation fault write dot fit header keyword observe hsc aware fix lsst code sh pprice@tiger3:~ gdb python gnu gdb gdb red hat enterprise linux 7.2 60.el6_4.1 gdb starting program hsc products-20130212 linux64 python/2.7.6 bin python lsst.afw.image import exposuref exp exposuref(1,1 exp.getmetadata().add(""a.b.c.d 12345 exp.writefits(""test.fits program receive signal sigsegv segmentation fault lsst::daf::base::propertyset::combine this=0x1eda370 source= src propertyset.cc:746 746 dj map.end gdb lsst::daf::base::propertyset::combine this=0x1eda370 source= src propertyset.cc:746 0x00002aaabac068dc lsst::daf::base::propertylist::deepcopy this=0x1ddd4c0 src propertylist.cc:74 0x00002aaab7ba559e lsst::afw::image::maskedimage::writefit this=0x1f1d8f0 fitsfile= metadata= imagemetadata= maskmetadata= variancemetadata= src image maskedimage.cc:569 0x00002aaab7b0a7f0 lsst::afw::image::exposure::writefit this=0x1f1d8d0 fitsfile= src image exposure.cc:251 0x00002aaab7b0a9fc lsst::afw::image::exposure::writefit this=0x1f1d8d0 filename=""test.fit src image exposure.cc:239 0x00002aaab6b91614 wrap_exposuref_writefits__swig_0 self= args= python lsst afw image imagelib_wrap.cc:160786 wrap_exposuref_writefits self= args= python lsst afw image imagelib_wrap.cc:29886 code appear affect dotted keyword support use hierarch support fail segfault","Segmentation fault from writing dotted FITS header keywords Observed on HSC, but I'm not aware of any fix for this on the LSST side either. {code:sh} pprice@tiger3:~ $ gdb python GNU gdb (GDB) Red Hat Enterprise Linux (7.2-60.el6_4.1) (gdb) r Starting program: /tigress/HSC/products-20130212/Linux64/python/2.7.6/bin/python >>> from lsst.afw.image import ExposureF >>> exp = ExposureF(1,1) >>> exp.getMetadata().add(""A.B.C.D"", 12345) >>> exp.writeFits(""test.fits"") Program received signal SIGSEGV, Segmentation fault. lsst::daf::base::PropertySet::combine (this=0x1eda370, source=...) at src/PropertySet.cc:746 746 if (dj == _map.end()) { (gdb) w #0 lsst::daf::base::PropertySet::combine (this=0x1eda370, source=...) at src/PropertySet.cc:746 #1 0x00002aaabac068dc in lsst::daf::base::PropertyList::deepCopy (this=0x1ddd4c0) at src/PropertyList.cc:74 #2 0x00002aaab7ba559e in lsst::afw::image::MaskedImage::writeFits (this=0x1f1d8f0, fitsfile=..., metadata=, imageMetadata=..., maskMetadata=..., varianceMetadata=...) at src/image/MaskedImage.cc:569 #3 0x00002aaab7b0a7f0 in lsst::afw::image::Exposure::writeFits (this=0x1f1d8d0, fitsfile=...) at src/image/Exposure.cc:251 #4 0x00002aaab7b0a9fc in lsst::afw::image::Exposure::writeFits (this=0x1f1d8d0, fileName=""test.fits"") at src/image/Exposure.cc:239 #5 0x00002aaab6b91614 in _wrap_ExposureF_writeFits__SWIG_0 (self=, args=) at python/lsst/afw/image/imageLib_wrap.cc:160786 #6 _wrap_ExposureF_writeFits (self=, args=) at python/lsst/afw/image/imageLib_wrap.cc:29886 {code} This appears to only affect dotted keywords (which should be supported through use of {{HIERARCH}}, and even if they're not supported, this should never fail with a segfault)."
"Optimize template engine used in configuration tool string.Template and string interpolation are quite weak, some additional feature would be welcomed :  - interpolation interprets wrongly ""%d"" in template files as template - string.template doesn't manage correctly non referenced template parameters present in template files.",4,DM-887,datamanagement,optimize template engine configuration tool string template string interpolation weak additional feature welcome interpolation interpret wrongly template file template string.template manage correctly non reference template parameter present template file,"Optimize template engine used in configuration tool string.Template and string interpolation are quite weak, some additional feature would be welcomed : - interpolation interprets wrongly ""%d"" in template files as template - string.template doesn't manage correctly non referenced template parameters present in template files."
"Use an existing qserv_run_dir with a new Qserv instance/binary Here's what should be added to qserv-configure.py :  - edit $QSERV_RUN_DIR/admin/qserv.conf and change Qserv instance dir to current one (which qserv-configure.sh), - check compliance of QSERV_RUN_DIR with new Qserv instance (version check ?) and/or update configuration files, - re-initialize services, if needed, without breaking already loaded data. ",4,DM-895,datamanagement,use exist qserv_run_dir new qserv instance binary add qserv-configure.py edit qserv_run_dir admin qserv.conf change qserv instance dir current qserv-configure.sh check compliance qserv_run_dir new qserv instance version check and/or update configuration file initialize service need break load datum,"Use an existing qserv_run_dir with a new Qserv instance/binary Here's what should be added to qserv-configure.py : - edit $QSERV_RUN_DIR/admin/qserv.conf and change Qserv instance dir to current one (which qserv-configure.sh), - check compliance of QSERV_RUN_DIR with new Qserv instance (version check ?) and/or update configuration files, - re-initialize services, if needed, without breaking already loaded data."
"unset BASH_ENV in newinstall.sh or surrounding instructions Nutbar users such as myself may have BASH_ENV set, which can mess with your carefully constructed shell environments.  Unsetting BASH_ENV should help.  This could perhaps go in newinstall.sh, or the documentation alongside where we suggest they unset other variables: https://confluence.lsstcorp.org/display/LSWUG/Building+the+v8.0+LSST+Stack+from+Source",1,DM-898,datamanagement,unset bash_env newinstall.sh surround instruction nutbar user bash_env set mess carefully construct shell environment unsette bash_env help newinstall.sh documentation alongside suggest unset variable https://confluence.lsstcorp.org/display/lswug/building+the+v8.0+lsst+stack+from+source,"unset BASH_ENV in newinstall.sh or surrounding instructions Nutbar users such as myself may have BASH_ENV set, which can mess with your carefully constructed shell environments. Unsetting BASH_ENV should help. This could perhaps go in newinstall.sh, or the documentation alongside where we suggest they unset other variables: https://confluence.lsstcorp.org/display/LSWUG/Building+the+v8.0+LSST+Stack+from+Source"
"Update PhoSim tutorial to use CatSim for creating instance catalogs Update the Process PhoSim Images tutorial (https://confluence.lsstcorp.org/display/LSWUG/Process+PhoSim+Images) to use CatSim to generate instance catalogs, once CatSim documentation is available. This will obviate the need for the helper script refCalCat.py. ",2,DM-900,datamanagement,update phosim tutorial use catsim create instance catalog update process phosim images tutorial https://confluence.lsstcorp.org/display/lswug/process+phosim+image use catsim generate instance catalog catsim documentation available obviate need helper script refcalcat.py,"Update PhoSim tutorial to use CatSim for creating instance catalogs Update the Process PhoSim Images tutorial (https://confluence.lsstcorp.org/display/LSWUG/Process+PhoSim+Images) to use CatSim to generate instance catalogs, once CatSim documentation is available. This will obviate the need for the helper script refCalCat.py."
"SourceDetectionTask should only add flags.negative if config.thresholdParity == ""both"" The SourceDetectionTask always adds ""flags.negative"" to the schema (if provided) but it is only used if config.thresholdParity == ""both"".  As adding a field to a schema requires that the table passed to the run method have that field this is a significant nuisance when reusing the task.  Please change the code to only modify the schema if it's going to set it. ",1,DM-903,datamanagement,sourcedetectiontask add flags.negative config.thresholdparity sourcedetectiontask add flags.negative schema provide config.thresholdparity add field schema require table pass run method field significant nuisance reuse task change code modify schema go set,"SourceDetectionTask should only add flags.negative if config.thresholdParity == ""both"" The SourceDetectionTask always adds ""flags.negative"" to the schema (if provided) but it is only used if config.thresholdParity == ""both"". As adding a field to a schema requires that the table passed to the run method have that field this is a significant nuisance when reusing the task. Please change the code to only modify the schema if it's going to set it."
Provide Task documentation for DipoleMeasurementTask See Summary. ,2,DM-911,datamanagement,provide task documentation dipolemeasurementtask summary,Provide Task documentation for DipoleMeasurementTask See Summary.
Provide Task documentation for PsfMatchTask See Description (it's currently called PsfMatch) ,4,DM-912,datamanagement,provide task documentation psfmatchtask description currently call psfmatch,Provide Task documentation for PsfMatchTask See Description (it's currently called PsfMatch)
Provide Task documentation for ImagePsfMatchTask See summary,2,DM-913,datamanagement,provide task documentation summary,Provide Task documentation for ImagePsfMatchTask See summary
Provide Task documentation for SnapPsfMatchTask See summary,2,DM-914,datamanagement,provide task documentation snappsfmatchtask summary,Provide Task documentation for SnapPsfMatchTask See summary
Provide Task documentation for AssembleCcdTask See summary,4,DM-915,datamanagement,provide task documentation assembleccdtask summary,Provide Task documentation for AssembleCcdTask See summary
Provide Task documentation for IsrTask See Summary,4,DM-916,datamanagement,provide task documentation isrtask summary,Provide Task documentation for IsrTask See Summary
Provide Task documentation for SourceDeblendTask See Summary,2,DM-926,datamanagement,provide task documentation sourcedeblendtask summary,Provide Task documentation for SourceDeblendTask See Summary
Provide Task documentation for CmdLineTask See Summary,4,DM-927,datamanagement,provide task documentation cmdlinetask summary,Provide Task documentation for CmdLineTask See Summary
Provide Task documentation for RepairTask See Summary,4,DM-928,datamanagement,provide task documentation repairtask summary,Provide Task documentation for RepairTask See Summary
"How to write your own command line task, including how-to-retarget sub-tasks Please provide documentation on how to write a command line task, and how to retarget tasks (e.g. reusing bits of IsrTask for a new camera)  The documentation should include a complete annotated example.  ",4,DM-929,datamanagement,write command line task include retarget sub task provide documentation write command line task retarget task e.g. reuse bit isrtask new camera documentation include complete annotated example,"How to write your own command line task, including how-to-retarget sub-tasks Please provide documentation on how to write a command line task, and how to retarget tasks (e.g. reusing bits of IsrTask for a new camera) The documentation should include a complete annotated example."
"Improve install/configuration/tests documentation and migrate it to reST format This ticket propose to migrate README and README-devel to reST format (see http://sphinx-doc.org/rest.html). The output is located here : http://lsst-web.ncsa.illinois.edu/~fjammes/qserv-doc/  Furthermore this ticket wil integrate Andy S. DM-622 value-added remarks about Qserv embedded documentation. {quote} README.txt needs a bit of formatting, whole ""NOTE FOR DEVELOPERS"" is one long line which may need scrolling depending on what do you use to read the file, same applies to README-devel.txt The install procedure in README.txt implies that the whole stack has to be installed including eups. If people have some part of it installed already the it would probably be better to reuse existing stack. Shall we spit install instructions into ""Install eups (if not installed already)"" and ""Install qserv""? README-devel.txt says ""Once Qserv is installed..."", I don't think that we need or want to install whole qserv before we start development (what if qserv is not available yet for the platform I'm trying to test). What probably needed is installed dependencies, and this should be covered by the comments before 'setup -r .' {quote} ",1,DM-930,datamanagement,improve install configuration test documentation migrate rest format ticket propose migrate readme readme devel rest format http://sphinx-doc.org/rest.html output locate http://lsst-web.ncsa.illinois.edu/~fjammes/qserv-doc/ furthermore ticket wil integrate andy s. dm-622 value add remark qserv embed documentation quote readme.txt need bit formatting note developer long line need scroll depend use read file apply readme-devel.txt install procedure readme.txt imply stack instal include eup people instal probably well reuse exist stack shall spit install instruction install eup instal install qserv readme-devel.txt say qserv instal think need want install qserv start development qserv available platform try test probably need instal dependency cover comment setup quote,"Improve install/configuration/tests documentation and migrate it to reST format This ticket propose to migrate README and README-devel to reST format (see http://sphinx-doc.org/rest.html). The output is located here : http://lsst-web.ncsa.illinois.edu/~fjammes/qserv-doc/ Furthermore this ticket wil integrate Andy S. DM-622 value-added remarks about Qserv embedded documentation. {quote} README.txt needs a bit of formatting, whole ""NOTE FOR DEVELOPERS"" is one long line which may need scrolling depending on what do you use to read the file, same applies to README-devel.txt The install procedure in README.txt implies that the whole stack has to be installed including eups. If people have some part of it installed already the it would probably be better to reuse existing stack. Shall we spit install instructions into ""Install eups (if not installed already)"" and ""Install qserv""? README-devel.txt says ""Once Qserv is installed..."", I don't think that we need or want to install whole qserv before we start development (what if qserv is not available yet for the platform I'm trying to test). What probably needed is installed dependencies, and this should be covered by the comments before 'setup -r .' {quote}"
"Photometric calibration uses a column ""flux"" not the specified filter unless a colour term is active The photometric calibration code uses a field ""flux"" in the reference catalog to impose a magnitude limit.  If a colour term is specified, it uses the primary and secondary filters to calculate the reference magnitude, but if there is no colour term it uses the column labelled ""flux"" and ignores the filtername.    Please change the code so that ""flux"" is ignored, and the flux associated with filterName is used.",1,DM-933,datamanagement,photometric calibration use column flux specify filter colour term active photometric calibration code use field flux reference catalog impose magnitude limit colour term specify use primary secondary filter calculate reference magnitude colour term use column label flux ignore filtername change code flux ignore flux associate filtername,"Photometric calibration uses a column ""flux"" not the specified filter unless a colour term is active The photometric calibration code uses a field ""flux"" in the reference catalog to impose a magnitude limit. If a colour term is specified, it uses the primary and secondary filters to calculate the reference magnitude, but if there is no colour term it uses the column labelled ""flux"" and ignores the filtername. Please change the code so that ""flux"" is ignored, and the flux associated with filterName is used."
"Replace getXXXKey for slots with returning functorKeys similar to existing compound Keys Make any changes in the Functor Key capabilities necessary to support getXXXKey, getXXXErrKey, and getXXXFlagKey for the 3 different types of slots.   Change these routines in Source.h.m4 to return FunctorKeys for both version 0 and version 1 tables.  Then fixup any compilation breaks on the C++ size which this causes.  Remove the version 1 specific accessors.   I am assigning this to Jim to confirm that the first part is done, then he can assign the remainder to Perry either by reassigning, or as a subtask.",4,DM-936,datamanagement,replace getxxxkey slot return functorkeys similar exist compound keys change functor key capability necessary support getxxxkey getxxxerrkey getxxxflagkey different type slot change routine source.h.m4 return functorkeys version version table fixup compilation break c++ size cause remove version specific accessor assign jim confirm assign remainder perry reassigning subtask,"Replace getXXXKey for slots with returning functorKeys similar to existing compound Keys Make any changes in the Functor Key capabilities necessary to support getXXXKey, getXXXErrKey, and getXXXFlagKey for the 3 different types of slots. Change these routines in Source.h.m4 to return FunctorKeys for both version 0 and version 1 tables. Then fixup any compilation breaks on the C++ size which this causes. Remove the version 1 specific accessors. I am assigning this to Jim to confirm that the first part is done, then he can assign the remainder to Perry either by reassigning, or as a subtask."
"Prevent conflict related to non-unique temporary files created during SciSQL install SciSQL install sometime fails with next message :  {code:bash} [1/2] MySqlScript: scripts/install.mysql [2/2] MySqlScript: scripts/demo.mysql Running testHtm                          : OK Running testSelect                       : OK Running testAngSep.py                    : OK Running testMedian.py                    : OK Running testPercentile.py                : OK Running testS2CPoly.py                   : OK Running testS2PtInBox.py                 : OK Running testS2PtInCircle.py              : OK Running testS2PtInEllipse.py             : OK Running docs.py                          : FAIL [see /usr/local/home/salnikov/qserv-run/u.fjammes.DM-622-g86a30ec72a/tmp/scisql-0.3.2/build/tools/docs.log]   9 tests passed, 1 failed, and 0 failed to run    One or more sciSQL unit tests failed make: *** [install] Error 1 -- CRITICAL: Error code returned by command : / u s r / l o c a l / h o m e / s a l n i k o v / q s e r v - r u n / u . f j a m m e s . D M - 6 2 2 - g 8 6 a 3 0 e c 7 2 a / t m p / c o n f i g u r e / s c i s q l . s h   $ cat /usr/local/home/salnikov/qserv-run/u.fjammes.DM-622-g86a30ec72a/tmp/scisql-0.3.2/build/tools/docs.log rm: cannot remove `/tmp/scisql_demo_ccds.tsv': Operation not permitted Failed to run documentation example:     rm -f /tmp/scisql_demo_ccds.tsv     ERROR 1086 (HY000) at line 4: File '/tmp/scisql_demo_ccds.tsv' already exists {code}  It looks like test uses non-unique temporary file name and someone already tried to run installation on the same host. ",2,DM-945,datamanagement,prevent conflict relate non unique temporary file create scisql install scisql install fail message code bash 1/2 mysqlscript script install.mysql 2/2 mysqlscript script demo.mysql running testhtm ok running testselect ok running ok run testmedian.py ok run testpercentile.py ok run tests2cpoly.py ok run tests2ptinbox.py ok run tests2ptincircle.py ok run tests2ptinellipse.py ok run docs.py fail /usr local home salnikov qserv run u.fjamme dm-622 g86a30ec72a tmp scisql-0.3.2 build tool docs.log test pass fail fail run scisql unit test fail install error critical error code return command cat /usr local home salnikov qserv run u.fjamme dm-622 g86a30ec72a tmp scisql-0.3.2 build tool docs.log rm remove /tmp scisql_demo_ccds.tsv operation permit fail run documentation example rm -f scisql_demo_ccds.tsv error 1086 hy000 line file /tmp scisql_demo_ccds.tsv exist code look like test use non unique temporary file try run installation host,"Prevent conflict related to non-unique temporary files created during SciSQL install SciSQL install sometime fails with next message : {code:bash} [1/2] MySqlScript: scripts/install.mysql [2/2] MySqlScript: scripts/demo.mysql Running testHtm : OK Running testSelect : OK Running testAngSep.py : OK Running testMedian.py : OK Running testPercentile.py : OK Running testS2CPoly.py : OK Running testS2PtInBox.py : OK Running testS2PtInCircle.py : OK Running testS2PtInEllipse.py : OK Running docs.py : FAIL [see /usr/local/home/salnikov/qserv-run/u.fjammes.DM-622-g86a30ec72a/tmp/scisql-0.3.2/build/tools/docs.log] 9 tests passed, 1 failed, and 0 failed to run One or more sciSQL unit tests failed make: *** [install] Error 1 -- CRITICAL: Error code returned by command : / u s r / l o c a l / h o m e / s a l n i k o v / q s e r v - r u n / u . f j a m m e s . D M - 6 2 2 - g 8 6 a 3 0 e c 7 2 a / t m p / c o n f i g u r e / s c i s q l . s h $ cat /usr/local/home/salnikov/qserv-run/u.fjammes.DM-622-g86a30ec72a/tmp/scisql-0.3.2/build/tools/docs.log rm: cannot remove `/tmp/scisql_demo_ccds.tsv': Operation not permitted Failed to run documentation example: rm -f /tmp/scisql_demo_ccds.tsv ERROR 1086 (HY000) at line 4: File '/tmp/scisql_demo_ccds.tsv' already exists {code} It looks like test uses non-unique temporary file name and someone already tried to run installation on the same host."
Improve management of qserv-run-dir in start/stop scripts. qserv-start.sh by default tries again to use /usr/local/home/salnikov/qserv-run/2014_05.0 directory. There may be a confusion if we have multiple run directories. Would it be better to install qserv start/stop scripts into run directory itself so that they don't need to guess anything?,2,DM-946,datamanagement,improve management qserv run dir start stop script qserv-start.sh default try use local home salnikov qserv run/2014_05.0 directory confusion multiple run directory well install qserv start stop script run directory need guess,Improve management of qserv-run-dir in start/stop scripts. qserv-start.sh by default tries again to use /usr/local/home/salnikov/qserv-run/2014_05.0 directory. There may be a confusion if we have multiple run directories. Would it be better to install qserv start/stop scripts into run directory itself so that they don't need to guess anything?
"Move qserv-testdata.py to qserv_testdata package Andy S. suggests this during DM-622 review, but this needs to be studied deeply, as qserv_testdata goal is to store large datafile, difficult to version in git.",3,DM-953,datamanagement,qserv-testdata.py qserv_testdata package andy s. suggest dm-622 review need study deeply qserv_testdata goal store large datafile difficult version git,"Move qserv-testdata.py to qserv_testdata package Andy S. suggests this during DM-622 review, but this needs to be studied deeply, as qserv_testdata goal is to store large datafile, difficult to version in git."
Clearer and shorter output for qserv-configure.py qserv-configure.py produces a lot of output which could be confusing if people try to look at it and understand it. It may be better to reduce it to something that just indicates that each step is completed successfully.,1,DM-954,datamanagement,clear short output qserv-configure.py qserv-configure.py produce lot output confusing people try look understand well reduce indicate step complete successfully,Clearer and shorter output for qserv-configure.py qserv-configure.py produces a lot of output which could be confusing if people try to look at it and understand it. It may be better to reduce it to something that just indicates that each step is completed successfully.
Buildbot email should state if the build used master only or included other branches Buildbot build status report currently doesn't state if the build was only for master or included other branches requested by the user.,4,DM-956,datamanagement,buildbot email state build master include branch buildbot build status report currently state build master include branch request user,Buildbot email should state if the build used master only or included other branches Buildbot build status report currently doesn't state if the build was only for master or included other branches requested by the user.
"Use aliases to clean up table version transition The addition of schema aliases on DM-417 should allow us to clean up some of the transitional code added on DM-545, as we can now alias new versions of fields to the old ones and vice versa.",2,DM-957,datamanagement,use alias clean table version transition addition schema alias dm-417 allow clean transitional code add dm-545 alias new version field old one vice versa,"Use aliases to clean up table version transition The addition of schema aliases on DM-417 should allow us to clean up some of the transitional code added on DM-545, as we can now alias new versions of fields to the old ones and vice versa."
"Move table versions to Schema We've added a version number to afw::table::BaseTable to help with the transition to a new approach with different naming conventions and FunctorKeys instead of compound keys.  However, it looks like Schema objects need to know about this version number as well, in order to change Subschema objects to split/join using underscores instead of periods.  That means we'll have to move the version down into the schema object, which may affect a lot of downstream code.  Other than touching a lot of code, the changes should be trivial.",4,DM-958,datamanagement,table version schema add version number afw::table::basetable help transition new approach different naming convention functorkeys instead compound key look like schema object need know version number order change subschema object split join underscore instead period mean version schema object affect lot downstream code touch lot code change trivial,"Move table versions to Schema We've added a version number to afw::table::BaseTable to help with the transition to a new approach with different naming conventions and FunctorKeys instead of compound keys. However, it looks like Schema objects need to know about this version number as well, in order to change Subschema objects to split/join using underscores instead of periods. That means we'll have to move the version down into the schema object, which may affect a lot of downstream code. Other than touching a lot of code, the changes should be trivial."
measAlg.interpolateOverDefects doesn't accept a python list of Defects The python binding of interpolateOverDefects should accept a python list of Defects as well as a swig-wrapped std::vector<Defect>; it doesn't (try using a python list in test818 in meas_algorithms/tests/Interp.py)  ,1,DM-963,datamanagement,measalg.interpolateoverdefect accept python list defects python bind interpolateoverdefects accept python list defect swig wrap std::vector try python list test818 meas_algorithm test interp.py,measAlg.interpolateOverDefects doesn't accept a python list of Defects The python binding of interpolateOverDefects should accept a python list of Defects as well as a swig-wrapped std::vector; it doesn't (try using a python list in test818 in meas_algorithms/tests/Interp.py)
Include aliases in Schema introspection Schema stringification and iteration should include aliases somehow.  Likewise the extract() Python methods.,1,DM-964,datamanagement,include alias schema introspection schema stringification iteration include alias likewise extract python method,Include aliases in Schema introspection Schema stringification and iteration should include aliases somehow. Likewise the extract() Python methods.
"fix int/long conversion on 32-bit systems and selected 64-bit systems tests/wrap.py fails in pex_config on 32-bit systems and some 64-bit systems (including Ubuntu 14.04) with the following: {code:no-linenum} tests/wrap.py  ...EE.E. ====================================================================== ERROR: testDefaults (__main__.NestedWrapTest) Test that C++ Control object defaults are correctly used as defaults for Config objects. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 89, in testDefaults     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testInt64 (__main__.NestedWrapTest) Test that we can wrap C++ Control objects with int64 members. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 95, in testInt64     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testReadControl (__main__.NestedWrapTest) Test reading the values from a C++ Control object into a Config object. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 82, in testReadControl     config.readControl(control)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 212, in readControl     __at=__at, __label=__label, __reset=__reset)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 217, in readControl     self.update(__at=__at, __label=__label, **values)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 515, in update     field.__set__(self, value, at=at, label=label)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 310, in __set__     raise FieldValidationError(self, instance, e.message) FieldValidationError: Field 'a.q' failed validation: Value 4 is of incorrect type long. Expected type int For more information read the Field definition at:   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 184, in makeConfigClass     fields[k] = FieldCls(doc=doc, dtype=dtype, optional=True) And the Config definition at:   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 131, in makeConfigClass     cls = type(name, (base,), {""__doc__"":doc})   ---------------------------------------------------------------------- Ran 8 tests in 0.017s  FAILED (errors=3) {code}  There is a partial fix on u/jbosch/intwrappers; this seems to work for Ubuntu 14.04, but not on 32-bit systems.",2,DM-966,datamanagement,"fix int long conversion 32 bit system select 64 bit system test wrap.py fail pex_config 32 bit system 64 bit system include ubuntu 14.04 following code linenum test wrap.py ee.e. error testdefault main__.nestedwraptest test c++ control object default correctly default config object traceback recent file test wrap.py line 89 testdefault self.assert_(testlib.checknestedcontrol(control config.a.p config.a.q config.b file /home boutigny cfht stack_5 build pex_config test testlib.py line 987 checknestedcontrol return testlib.checknestedcontrol(*args typeerror method checknestedcontrol argument type double error testint64 main__.nestedwraptest test wrap c++ control object int64 member traceback recent file test wrap.py line 95 testint64 self.assert_(testlib.checknestedcontrol(control config.a.p config.a.q config.b file /home boutigny cfht stack_5 build pex_config test testlib.py line 987 checknestedcontrol return testlib.checknestedcontrol(*args typeerror method checknestedcontrol argument type double error testreadcontrol main__.nestedwraptest test read value c++ control object config object traceback recent file test wrap.py line 82 testreadcontrol file /home boutigny cfht stack_5 build pex_config python lsst pex config wrap.py line 212 readcontrol at=__at label=__label reset=__reset file /home boutigny cfht stack_5 build pex_config python lsst pex config wrap.py line 217 readcontrol self.update(__at=__at label=__label value file /home boutigny cfht stack_5 build pex_config python lsst pex config config.py line 515 update field.__set__(self value label label file /home boutigny cfht stack_5 build pex_config python lsst pex config config.py line 310 set raise fieldvalidationerror(self instance e.message fieldvalidationerror field a.q failed validation value incorrect type long expect type int information read field definition file /home boutigny cfht stack_5 build pex_config python lsst pex config wrap.py line 184 makeconfigclass fields[k fieldcls(doc doc dtype dtype optional true config definition file /home boutigny cfht stack_5 build pex_config python lsst pex config wrap.py line 131 makeconfigclass cls type(name base doc__"":doc ran test 0.017s failed errors=3 code partial fix jbosch intwrapper work ubuntu 14.04 32 bit system","fix int/long conversion on 32-bit systems and selected 64-bit systems tests/wrap.py fails in pex_config on 32-bit systems and some 64-bit systems (including Ubuntu 14.04) with the following: {code:no-linenum} tests/wrap.py ...EE.E. ====================================================================== ERROR: testDefaults (__main__.NestedWrapTest) Test that C++ Control object defaults are correctly used as defaults for Config objects. ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/wrap.py"", line 89, in testDefaults self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b)) File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double' ====================================================================== ERROR: testInt64 (__main__.NestedWrapTest) Test that we can wrap C++ Control objects with int64 members. ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/wrap.py"", line 95, in testInt64 self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b)) File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double' ====================================================================== ERROR: testReadControl (__main__.NestedWrapTest) Test reading the values from a C++ Control object into a Config object. ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/wrap.py"", line 82, in testReadControl config.readControl(control) File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 212, in readControl __at=__at, __label=__label, __reset=__reset) File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 217, in readControl self.update(__at=__at, __label=__label, **values) File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 515, in update field.__set__(self, value, at=at, label=label) File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 310, in __set__ raise FieldValidationError(self, instance, e.message) FieldValidationError: Field 'a.q' failed validation: Value 4 is of incorrect type long. Expected type int For more information read the Field definition at: File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 184, in makeConfigClass fields[k] = FieldCls(doc=doc, dtype=dtype, optional=True) And the Config definition at: File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 131, in makeConfigClass cls = type(name, (base,), {""__doc__"":doc}) ---------------------------------------------------------------------- Ran 8 tests in 0.017s FAILED (errors=3) {code} There is a partial fix on u/jbosch/intwrappers; this seems to work for Ubuntu 14.04, but not on 32-bit systems."
"qserv-configure.py is broken in master It looks like there was a bug introduced either during the merge of DM-622 with master or right before that. Running {{qserv-configure.py}} from master fails now: {code} $ qserv-configure.py    File ""/usr/local/home/salnikov/qserv-master/build/dist/bin/qserv-configure.py"", line 229     (""Do you want to update user configuration file (currently pointing                                                                       ^ SyntaxError: EOL while scanning string literal {code} I assign this to myself, Fabrice is on vacation now and we need to fix this quickly.",1,DM-967,datamanagement,qserv-configure.py break master look like bug introduce merge dm-622 master right run qserv-configure.py master fail code qserv-configure.py file /usr local home salnikov qserv master build dist bin qserv configure.py line 229 want update user configuration file currently point syntaxerror eol scan string literal code assign fabrice vacation need fix quickly,"qserv-configure.py is broken in master It looks like there was a bug introduced either during the merge of DM-622 with master or right before that. Running {{qserv-configure.py}} from master fails now: {code} $ qserv-configure.py File ""/usr/local/home/salnikov/qserv-master/build/dist/bin/qserv-configure.py"", line 229 (""Do you want to update user configuration file (currently pointing ^ SyntaxError: EOL while scanning string literal {code} I assign this to myself, Fabrice is on vacation now and we need to fix this quickly."
"Create a ""stub"" package that checks for system dependencies for Qserv Create a special ""stab"" package that checks and reports missing dependencies.",2,DM-973,datamanagement,create stub package check system dependency qserv create special stab package check report miss dependency,"Create a ""stub"" package that checks for system dependencies for Qserv Create a special ""stab"" package that checks and reports missing dependencies."
"setup standard aliases for frequently-used measurements Frequently-used measurement fields such as classification and pixel flags should have shortened aliases that can be used instead of their full, package-qualified versions.  In some cases, these may serve as a sort of slot (e.g. we may have multiple classifications algorithms someday).  In this issue, we should audit all current measurement algorithm fields that don't already have a slot that works for them and consider whether there should be a standard alias.  We also need to work out a system for defining these aliases, probably in the config for SingleFrameMeasurementTask.",4,DM-975,datamanagement,setup standard alias frequently measurement frequently measurement field classification pixel flag shorten alias instead package qualified version case serve sort slot e.g. multiple classification algorithm someday issue audit current measurement algorithm field slot work consider standard alias need work system define alias probably config singleframemeasurementtask,"setup standard aliases for frequently-used measurements Frequently-used measurement fields such as classification and pixel flags should have shortened aliases that can be used instead of their full, package-qualified versions. In some cases, these may serve as a sort of slot (e.g. we may have multiple classifications algorithms someday). In this issue, we should audit all current measurement algorithm fields that don't already have a slot that works for them and consider whether there should be a standard alias. We also need to work out a system for defining these aliases, probably in the config for SingleFrameMeasurementTask."
Detailed documentation for meas_base tasks We should follow RHL's example for detailed task documentation and document all meas_base tasks.,2,DM-976,datamanagement,detailed documentation meas_base task follow rhl example detailed task documentation document meas_base task,Detailed documentation for meas_base tasks We should follow RHL's example for detailed task documentation and document all meas_base tasks.
"Documentation audit and cleanup for meas_base plugins Many meas_base Plugins and Algorithms have poor documentation, including several whose documentation is a copy/paste relic from some other algorithm.  These need to be fixed.",2,DM-977,datamanagement,documentation audit cleanup meas_base plugin meas_base plugins algorithms poor documentation include documentation copy paste relic algorithm need fix,"Documentation audit and cleanup for meas_base plugins Many meas_base Plugins and Algorithms have poor documentation, including several whose documentation is a copy/paste relic from some other algorithm. These need to be fixed."
"add base class for measurement tasks We should consider adding a base class for measurement tasks (SingleFrameMeasurementTask, ForcedMeasuremedTask) that includes the callMeasure methods.  I'm hoping this will help cleanup callMeasure and improve code reuse.",1,DM-978,datamanagement,add base class measurement task consider add base class measurement task singleframemeasurementtask forcedmeasuremedtask include callmeasure method hope help cleanup callmeasure improve code reuse,"add base class for measurement tasks We should consider adding a base class for measurement tasks (SingleFrameMeasurementTask, ForcedMeasuremedTask) that includes the callMeasure methods. I'm hoping this will help cleanup callMeasure and improve code reuse."
"Add FunctorKey to replace Coord compound keys Unlike previous FunctorKey replacements, Coord compound Keys are used in the minimal schema for SimpleTable and SourceTable, which makes removing them problematic.",1,DM-979,datamanagement,add functorkey replace coord compound key unlike previous functorkey replacement coord compound keys minimal schema simpletable sourcetable make remove problematic,"Add FunctorKey to replace Coord compound keys Unlike previous FunctorKey replacements, Coord compound Keys are used in the minimal schema for SimpleTable and SourceTable, which makes removing them problematic."
convert measurement algorithms in ip_diffim ip_diffim includes a few measurement algorithms which need to be converted to the new framework.,5,DM-980,datamanagement,convert measurement algorithm ip_diffim ip_diffim include measurement algorithm need convert new framework,convert measurement algorithms in ip_diffim ip_diffim includes a few measurement algorithms which need to be converted to the new framework.
"convert measurement algorithms in meas_extensions_shapeHSM This is a low-priority ticket to replace the old-style plugins in meas_extensions_shapeHSM with new ones compatible with meas_base.  As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete.",3,DM-981,datamanagement,convert measurement algorithm meas_extensions_shapehsm low priority ticket replace old style plugin meas_extensions_shapehsm new one compatible meas_base main line stack delay meas_base conversion nearly fully complete,"convert measurement algorithms in meas_extensions_shapeHSM This is a low-priority ticket to replace the old-style plugins in meas_extensions_shapeHSM with new ones compatible with meas_base. As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete."
"convert meas_extensions_photometryKron to new measurement framework This is a low-priority ticket to replace the old-style plugins in meas_extensions_photometryKron with new ones compatible with meas_base.  As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete.",3,DM-982,datamanagement,convert meas_extensions_photometrykron new measurement framework low priority ticket replace old style plugin meas_extensions_photometrykron new one compatible meas_base main line stack delay meas_base conversion nearly fully complete,"convert meas_extensions_photometryKron to new measurement framework This is a low-priority ticket to replace the old-style plugins in meas_extensions_photometryKron with new ones compatible with meas_base. As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete."
Finish the Log packaging Finish the work started by Bill (DM-778),6,DM-983,datamanagement,finish log packaging finish work start bill dm-778,Finish the Log packaging Finish the work started by Bill (DM-778)
allow partial measurement results to be set when error flag is set We need to be able to return values at the same time that an error flag is set.  The easiest way to do this is to have Algorithms take a Result object as an output argument rather than return it.  We'll revisit this design later. ,2,DM-984,datamanagement,allow partial measurement result set error flag set need able return value time error flag set easy way algorithms result object output argument return revisit design later,allow partial measurement results to be set when error flag is set We need to be able to return values at the same time that an error flag is set. The easiest way to do this is to have Algorithms take a Result object as an output argument rather than return it. We'll revisit this design later.
".my.cnf in user HOME directory breaks setup script Presence of {{.my.cnf}} file in the user HOME directory crashes {{qserv-configure.py}} script if parameters in {{.my.cnf}} conflict with parameters in {{qserv.conf}}.  How to reproduce: * create .my.cnf file in the home directory: {code} [client] user     = anything # host/port and/or socket host     = 127.0.0.1 port     = 3306 socket   = /tmp/mysql.sock {code} * try to run {{qserv-configure}}, it fails with error: {code} /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: connect: Connection refused /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: line 13: /dev/tcp/127.0.0.1/23306: Connection refused ERROR 2003 (HY000): Can't connect to MySQL server on '127.0.0.1' (111) {code}  It looks like {{~/.my.cnf}} may be a left-over from some earlier qserv installation. If I remove it and re-run {{qserv-configure.py}} now it's not created anymore. Maybe worth adding some kind of protection to {{qserv-configure.py}} in case other users have this file in their home directory.",2,DM-989,datamanagement,user home directory break setup script presence .my.cnf file user home directory crash qserv-configure.py script parameter .my.cnf conflict parameter qserv.conf reproduce create file home directory code client user host port and/or socket host 127.0.0.1 port 3306 socket /tmp mysql.sock code try run qserv configure fail error code /usr local home salnikov qserv run u.salnikov dm-595 tmp configure mysql.sh connect connection refuse local home salnikov qserv run u.salnikov dm-595 tmp configure mysql.sh line 13 tcp/127.0.0.1/23306 connection refuse error 2003 hy000 connect mysql server 127.0.0.1 111 code look like ~/.my.cnf left early qserv installation remove run qserv-configure.py create anymore maybe worth add kind protection qserv-configure.py case user file home directory,".my.cnf in user HOME directory breaks setup script Presence of {{.my.cnf}} file in the user HOME directory crashes {{qserv-configure.py}} script if parameters in {{.my.cnf}} conflict with parameters in {{qserv.conf}}. How to reproduce: * create .my.cnf file in the home directory: {code} [client] user = anything # host/port and/or socket host = 127.0.0.1 port = 3306 socket = /tmp/mysql.sock {code} * try to run {{qserv-configure}}, it fails with error: {code} /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: connect: Connection refused /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: line 13: /dev/tcp/127.0.0.1/23306: Connection refused ERROR 2003 (HY000): Can't connect to MySQL server on '127.0.0.1' (111) {code} It looks like {{~/.my.cnf}} may be a left-over from some earlier qserv installation. If I remove it and re-run {{qserv-configure.py}} now it's not created anymore. Maybe worth adding some kind of protection to {{qserv-configure.py}} in case other users have this file in their home directory."
add query involving a blob to the integration tests We need to add a query (or more?) to the qserv_testdata that involve blobs. Blobs are interesting because they might break some parts of the qserv if we failed to escape things properly etc. ,2,DM-991,datamanagement,add query involve blob integration test need add query qserv_testdata involve blob blob interesting break part qserv fail escape thing properly etc,add query involving a blob to the integration tests We need to add a query (or more?) to the qserv_testdata that involve blobs. Blobs are interesting because they might break some parts of the qserv if we failed to escape things properly etc.
"improve message from qserv_testdata Currently, when I try to run qserv-benchmark but qserv_testdata was not setup, I am getting  {code} CRITICAL Unable to find tests datasets. -- FOR EUPS USERS : Please run :    eups distrib install qserv_testdata    setup qserv_testdata FOR NON-EUPS USERS : Please fill 'testdata_dir' value in ~/.lsst/qserv.conf with the path of the directory containing tests datasets or use --testdata-dir option. {code}  It is important to note in the section for eups users that this has to be called BEFORE qserv is setup, otherwise it has no effect. ",1,DM-993,datamanagement,improve message qserv_testdata currently try run qserv benchmark qserv_testdata setup get code critical unable find test dataset eups user run eup distrib install qserv_testdata setup qserv_testdata non eups user fill testdata_dir value ~/.lsst qserv.conf path directory contain test dataset use --testdata dir option code important note section eup user call qserv setup effect,"improve message from qserv_testdata Currently, when I try to run qserv-benchmark but qserv_testdata was not setup, I am getting {code} CRITICAL Unable to find tests datasets. -- FOR EUPS USERS : Please run : eups distrib install qserv_testdata setup qserv_testdata FOR NON-EUPS USERS : Please fill 'testdata_dir' value in ~/.lsst/qserv.conf with the path of the directory containing tests datasets or use --testdata-dir option. {code} It is important to note in the section for eups users that this has to be called BEFORE qserv is setup, otherwise it has no effect."
"make slot config validation more intelligent Slot config validation currently assumes that field names match plugin names, which is not always a safe assumption.  This can prevent certain algorithms from being used in slots.  We probably can't do this validation in Config.validate(); we need to check in the measurement Task constructor after the schema has already been constructed.",1,DM-994,datamanagement,slot config validation intelligent slot config validation currently assume field name match plugin name safe assumption prevent certain algorithm slot probably validation config.validate need check measurement task constructor schema construct,"make slot config validation more intelligent Slot config validation currently assumes that field names match plugin names, which is not always a safe assumption. This can prevent certain algorithms from being used in slots. We probably can't do this validation in Config.validate(); we need to check in the measurement Task constructor after the schema has already been constructed."
"Make NoiseReplacer a context manager I think the API for NoiseReplacer could be made more idiomatic (and possibly safer) by turning it into a ""context manager"" (i.e. so it can be used with the ""with"" statement).",1,DM-995,datamanagement,noisereplacer context manager think api noisereplacer idiomatic possibly safe turn context manager i.e. statement,"Make NoiseReplacer a context manager I think the API for NoiseReplacer could be made more idiomatic (and possibly safer) by turning it into a ""context manager"" (i.e. so it can be used with the ""with"" statement)."
"rename config file(s) in Qserv Rename local.qserv.cnf to qserv-czar.cnf. It is quite likely there are some other config files that would make sense to rename. If you see some candidates, let's discussion on qserv-l and do the renames.",1,DM-999,datamanagement,rename config file(s qserv rename local.qserv.cnf qserv-czar.cnf likely config file sense rename candidate let discussion qserv rename,"rename config file(s) in Qserv Rename local.qserv.cnf to qserv-czar.cnf. It is quite likely there are some other config files that would make sense to rename. If you see some candidates, let's discussion on qserv-l and do the renames."
"Modify assertAlmostEqual in ip_diffim subtractExposures.py unit test In unit test, the comparison     self.assertAlmostEqual(skp1[nk][np], skp2[nk][np], 4)   fails.  However if changed to    self.assertTrue(abs(skp1[nk][np]-skp2[nk][np]) < 10**-4)  which is the desired test, this succeeds.   This ticket will remove all assertAlmostEquals from subtractExposure.py and replace with the fundamental comparison operator of the absolute value of the differences.",1,DM-1001,datamanagement,modify assertalmostequal ip_diffim subtractexposures.py unit test unit test comparison self.assertalmostequal(skp1[nk][np skp2[nk][np fail change self.asserttrue(abs(skp1[nk][np]-skp2[nk][np 10**-4 desire test succeed ticket remove assertalmostequal subtractexposure.py replace fundamental comparison operator absolute value difference,"Modify assertAlmostEqual in ip_diffim subtractExposures.py unit test In unit test, the comparison self.assertAlmostEqual(skp1[nk][np], skp2[nk][np], 4) fails. However if changed to self.assertTrue(abs(skp1[nk][np]-skp2[nk][np]) < 10**-4) which is the desired test, this succeeds. This ticket will remove all assertAlmostEquals from subtractExposure.py and replace with the fundamental comparison operator of the absolute value of the differences."
Research Apache Mesos and Google Kubernetes It'd be good to check out Apache Mesos and Google Kubernetes and see how relevant they are for Qserv / LSST Database ,4,DM-1002,datamanagement,research apache mesos google kubernetes good check apache mesos google kubernetes relevant qserv lsst database,Research Apache Mesos and Google Kubernetes It'd be good to check out Apache Mesos and Google Kubernetes and see how relevant they are for Qserv / LSST Database
Provide Task documentation for ModelPsfMatchTask See Description (it's currently called PsfMatch) ,2,DM-1004,datamanagement,provide task documentation modelpsfmatchtask description currently call psfmatch,Provide Task documentation for ModelPsfMatchTask See Description (it's currently called PsfMatch)
fix names of meas_base plugins to match new naming standards Some meas_base plugins still have old-style algorithm names.,1,DM-1010,datamanagement,fix name meas_base plugin match new naming standard meas_base plugin old style algorithm names,fix names of meas_base plugins to match new naming standards Some meas_base plugins still have old-style algorithm names.
"Remove use of compound fields in minimal schema If we want to ultimately remove compound fields, we need to remove the ""coord"" field from the SimpleTable/SourceTable minimal schema, and provide a different way to get ra,dec from a source.",4,DM-1011,datamanagement,remove use compound field minimal schema want ultimately remove compound field need remove coord field simpletable sourcetable minimal schema provide different way ra dec source,"Remove use of compound fields in minimal schema If we want to ultimately remove compound fields, we need to remove the ""coord"" field from the SimpleTable/SourceTable minimal schema, and provide a different way to get ra,dec from a source."
"remove temporary workaround in new SkyCoord algorithm SingleFrameSkyCoordPlugin is using the Footprint Peak, not the centroid slot.  According to comments in the code, this is a workaround for some problem with centroids.  This needs to be fixed.",1,DM-1012,datamanagement,remove temporary workaround new skycoord algorithm singleframeskycoordplugin footprint peak centroid slot accord comment code workaround problem centroid need fix,"remove temporary workaround in new SkyCoord algorithm SingleFrameSkyCoordPlugin is using the Footprint Peak, not the centroid slot. According to comments in the code, this is a workaround for some problem with centroids. This needs to be fixed."
"Classification should set flags upon failure The classification algorithm claims it can never fail.  It can, and should report this.",2,DM-1013,datamanagement,classification set flag failure classification algorithm claim fail report,"Classification should set flags upon failure The classification algorithm claims it can never fail. It can, and should report this."
"Detailed documentation for GaussianCentroid We need more detailed documentation for the GaussianCentroid algorithm, in terms of how it actually computes the centroid.  We (Jim and Perry) have done what we can, but we need help from whoever actually wrote it (RHL, we think) to provide the rest.  In particular:  - Additional detail should be filled in in the class Doxygen for GaussianCentroidAlgorithm, in GaussianCentroid.h  - The ""noPeak"" flag field description and name should be compared to what the algorithm actually does with it.  It looks to me like it's a bit misnamed (and maybe shouldn't be considered an error condition at all, if we want to run this on difference images), but I'm not sure.",1,DM-1014,datamanagement,detailed documentation gaussiancentroid need detailed documentation gaussiancentroid algorithm term actually compute centroid jim perry need help actually write rhl think provide rest particular additional detail fill class doxygen gaussiancentroidalgorithm gaussiancentroid.h nopeak flag field description compare algorithm actually look like bit misname maybe consider error condition want run difference image sure,"Detailed documentation for GaussianCentroid We need more detailed documentation for the GaussianCentroid algorithm, in terms of how it actually computes the centroid. We (Jim and Perry) have done what we can, but we need help from whoever actually wrote it (RHL, we think) to provide the rest. In particular: - Additional detail should be filled in in the class Doxygen for GaussianCentroidAlgorithm, in GaussianCentroid.h - The ""noPeak"" flag field description and name should be compared to what the algorithm actually does with it. It looks to me like it's a bit misnamed (and maybe shouldn't be considered an error condition at all, if we want to run this on difference images), but I'm not sure."
"convert GaussianFlux to use shape, centroid slots We should cleanup and simplify the GaussianFlux algorithm to simply use the shape and centroid slot values instead of either computing its own or having configurable field names for where to look these up.",1,DM-1015,datamanagement,convert gaussianflux use shape centroid slot cleanup simplify gaussianflux algorithm simply use shape centroid slot value instead compute have configurable field name look,"convert GaussianFlux to use shape, centroid slots We should cleanup and simplify the GaussianFlux algorithm to simply use the shape and centroid slot values instead of either computing its own or having configurable field names for where to look these up."
"Detailed documentation for SdssCentroid We needed detailed documentation for how SdssCentroid actually works.  I think it involves fitting something like a symmetrized PSF model, but I'm sufficiently unsure of the details that RHL should write this, not me.  It should go in the Doxygen class docs for SdssCentroidAlgorithm in SdssCentroid.h in meas_base.",1,DM-1016,datamanagement,detailed documentation sdsscentroid need detailed documentation sdsscentroid actually work think involve fit like symmetrized psf model sufficiently unsure detail rhl write doxygen class doc sdsscentroidalgorithm sdsscentroid.h meas_base,"Detailed documentation for SdssCentroid We needed detailed documentation for how SdssCentroid actually works. I think it involves fitting something like a symmetrized PSF model, but I'm sufficiently unsure of the details that RHL should write this, not me. It should go in the Doxygen class docs for SdssCentroidAlgorithm in SdssCentroid.h in meas_base."
"fix testForced.py testForced.py is currently passing even though it probably should be failing: it's trying to get centroid values from a source which has neither a valid centroid slot or a Footprint with Peaks (I suspect because transforming a footprint might remove the peaks).  Prior to DM-976, that would have caused a segfault; on DM-976, I've turned it into an exception, which is then turned into a warning by the measurement framework.",2,DM-1017,datamanagement,fix testforced.py testforced.py currently pass probably fail try centroid value source valid centroid slot footprint peaks suspect transform footprint remove peak prior dm-976 cause segfault dm-976 turn exception turn warning measurement framework,"fix testForced.py testForced.py is currently passing even though it probably should be failing: it's trying to get centroid values from a source which has neither a valid centroid slot or a Footprint with Peaks (I suspect because transforming a footprint might remove the peaks). Prior to DM-976, that would have caused a segfault; on DM-976, I've turned it into an exception, which is then turned into a warning by the measurement framework."
"Fix incorrect eupspkg config for astrometry_net The clang patch from 8.0.0. version was (correctly) deleted. However, the patch identity was still left in the eupspkg config's protocol.  This will delete the last vestige of the formerly necessary clang patch.",2,DM-1018,datamanagement,fix incorrect eupspkg config astrometry_net clang patch 8.0.0 version correctly delete patch identity leave eupspkg config protocol delete vestige necessary clang patch,"Fix incorrect eupspkg config for astrometry_net The clang patch from 8.0.0. version was (correctly) deleted. However, the patch identity was still left in the eupspkg config's protocol. This will delete the last vestige of the formerly necessary clang patch."
"fix warnings related to libraries pulled through dependent package This came up during migrating qserv to the new logging system, and it can be reproduced by taking log4cxx, see DM-983, essentially:  {code} eups distrib install -c log4cxx 0.10.0.lsst1 -r http://lsst-web.ncsa.illinois.edu/~becla/distrib -r http://sw.lsstcorp.org/eupspkg {code}  cloning log package (contrib/log.git), building it and installing in your stack, and finally taking the branch u/jbecla/DM-207 of qserv and building it.  The warnings looks like:  {code}/usr/bin/ld: warning: libutils.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libpex_exceptions.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libbase.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) {code}  and they show up when I build qserv package, and are triggered by the liblog. I suspect sconsUtils deal with that sort of issues, but since we have our own scons system for qserv it is not handled. Fabrice, can you try to find a reasonable solution for that? Thanks!",1,DM-1022,datamanagement,fix warning relate library pull dependent package come migrate qserv new log system reproduce take log4cxx dm-983 essentially code eup distrib install log4cxx 0.10.0.lsst1 http://lsst-web.ncsa.illinois.edu/~becla/distrib -r http://sw.lsstcorp.org/eupspkg code clone log package contrib log.git build instal stack finally take branch jbecla dm-207 qserv build warning look like code}/usr bin ld warning libutils.so need /usr local home becla qservdev linux64 log/1.0.0 lib liblog.so find try -rpath -rpath link bin ld warning libpex_exceptions.so need /usr local home becla qservdev linux64 log/1.0.0 lib liblog.so find try -rpath -rpath link bin ld warning libbase.so need /usr local home becla qservdev linux64 log/1.0.0 lib liblog.so find try -rpath -rpath link code build qserv package trigger liblog suspect sconsutils deal sort issue scon system qserv handle fabrice try find reasonable solution thank,"fix warnings related to libraries pulled through dependent package This came up during migrating qserv to the new logging system, and it can be reproduced by taking log4cxx, see DM-983, essentially: {code} eups distrib install -c log4cxx 0.10.0.lsst1 -r http://lsst-web.ncsa.illinois.edu/~becla/distrib -r http://sw.lsstcorp.org/eupspkg {code} cloning log package (contrib/log.git), building it and installing in your stack, and finally taking the branch u/jbecla/DM-207 of qserv and building it. The warnings looks like: {code}/usr/bin/ld: warning: libutils.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libpex_exceptions.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libbase.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) {code} and they show up when I build qserv package, and are triggered by the liblog. I suspect sconsUtils deal with that sort of issues, but since we have our own scons system for qserv it is not handled. Fabrice, can you try to find a reasonable solution for that? Thanks!"
"make use of MeasurementDataFlags We started adding a system to allow algorithms to declare what kind of data they can run on, but never really put it in place.  To do this, we should:  - Add more flags (at least NO_CALIB).  - Pass these flags in pipe_tasks and other places with tasks that use measurement tasks as subtasks (for instance, we should set NO_WCS and NO_CALIB during calibrate.initialMeasurement, and COADD in processCoadd).  - Add checks for these flags in appropriate algorithms.  For instance, PsfFlux should fail in construction if NO_PSF is set, and PeakLikelihoodFlux should fail if PRECONVOLVED is not set.",6,DM-1026,datamanagement,use measurementdataflag start add system allow algorithm declare kind datum run place add flag no_calib pass flag pipe_task place task use measurement task subtask instance set no_wcs no_calib calibrate.initialmeasurement coadd processcoadd add check flag appropriate algorithm instance psfflux fail construction no_psf set peaklikelihoodflux fail preconvolved set,"make use of MeasurementDataFlags We started adding a system to allow algorithms to declare what kind of data they can run on, but never really put it in place. To do this, we should: - Add more flags (at least NO_CALIB). - Pass these flags in pipe_tasks and other places with tasks that use measurement tasks as subtasks (for instance, we should set NO_WCS and NO_CALIB during calibrate.initialMeasurement, and COADD in processCoadd). - Add checks for these flags in appropriate algorithms. For instance, PsfFlux should fail in construction if NO_PSF is set, and PeakLikelihoodFlux should fail if PRECONVOLVED is not set."
qserv-version.sh produces incorrect version number I have just installed qserv on a clean machine (this is in a new virtual machine running Ubuntu12.04) which got me version 2014_07.0 installed: {code} $ eups list qserv    2014_07.0    current b76 $ setup qserv $ eups list qserv    2014_07.0    current b76 setup $ echo $QSERV_DIR /opt/salnikov/STACK/Linux64/qserv/2014_07.0 {code}  but the {{qserv-version.sh}} script still thinks that I'm running older version: {code} $ qserv-version.sh 2014_05.0 {code},2,DM-1028,datamanagement,qserv-version.sh produce incorrect version number instal qserv clean machine new virtual machine run ubuntu12.04 get version 2014_07.0 instal code eup list qserv 2014_07.0 current b76 setup qserv eup list qserv 2014_07.0 current b76 setup echo qserv_dir salnikov stack linux64 qserv/2014_07.0 code qserv-version.sh script think run old version code qserv-version.sh 2014_05.0 code,qserv-version.sh produces incorrect version number I have just installed qserv on a clean machine (this is in a new virtual machine running Ubuntu12.04) which got me version 2014_07.0 installed: {code} $ eups list qserv 2014_07.0 current b76 $ setup qserv $ eups list qserv 2014_07.0 current b76 setup $ echo $QSERV_DIR /opt/salnikov/STACK/Linux64/qserv/2014_07.0 {code} but the {{qserv-version.sh}} script still thinks that I'm running older version: {code} $ qserv-version.sh 2014_05.0 {code}
"""source"" command is not in standard shell {{qserv-start.sh}} script fails when installed on Ubuntu12.04: {code} $ ~/qserv-run/2014_05.0/bin/qserv-start.sh /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 4: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: source: not found /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 6: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: check_qserv_run_dir: not found {code}  It complains about {{source}} command. {{source}} is not standard POSIX shell command, it is an extension which exists in many shells. Apparently in older Ubuntu version {{/bin/sh}} is stricter about non-standard features.   To fix the script one either has to use standard . (dot) command or change shebang to {{#!/bin/bash}}. This of course applies to all our executable scripts.",2,DM-1029,datamanagement,source command standard shell qserv-start.sh script fail instal ubuntu12.04 code ~/qserv run/2014_05.0 bin qserv start.sh /home salnikov qserv run/2014_05.0 bin qserv start.sh /home salnikov qserv run/2014_05.0 bin qserv start.sh source find salnikov qserv run/2014_05.0 bin qserv start.sh /home salnikov qserv run/2014_05.0 bin qserv start.sh check_qserv_run_dir find code complain source command source standard posix shell command extension exist shell apparently old ubuntu version /bin sh strict non standard feature fix script use standard dot command change shebang /bin bash course apply executable script,"""source"" command is not in standard shell {{qserv-start.sh}} script fails when installed on Ubuntu12.04: {code} $ ~/qserv-run/2014_05.0/bin/qserv-start.sh /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 4: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: source: not found /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 6: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: check_qserv_run_dir: not found {code} It complains about {{source}} command. {{source}} is not standard POSIX shell command, it is an extension which exists in many shells. Apparently in older Ubuntu version {{/bin/sh}} is stricter about non-standard features. To fix the script one either has to use standard . (dot) command or change shebang to {{#!/bin/bash}}. This of course applies to all our executable scripts."
"eliminate confusing config side-effects in CalibrateTask CalibrateTask does some unexpected things differently if you configure it certain ways, because it perceives certain processing as only being necessary to feed other steps.  In particular, if you disable astrometry and photometric calibration, it only runs measurement once, because it assumes the only purpose of the post-PSF measurement is to feed those algorithms.  This (as well as poor test coverage) made it easy to break CalibrateTask in the case where those options are disabled a few branches back.  After conferring with Simon and Andy, we think the best solution is to remove this sort of conditional processing from CalibrateTask, which should also make it much easier to read.  Instead, we'll always do both the initial and final phase of measurement, even if one of those phases is not explicitly being used within CalibrateTask itself.",1,DM-1041,datamanagement,eliminate confuse config effect calibratetask calibratetask unexpected thing differently configure certain way perceive certain processing necessary feed step particular disable astrometry photometric calibration run measurement assume purpose post psf measurement feed algorithm poor test coverage easy break calibratetask case option disable branch confer simon andy think good solution remove sort conditional processing calibratetask easy read instead initial final phase measurement phase explicitly calibratetask,"eliminate confusing config side-effects in CalibrateTask CalibrateTask does some unexpected things differently if you configure it certain ways, because it perceives certain processing as only being necessary to feed other steps. In particular, if you disable astrometry and photometric calibration, it only runs measurement once, because it assumes the only purpose of the post-PSF measurement is to feed those algorithms. This (as well as poor test coverage) made it easy to break CalibrateTask in the case where those options are disabled a few branches back. After conferring with Simon and Andy, we think the best solution is to remove this sort of conditional processing from CalibrateTask, which should also make it much easier to read. Instead, we'll always do both the initial and final phase of measurement, even if one of those phases is not explicitly being used within CalibrateTask itself."
Create a permanent and accessible mapping of the BB# and the bNNN.  Create a permanent and accessible mapping of the BB# and the bNNN.   The users are interested in the BB# since is is used to point to the STDIO file form the entire stack build. The bNNN is needed because the daily life of the developer revolves around the stack tagged alternately by the bNNN tags and/or the DM Release tags. ,2,DM-1045,datamanagement,create permanent accessible mapping bb bnnn create permanent accessible mapping bb bnnn user interested bb point stdio file form entire stack build bnnn need daily life developer revolve stack tag alternately bnnn tag and/or dm release tag,Create a permanent and accessible mapping of the BB# and the bNNN. Create a permanent and accessible mapping of the BB# and the bNNN. The users are interested in the BB# since is is used to point to the STDIO file form the entire stack build. The bNNN is needed because the daily life of the developer revolves around the stack tagged alternately by the bNNN tags and/or the DM Release tags.
"init.d/qserv-czar needs LD_LIBRARY path With the addition of log we now need to find some shared libraries from stack. Current version of qserv-czar init.d script does not capture LD_LIBRARY_PATH, so we should add it there. ",1,DM-1054,datamanagement,init.d qserv czar need ld_library path addition log need find share library stack current version qserv czar init.d script capture ld_library_path add,"init.d/qserv-czar needs LD_LIBRARY path With the addition of log we now need to find some shared libraries from stack. Current version of qserv-czar init.d script does not capture LD_LIBRARY_PATH, so we should add it there."
"Remove unnecessary pieces from qserv czar config The config file for the qserv czar has some items that are no longer relevant, and in this issue, we focus on the ones that are clearly the responsibility of our qserv css.  This ticket includes: -- removing these items from the installation/configuration templates -- removing these items from sample configuration files -- removing these items from the code that reads in the configuration file and sets defaults for these items -- fixing things that seem to break as a result of this cleanup.  danielw volunteers to assist on the last item, as needed.  ",2,DM-1055,datamanagement,remove unnecessary piece qserv czar config config file qserv czar item long relevant issue focus one clearly responsibility qserv css ticket include remove item installation configuration template remove item sample configuration file remove item code read configuration file set default item fix thing break result cleanup danielw volunteer assist item need,"Remove unnecessary pieces from qserv czar config The config file for the qserv czar has some items that are no longer relevant, and in this issue, we focus on the ones that are clearly the responsibility of our qserv css. This ticket includes: -- removing these items from the installation/configuration templates -- removing these items from sample configuration files -- removing these items from the code that reads in the configuration file and sets defaults for these items -- fixing things that seem to break as a result of this cleanup. danielw volunteers to assist on the last item, as needed."
"fix SubSchema handling of ""."" and ""_"" SubSchema didn't get included in the rest of the switch from ""."" to ""_"" as a field name separator.  As part of fixing this, we should also be able to simplify the code in the slot definers in SourceTable.",1,DM-1058,datamanagement,fix subschema handling subschema include rest switch field separator fix able simplify code slot definer sourcetable,"fix SubSchema handling of ""."" and ""_"" SubSchema didn't get included in the rest of the switch from ""."" to ""_"" as a field name separator. As part of fixing this, we should also be able to simplify the code in the slot definers in SourceTable."
"track down difference in SdssShape implementation The meas_base version of SdssShape produces slightly different outputs from the original version in meas_algorithms, but these should be identical.  We should understand this difference rather than assume its benign just because it's small.",2,DM-1059,datamanagement,track difference sdssshape implementation meas_base version sdssshape produce slightly different output original version meas_algorithm identical understand difference assume benign small,"track down difference in SdssShape implementation The meas_base version of SdssShape produces slightly different outputs from the original version in meas_algorithms, but these should be identical. We should understand this difference rather than assume its benign just because it's small."
"move algorithm implementations out of separate subdirectory We should move the code in the algorithms subdirectory (and namespace) into the .cc files that correspond to individual algorithms.  They should generally go into anonymous namespaces there.  After doing so, we should do one more test to compare the meas_base and meas_algorithms implementations.",1,DM-1067,datamanagement,algorithm implementation separate subdirectory code algorithm subdirectory namespace .cc file correspond individual algorithm generally anonymous namespace test compare meas_base meas_algorithms implementation,"move algorithm implementations out of separate subdirectory We should move the code in the algorithms subdirectory (and namespace) into the .cc files that correspond to individual algorithms. They should generally go into anonymous namespaces there. After doing so, we should do one more test to compare the meas_base and meas_algorithms implementations."
"audit and clean up algorithm flag and config usage Check that meas_base plugins and algorithms have appropriate config options and flags (mainly, check that there are no unused config options or flags due to copy/paste relics).",1,DM-1068,datamanagement,audit clean algorithm flag config usage check meas_base plugin algorithm appropriate config option flag mainly check unused config option flag copy paste relic,"audit and clean up algorithm flag and config usage Check that meas_base plugins and algorithms have appropriate config options and flags (mainly, check that there are no unused config options or flags due to copy/paste relics)."
"switch default table version to 1 Now that all tasks that use catalogs explicitly set the table version, it should be relatively straightforward to set the default version to 1 in afw.  Code that cannot handle version > 0 tables should continue to explicitly set version=0.",2,DM-1070,datamanagement,switch default table version task use catalog explicitly set table version relatively straightforward set default version afw code handle version table continue explicitly set version=0,"switch default table version to 1 Now that all tasks that use catalogs explicitly set the table version, it should be relatively straightforward to set the default version to 1 in afw. Code that cannot handle version > 0 tables should continue to explicitly set version=0."
"Switch default measurement tasks to meas_base We should set the default measurement task in ProcessImageTask to SingleFrameMeasurementTask, and note that SourceMeasurementTask and the old forced photometry drivers are deprecated.",2,DM-1071,datamanagement,switch default measurement task meas_base set default measurement task processimagetask singleframemeasurementtask note sourcemeasurementtask old force photometry driver deprecate,"Switch default measurement tasks to meas_base We should set the default measurement task in ProcessImageTask to SingleFrameMeasurementTask, and note that SourceMeasurementTask and the old forced photometry drivers are deprecated."
create forced wrappers for algorithms We have multiple algorithms in meas_base which could be used in forced mode but have no forced plugin.  We should go through the algorithms we have implemented and create forced plugin wrappers for these.,1,DM-1072,datamanagement,create force wrapper algorithm multiple algorithm meas_base force mode force plugin algorithm implement create force plugin wrapper,create forced wrappers for algorithms We have multiple algorithms in meas_base which could be used in forced mode but have no forced plugin. We should go through the algorithms we have implemented and create forced plugin wrappers for these.
"remove old forced photometry tasks After meas_base has been fully integrated, remove the old forced photometry tasks from pipe_tasks",1,DM-1073,datamanagement,remove old force photometry task meas_base fully integrate remove old force photometry task pipe_task,"remove old forced photometry tasks After meas_base has been fully integrated, remove the old forced photometry tasks from pipe_tasks"
"convert afw::table unit tests to version 1 Most afw::table unit tests explicitly set version 0.  We should change these to test the new behaviors, not the deprecated ones.",2,DM-1076,datamanagement,convert afw::table unit test version afw::table unit test explicitly set version change test new behavior deprecate one,"convert afw::table unit tests to version 1 Most afw::table unit tests explicitly set version 0. We should change these to test the new behaviors, not the deprecated ones."
"Audit TCT recommendations to ensure that all standards updates were installed into Standards documents. Audit TCT recommendations to ensure that all standards updates were installed into Standards documents.  It was found that the meeting recorded in: [https://dev.lsstcorp.org/trac/wiki/Winter2012/CodingStandardsChanges] failed to include two recommendations:   * recommended: 3-30: I find the Error suffix to be usually more appropriate than Exception. ** current: 3-30. Exception classes SHOULD be suffixed with Exception.  * recommended but not specifically included: Namespaces in source files: we should use namespace blocks in source files, and prefer unqualified (or less-qualified) names within those blocks over global-namespace aliases. ** Rule 3-6 is an amalgam of namespace rules which doesn't quite have the particulars desired. FYI: The actual vote was to:  ""Allow namespace blocks in source code (cc) files.""  To simplify the future audit, all other recommendations in that specific meeting were verified as installed into the standards.",2,DM-1077,datamanagement,audit tct recommendation ensure standard update instal standards document audit tct recommendation ensure standard update instal standards document find meeting record https://dev.lsstcorp.org/trac/wiki/winter2012/codingstandardschanges fail include recommendation recommend 30 find error suffix usually appropriate exception current 30 exception class suffix exception recommend specifically include namespace source file use namespace block source file prefer unqualified qualify name block global namespace alias rule amalgam namespace rule particular desire fyi actual vote allow namespace block source code cc file simplify future audit recommendation specific meeting verify instal standard,"Audit TCT recommendations to ensure that all standards updates were installed into Standards documents. Audit TCT recommendations to ensure that all standards updates were installed into Standards documents. It was found that the meeting recorded in: [https://dev.lsstcorp.org/trac/wiki/Winter2012/CodingStandardsChanges] failed to include two recommendations: * recommended: 3-30: I find the Error suffix to be usually more appropriate than Exception. ** current: 3-30. Exception classes SHOULD be suffixed with Exception. * recommended but not specifically included: Namespaces in source files: we should use namespace blocks in source files, and prefer unqualified (or less-qualified) names within those blocks over global-namespace aliases. ** Rule 3-6 is an amalgam of namespace rules which doesn't quite have the particulars desired. FYI: The actual vote was to: ""Allow namespace blocks in source code (cc) files."" To simplify the future audit, all other recommendations in that specific meeting were verified as installed into the standards."
add batch flag to newinstall.sh In some cases (installing in some script environment) it is nice to be able to run without any interaction.  This will add a flag to tell the script to install without asking about anaconda and git.  It will assume the answer is 'yes' to any question it would have asked.,1,DM-1078,datamanagement,add batch flag newinstall.sh case instal script environment nice able run interaction add flag tell script install ask anaconda git assume answer yes question ask,add batch flag to newinstall.sh In some cases (installing in some script environment) it is nice to be able to run without any interaction. This will add a flag to tell the script to install without asking about anaconda and git. It will assume the answer is 'yes' to any question it would have asked.
"Fix overload problems in SourceCatalog.append and .extend This example fails with an exception: {code:py} import lsst.afw.table as afwTable schema = afwTable.SourceTable.makeMinimalSchema() st = afwTable.SourceTable.make(schema) cat = afwTable.SourceCatalog(st) tmp = afwTable.SourceCatalog(cat.getTable()) cat.extend(tmp) {code}  Expected behavior is that the last line is equivalent to {{cat.extend(tmp, deep=False)}}.",1,DM-1083,datamanagement,fix overload problem sourcecatalog.append example fail exception code py import lsst.afw.table afwtable schema afwtable sourcetable.makeminimalschema st afwtable sourcetable.make(schema cat afwtable sourcecatalog(st tmp afwtable sourcecatalog(cat.gettable cat.extend(tmp code expect behavior line equivalent cat.extend(tmp deep false,"Fix overload problems in SourceCatalog.append and .extend This example fails with an exception: {code:py} import lsst.afw.table as afwTable schema = afwTable.SourceTable.makeMinimalSchema() st = afwTable.SourceTable.make(schema) cat = afwTable.SourceCatalog(st) tmp = afwTable.SourceCatalog(cat.getTable()) cat.extend(tmp) {code} Expected behavior is that the last line is equivalent to {{cat.extend(tmp, deep=False)}}."
"Update the DMS and Astro Glossaries The DMS and Astro Glossaries in Confluence define a set of technical terms used in their respective domains. Some of the definitions are placeholders, and other terms used in the SWUG, DM Space, and DM Developer Guide have yet to be defined in one glossary or the other. It is time to update these docs.",2,DM-1085,datamanagement,update dms astro glossaries dms astro glossaries confluence define set technical term respective domain definition placeholder term swug dm space dm developer guide define glossary time update doc,"Update the DMS and Astro Glossaries The DMS and Astro Glossaries in Confluence define a set of technical terms used in their respective domains. Some of the definitions are placeholders, and other terms used in the SWUG, DM Space, and DM Developer Guide have yet to be defined in one glossary or the other. It is time to update these docs."
Determine problem with Mac OS X processCCD output when compared to 'identical' dataset generated on RHEL6 The benchmark file for the summer2012 demo is generated on  RHEL6. When using the dataset to compare stack output generated on Mac OSX 10.8.5 (and 10.9)  there are significant deviations.   Find out where the problem arises...in the stack during processing or in the comparison.,4,DM-1086,datamanagement,determine problem mac os processccd output compare identical dataset generate rhel6 benchmark file summer2012 demo generate rhel6 dataset compare stack output generate mac osx 10.8.5 10.9 significant deviation find problem arise stack processing comparison,Determine problem with Mac OS X processCCD output when compared to 'identical' dataset generated on RHEL6 The benchmark file for the summer2012 demo is generated on RHEL6. When using the dataset to compare stack output generated on Mac OSX 10.8.5 (and 10.9) there are significant deviations. Find out where the problem arises...in the stack during processing or in the comparison.
"Research how to kill query in mysql In the port to the new Xrootd Ssi API, worker-side squashing was lost in the shuffle. The plumbing is different, and re-implementing squash functionality is not entirely straightforward, especially because the new API is still missing documentation and examples for implementing cancellation.  The consequences of not implementing this are minor--some extra work may be done by the worker, but not a whole lot, because user-level cancellation has not been implemented.  First step is to understand how mysql code is handling query killing.",4,DM-1087,datamanagement,research kill query mysql port new xrootd ssi api worker squashing lose shuffle plumbing different implement squash functionality entirely straightforward especially new api miss documentation example implement cancellation consequence implement minor extra work worker lot user level cancellation implement step understand mysql code handle query kill,"Research how to kill query in mysql In the port to the new Xrootd Ssi API, worker-side squashing was lost in the shuffle. The plumbing is different, and re-implementing squash functionality is not entirely straightforward, especially because the new API is still missing documentation and examples for implementing cancellation. The consequences of not implementing this are minor--some extra work may be done by the worker, but not a whole lot, because user-level cancellation has not been implemented. First step is to understand how mysql code is handling query killing."
"Investigate HTCondor config settings to control speed of ClassAd propagation With default settings we do not have good visibility as to whether an updated ClassAd on a compute node (e.g., CacheDataList now has ccd ""S00"") will be in effect on the submit node in time for a Job to be matched to an optimal HTCondor node/slot.   There are several components (negotiator, schedd, startd) and their associated activities that could impact the time that it takes for a new ClassAd on a worker node to 'propagate' back to the submit side. We investigate these configuration settings to try to determine what thresholds for configuration settings are required to meet a given time cadence of job submissions.",2,DM-1088,datamanagement,investigate htcondor config setting control speed classad propagation default setting good visibility update classad compute node e.g. cachedatalist ccd s00 effect submit node time job match optimal htcondor node slot component negotiator schedd startd associate activity impact time take new classad worker node propagate submit investigate configuration setting try determine threshold configuration setting require meet give time cadence job submission,"Investigate HTCondor config settings to control speed of ClassAd propagation With default settings we do not have good visibility as to whether an updated ClassAd on a compute node (e.g., CacheDataList now has ccd ""S00"") will be in effect on the submit node in time for a Job to be matched to an optimal HTCondor node/slot. There are several components (negotiator, schedd, startd) and their associated activities that could impact the time that it takes for a new ClassAd on a worker node to 'propagate' back to the submit side. We investigate these configuration settings to try to determine what thresholds for configuration settings are required to meet a given time cadence of job submissions."
"Enhance SWUG chapter on measurement A basic description of measurement in the LSST Stack was created for the SWUG (see DM-692). However, this chapter lacks specifics about how the measurement algorithms work and does not yet mention some of the available algoritms. This task is intended to provide the next level of detail, with references to source-level descriptions that exist or are in progress. ",6,DM-1089,datamanagement,enhance swug chapter measurement basic description measurement lsst stack create swug dm-692 chapter lack specific measurement algorithm work mention available algoritms task intend provide level detail reference source level description exist progress,"Enhance SWUG chapter on measurement A basic description of measurement in the LSST Stack was created for the SWUG (see DM-692). However, this chapter lacks specifics about how the measurement algorithms work and does not yet mention some of the available algoritms. This task is intended to provide the next level of detail, with references to source-level descriptions that exist or are in progress."
Design unit-test data package Gather requirements for unit test datasets and determine how to organize and generate the data.,4,DM-1103,datamanagement,design unit test datum package gather requirement unit test dataset determine organize generate datum,Design unit-test data package Gather requirements for unit test datasets and determine how to organize and generate the data.
"Custom mapper for unit test data package We need a custom mapper for the test data package, so we can run unit tests against it without depending on any particular obs_* package.",6,DM-1105,datamanagement,custom mapper unit test datum package need custom mapper test datum package run unit test depend particular obs package,"Custom mapper for unit test data package We need a custom mapper for the test data package, so we can run unit tests against it without depending on any particular obs_* package."
"Improve documentation of code from DM-70 DM-70 was merged before its code documentation was completely ready, in an effort to not leave such a large amount of code un-merged.  This ticket exists as the second half of DM-70 to clean up its documentation.",6,DM-1118,datamanagement,improve documentation code dm-70 dm-70 merge code documentation completely ready effort leave large code un merged ticket exist second half dm-70 clean documentation,"Improve documentation of code from DM-70 DM-70 was merged before its code documentation was completely ready, in an effort to not leave such a large amount of code un-merged. This ticket exists as the second half of DM-70 to clean up its documentation."
"Chebyshev approximation object for aperture corrections Using the interface defined in DM-740, implement a Chebyshev-based implementation to be used to interpolate aperture corrections across CCD-level interfaces.  A prototype is available on the HSC fork, which can be copied directly, modulo any interface changes on DM-740.  For more information, see the HSC Jira issue (which also includes the work associated with DM-740): https://hsc-jira.astro.princeton.edu/jira/browse/HSC-796 and the HSC git commits https://github.com/HyperSuprime-Cam/afw/compare/releases/S14A_0...tickets/DM-796",3,DM-1124,datamanagement,chebyshev approximation object aperture correction interface define dm-740 implement chebyshev base implementation interpolate aperture correction ccd level interface prototype available hsc fork copy directly modulo interface change dm-740 information hsc jira issue include work associate dm-740 https://hsc-jira.astro.princeton.edu/jira/browse/hsc-796 hsc git commit https://github.com/hypersuprime-cam/afw/compare/releases/s14a_0...tickets/dm-796,"Chebyshev approximation object for aperture corrections Using the interface defined in DM-740, implement a Chebyshev-based implementation to be used to interpolate aperture corrections across CCD-level interfaces. A prototype is available on the HSC fork, which can be copied directly, modulo any interface changes on DM-740. For more information, see the HSC Jira issue (which also includes the work associated with DM-740): https://hsc-jira.astro.princeton.edu/jira/browse/HSC-796 and the HSC git commits https://github.com/HyperSuprime-Cam/afw/compare/releases/S14A_0...tickets/DM-796"
"avoid usage of measurement framework in star selectors At least one of the star selectors uses the old measurement framework system to measure the moments of a cloud of points.  With the new versions of all the measurement plugins, it should be much easier (and cleaner) to just call the SdssShape algorithm directly, instead of dealing with the complexity of applying the measurement framework to something that isn't really an image.",3,DM-1125,datamanagement,avoid usage measurement framework star selector star selector use old measurement framework system measure moment cloud point new version measurement plugin easy clean sdssshape algorithm directly instead deal complexity apply measurement framework image,"avoid usage of measurement framework in star selectors At least one of the star selectors uses the old measurement framework system to measure the moments of a cloud of points. With the new versions of all the measurement plugins, it should be much easier (and cleaner) to just call the SdssShape algorithm directly, instead of dealing with the complexity of applying the measurement framework to something that isn't really an image."
"design new Footprint API This issue is for *planning* (not implementing) some changes to Footprint's interface, including the following:  - make Footprint immutable  - create a separate SpanRegion class that holds Spans and provides geometric operators does not hold Peaks or a ""region"" bbox (Footprint would then hold one of these).  - many operations currently implemented as free functions should be moved to methods  - we should switch the container from vector<PTR(Span)> to simply vector<Span>, as Span is nonpolymorphic and at last as cheap to copy as a shared_ptr.  The output of this issue will be a set of header files that define the new interface, signed off by an SAT design review.  Other issues will be responsible for implementing the new interface and fixing code broken by the change.",8,DM-1126,datamanagement,design new footprint api issue plan implement change footprint interface include follow footprint immutable create separate spanregion class hold spans provide geometric operator hold peaks region bbox footprint hold operation currently implement free function move method switch container vector simply vector span nonpolymorphic cheap copy shared_ptr output issue set header file define new interface sign sat design review issue responsible implement new interface fix code break change,"design new Footprint API This issue is for *planning* (not implementing) some changes to Footprint's interface, including the following: - make Footprint immutable - create a separate SpanRegion class that holds Spans and provides geometric operators does not hold Peaks or a ""region"" bbox (Footprint would then hold one of these). - many operations currently implemented as free functions should be moved to methods - we should switch the container from vector to simply vector, as Span is nonpolymorphic and at last as cheap to copy as a shared_ptr. The output of this issue will be a set of header files that define the new interface, signed off by an SAT design review. Other issues will be responsible for implementing the new interface and fixing code broken by the change."
"create PSF simulations for shapelet approximation test To determine the number and order of shapelet expansions needed to approximate the LSST PSF, we need simulations of LSST PSFs, presumably generated using PhoSim.  I think we want something like 50-100 extremely high-SNR stars, drawn from realistic distributions of anything that would affect the PSF, including position on the focal plane and center position within a pixel.  I only care about getting these out as postage stamps, so I'll leave the question of how best to run PhoSim to get these up to someone else.  While this is part of a mostly-Princeton Epic, I'm assigning this issue to Simon as he'd be able to do it much faster than I could.  I'll also let him review and update the story points estimate.",6,DM-1131,datamanagement,create psf simulation shapelet approximation test determine number order shapelet expansion need approximate lsst psf need simulation lsst psf presumably generate phosim think want like 50 100 extremely high snr star draw realistic distribution affect psf include position focal plane center position pixel care get postage stamp leave question good run phosim princeton epic assign issue simon able fast let review update story point estimate,"create PSF simulations for shapelet approximation test To determine the number and order of shapelet expansions needed to approximate the LSST PSF, we need simulations of LSST PSFs, presumably generated using PhoSim. I think we want something like 50-100 extremely high-SNR stars, drawn from realistic distributions of anything that would affect the PSF, including position on the focal plane and center position within a pixel. I only care about getting these out as postage stamps, so I'll leave the question of how best to run PhoSim to get these up to someone else. While this is part of a mostly-Princeton Epic, I'm assigning this issue to Simon as he'd be able to do it much faster than I could. I'll also let him review and update the story points estimate."
"implement SDSS PSF residual trick SDSS galaxy fitting approximated the convolution of a galaxy model with the PSF as the convolution of the galaxy model with a simplified approximation to the PSF added to the difference between the PSF approximation and the true PSF.  We should do the same, as it'll be no worse than ignoring the difference between the PSF approximation and the true PSF, and it may greatly reduce the complexity needed in the approximation.",8,DM-1134,datamanagement,implement sdss psf residual trick sdss galaxy fitting approximate convolution galaxy model psf convolution galaxy model simplified approximation psf add difference psf approximation true psf bad ignore difference psf approximation true psf greatly reduce complexity need approximation,"implement SDSS PSF residual trick SDSS galaxy fitting approximated the convolution of a galaxy model with the PSF as the convolution of the galaxy model with a simplified approximation to the PSF added to the difference between the PSF approximation and the true PSF. We should do the same, as it'll be no worse than ignoring the difference between the PSF approximation and the true PSF, and it may greatly reduce the complexity needed in the approximation."
"Demonstrate & iterate with team on documentation toolchain    Following from DM-1137, this epic relates to demonstrating various options for documentation tools workflows to the team, gathering input as to the preferred solution, adopting a workflow, and defining any specific implementation choices.   This is part of curating our documentation infrastructure. ",5,DM-1138,datamanagement,demonstrate iterate team documentation toolchain follow dm-1137 epic relate demonstrate option documentation tool workflow team gather input preferred solution adopt workflow define specific implementation choice curate documentation infrastructure,"Demonstrate & iterate with team on documentation toolchain Following from DM-1137, this epic relates to demonstrating various options for documentation tools workflows to the team, gathering input as to the preferred solution, adopting a workflow, and defining any specific implementation choices. This is part of curating our documentation infrastructure."
Investigate automatic MacOS X build/deploy This item is to set up the same continuous integration process we have on Linux on a MacOSX test server.   JK: In PMCS this would be Economou F and New Hire LS3,4,DM-1140,datamanagement,investigate automatic macos build deploy item set continuous integration process linux macosx test server jk pmcs economou new hire ls3,Investigate automatic MacOS X build/deploy This item is to set up the same continuous integration process we have on Linux on a MacOSX test server. JK: In PMCS this would be Economou F and New Hire LS3
"Regularise Nightly and Weekly builds  The intent here is to create two seperate automated deployment environments, one based on a nightly (or ad-hoc) build, one one  aslower cadence (eg weekly). This will allow us to do intergration/QA runs on a bleeding or trailing edge as required.   JK: In PMCS this would be Economou F and New Hire LS3",6,DM-1142,datamanagement,regularise nightly weekly build intent create seperate automate deployment environment base nightly ad hoc build aslower cadence eg weekly allow intergration qa run bleeding trail edge require jk pmcs economou new hire ls3,"Regularise Nightly and Weekly builds The intent here is to create two seperate automated deployment environments, one based on a nightly (or ad-hoc) build, one one aslower cadence (eg weekly). This will allow us to do intergration/QA runs on a bleeding or trailing edge as required. JK: In PMCS this would be Economou F and New Hire LS3"
"Create a top-level qserv_distrib package qserv_distrib will be a meta-package embedding qserv, qserv_testdata and partition.",2,DM-1147,datamanagement,create level qserv_distrib package qserv_distrib meta package embed qserv qserv_testdata partition,"Create a top-level qserv_distrib package qserv_distrib will be a meta-package embedding qserv, qserv_testdata and partition."
Fix example of IsrTask to be callable with data on disk Currently the example of the IsrTask takes a fake dataref.  This is hard to use with real data.  In DM-1113 we will update IsrTask to not take a dataRef.  This will make it easy to update the example script to work with real data.  This ticket will also include removing from the unit tests any fake dataRefs that have become unnecessary as a result of DM-1299.  ,2,DM-1151,datamanagement,fix example isrtask callable datum disk currently example isrtask take fake dataref hard use real datum dm-1113 update isrtask dataref easy update example script work real datum ticket include remove unit test fake dataref unnecessary result dm-1299,Fix example of IsrTask to be callable with data on disk Currently the example of the IsrTask takes a fake dataref. This is hard to use with real data. In DM-1113 we will update IsrTask to not take a dataRef. This will make it easy to update the example script to work with real data. This ticket will also include removing from the unit tests any fake dataRefs that have become unnecessary as a result of DM-1299.
"Css C++ client needs to auto-reconnect The zookeeper client in C++ that the czar uses doesn't auto-reconnect. This is a capability provided in the kazoo library that qserv's python layer provides, but isn't provided in the c++ client.  The zookeeper client disconnects pretty easily: if you step through your code in gdb, the zk client will probably disconnect because its threads expect to keep running. zk sessions may expire too. Our layer should reconnect unless there is really no way to recover without assistance from the calling code (e.g. configuration is wrong, etc.).  This ticket includes only basic reconnection attempting, throwing an exception only when some ""reconnection-is-impossible"" condition is met.",2,DM-1152,datamanagement,css c++ client need auto reconnect zookeeper client c++ czar use auto reconnect capability provide kazoo library qserv python layer provide provide c++ client zookeeper client disconnect pretty easily step code gdb zk client probably disconnect thread expect run zk session expire layer reconnect way recover assistance call code e.g. configuration wrong etc ticket include basic reconnection attempt throw exception reconnection impossible condition meet,"Css C++ client needs to auto-reconnect The zookeeper client in C++ that the czar uses doesn't auto-reconnect. This is a capability provided in the kazoo library that qserv's python layer provides, but isn't provided in the c++ client. The zookeeper client disconnects pretty easily: if you step through your code in gdb, the zk client will probably disconnect because its threads expect to keep running. zk sessions may expire too. Our layer should reconnect unless there is really no way to recover without assistance from the calling code (e.g. configuration is wrong, etc.). This ticket includes only basic reconnection attempting, throwing an exception only when some ""reconnection-is-impossible"" condition is met."
"Minor problems in lsstsw, related to Qserv offline install procedure - on lsstsw master branch tip, ./stack/Linux64/lsst/9.2/bin/newinstall.sh doesn't seems to be the last version (it still install eups-1.3.0 instead od eups-1.5.0)   - on Fedora19, flock from util-linux 2.23.1 doesn't support next options : {code:bash} $ flock -w 0 200 flock: timeout cannot be zero {code} but {code:bash} flock -w 1 200 {code} works,  - newinstall.sh : would it be possible to enable automatic answers to git and anaconda install questions. For example, in order to easily enable automatic install on 300 nodes clusters ? (cancelled : covered by DM-1078)  - loadLSST.sh appends automatically http://sw.lsstcorp.org/eupspkg to EUPS_PKGROOT, and if first url in EUPS_PKGROOT isn't available eups fails without trying next ones => this isn't compliant with offline mode and introduce a work-around in Qserv offline mode install scripts. Would it be possible to define a lightly different behaviour for loadLSST.sh ?  - In newinstall.sh, l. 167, if Python version isn't correct, then exit *with error code*. ",3,DM-1153,datamanagement,minor problem lsstsw relate qserv offline install procedure lsstsw master branch tip ./stack linux64 lsst/9.2 bin newinstall.sh version install eups-1.3.0 instead od eups-1.5.0 fedora19 flock util linux 2.23.1 support option code bash flock -w 200 flock timeout zero code code bash flock 200 code work newinstall.sh possible enable automatic answer git anaconda install question example order easily enable automatic install 300 node cluster cancel cover dm-1078 loadlsst.sh append automatically http://sw.lsstcorp.org/eupspkg eups_pkgroot url eups_pkgroot available eup fail try one compliant offline mode introduce work qserv offline mode install script possible define lightly different behaviour loadlsst.sh newinstall.sh l. 167 python version correct exit error code,"Minor problems in lsstsw, related to Qserv offline install procedure - on lsstsw master branch tip, ./stack/Linux64/lsst/9.2/bin/newinstall.sh doesn't seems to be the last version (it still install eups-1.3.0 instead od eups-1.5.0) - on Fedora19, flock from util-linux 2.23.1 doesn't support next options : {code:bash} $ flock -w 0 200 flock: timeout cannot be zero {code} but {code:bash} flock -w 1 200 {code} works, - newinstall.sh : would it be possible to enable automatic answers to git and anaconda install questions. For example, in order to easily enable automatic install on 300 nodes clusters ? (cancelled : covered by DM-1078) - loadLSST.sh appends automatically http://sw.lsstcorp.org/eupspkg to EUPS_PKGROOT, and if first url in EUPS_PKGROOT isn't available eups fails without trying next ones => this isn't compliant with offline mode and introduce a work-around in Qserv offline mode install scripts. Would it be possible to define a lightly different behaviour for loadLSST.sh ? - In newinstall.sh, l. 167, if Python version isn't correct, then exit *with error code*."
SUI:  Query and display LSST image  Define the APIs for image query with database group.   Depend on the implementation of APIs  Exercise the image cutout service DM-1977 been developed in SLAC.     ,6,DM-1156,datamanagement,sui query display lsst image define api image query database group depend implementation api exercise image cutout service dm-1977 develop slac,SUI: Query and display LSST image Define the APIs for image query with database group. Depend on the implementation of APIs Exercise the image cutout service DM-1977 been developed in SLAC.
"Cleanup SdssShape We should do a comprehensive cleanup of the SdssShapeAlgorithm class.  This includes removing the SdssShapeImpl interface (never supposed to have been public, but it became public) from other code that uses it, and integrating this code directly into the algorithm class.  We should also ensure that the source from which the algorithm is derived is clearly cited -- that's Bernstein and Jarvis (2002, http://adsabs.harvard.edu/abs/2002AJ....123..583B); see also DM-2304.",8,DM-1161,datamanagement,cleanup sdssshape comprehensive cleanup sdssshapealgorithm class include remove sdssshapeimpl interface suppose public public code use integrate code directly algorithm class ensure source algorithm derive clearly cite bernstein jarvis 2002 http://adsabs.harvard.edu/abs/2002aj....123..583b dm-2304,"Cleanup SdssShape We should do a comprehensive cleanup of the SdssShapeAlgorithm class. This includes removing the SdssShapeImpl interface (never supposed to have been public, but it became public) from other code that uses it, and integrating this code directly into the algorithm class. We should also ensure that the source from which the algorithm is derived is clearly cited -- that's Bernstein and Jarvis (2002, http://adsabs.harvard.edu/abs/2002AJ....123..583B); see also DM-2304."
"Port meas_algorithms unit tests for plugins While test coverage isn't complete, there are many unit tests for measurement plugin algorithms in meas_algorithms that have not yet been ported to meas_base.  We should make sure any of these tests that aren't already covered in meas_base are moved over before we remove the meas_algorithms versions of things.",6,DM-1162,datamanagement,port meas_algorithm unit test plugin test coverage complete unit test measurement plugin algorithm meas_algorithm port meas_base sure test cover meas_base move remove meas_algorithm version thing,"Port meas_algorithms unit tests for plugins While test coverage isn't complete, there are many unit tests for measurement plugin algorithms in meas_algorithms that have not yet been ported to meas_base. We should make sure any of these tests that aren't already covered in meas_base are moved over before we remove the meas_algorithms versions of things."
"remove dependency between sconsUtils and eups sconsUtils currently depends on eups, (and as far as I understand it, it should not...)",4,DM-1164,datamanagement,remove dependency sconsutil eup sconsutils currently depend eup far understand,"remove dependency between sconsUtils and eups sconsUtils currently depends on eups, (and as far as I understand it, it should not...)"
"experiment with building MyISAM as a shared library Per Monty (phone call Aug 28, 2014), this should be easy, code to look at is in storage/myisam/mi_test*",2,DM-1167,datamanagement,experiment build myisam share library monty phone aug 28 2014 easy code look storage myisam mi_test,"experiment with building MyISAM as a shared library Per Monty (phone call Aug 28, 2014), this should be easy, code to look at is in storage/myisam/mi_test*"
"zookeeper port numbers should be configurable The test programs in core/modules/css/ has hardcoded zookeeper port numbers. That needs to be fixed, it needs to be configurable.",1,DM-1168,datamanagement,zookeeper port number configurable test program core module css/ hardcode zookeeper port number need fix need configurable,"zookeeper port numbers should be configurable The test programs in core/modules/css/ has hardcoded zookeeper port numbers. That needs to be fixed, it needs to be configurable."
"User-friendly install/configure/test scripts - stdout, stderr output should be colorized, whereas redirecting it into a file shouldn't.",3,DM-1169,datamanagement,user friendly install configure test script stdout stderr output colorize redirect file,"User-friendly install/configure/test scripts - stdout, stderr output should be colorized, whereas redirecting it into a file shouldn't."
"Make default image origin PARENT in all cases After DM-840 is finished, and code updated for it (DM-846), make the new default image origin PARENT, remove the UNDEFINED image origin enum, and update at least some of the code that explicitly specifies PARENT (there is little harm in leaving this explicit, other than it does not set an optimal example).",4,DM-1176,datamanagement,default image origin parent case dm-840 finish code update dm-846 new default image origin parent remove undefined image origin enum update code explicitly specify parent little harm leave explicit set optimal example,"Make default image origin PARENT in all cases After DM-840 is finished, and code updated for it (DM-846), make the new default image origin PARENT, remove the UNDEFINED image origin enum, and update at least some of the code that explicitly specifies PARENT (there is little harm in leaving this explicit, other than it does not set an optimal example)."
"Channel was inactive for too long error There as an issue with the receiveEvent call that can cause an exception ""Channel was inactive for too long"".  The fix for this (according to the ActiveMQ users list and archives) is to either completely disable the inactivity monitor or to increase the inactivity limit to something extremely high.  The fix is adding:  ""wireFormat.maxInactivityDuration=0"" to the URL in establishing the connection.  There might also be a way of doing this directly in the ActiveMQ broker, but so far I haven't seen anything that would let me specify that.",2,DM-1185,datamanagement,channel inactive long error issue receiveevent cause exception channel inactive long fix accord activemq user list archive completely disable inactivity monitor increase inactivity limit extremely high fix add wireformat.maxinactivityduration=0 url establish connection way directly activemq broker far see let specify,"Channel was inactive for too long error There as an issue with the receiveEvent call that can cause an exception ""Channel was inactive for too long"". The fix for this (according to the ActiveMQ users list and archives) is to either completely disable the inactivity monitor or to increase the inactivity limit to something extremely high. The fix is adding: ""wireFormat.maxInactivityDuration=0"" to the URL in establishing the connection. There might also be a way of doing this directly in the ActiveMQ broker, but so far I haven't seen anything that would let me specify that."
"Learn the OCS middleware OCS will deliver the OCS Middleware software in November 2014.  There will be a workshop held at SLAC the week of November 10th.    I spoke with K-T and we should be able to attend this remotely, if necessary.  We need to spend some time getting familiar with this software and how it will integrate with the Base DMCS for AP.",6,DM-1187,datamanagement,learn ocs middleware ocs deliver ocs middleware software november 2014 workshop hold slac week november 10th speak able attend remotely necessary need spend time get familiar software integrate base dmcs ap,"Learn the OCS middleware OCS will deliver the OCS Middleware software in November 2014. There will be a workshop held at SLAC the week of November 10th. I spoke with K-T and we should be able to attend this remotely, if necessary. We need to spend some time getting familiar with this software and how it will integrate with the Base DMCS for AP."
"rewrite low-level shapelet evaluation code While trying to track down some bugs on DM-641, I've grown frustrated with the difficulty of testing the deeply-buried (i.e. interfaces I want to test are private) shapelet evaluation code there.  That sort of code really belongs in the shapelet package (not meas_multifit) anyway, where I have a lot of similar code, so on this issue I'm going to move it there and refactor the existing code so it all fits together better.",2,DM-1188,datamanagement,rewrite low level shapelet evaluation code try track bug dm-641 grow frustrate difficulty test deeply bury i.e. interface want test private shapelet evaluation code sort code belong shapelet package meas_multifit lot similar code issue go refactor exist code fit well,"rewrite low-level shapelet evaluation code While trying to track down some bugs on DM-641, I've grown frustrated with the difficulty of testing the deeply-buried (i.e. interfaces I want to test are private) shapelet evaluation code there. That sort of code really belongs in the shapelet package (not meas_multifit) anyway, where I have a lot of similar code, so on this issue I'm going to move it there and refactor the existing code so it all fits together better."
There is a bug in the prescan bbox for megacam. The bounding box of the prescan region in the megacam camera should have zero y extent (I think).  Instead it goes from y=-1 to y=2.  This is either a bug in the generation of the ampInfoTables or in the way the bounding boxes are interpreted.,1,DM-1195,datamanagement,bug prescan bbox megacam bounding box prescan region megacam camera zero extent think instead go y=-1 y=2 bug generation ampinfotable way bounding box interpret,There is a bug in the prescan bbox for megacam. The bounding box of the prescan region in the megacam camera should have zero y extent (I think). Instead it goes from y=-1 to y=2. This is either a bug in the generation of the ampInfoTables or in the way the bounding boxes are interpreted.
"exampleUtils in ip_isr is wrong about read corner https://dev.lsstcorp.org/cgit/LSST/DMS/ip_isr.git/tree/examples/exampleUtils.py#n95 Says that the read corner is in assembled coordinates.  This is not true, it is in the coordinates of the raw amp.  That is, if the raw amp is in electronic coordinates (like the lsstSim images) it is always LL, but if it is pre-assembled, it may be some other corner.  This should probably use the methods in cameraGeom.utils to do the image generation.",1,DM-1196,datamanagement,exampleutil ip_isr wrong read corner https://dev.lsstcorp.org/cgit/lsst/dms/ip_isr.git/tree/examples/exampleutils.py#n95 say read corner assembled coordinate true coordinate raw amp raw amp electronic coordinate like lsstsim image ll pre assemble corner probably use method camerageom.util image generation,"exampleUtils in ip_isr is wrong about read corner https://dev.lsstcorp.org/cgit/LSST/DMS/ip_isr.git/tree/examples/exampleUtils.py#n95 Says that the read corner is in assembled coordinates. This is not true, it is in the coordinates of the raw amp. That is, if the raw amp is in electronic coordinates (like the lsstSim images) it is always LL, but if it is pre-assembled, it may be some other corner. This should probably use the methods in cameraGeom.utils to do the image generation."
"Support some mixed-type operations for Point and Extent The current lack of automatic conversions in python is pretty irritating, and I think it's a big enough issue for people writing scripts that we should fix it.  In particular, allow {code} Point2D + Extent2I Point2D - Extent2I Point2D - Point2I  Extend2D + Extent2I Extend2D - Extent2I {code} (and the respective operations in the opposite order where well defined) It would also be good to allow the all functions expecting PointD to accept PointI, but I'm not sure if swig makes this possible.  It's probably not worth providing C++ overloads for all of these functions (and to be consistent we should probably do all or none).  I realize that you invented these types to avoid bare 2-tuples, but I'm not convinced that we shouldn't also provide overloads to transparently convert tuples to afwGeom objects.",2,DM-1197,datamanagement,support mixed type operation point extent current lack automatic conversion python pretty irritating think big issue people write script fix particular allow code point2d extent2i point2d extent2i point2d point2i extend2d extent2i extend2d extent2i code respective operation opposite order define good allow function expect pointd accept pointi sure swig make possible probably worth provide c++ overload function consistent probably realize invent type avoid bare tuple convince provide overload transparently convert tuple afwgeom object,"Support some mixed-type operations for Point and Extent The current lack of automatic conversions in python is pretty irritating, and I think it's a big enough issue for people writing scripts that we should fix it. In particular, allow {code} Point2D + Extent2I Point2D - Extent2I Point2D - Point2I Extend2D + Extent2I Extend2D - Extent2I {code} (and the respective operations in the opposite order where well defined) It would also be good to allow the all functions expecting PointD to accept PointI, but I'm not sure if swig makes this possible. It's probably not worth providing C++ overloads for all of these functions (and to be consistent we should probably do all or none). I realize that you invented these types to avoid bare 2-tuples, but I'm not convinced that we shouldn't also provide overloads to transparently convert tuples to afwGeom objects."
"add ColumnView support to FunctorKeys FunctorKeys currently only work with individual records, but at least some could work with columns as well.  Need to add an interface for this and decide how to handle cases where it the FunctorKey doesn't support it.",2,DM-1201,datamanagement,add columnview support functorkeys functorkeys currently work individual record work column need add interface decide handle case functorkey support,"add ColumnView support to FunctorKeys FunctorKeys currently only work with individual records, but at least some could work with columns as well. Need to add an interface for this and decide how to handle cases where it the FunctorKey doesn't support it."
"anaconda is too outdated to work with pip The version of anaconda distributed with the stack is too outdated to be used with pip (and probably other things). The issue is an unsafe version of ssh.  A workaround is to issue this command while anaconda is setup: {code} conda update conda {code} Warning: it is unwise to try to update anaconda itself (with ""conda update anaconda"") because that will revert some of the changes and may result in an unusable anaconda.  I think what is required is an obvious change to ups/eupspkg.cfg.sh  The current version of anaconda is 2.0.1 based on http://repo.continuum.io/archive/  Note: there is no component for anaconda. I will submit another ticket.",2,DM-1211,datamanagement,anaconda outdated work pip version anaconda distribute stack outdated pip probably thing issue unsafe version ssh workaround issue command anaconda setup code conda update conda code warning unwise try update anaconda conda update anaconda revert change result unusable anaconda think require obvious change ups eupspkg.cfg.sh current version anaconda 2.0.1 base http://repo.continuum.io/archive/ note component anaconda submit ticket,"anaconda is too outdated to work with pip The version of anaconda distributed with the stack is too outdated to be used with pip (and probably other things). The issue is an unsafe version of ssh. A workaround is to issue this command while anaconda is setup: {code} conda update conda {code} Warning: it is unwise to try to update anaconda itself (with ""conda update anaconda"") because that will revert some of the changes and may result in an unusable anaconda. I think what is required is an obvious change to ups/eupspkg.cfg.sh The current version of anaconda is 2.0.1 based on http://repo.continuum.io/archive/ Note: there is no component for anaconda. I will submit another ticket."
"cleanup order/grouping of header files We want: * header for the class * then system * then third party * then lsst * then qserv  We currently don't have the ""lsst"" group (with a few exceptions), and we call the last one ""local"" in most places.",1,DM-1213,datamanagement,cleanup order grouping header file want header class system party lsst qserv currently lsst group exception local place,"cleanup order/grouping of header files We want: * header for the class * then system * then third party * then lsst * then qserv We currently don't have the ""lsst"" group (with a few exceptions), and we call the last one ""local"" in most places."
"makeMaskedImage leaks memory Calling afw.image.makeMaskedImage leaks memory when called from Python, because it returns a raw pointer without telling Swig %newobject.  To fix it, it'd be better to just have it return by value instead of by pointer, though this might involve fixing some downstream C\+\+ code (Python code should not be affected).",1,DM-1215,datamanagement,makemaskedimage leak memory calling afw.image.makemaskedimage leak memory call python return raw pointer tell swig newobject fix well return value instead pointer involve fix downstream c\+\+ code python code affect,"makeMaskedImage leaks memory Calling afw.image.makeMaskedImage leaks memory when called from Python, because it returns a raw pointer without telling Swig %newobject. To fix it, it'd be better to just have it return by value instead of by pointer, though this might involve fixing some downstream C\+\+ code (Python code should not be affected)."
"compute linear parameter derivatives more intelligently in optimizer The numerical derivatives computed by the optimizer currently don't distinguish between the linear parameters (for which derivatives are trivial) and nonlinear parameters (for which they're hard), because we don't pass the information that distinguishes them to the object that computes the derivatives.  If we move the computation of derivatives from the Optimizer class to the Objective class, we should be able to compute the derivatives much more efficiently.  While this doesn't matter much when fitting single component galaxy models (because there's only one linear parameter in that case), it should matter quite a bit when fitting high-order shapelets to PSF models.",4,DM-1216,datamanagement,compute linear parameter derivative intelligently optimizer numerical derivative compute optimizer currently distinguish linear parameter derivative trivial nonlinear parameter hard pass information distinguish object compute derivative computation derivative optimizer class objective class able compute derivative efficiently matter fitting single component galaxy model linear parameter case matter bit fitting high order shapelet psf model,"compute linear parameter derivatives more intelligently in optimizer The numerical derivatives computed by the optimizer currently don't distinguish between the linear parameters (for which derivatives are trivial) and nonlinear parameters (for which they're hard), because we don't pass the information that distinguishes them to the object that computes the derivatives. If we move the computation of derivatives from the Optimizer class to the Objective class, we should be able to compute the derivatives much more efficiently. While this doesn't matter much when fitting single component galaxy models (because there's only one linear parameter in that case), it should matter quite a bit when fitting high-order shapelets to PSF models."
"Refactor meas_base Python wrappers and plugin registration meas_base currently has a single Swig library (like most packages), defined within a single .i file (like some packages).  It also registers all of its plugins in a single python module, plugins.py.  Instead, it should:  - Have two Swig libraries: one for the interfaces and helper classes, and one for plugin algorithms.  Most downstream packages will only want to %import (and hence #include) the interface, and having them build against everything slows the build down unnecessarily.  The package __init__.py should import all symbols from both libraries, so the change would be transparent to the user.  - Have separate .i files for each algorithm or small group of algorithms.  Each of these could %import the interface library file and the pure-Python registry code, and then register the plugins wrapped there within a %pythoncode block.  That'd make the implementation of the algorithms a bit less scattered throughout the package, making them easier to maintain and better examples for new plugins.",3,DM-1217,datamanagement,refactor meas_base python wrapper plugin registration meas_base currently single swig library like package define single .i file like package register plugin single python module plugins.py instead swig library interface helper class plugin algorithm downstream package want import include interface have build slow build unnecessarily package init__.py import symbol library change transparent user separate .i file algorithm small group algorithm import interface library file pure python registry code register plugin wrap pythoncode block implementation algorithm bit scatter package make easy maintain well example new plugin,"Refactor meas_base Python wrappers and plugin registration meas_base currently has a single Swig library (like most packages), defined within a single .i file (like some packages). It also registers all of its plugins in a single python module, plugins.py. Instead, it should: - Have two Swig libraries: one for the interfaces and helper classes, and one for plugin algorithms. Most downstream packages will only want to %import (and hence #include) the interface, and having them build against everything slows the build down unnecessarily. The package __init__.py should import all symbols from both libraries, so the change would be transparent to the user. - Have separate .i files for each algorithm or small group of algorithms. Each of these could %import the interface library file and the pure-Python registry code, and then register the plugins wrapped there within a %pythoncode block. That'd make the implementation of the algorithms a bit less scattered throughout the package, making them easier to maintain and better examples for new plugins."
"Support multiple-aperture fluxes in slots We should be able to use multiple-aperture flux results in slots.  While this is technically possible already by setting specific aliases, it doesn't work through the usual mechanisms for setting up slots (the define methods in SourceTable and the SourceSlotConfig in meas_base).  After addressing this, we should remove the old SincFlux and NaiveFlux algorithms, as the new CircularApertureFlux algorithm will be able to do everything they can do.",2,DM-1218,datamanagement,support multiple aperture flux slot able use multiple aperture flux result slot technically possible set specific alias work usual mechanism set slot define method sourcetable sourceslotconfig meas_base address remove old sincflux naiveflux algorithm new circularapertureflux algorithm able,"Support multiple-aperture fluxes in slots We should be able to use multiple-aperture flux results in slots. While this is technically possible already by setting specific aliases, it doesn't work through the usual mechanisms for setting up slots (the define methods in SourceTable and the SourceSlotConfig in meas_base). After addressing this, we should remove the old SincFlux and NaiveFlux algorithms, as the new CircularApertureFlux algorithm will be able to do everything they can do."
"LSE-68: Bring pull interface to CCB approval Bring a long-pending set of changes, primarily the adoption of the ""pull interface"" for all DM-Camera image requests, to CCB approval",6,DM-1228,datamanagement,lse-68 bring pull interface ccb approval bring long pende set change primarily adoption pull interface dm camera image request ccb approval,"LSE-68: Bring pull interface to CCB approval Bring a long-pending set of changes, primarily the adoption of the ""pull interface"" for all DM-Camera image requests, to CCB approval"
"LSE-69: Bring Summer 2014 work to CCB approval Bring Summer 2014 work on LSE-69 to CCB approval, ideally by 20 October in time to be part of the CD-2 document package.",6,DM-1230,datamanagement,lse-69 bring summer 2014 work ccb approval bring summer 2014 work lse-69 ccb approval ideally 20 october time cd-2 document package,"LSE-69: Bring Summer 2014 work to CCB approval Bring Summer 2014 work on LSE-69 to CCB approval, ideally by 20 October in time to be part of the CD-2 document package."
"LSE-72: Bring Summer 2014 work to CCB approval Remaining work is to proofread the SysML-ization by Brian Selvy of the LSE-72 draft, do any required cleanup in conjunction with the OCS team, and advocate for LCR-202 (already exists) at the CCB.",3,DM-1232,datamanagement,lse-72 bring summer 2014 work ccb approval remain work proofread sysml ization brian selvy lse-72 draft require cleanup conjunction ocs team advocate lcr-202 exist ccb,"LSE-72: Bring Summer 2014 work to CCB approval Remaining work is to proofread the SysML-ization by Brian Selvy of the LSE-72 draft, do any required cleanup in conjunction with the OCS team, and advocate for LCR-202 (already exists) at the CCB."
"LSE-75: Bring Summer 2014 work to CCB Bring a small set of technical changes (e.g., pointing notice moved to OCS ICD) and the addition of PSF reporting to T&S into the SysML version of LSE-75 and submit a change request to get this approved.",6,DM-1236,datamanagement,lse-75 bring summer 2014 work ccb bring small set technical change e.g. point notice move ocs icd addition psf report t&s sysml version lse-75 submit change request approve,"LSE-75: Bring Summer 2014 work to CCB Bring a small set of technical changes (e.g., pointing notice moved to OCS ICD) and the addition of PSF reporting to T&S into the SysML version of LSE-75 and submit a change request to get this approved."
LSE-75: Refine WCS and PSF requirements Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis.  Depends on the ability of the T&S group to engage with this subject during the Winter 2015 period.  Can be deferred to Summer 2015 without major impacts.  Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.,8,DM-1237,datamanagement,lse-75 refine wcs psf requirement clarify data format precision requirement tcs telescope site component reporting wcs psf information dm image basis depend ability t&s group engage subject winter 2015 period defer summer 2015 major impact current pmcs deadline phase readiness lse-75 29 sep-2015,LSE-75: Refine WCS and PSF requirements Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis. Depends on the ability of the T&S group to engage with this subject during the Winter 2015 period. Can be deferred to Summer 2015 without major impacts. Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.
"LSE-140: Bring Summer 2014 work to CCB approval Convert the existing LSE-140 draft to SysML, produce a docgen, and review with Jacques Sebag.  Bring to CCB meeting on 8 October under existing LCR-201.",6,DM-1240,datamanagement,lse-140 bring summer 2014 work ccb approval convert exist lse-140 draft sysml produce docgen review jacques sebag bring ccb meeting october exist lcr-201,"LSE-140: Bring Summer 2014 work to CCB approval Convert the existing LSE-140 draft to SysML, produce a docgen, and review with Jacques Sebag. Bring to CCB meeting on 8 October under existing LCR-201."
"Risk Register refresh 1/2015 Periodic review of DM risk register contents.  Covers preparation for a review expected at the end of January 2015, the only one during Winter 2015.",3,DM-1242,datamanagement,risk register refresh 1/2015 periodic review dm risk register content cover preparation review expect end january 2015 winter 2015,"Risk Register refresh 1/2015 Periodic review of DM risk register contents. Covers preparation for a review expected at the end of January 2015, the only one during Winter 2015."
Install scisql plugin (shared library) outside of eups stack. sciSQL plugin is currently deployed in eups stack (i.e. $MYSQL_DIR/lib/plugin) during configuration step. Nevertheless eups stack should be immutable during configuration step.  MySQL plugin-dir option may allow to deploy sciSQL plugin outside of eups stack (for example in QSERV_RUN_DIR).,3,DM-1245,datamanagement,install scisql plugin share library outside eup stack scisql plugin currently deploy eup stack i.e. mysql_dir lib plugin configuration step eup stack immutable configuration step mysql plugin dir option allow deploy scisql plugin outside eup stack example qserv_run_dir,Install scisql plugin (shared library) outside of eups stack. sciSQL plugin is currently deployed in eups stack (i.e. $MYSQL_DIR/lib/plugin) during configuration step. Nevertheless eups stack should be immutable during configuration step. MySQL plugin-dir option may allow to deploy sciSQL plugin outside of eups stack (for example in QSERV_RUN_DIR).
"add per-exposure callbacks for measurement plugins We should give measurement plugins an opportunity to do some work whenever we start processing a new Exposure.  This give them a good time to throw exceptions when there's something obviously wrong with the exposure (e.g. it's missing a Psf).  It also gives them an opportunity to do work that could be used to speed up per-source processing.  One specific case I have in mind is in shapelet PSF approximation for galaxy fitting, where it could be very valuable to use an initial fit to an average PSF to initialize (and speed up) the first to per-source PSFs.  One question here is how to to allow information to be passed from the per-Exposure method to the per-Source methods - we should not do that via plugin instance attributes, which means we probably want to have the per-Exposure method return an arbitrary object that would be passed to the per-Source methods.  Unfortunately, I don't see a way to avoid having that change the signature for those methods for all existing plugins, so this should be done relatively early, before we have too many user-contributed plugins.",3,DM-1246,datamanagement,add exposure callback measurement plugin measurement plugin opportunity work start process new exposure good time throw exception obviously wrong exposure e.g. miss psf give opportunity work speed source processing specific case mind shapelet psf approximation galaxy fitting valuable use initial fit average psf initialize speed source psf question allow information pass exposure method source method plugin instance attribute mean probably want exposure method return arbitrary object pass source method unfortunately way avoid have change signature method exist plugin relatively early user contribute plugin,"add per-exposure callbacks for measurement plugins We should give measurement plugins an opportunity to do some work whenever we start processing a new Exposure. This give them a good time to throw exceptions when there's something obviously wrong with the exposure (e.g. it's missing a Psf). It also gives them an opportunity to do work that could be used to speed up per-source processing. One specific case I have in mind is in shapelet PSF approximation for galaxy fitting, where it could be very valuable to use an initial fit to an average PSF to initialize (and speed up) the first to per-source PSFs. One question here is how to to allow information to be passed from the per-Exposure method to the per-Source methods - we should not do that via plugin instance attributes, which means we probably want to have the per-Exposure method return an arbitrary object that would be passed to the per-Source methods. Unfortunately, I don't see a way to avoid having that change the signature for those methods for all existing plugins, so this should be done relatively early, before we have too many user-contributed plugins."
CSS design for query metadata v1 The goal of this ticket (and DM-1250) is to try to understand what kind of per-query metadata is necessary to provide client-transparent query processing in case when czar/proxy could die or be restarted.  Some relevant info is in the Trac: https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS/RunTimeState,3,DM-1251,datamanagement,css design query metadata v1 goal ticket dm-1250 try understand kind query metadata necessary provide client transparent query process case czar proxy die restart relevant info trac https://dev.lsstcorp.org/trac/wiki/db/qserv/css/runtimestate,CSS design for query metadata v1 The goal of this ticket (and DM-1250) is to try to understand what kind of per-query metadata is necessary to provide client-transparent query processing in case when czar/proxy could die or be restarted. Some relevant info is in the Trac: https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS/RunTimeState
Metadata Store - design v1 Research potential off-the-shelf candidates. Propose initial version of metadata design. ,5,DM-1254,datamanagement,metadata store design v1 research potential shelf candidate propose initial version metadata design,Metadata Store - design v1 Research potential off-the-shelf candidates. Propose initial version of metadata design.
"Metadata Store - experimental prototype v2 (DataCat) Integrate prototype v1 with Fermi's DataCat (e.g., reuse logic for reading headers using afw, store in DataCat). Experiment with foreign tables.",6,DM-1257,datamanagement,metadata store experimental prototype v2 datacat integrate prototype v1 fermi datacat e.g. reuse logic read header afw store datacat experiment foreign table,"Metadata Store - experimental prototype v2 (DataCat) Integrate prototype v1 with Fermi's DataCat (e.g., reuse logic for reading headers using afw, store in DataCat). Experiment with foreign tables."
Update documentation and automatic install script w.r.t. Qserv 2014_09.0 release Creation of qserv_distrib and distribution of Qserv via official LSST repositories have to be taken into account in Qserv documentation and automatic install script.,4,DM-1258,datamanagement,update documentation automatic install script w.r.t qserv 2014_09.0 release creation qserv_distrib distribution qserv official lsst repository take account qserv documentation automatic install script,Update documentation and automatic install script w.r.t. Qserv 2014_09.0 release Creation of qserv_distrib and distribution of Qserv via official LSST repositories have to be taken into account in Qserv documentation and automatic install script.
incorporation of sizing model into data center requirements Deliverable: incorporation of sizing model into data center requirements Freemon M 100% ,7,DM-1261,datamanagement,incorporation size model datum center requirement deliverable incorporation size model datum center requirement freemon 100,incorporation of sizing model into data center requirements Deliverable: incorporation of sizing model into data center requirements Freemon M 100%
completed governance of security plan for review Deliverable: completed governance of security plan for review Petravick D 10% ,1,DM-1263,datamanagement,complete governance security plan review deliverable complete governance security plan review petravick 10,completed governance of security plan for review Deliverable: completed governance of security plan for review Petravick D 10%
security plan october. Deliverable: security plan Ephibian 10% ,1,DM-1264,datamanagement,security plan october deliverable security plan ephibian 10,security plan october. Deliverable: security plan Ephibian 10%
"addition of procurement of physical goods to contract Deliverable: addition of procurement of physical goods to contract Petravick D 5%, Gelman M 10% ",6,DM-1265,datamanagement,addition procurement physical good contract deliverable addition procurement physical good contract petravick gelman 10,"addition of procurement of physical goods to contract Deliverable: addition of procurement of physical goods to contract Petravick D 5%, Gelman M 10%"
report of traffic shaping Deliverable: report of traffic shaping Freemon M 100% ,7,DM-1267,datamanagement,report traffic shaping deliverable report traffic shape freemon 100,report of traffic shaping Deliverable: report of traffic shaping Freemon M 100%
"more efficient VMware infrastructure Deliverable: more efficient VMware infrastructure  W15: Glick B 50%, Elliott M 25%, Mather B 10% 38 SP estimated  S15: Mather B 40%, Glick B 25% 6 SP estimated",6,DM-1270,datamanagement,efficient vmware infrastructure deliverable efficient vmware infrastructure w15 glick 50 elliott 25 mather 10 38 sp estimate s15 mather 40 glick 25 sp estimate,"more efficient VMware infrastructure Deliverable: more efficient VMware infrastructure W15: Glick B 50%, Elliott M 25%, Mather B 10% 38 SP estimated S15: Mather B 40%, Glick B 25% 6 SP estimated"
"Fix Scisql deployment test error (doc.py) Deployment test in tools/docs.py fails due to a wrong ""scisql_index"" path in scisql documentation.  Fortunately, qserv-configure.py doesn't stop on this error.",2,DM-1279,datamanagement,fix scisql deployment test error doc.py deployment test tool docs.py fail wrong scisql_index path scisql documentation fortunately qserv-configure.py stop error,"Fix Scisql deployment test error (doc.py) Deployment test in tools/docs.py fails due to a wrong ""scisql_index"" path in scisql documentation. Fortunately, qserv-configure.py doesn't stop on this error."
"meas_base ResultMappers should be FunctorKeys The ResultMapper classes in meas_base should inherit from FunctorKey, and support bidirectional transfers involving the Result structs and records.",3,DM-1280,datamanagement,meas_base resultmappers functorkeys resultmapper class meas_base inherit functorkey support bidirectional transfer involve result struct record,"meas_base ResultMappers should be FunctorKeys The ResultMapper classes in meas_base should inherit from FunctorKey, and support bidirectional transfers involving the Result structs and records."
"add Schema method to join strings using the appropriate delimiter Delimiters in Schema field names are version-dependent.  One can currently use {{schema[""a""][""b""].getPrefix()}} to join fields using the appropriate delimiter, but this is confusing to read.",1,DM-1281,datamanagement,"add schema method join string appropriate delimiter delimiters schema field name version dependent currently use schema[""a""][""b""].getprefix join field appropriate delimiter confusing read","add Schema method to join strings using the appropriate delimiter Delimiters in Schema field names are version-dependent. One can currently use {{schema[""a""][""b""].getPrefix()}} to join fields using the appropriate delimiter, but this is confusing to read."
multi-level replacement in Schema aliases Schema aliases should support more than one level (i.e. an alias may resolve to another alias).,2,DM-1282,datamanagement,multi level replacement schema alias schema alias support level i.e. alias resolve alia,multi-level replacement in Schema aliases Schema aliases should support more than one level (i.e. an alias may resolve to another alias).
Improve Startup of HTCondor Jobs Adjust configuration parameters of HTCondor config and/or submission files to improve speed at which HTCondor jobs start in both the replicator pool and worker pool.,2,DM-1285,datamanagement,improve startup htcondor jobs adjust configuration parameter htcondor config and/or submission file improve speed htcondor job start replicator pool worker pool,Improve Startup of HTCondor Jobs Adjust configuration parameters of HTCondor config and/or submission files to improve speed at which HTCondor jobs start in both the replicator pool and worker pool.
"Improve worker fault tolerance of missing distributor data Instances of worker jobs might ask the Archive DMCS which distributor to connect to, only to connect to that distributor which has recently been rebooted, so the distributor might not have that information.   At this point, the Distributor could send an “expired” notice of some kind to the Archive DMCS to clear it’s cache, and also tell the worker that it has no file of the type it’s looking for.  The worker would then go back to the Archive DMCS. ",6,DM-1286,datamanagement,improve worker fault tolerance miss distributor datum instances worker job ask archive dmcs distributor connect connect distributor recently reboot distributor information point distributor send expired notice kind archive dmcs clear cache tell worker file type look worker archive dmcs,"Improve worker fault tolerance of missing distributor data Instances of worker jobs might ask the Archive DMCS which distributor to connect to, only to connect to that distributor which has recently been rebooted, so the distributor might not have that information. At this point, the Distributor could send an expired notice of some kind to the Archive DMCS to clear it s cache, and also tell the worker that it has no file of the type it s looking for. The worker would then go back to the Archive DMCS."
Propose and document a recipe to build Qserv in eups In-place build is available and documented.,2,DM-1287,datamanagement,propose document recipe build qserv eup place build available document,Propose and document a recipe to build Qserv in eups In-place build is available and documented.
Identify test data and camera To test processCcd we need to identify a set of data to process (possibly mocked).  This implies that there will also need to be a minimal camera to go with the minimal data.  This task is to identify the minimal data and find a location for it.,4,DM-1291,datamanagement,identify test datum camera test processccd need identify set datum process possibly mock imply need minimal camera minimal datum task identify minimal datum find location,Identify test data and camera To test processCcd we need to identify a set of data to process (possibly mocked). This implies that there will also need to be a minimal camera to go with the minimal data. This task is to identify the minimal data and find a location for it.
Implement designed tests for processCcd Implement the designed tests with the installed data.,8,DM-1293,datamanagement,implement design test processccd implement design test instal datum,Implement designed tests for processCcd Implement the designed tests with the installed data.
Generate use case We should first sit down with a Camera team rep (Jim C. maybe) to define the tool they need.  How dynamic are the data?  What is the layout of the data?  We should also get some example test data to work with.,6,DM-1294,datamanagement,generate use case sit camera team rep jim c. maybe define tool need dynamic datum layout datum example test datum work,Generate use case We should first sit down with a Camera team rep (Jim C. maybe) to define the tool they need. How dynamic are the data? What is the layout of the data? We should also get some example test data to work with.
Sync test data headers with standards The headers will need to be vetted against known FITS standards.  The headers should be valid and should not contain any non-standard keywords.  There should also be a census of FITS standards that may be used for defining sensor layout.,8,DM-1295,datamanagement,sync test datum header standard header need vet know fits standard header valid contain non standard keyword census fits standard define sensor layout,Sync test data headers with standards The headers will need to be vetted against known FITS standards. The headers should be valid and should not contain any non-standard keywords. There should also be a census of FITS standards that may be used for defining sensor layout.
Verify use of the tool with the camera team We will need to run the tool in the camera team's system to make sure the interfaces are as they expected.  It will also help to make sure that the tool addresses all of the cases.,6,DM-1297,datamanagement,verify use tool camera team need run tool camera team system sure interface expect help sure tool address case,Verify use of the tool with the camera team We will need to run the tool in the camera team's system to make sure the interfaces are as they expected. It will also help to make sure that the tool addresses all of the cases.
"Design the API The API will have to be able to accommodate data we know about, so will need to deal with reasonable missing data.  It should also not preclude extension.  This will need to be RFC'd since it is an API change.",8,DM-1298,datamanagement,design api api able accommodate datum know need deal reasonable missing datum preclude extension need rfc'd api change,"Design the API The API will have to be able to accommodate data we know about, so will need to deal with reasonable missing data. It should also not preclude extension. This will need to be RFC'd since it is an API change."
"C++ code changes required for --std=c++11 Some C++ code requires changes for modern C++ compilers if it is to be compatible with C++11 and the older standard. Here is my list, so far (ignoring the known issue of MacOS having two different standard C++ libraries)  Implicit conversion of shared_ptr to bool no longer works: <http://stackoverflow.com/questions/7580009/gcc-error-cannot-convert-const-shared-ptr-to-bool-in-return> In C++11, shared_ptr has an explicit operator bool which means that a shared_ptr can't be implicitly converted to a bool. This is likely to show up in a lot of packages. So far found in: - pex_policy - meas_algorithms  warning: adding 'int' to a string does not append to the string seen in daf_persistence  warning: 'va_start' has undefined behavior with reference types seen in pex_logging Trace.h and one other place. Fixed by not using references (and thus copying the arguments), rather than using pointers, to avoid changing the APIs. this is an old issue; but not all compilers warned about it: See <http://stackoverflow.com/questions/222195/are-there-gotchas-using-varargs-with-reference-parameters>  warning: 'register' storage class specifier is deprecated seen in: - boost 1.55.0.1 - Eigen 3.2.0 - a pex_logging .i file I silenced this warning in sconsUtils because it's too much trouble to upgrade boost and Eigen, and the warnings are very obtrusive  in daf_persistence: {code} src/DbStorageImpl.cc:87:35: warning: first declaration of static data member specialization of 'mysqlType' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions]     dafPer::IntegerTypeTraits<1>::mysqlType = MYSQL_TYPE_TINY; {code} I have not figured out how to fix this one.  clang 6 is pickier about the order of instantiation: in afw (fixed in DM-1302) Many errors such as the following: {code} include/lsst/afw/table/Flag.h:27:8: error: explicit specialization of 'lsst::afw::table::FieldBase<lsst::afw::table::Flag>' after instantiation struct FieldBase<Flag> {        ^~~~~~~~~~~~~~~ include/lsst/afw/table/Key.h:49:39: note: implicit instantiation first required here {code} Fixed by including Flag.h in Key.h  clang 6 warns about ambiguous unsequenced modifications. Seen in shapelet, for example: {code} src/HermiteTransformMatrix.cc:107:59: warning: multiple unsequenced modifications to 'kn' [-Wunsequenced]         for (int kn=jn, koff=joff; kn <= order; (koff += (++kn)) += (++kn)) {                                                           ^          ~~ {code} Fixed by using a += ++b, a += ++b, since it was the cleanest solution I could find.  clang 6 is pickier about instantiation classes in the wrong namespace: in shapelet (fixed by not instantiating the anonymous classes): {code} src/MatrixBuilder.cc:945:1: error: explicit instantiation of 'lsst::shapelet::<anonymous namespace>::SimpleImpl' must occur in namespace '' INSTANTIATE(float); {code}  meas_algorithms produced this warning because the class member _wcsPtr was a reference (which was unecessary): {code} src/ShapeletKernel.cc:188:34: warning: binding reference member '_wcsPtr' to a temporary value [-Wdangling-field]         _interp(interp), _wcsPtr(wcsPtr->clone())                                  ^~~~~~~~~~~~~~~ {code}  clang 6 warns about expressions such as if (!a == 0) because the ! is applied to ""a"", not the result of the ""a == 0"". Fixed in the obvious way in several places.  meas_algorithms seg-faults on loading unless boost is built with C++11 support. See ticket DM-1361 ",4,DM-1302,datamanagement,c++ code change require --std c++11 c++ code require change modern c++ compiler compatible c++11 old standard list far ignore know issue macos have different standard c++ library implicit conversion shared_ptr bool long work c++11 shared_ptr explicit operator bool mean shared_ptr implicitly convert bool likely lot package far find pex_policy meas_algorithm warning add int string append string see daf_persistence warning va_start undefined behavior reference type see pex_logge trace.h place fix reference copy argument pointer avoid change api old issue compiler warn warn register storage class specifi deprecate see boost 1.55.0.1 eigen 3.2.0 pex_logging .i file silence warning sconsutil trouble upgrade boost eigen warning obtrusive daf_persistence code src dbstorageimpl.cc:87:35 warn declaration static datum member specialization mysqltype outside namespace persistence c++11 extension -wc++11 extension dafper::integertypetraits<1>::mysqltype mysql_type_tiny code figure fix clang picky order instantiation afw fix dm-1302 error following code include lsst afw table flag.h:27:8 error explicit specialization lsst::afw::table::fieldbase instantiation struct fieldbase ^~~~~~~~~~~~~~~ include lsst afw table key.h:49:39 note implicit instantiation require code fix include flag.h key.h clang warn ambiguous unsequenced modification see shapelet example code src hermitetransformmatrix.cc:107:59 warning multiple unsequenced modification kn -wunsequence int kn jn koff joff kn order koff kn kn code fix clean solution find clang picky instantiation class wrong namespace shapelet fix instantiate anonymous class code src matrixbuilder.cc:945:1 error explicit instantiation lsst::shapelet::::simpleimpl occur namespace code meas_algorithm produce warning class member wcsptr reference unecessary code src shapeletkernel.cc:188:34 warn bind reference member wcsptr temporary value -wdangling field interp(interp code clang warn expression apply result fix obvious way place meas_algorithms seg fault loading boost build c++11 support ticket dm-1361,"C++ code changes required for --std=c++11 Some C++ code requires changes for modern C++ compilers if it is to be compatible with C++11 and the older standard. Here is my list, so far (ignoring the known issue of MacOS having two different standard C++ libraries) Implicit conversion of shared_ptr to bool no longer works:  In C++11, shared_ptr has an explicit operator bool which means that a shared_ptr can't be implicitly converted to a bool. This is likely to show up in a lot of packages. So far found in: - pex_policy - meas_algorithms warning: adding 'int' to a string does not append to the string seen in daf_persistence warning: 'va_start' has undefined behavior with reference types seen in pex_logging Trace.h and one other place. Fixed by not using references (and thus copying the arguments), rather than using pointers, to avoid changing the APIs. this is an old issue; but not all compilers warned about it: See  warning: 'register' storage class specifier is deprecated seen in: - boost 1.55.0.1 - Eigen 3.2.0 - a pex_logging .i file I silenced this warning in sconsUtils because it's too much trouble to upgrade boost and Eigen, and the warnings are very obtrusive in daf_persistence: {code} src/DbStorageImpl.cc:87:35: warning: first declaration of static data member specialization of 'mysqlType' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] dafPer::IntegerTypeTraits<1>::mysqlType = MYSQL_TYPE_TINY; {code} I have not figured out how to fix this one. clang 6 is pickier about the order of instantiation: in afw (fixed in DM-1302) Many errors such as the following: {code} include/lsst/afw/table/Flag.h:27:8: error: explicit specialization of 'lsst::afw::table::FieldBase' after instantiation struct FieldBase { ^~~~~~~~~~~~~~~ include/lsst/afw/table/Key.h:49:39: note: implicit instantiation first required here {code} Fixed by including Flag.h in Key.h clang 6 warns about ambiguous unsequenced modifications. Seen in shapelet, for example: {code} src/HermiteTransformMatrix.cc:107:59: warning: multiple unsequenced modifications to 'kn' [-Wunsequenced] for (int kn=jn, koff=joff; kn <= order; (koff += (++kn)) += (++kn)) { ^ ~~ {code} Fixed by using a += ++b, a += ++b, since it was the cleanest solution I could find. clang 6 is pickier about instantiation classes in the wrong namespace: in shapelet (fixed by not instantiating the anonymous classes): {code} src/MatrixBuilder.cc:945:1: error: explicit instantiation of 'lsst::shapelet::::SimpleImpl' must occur in namespace '' INSTANTIATE(float); {code} meas_algorithms produced this warning because the class member _wcsPtr was a reference (which was unecessary): {code} src/ShapeletKernel.cc:188:34: warning: binding reference member '_wcsPtr' to a temporary value [-Wdangling-field] _interp(interp), _wcsPtr(wcsPtr->clone()) ^~~~~~~~~~~~~~~ {code} clang 6 warns about expressions such as if (!a == 0) because the ! is applied to ""a"", not the result of the ""a == 0"". Fixed in the obvious way in several places. meas_algorithms seg-faults on loading unless boost is built with C++11 support. See ticket DM-1361"
"Tests fail in shapelet when building on OS X 10.9 When building the master on pugsley.ncsa.illinois.edu, shapelet builds successfully, but two tests fail:  {code} pugsley:lsstsw mjuric$ cat build/shapelet/tests/.tests/*.failed tests/testMatrixBuilder.py  .F..... ====================================================================== FAIL: testConvolvedCompoundMatrixBuilder (__main__.MatrixBuilderTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMatrixBuilder.py"", line 310, in testConvolvedCompoundMatrixBuilder     self.assertClose(numpy.dot(matrix1D, coefficients), checkVector, rtol=1E-14)   File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/50 elements differ with rtol=1e-14, atol=2.22044604925e-16 0.175869366369 != 0.175869366369 (diff=1.99840144433e-15/0.175869366369=1.13629876856e-14)  ---------------------------------------------------------------------- Ran 7 tests in 0.323s  FAILED (failures=1) tests/testMultiShapelet.py  ...F... ====================================================================== FAIL: testConvolveGaussians (__main__.MultiShapeletTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMultiShapelet.py"", line 88, in testConvolveGaussians     self.compareMultiShapeletFunctions(msf3a, msf3b)   File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 107, in compareMultiShapeletFunctions     self.compareShapeletFunctions(sa, sb, rtolEllipse=rtolEllipse, rtolCoeff=rtolCoeff)   File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 86, in compareShapeletFunctions     rtol=rtolEllipse)   File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/5 elements differ with rtol=1e-14, atol=2.22044604925e-16 2.44929359829e-16 != 1.13310777953e-15 (diff=8.881784197e-16/1.13310777953e-15=0.783842839795)  ---------------------------------------------------------------------- Ran 7 tests in 0.131s  FAILED (failures=1) {code}  ===============  More info on pugsley.ncsa.illinois.edu:  	pugsley:lsstsw mjuric$ sw_vers 	ProductName:	Mac OS X 	ProductVersion:	10.9.5 	BuildVersion:	13F34  	pugsley:lsstsw mjuric$ clang -v 	Apple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn) 	Target: x86_64-apple-darwin13.4.0 	Thread model: posix  ============  The files are in {{/Users/mjuric/test/lsstsw/build/shapelet/}}.",1,DM-1305,datamanagement,"test fail shapelet build os 10.9 build master pugsley.ncsa.illinois.edu shapelet build successfully test fail code pugsley lsstsw mjuric$ cat build shapelet tests/.tests/*.failed test testmatrixbuilder.py .f fail testconvolvedcompoundmatrixbuilder main__.matrixbuildertestcase traceback recent file test testmatrixbuilder.py line 310 testconvolvedcompoundmatrixbuilder self.assertclose(numpy.dot(matrix1d coefficient checkvector rtol=1e-14 file /users mjuric test lsstsw stack darwinx86 utils/9.2 python lsst util tests.py line 328 assertclose testcase.assertfalse(faile msg=""\n"".join(msg assertionerror 1/50 element differ rtol=1e-14 atol=2.22044604925e-16 0.175869366369 0.175869366369 diff=1.99840144433e-15/0.175869366369=1.13629876856e-14 ran test 0.323s fail failures=1 test testmultishapelet.py fail testconvolvegaussian main__.multishapelettestcase traceback recent file test testmultishapelet.py line 88 testconvolvegaussians self.comparemultishapeletfunctions(msf3a msf3b file /users mjuric test lsstsw build shapelet python lsst shapelet tests.py line 107 comparemultishapeletfunction self.compareshapeletfunctions(sa sb rtolellipse rtolellipse rtolcoeff rtolcoeff file /users mjuric test lsstsw build shapelet python lsst shapelet tests.py line 86 compareshapeletfunction rtol rtolellipse file /users mjuric test lsstsw stack darwinx86 utils/9.2 python lsst util tests.py line 328 assertclose testcase.assertfalse(faile msg=""\n"".join(msg assertionerror 1/5 element differ rtol=1e-14 atol=2.22044604925e-16 2.44929359829e-16 1.13310777953e-15 diff=8.881784197e-16/1.13310777953e-15=0.783842839795 ran test 0.131s fail failures=1 code info pugsley.ncsa.illinois.edu pugsley lsstsw mjuric$ sw_vers productname mac os productversion 10.9.5 buildversion 13f34 pugsley lsstsw mjuric$ clang apple llvm version 5.1 clang-503.0.40 base llvm 3.4svn target x86_64 apple darwin13.4.0 thread model posix file /users mjuric test lsstsw build shapelet/","Tests fail in shapelet when building on OS X 10.9 When building the master on pugsley.ncsa.illinois.edu, shapelet builds successfully, but two tests fail: {code} pugsley:lsstsw mjuric$ cat build/shapelet/tests/.tests/*.failed tests/testMatrixBuilder.py .F..... ====================================================================== FAIL: testConvolvedCompoundMatrixBuilder (__main__.MatrixBuilderTestCase) ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/testMatrixBuilder.py"", line 310, in testConvolvedCompoundMatrixBuilder self.assertClose(numpy.dot(matrix1D, coefficients), checkVector, rtol=1E-14) File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/50 elements differ with rtol=1e-14, atol=2.22044604925e-16 0.175869366369 != 0.175869366369 (diff=1.99840144433e-15/0.175869366369=1.13629876856e-14) ---------------------------------------------------------------------- Ran 7 tests in 0.323s FAILED (failures=1) tests/testMultiShapelet.py ...F... ====================================================================== FAIL: testConvolveGaussians (__main__.MultiShapeletTestCase) ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/testMultiShapelet.py"", line 88, in testConvolveGaussians self.compareMultiShapeletFunctions(msf3a, msf3b) File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 107, in compareMultiShapeletFunctions self.compareShapeletFunctions(sa, sb, rtolEllipse=rtolEllipse, rtolCoeff=rtolCoeff) File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 86, in compareShapeletFunctions rtol=rtolEllipse) File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/5 elements differ with rtol=1e-14, atol=2.22044604925e-16 2.44929359829e-16 != 1.13310777953e-15 (diff=8.881784197e-16/1.13310777953e-15=0.783842839795) ---------------------------------------------------------------------- Ran 7 tests in 0.131s FAILED (failures=1) {code} =============== More info on pugsley.ncsa.illinois.edu: pugsley:lsstsw mjuric$ sw_vers ProductName: Mac OS X ProductVersion: 10.9.5 BuildVersion: 13F34 pugsley:lsstsw mjuric$ clang -v Apple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn) Target: x86_64-apple-darwin13.4.0 Thread model: posix ============ The files are in {{/Users/mjuric/test/lsstsw/build/shapelet/}}."
"Edit agreed-upon changes into Word version of LSE-69 A meeting around 9/26/2014 agreed on a set of revisions to LSE-69, with some language still needed from [~gpdf].  This action is to edit the tracked-changes Word version of LSE-69 containing the notes from that meeting into a final copy that can be reviewed by the Camera team and used as input to editing the SysML version of the ICD.",3,DM-1309,datamanagement,edit agree change word version lse-69 meeting 9/26/2014 agree set revision lse-69 language need ~gpdf action edit track change word version lse-69 contain note meeting final copy review camera team input edit sysml version icd,"Edit agreed-upon changes into Word version of LSE-69 A meeting around 9/26/2014 agreed on a set of revisions to LSE-69, with some language still needed from [~gpdf]. This action is to edit the tracked-changes Word version of LSE-69 containing the notes from that meeting into a final copy that can be reviewed by the Camera team and used as input to editing the SysML version of the ICD."
Create change request for LSE-69 Create a change request to bring LSE-69 up to date and capture the Summer 2014 work.,1,DM-1310,datamanagement,create change request lse-69 create change request bring lse-69 date capture summer 2014 work,Create change request for LSE-69 Create a change request to bring LSE-69 up to date and capture the Summer 2014 work.
"Enter LSE-69 update into EA as SysML Covers entering the contents of the LSE-69 update into EA as SysML, with associated updating of diagrams, and the creation of a docgen'ed version for CCB action.",1,DM-1311,datamanagement,enter lse-69 update ea sysml cover enter content lse-69 update ea sysml associate updating diagram creation docgen'ed version ccb action,"Enter LSE-69 update into EA as SysML Covers entering the contents of the LSE-69 update into EA as SysML, with associated updating of diagrams, and the creation of a docgen'ed version for CCB action."
Proofread docgen'ed version of LSE-72 Brian Selvy is producing a SysML version of the LSE-72 updated edited by [~gpdf].  The action here is to proofread the docgen of that version once it is ready.,2,DM-1312,datamanagement,proofread docgen'ed version lse-72 brian selvy produce sysml version lse-72 update edit ~gpdf action proofread docgen version ready,Proofread docgen'ed version of LSE-72 Brian Selvy is producing a SysML version of the LSE-72 updated edited by [~gpdf]. The action here is to proofread the docgen of that version once it is ready.
"Identify Conditions information in LSE-130 that is required for Alert Production LSE-69 declares that there are two categories of Conditions data (telemetry) required by DM from the Camera: those items that are needed for Alert Production (for which the AP components at the Base will need a whitelist, and for which the Camera has a tighter latency requirement), and those that are not (but are then presumably needed in DRP or other deferred productions).  It states that the subset needed for AP should be enumerated in LSE-130.  The action here is to create an initial version of that list.",2,DM-1313,datamanagement,identify conditions information lse-130 require alert production lse-69 declare category conditions datum telemetry require dm camera item need alert production ap component base need whitelist camera tight latency requirement presumably need drp deferred production state subset need ap enumerate lse-130 action create initial version list,"Identify Conditions information in LSE-130 that is required for Alert Production LSE-69 declares that there are two categories of Conditions data (telemetry) required by DM from the Camera: those items that are needed for Alert Production (for which the AP components at the Base will need a whitelist, and for which the Camera has a tighter latency requirement), and those that are not (but are then presumably needed in DRP or other deferred productions). It states that the subset needed for AP should be enumerated in LSE-130. The action here is to create an initial version of that list."
"Publish Qserv S14 version on lsst distribution server In order to publish this version please tag Qserv master tip with ""2014_09.0"" and then run: {code:bash} ssh lsstsw@lsst-dev # command below can't be runned in buildbot, as it doesn't support qserv_distrib build rebuild -t 2014_09.0 qserv_distrib # bXXX is provided by previous command publish -t qserv -b bXXX qserv_distrib publish -t 2014_09 -b bXXX qserv_distrib {code} ",1,DM-1314,datamanagement,publish qserv s14 version lsst distribution server order publish version tag qserv master tip 2014_09.0 run code bash ssh lsstsw@lsst dev command runne buildbot support qserv_distrib build rebuild 2014_09.0 qserv_distrib bxxx provide previous command publish qserv bxxx qserv_distrib publish 2014_09 bxxx qserv_distrib code,"Publish Qserv S14 version on lsst distribution server In order to publish this version please tag Qserv master tip with ""2014_09.0"" and then run: {code:bash} ssh lsstsw@lsst-dev # command below can't be runned in buildbot, as it doesn't support qserv_distrib build rebuild -t 2014_09.0 qserv_distrib # bXXX is provided by previous command publish -t qserv -b bXXX qserv_distrib publish -t 2014_09 -b bXXX qserv_distrib {code}"
advance to assigning tier-2 and tier-3 reliability levels  Accommodated Ron Lambert's input on networking equipment. Assigned tier-2 and tier-3 levels to processing systems. ,4,DM-1315,datamanagement,advance assign tier-2 tier-3 reliability level accommodate ron lambert input networking equipment assign tier-2 tier-3 level processing system,advance to assigning tier-2 and tier-3 reliability levels Accommodated Ron Lambert's input on networking equipment. Assigned tier-2 and tier-3 levels to processing systems.
"Deploy LSST stack within OpenStack instances on ISL testbed Deploy the LSST Stack within OpenStack instances within the ISL testbed -- this could be for multiple flavors CentOS, Ubuntu, etc, and this could be done by pulling Docker Images to the instances.   There will also likely be some initial debugging of starting instances within the ISL platform as a new installation has been stood up Sept 2014. ",1,DM-1316,datamanagement,deploy lsst stack openstack instance isl testbe deploy lsst stack openstack instance isl testbe multiple flavor centos ubuntu etc pull docker images instance likely initial debugging start instance isl platform new installation stand sept 2014,"Deploy LSST stack within OpenStack instances on ISL testbed Deploy the LSST Stack within OpenStack instances within the ISL testbed -- this could be for multiple flavors CentOS, Ubuntu, etc, and this could be done by pulling Docker Images to the instances. There will also likely be some initial debugging of starting instances within the ISL platform as a new installation has been stood up Sept 2014."
"Create Docker Image / Dockerfile for LSST Stack for ubuntu Create an installation of the LSST Stack v9_2  within a Docker Image for ubuntu for easing the import of LSST software into an OpenStack instance,  We create  the image utilizing a Dockerfile to make systematic  the creation of such images. ",2,DM-1317,datamanagement,create docker image dockerfile lsst stack ubuntu create installation lsst stack v9_2 docker image ubuntu ease import lsst software openstack instance create image utilize dockerfile systematic creation image,"Create Docker Image / Dockerfile for LSST Stack for ubuntu Create an installation of the LSST Stack v9_2 within a Docker Image for ubuntu for easing the import of LSST software into an OpenStack instance, We create the image utilizing a Dockerfile to make systematic the creation of such images."
update expected results file in SDSS demo test Update the expected outputs in the SDSS DM stack demo repo to match what we expect from the new meas_base framework.,1,DM-1318,datamanagement,update expect result file sdss demo test update expect output sdss dm stack demo repo match expect new meas_base framework,update expected results file in SDSS demo test Update the expected outputs in the SDSS DM stack demo repo to match what we expect from the new meas_base framework.
"Setup temporary NFS datastore for VMs We expect the primary datastores will be local to the VM compute node, but this NFS datastore will allow us to live migrate a handful of VMs between compute nodes.  We may also use this or another datastore as place to create backup snapshots of particular VMs.",2,DM-1320,datamanagement,setup temporary nfs datastore vms expect primary datastore local vm compute node nfs datastore allow live migrate handful vms compute nodes use datastore place create backup snapshot particular vms,"Setup temporary NFS datastore for VMs We expect the primary datastores will be local to the VM compute node, but this NFS datastore will allow us to live migrate a handful of VMs between compute nodes. We may also use this or another datastore as place to create backup snapshots of particular VMs."
"Setup new VM compute nodes Once the new VM compute nodes arrive, the hardware needs to be installed, ESXi installed, and networked for use by vSphere.",6,DM-1321,datamanagement,setup new vm compute node new vm compute nodes arrive hardware need instal esxi instal network use vsphere,"Setup new VM compute nodes Once the new VM compute nodes arrive, the hardware needs to be installed, ESXi installed, and networked for use by vSphere."
"Expire Workers that receive no files In instances where the files the worker expected to get never arrive, there should be a way of the worker to recognize this (say, a timeout after waiting for the Archive DMCS for some time), and exit.",4,DM-1322,datamanagement,expire workers receive file instance file worker expect arrive way worker recognize timeout wait archive dmcs time exit,"Expire Workers that receive no files In instances where the files the worker expected to get never arrive, there should be a way of the worker to recognize this (say, a timeout after waiting for the Archive DMCS for some time), and exit."
"Automatic expiration of replicator jobs Replicator jobs that receive no data specifically for the visit, raft, and exposure sequence ID within a certain amount of time should self expire to prevent future jobs from running.  If this is not done, jobs will back up in the HTCondor queue.",4,DM-1326,datamanagement,automatic expiration replicator job replicator job receive datum specifically visit raft exposure sequence id certain time self expire prevent future job run job htcondor queue,"Automatic expiration of replicator jobs Replicator jobs that receive no data specifically for the visit, raft, and exposure sequence ID within a certain amount of time should self expire to prevent future jobs from running. If this is not done, jobs will back up in the HTCondor queue."
Order new NFS servers We are waiting on the UIUC Business Offices to provide us a new account number for ordering these servers.  As soon as we get that account number we need to order the hardware from KOI Computing.,6,DM-1328,datamanagement,order new nfs server wait uiuc business offices provide new account number order server soon account number need order hardware koi computing,Order new NFS servers We are waiting on the UIUC Business Offices to provide us a new account number for ordering these servers. As soon as we get that account number we need to order the hardware from KOI Computing.
Plan new NFS mounts & data organization This is a task of deciding the new client mounts for the 3 new NFS servers.  These should match up with the new storage hierarchy and classification <https://wiki.ncsa.illinois.edu/display/LSST/Storage+Structure>.,1,DM-1329,datamanagement,plan new nfs mount datum organization task decide new client mount new nfs server match new storage hierarchy classification,Plan new NFS mounts & data organization This is a task of deciding the new client mounts for the 3 new NFS servers. These should match up with the new storage hierarchy and classification .
"squash edge errors in SdssCentroid SdssCentroid doesn't trap exceptions that are thrown due to being too close to the edge, resulting in noisy warnings in the logs.  Instead, it should catch the low-level exception and re-throw as MeasurementError, after defining a flag field for this specific failure mode.",1,DM-1331,datamanagement,squash edge error sdsscentroid sdsscentroid trap exception throw close edge result noisy warning log instead catch low level exception throw measurementerror define flag field specific failure mode,"squash edge errors in SdssCentroid SdssCentroid doesn't trap exceptions that are thrown due to being too close to the edge, resulting in noisy warnings in the logs. Instead, it should catch the low-level exception and re-throw as MeasurementError, after defining a flag field for this specific failure mode."
"address no-shape warnings in GaussianFlux GaussianFlux relies on the shape slot, and puts noisy warnings in the logs when the shape slot fails.  However, we probably don't want to add a new flag for GaussianFlux to indicate this failure mode, because it'd be entirely redundant with the shape slot flag.  We should figure out some other way to squash this warning - how we do that may depend on whether this is addressed before or after the C++ redesign.  We should also consider having GaussianFlux add an alias to the schema to point back at the shape slot flag, creating what looks like a specific flag for this failure while actually just being a link back to the shape slot flag.  That's probably not worth doing within the current C++ interface, however, as it'd require some unpleasant mucking around with ResultMappers.",2,DM-1332,datamanagement,address shape warning gaussianflux gaussianflux rely shape slot put noisy warning log shape slot fail probably want add new flag gaussianflux indicate failure mode entirely redundant shape slot flag figure way squash warning depend address c++ redesign consider have gaussianflux add alia schema point shape slot flag create look like specific flag failure actually link shape slot flag probably worth current c++ interface require unpleasant mucking resultmappers,"address no-shape warnings in GaussianFlux GaussianFlux relies on the shape slot, and puts noisy warnings in the logs when the shape slot fails. However, we probably don't want to add a new flag for GaussianFlux to indicate this failure mode, because it'd be entirely redundant with the shape slot flag. We should figure out some other way to squash this warning - how we do that may depend on whether this is addressed before or after the C++ redesign. We should also consider having GaussianFlux add an alias to the schema to point back at the shape slot flag, creating what looks like a specific flag for this failure while actually just being a link back to the shape slot flag. That's probably not worth doing within the current C++ interface, however, as it'd require some unpleasant mucking around with ResultMappers."
"resolve factor of two difference in GaussianFlux After changing the implementation of GaussianFlux to use the shape slot rather than estimate the shape itself by re-running the SdssShape code, Perry saw a 5-15% difference in the fluxes (I'm not sure of the sign).  The new behavior (using the shape) is consistent with what we'd have gotten with the old code when the little-used ""fixed"" config option was enabled (not surprising, as that just looked up the SdssShape measurement by name, instead of via slots).  I suspect the difference is coming in because of the factor of two between SdssShape's ""raw"" measurements - the actual Gaussian-weighted moments - and the factor of 2 it applies to make its measurements equivalent to ideal unweighted moments.  The correct weight function to use for GaussianFlux includes this factor of 2 (i.e. it's larger than the ""raw"" moments), and it's likely either the old code wasn't including this or the new code isn't.  We need to determine which one, and if necessary, fix the new code.",2,DM-1333,datamanagement,resolve factor difference gaussianflux change implementation gaussianflux use shape slot estimate shape run sdssshape code perry see 15 difference flux sure sign new behavior shape consistent get old code little fix config option enable surprising look sdssshape measurement instead slot suspect difference come factor sdssshape raw measurement actual gaussian weight moment factor apply measurement equivalent ideal unweighted moment correct weight function use gaussianflux include factor i.e. large raw moment likely old code include new code need determine necessary fix new code,"resolve factor of two difference in GaussianFlux After changing the implementation of GaussianFlux to use the shape slot rather than estimate the shape itself by re-running the SdssShape code, Perry saw a 5-15% difference in the fluxes (I'm not sure of the sign). The new behavior (using the shape) is consistent with what we'd have gotten with the old code when the little-used ""fixed"" config option was enabled (not surprising, as that just looked up the SdssShape measurement by name, instead of via slots). I suspect the difference is coming in because of the factor of two between SdssShape's ""raw"" measurements - the actual Gaussian-weighted moments - and the factor of 2 it applies to make its measurements equivalent to ideal unweighted moments. The correct weight function to use for GaussianFlux includes this factor of 2 (i.e. it's larger than the ""raw"" moments), and it's likely either the old code wasn't including this or the new code isn't. We need to determine which one, and if necessary, fix the new code."
"Test the creation of basic OpenStack instances on the new ISL testbed [IceHouse] A new version & implementation of the ISL OpenStack testbed is up and running. The new cloud is using IceHouse, the ninth OpenStack release. We get started on this platform by verifying that basic instance creation is working.  We target the creation of an instance through the (Horizon) GUI interface,  and via the nova CLI. ",1,DM-1334,datamanagement,test creation basic openstack instance new isl testbe icehouse new version implementation isl openstack testbe run new cloud icehouse ninth openstack release start platform verify basic instance creation work target creation instance horizon gui interface nova cli,"Test the creation of basic OpenStack instances on the new ISL testbed [IceHouse] A new version & implementation of the ISL OpenStack testbed is up and running. The new cloud is using IceHouse, the ninth OpenStack release. We get started on this platform by verifying that basic instance creation is working. We target the creation of an instance through the (Horizon) GUI interface, and via the nova CLI."
"Create instance with a Floating IP Associated through the nova CLI We see that in working with the Horizon GUI, it is fairly straightforward to give an instance a public IP address by associating a Floating IP with the current local IP.    However, we will want to be able to accomplish this task both remotely and programmatically within workflow.  As a step towards this, we target the solution of this via the nova CLI.",2,DM-1335,datamanagement,create instance float ip associated nova cli work horizon gui fairly straightforward instance public ip address associate float ip current local ip want able accomplish task remotely programmatically workflow step target solution nova cli,"Create instance with a Floating IP Associated through the nova CLI We see that in working with the Horizon GUI, it is fairly straightforward to give an instance a public IP address by associating a Floating IP with the current local IP. However, we will want to be able to accomplish this task both remotely and programmatically within workflow. As a step towards this, we target the solution of this via the nova CLI."
"Review advance doc to review with the data center working group Consult with M. Freemon on the sizing model given the generic description of the computing needed tier-3 and tier-2 support. Make a pass through the underlying data center standards doc for things  ""any competent designer should know""  for example wall plugs on separate circuits form computer circuits.  add that to the document, and call meeting of working group for review in late Oct.",4,DM-1336,datamanagement,review advance doc review datum center working group consult m. freemon sizing model give generic description computing need tier-3 tier-2 support pass underlie data center standard doc thing competent designer know example wall plug separate circuit form computer circuit add document meeting working group review late oct.,"Review advance doc to review with the data center working group Consult with M. Freemon on the sizing model given the generic description of the computing needed tier-3 and tier-2 support. Make a pass through the underlying data center standards doc for things ""any competent designer should know"" for example wall plugs on separate circuits form computer circuits. add that to the document, and call meeting of working group for review in late Oct."
"Make QSERV_RUN_DIR scripts able to detect qserv install paths using eups Ticket related to Mario email (subject: [QSERV-L] Some points/actions from the discussion today) :  Making your ""qserv data"" directory independent of where qserv is installed   I think this is a big one, and largely independent of EUPS. You have a problem where you want to use one set of test data potentially with different qserv binaries (not at the same time, of course). I'd argue you should refactor the scripts generated by qserv-configure to either:   * get _all_ their information about various paths from a _single_ file, for example, from etc/paths.cfg.sh. Then you can easily regenerate just that file when you need to switch to a different qserv (or zookeper, or what not), or... * refactor the generated scripts to learn from the environment which binaries to run. I.e., if $QSERV_DIR is defined, use that qserv, etc. This will let you switch binaries by simply setup-ing the new one with EUPS.   The two are not mutually exclusive -- e.g., all of this logic could be in etc/paths.cfg.sh, and depending on whether this is a development build or a ""non-EUPS"" build, it can either pick up the paths from the environment or hardcode them.   Assuming you did that, your development loop may look something like this:   {code}     # assuming that qserv-configure.py has already been run     # in ../qserv-run       # do something with qserv-a clone     cd qserv-a     setup -r .     ... do some edits ...     scons       # now do the tests     cd ../qserv-run     ./bin/qserv-start.sh     ... do tests ...     ./bin/qserv-stop.sh       # now switch to qserv-b clone     cd ../qserv-b     setup -r .       # and do the tests again     cd ../qserv-run     ./bin/qserv-start.sh     ... do tests ...     ./bin/qserv-stop.sh  {code}  that is, as qserv-start picks up the relevant products from the environment, there's no need to rebuild/reconfigure the qserv-rundirectory each time. ",7,DM-1338,datamanagement,qserv_run_dir script able detect qserv install path eup ticket relate mario email subject qserv point action discussion today make qserv data directory independent qserv instal think big largely independent eups problem want use set test datum potentially different qserv binary time course argue refactor script generate qserv configure information path single file example etc paths.cfg.sh easily regenerate file need switch different qserv zookeper refactor generate script learn environment binarie run i.e. qserv_dir define use qserv etc let switch binary simply setup ing new eups mutually exclusive e.g. logic etc paths.cfg.sh depend development build non eups build pick path environment hardcode assume development loop look like code assume qserv-configure.py run /qserv run qserv clone cd qserv setup edit scon test cd run ./bin qserv start.sh test qserv stop.sh switch qserv clone cd /qserv setup test cd run ./bin qserv start.sh test qserv stop.sh code qserv start pick relevant product environment need rebuild reconfigure qserv rundirectory time,"Make QSERV_RUN_DIR scripts able to detect qserv install paths using eups Ticket related to Mario email (subject: [QSERV-L] Some points/actions from the discussion today) : Making your ""qserv data"" directory independent of where qserv is installed I think this is a big one, and largely independent of EUPS. You have a problem where you want to use one set of test data potentially with different qserv binaries (not at the same time, of course). I'd argue you should refactor the scripts generated by qserv-configure to either: * get _all_ their information about various paths from a _single_ file, for example, from etc/paths.cfg.sh. Then you can easily regenerate just that file when you need to switch to a different qserv (or zookeper, or what not), or... * refactor the generated scripts to learn from the environment which binaries to run. I.e., if $QSERV_DIR is defined, use that qserv, etc. This will let you switch binaries by simply setup-ing the new one with EUPS. The two are not mutually exclusive -- e.g., all of this logic could be in etc/paths.cfg.sh, and depending on whether this is a development build or a ""non-EUPS"" build, it can either pick up the paths from the environment or hardcode them. Assuming you did that, your development loop may look something like this: {code} # assuming that qserv-configure.py has already been run # in ../qserv-run # do something with qserv-a clone cd qserv-a setup -r . ... do some edits ... scons # now do the tests cd ../qserv-run ./bin/qserv-start.sh ... do tests ... ./bin/qserv-stop.sh # now switch to qserv-b clone cd ../qserv-b setup -r . # and do the tests again cd ../qserv-run ./bin/qserv-start.sh ... do tests ... ./bin/qserv-stop.sh {code} that is, as qserv-start picks up the relevant products from the environment, there's no need to rebuild/reconfigure the qserv-rundirectory each time."
Read through log4cxx documentation and log.git code Read through the log4cxx documentation and become familiar with how the log.git package is set up.,2,DM-1340,datamanagement,read log4cxx documentation log.git code read log4cxx documentation familiar log.git package set,Read through log4cxx documentation and log.git code Read through the log4cxx documentation and become familiar with how the log.git package is set up.
"Write/configure tests with existing configurations and appenders In order to be come more familiar with how to use the log.git package, write some tests to see how existing configurations and appenders are used by the log.git package.",4,DM-1341,datamanagement,write configure test exist configuration appender order come familiar use log.git package write test exist configuration appender log.git package,"Write/configure tests with existing configurations and appenders In order to be come more familiar with how to use the log.git package, write some tests to see how existing configurations and appenders are used by the log.git package."
"Write Unit test for new DM message appender class Write unit tests for DM message appender class.  This might also require some tests for a configurator class, if that class is created.",2,DM-1343,datamanagement,write unit test new dm message appender class write unit test dm message appender class require test configurator class class create,"Write Unit test for new DM message appender class Write unit tests for DM message appender class. This might also require some tests for a configurator class, if that class is created."
"Define gather/scatter mechanism for tasks with generic API Define gather/scatter mechanism for tasks with generic API.  This will use event services (DM Messages) initially, but we would like to support MPI or other communication mechanisms as well.  This will involve consulting with Paul about how the existing code is structured. ",8,DM-1344,datamanagement,define gather scatter mechanism task generic api define gather scatter mechanism task generic api use event service dm messages initially like support mpi communication mechanism involve consult paul exist code structure,"Define gather/scatter mechanism for tasks with generic API Define gather/scatter mechanism for tasks with generic API. This will use event services (DM Messages) initially, but we would like to support MPI or other communication mechanisms as well. This will involve consulting with Paul about how the existing code is structured."
OCS Middleware workshop Attend OCS Middleware workshop.  This will probably have be done remotely because I have a personal conflict with that time that will prevent me from attending in person.,4,DM-1345,datamanagement,ocs middleware workshop attend ocs middleware workshop probably remotely personal conflict time prevent attend person,OCS Middleware workshop Attend OCS Middleware workshop. This will probably have be done remotely because I have a personal conflict with that time that will prevent me from attending in person.
"Write example programs for OCS Middleware Write some example programs to get familiar with the OCS middleware software. The OCS middleware will later be integrated with the AP, in the base dmcs and replicator jobs.",4,DM-1346,datamanagement,write example program ocs middleware write example program familiar ocs middleware software ocs middleware later integrate ap base dmcs replicator job,"Write example programs for OCS Middleware Write some example programs to get familiar with the OCS middleware software. The OCS middleware will later be integrated with the AP, in the base dmcs and replicator jobs."
"Refine Event base class to allow ActiveMQ filterable settings The current Event.cc base class needs to be refined to remove and old-style data release terms that aren't used anymore.  Plus, it needs to be easily extensible to allow other types of dictionaries of terms that will be used in the message headers to make them filterable on the server side.",2,DM-1347,datamanagement,refine event base class allow activemq filterable setting current event.cc base class need refine remove old style datum release term anymore plus need easily extensible allow type dictionary term message header filterable server,"Refine Event base class to allow ActiveMQ filterable settings The current Event.cc base class needs to be refined to remove and old-style data release terms that aren't used anymore. Plus, it needs to be easily extensible to allow other types of dictionaries of terms that will be used in the message headers to make them filterable on the server side."
Update tests to use unit test framework The tests for this package predate the unit test framework that other package use.  Update the tests to uses the unit test framework and get rid of any duplicate  or obsolete tests.,5,DM-1348,datamanagement,update test use unit test framework test package predate unit test framework package use update test use unit test framework rid duplicate obsolete test,Update tests to use unit test framework The tests for this package predate the unit test framework that other package use. Update the tests to uses the unit test framework and get rid of any duplicate or obsolete tests.
"ORIGINATORID value can churn too quickly. The ORIGINATORID is a 64-bit word consisting of an IPv4 host address, 16-bit process id, and 16-bit local value.   In addition to the 16-bit process id not being standard across platforms (Mac OS >Leopard goes to 99999), the churn rate for the local value should be much higher than just 16 bits.  This could be fixed to changing ORIGINATORID to a 32-bit process id and a separate value for the local value, which would be specified together in the DM event selector.   I have to look into this more to see if this is a viable solution.  This might need to go to three separate values to future proof it (i.e., ipv6).",8,DM-1352,datamanagement,originatorid value churn quickly originatorid 64 bit word consist ipv4 host address 16 bit process 16 bit local value addition 16 bit process standard platform mac os leopard go 99999 churn rate local value high 16 bit fix change originatorid 32 bit process separate value local value specify dm event selector look viable solution need separate value future proof i.e. ipv6,"ORIGINATORID value can churn too quickly. The ORIGINATORID is a 64-bit word consisting of an IPv4 host address, 16-bit process id, and 16-bit local value. In addition to the 16-bit process id not being standard across platforms (Mac OS >Leopard goes to 99999), the churn rate for the local value should be much higher than just 16 bits. This could be fixed to changing ORIGINATORID to a 32-bit process id and a separate value for the local value, which would be specified together in the DM event selector. I have to look into this more to see if this is a viable solution. This might need to go to three separate values to future proof it (i.e., ipv6)."
"Install docker 1.1.2 in an ISL OpenStack CentOS  instance, perform basic checks Install docker 1.1.2 in an ISL OpenStack CentOS 6.5 instance, and perform  basic checks such stopping and starting the docker daemon, changing default settings such as size limit of containers/images,  pulling  standard images from docker hub, starting containers from these images, etc. ",2,DM-1354,datamanagement,install docker 1.1.2 isl openstack centos instance perform basic check install docker 1.1.2 isl openstack centos 6.5 instance perform basic check stop start docker daemon change default setting size limit container image pull standard image docker hub start container image etc,"Install docker 1.1.2 in an ISL OpenStack CentOS instance, perform basic checks Install docker 1.1.2 in an ISL OpenStack CentOS 6.5 instance, and perform basic checks such stopping and starting the docker daemon, changing default settings such as size limit of containers/images, pulling standard images from docker hub, starting containers from these images, etc."
"Re-arrange how Qserv directories are installed we already touched question about installation directory structure at a meeting, maybe we can improve things by re-arranging how things are installed      we are currently installing stuff into four directories: cfg, bin, lib, proxy     to make it look more standard and to avoid clash with qserv source directories we could move cfg and proxy to a different location (something like share/qserv to make it more root-install-friendly in case we ever want to install under /usr)     this change (if you want to do it) deserves separate ticket, do not do it in this ticket ",3,DM-1355,datamanagement,arrange qserv directory instal touch question installation directory structure meeting maybe improve thing arrange thing instal currently instal stuff directory cfg bin lib proxy look standard avoid clash qserv source directory cfg proxy different location like share qserv root install friendly case want install change want deserve separate ticket ticket,"Re-arrange how Qserv directories are installed we already touched question about installation directory structure at a meeting, maybe we can improve things by re-arranging how things are installed we are currently installing stuff into four directories: cfg, bin, lib, proxy to make it look more standard and to avoid clash with qserv source directories we could move cfg and proxy to a different location (something like share/qserv to make it more root-install-friendly in case we ever want to install under /usr) this change (if you want to do it) deserves separate ticket, do not do it in this ticket"
"End-to-end demo  fails to exit with the correct status  when Warning would be correct. The output  of demo2012 results in an output file which is compared against a benchmarked file. Currently the comparison allows a deviation from the benchmark based ""on the the number of digits in the significands used in multiple precision arithmetic"";  that  number is currently set to 11.  An example using that setting is: @ Absolute error = 5.9973406400e-1, Relative error = 9.1865836000e-4 ##2566    #:25  <== 29.7751550737 ##2566    #:25  ==> 29.7478269835  Additionally, the current use returns: a 'Fatal' error if the code itself fails to execute correctly; a ""Warning' error if the any of the benchmarked quantities do not meet the comparison criteria; 'Success' if the comparison meets all criteria.  It is noted that the buildbot 'Warning' color indicator is not currently being displayed when the comparison fails. That is a coding error.  This Issue will: * ensure that BUILDBOT_WARNING(S) causes the correct display color when appropriate.   This Ticket has been split into two parts. This is part 1.  Part 2 is DM-1379 ",2,DM-1358,datamanagement,end end demo fail exit correct status warning correct output demo2012 result output file compare benchmarked file currently comparison allow deviation benchmark base number digit significand multiple precision arithmetic number currently set 11 example setting absolute error 5.9973406400e-1 relative error 9.1865836000e-4 2566 25 29.7751550737 2566 25 29.7478269835 additionally current use return fatal error code fail execute correctly warning error benchmarke quantity meet comparison criterion success comparison meet criterion note buildbot warning color indicator currently display comparison fail code error issue ensure buildbot_warning(s cause correct display color appropriate ticket split part dm-1379,"End-to-end demo fails to exit with the correct status when Warning would be correct. The output of demo2012 results in an output file which is compared against a benchmarked file. Currently the comparison allows a deviation from the benchmark based ""on the the number of digits in the significands used in multiple precision arithmetic""; that number is currently set to 11. An example using that setting is: @ Absolute error = 5.9973406400e-1, Relative error = 9.1865836000e-4 ##2566 #:25 <== 29.7751550737 ##2566 #:25 ==> 29.7478269835 Additionally, the current use returns: a 'Fatal' error if the code itself fails to execute correctly; a ""Warning' error if the any of the benchmarked quantities do not meet the comparison criteria; 'Success' if the comparison meets all criteria. It is noted that the buildbot 'Warning' color indicator is not currently being displayed when the comparison fails. That is a coding error. This Issue will: * ensure that BUILDBOT_WARNING(S) causes the correct display color when appropriate. This Ticket has been split into two parts. This is part 1. Part 2 is DM-1379"
"improvements to PsfFlux While using it as an example for the redesign of DM-829, I came up with some ideas for how to improve PsfFlux's handling of edge/bad pixels:  - Add a flag indicating that at least one pixel was rejected  - Add thresholds (number? fraction of PSF model size?) for how many non-rejected, non-edge pixels we need to even attempt a fit.",1,DM-1359,datamanagement,improvement psfflux example redesign dm-829 come idea improve psfflux handling edge bad pixel add flag indicate pixel reject add threshold number fraction psf model size non reject non edge pixel need attempt fit,"improvements to PsfFlux While using it as an example for the redesign of DM-829, I came up with some ideas for how to improve PsfFlux's handling of edge/bad pixels: - Add a flag indicating that at least one pixel was rejected - Add thresholds (number? fraction of PSF model size?) for how many non-rejected, non-edge pixels we need to even attempt a fit."
"Fix minor loose ends from new result plumbing Some of the more minor issues raised in comments from DM-199 were left undone prior to merging. This ticket addresses them.  For more information, please see:  https://github.com/LSST/qserv/pull/2 . ",1,DM-1360,datamanagement,fix minor loose end new result plumb minor issue raise comment dm-199 leave undone prior merge ticket address information https://github.com/lsst/qserv/pull/2,"Fix minor loose ends from new result plumbing Some of the more minor issues raised in comments from DM-199 were left undone prior to merging. This ticket addresses them. For more information, please see: https://github.com/LSST/qserv/pull/2 ."
"Edit pull interface and other Summer 2014 work into LSE-68 in Word Deliverable: circulate a Word-based draft of LSE-68 in which the ""push"" interface is removed, and the ""pull"" interface is refined to include the guider and other Summer 2014 work.  Note that the use of ""pull"" for the guider applies whether or not the proposed guider redesign is accepted.",2,DM-1362,datamanagement,edit pull interface summer 2014 work lse-68 word deliverable circulate word base draft lse-68 push interface remove pull interface refine include guider summer 2014 work note use pull guider apply propose guider redesign accept,"Edit pull interface and other Summer 2014 work into LSE-68 in Word Deliverable: circulate a Word-based draft of LSE-68 in which the ""push"" interface is removed, and the ""pull"" interface is refined to include the guider and other Summer 2014 work. Note that the use of ""pull"" for the guider applies whether or not the proposed guider redesign is accepted."
"Avoid use of ~/.my.cnf (used by css watcher) See https://jira.lsstcorp.org/browse/DM-1258?focusedCommentId=29230&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-29230.  {{~/.my.cnf}} is used by css watcher, an optional tool used for monitoring css. It is a symlink to $QSERV_RUN_DIR/etc/my-client.cnf.  The css watcher could use MySQL credentials located in ~/.lsst/qserv.conf (used by integration tests wich are a Qserv/MySQL client)",2,DM-1363,datamanagement,avoid use ~/.my.cnf css watcher https://jira.lsstcorp.org/browse/dm-1258?focusedcommentid=29230&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-29230 ~/.my.cnf css watcher optional tool monitor css symlink qserv_run_dir etc client.cnf css watcher use mysql credential locate ~/.lsst qserv.conf integration test wich qserv mysql client,"Avoid use of ~/.my.cnf (used by css watcher) See https://jira.lsstcorp.org/browse/DM-1258?focusedCommentId=29230&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-29230. {{~/.my.cnf}} is used by css watcher, an optional tool used for monitoring css. It is a symlink to $QSERV_RUN_DIR/etc/my-client.cnf. The css watcher could use MySQL credentials located in ~/.lsst/qserv.conf (used by integration tests wich are a Qserv/MySQL client)"
"replace ""bad data"" flag in SdssCentroid SdssCentroid has a ""bad data"" flag that doesn't actually convey any information about what went wrong.  This should be replaced with one or more flags that provide more information.",3,DM-1364,datamanagement,replace bad datum flag sdsscentroid sdsscentroid bad data flag actually convey information go wrong replace flag provide information,"replace ""bad data"" flag in SdssCentroid SdssCentroid has a ""bad data"" flag that doesn't actually convey any information about what went wrong. This should be replaced with one or more flags that provide more information."
"Errors in testHealpixSkyMap.py There is a failing unit test when healpy is supplied.  The problem is that the method boundary is not defined in the version of healpy we supply for the sims, however boundaries does exist.  If I replace boundary with boundaries, the test passes.",1,DM-1372,datamanagement,error testhealpixskymap.py fail unit test healpy supply problem method boundary define version healpy supply sim boundary exist replace boundary boundary test pass,"Errors in testHealpixSkyMap.py There is a failing unit test when healpy is supplied. The problem is that the method boundary is not defined in the version of healpy we supply for the sims, however boundaries does exist. If I replace boundary with boundaries, the test passes."
Create Docker Image / Dockerfile for LSST Stack for CentOS6.5 Make a Dockerfile for systematic generation of docker images using a Centos6.5 base image  containing the LSST Stack (v9_2 at the moment) and library dependencies.,2,DM-1375,datamanagement,create docker image dockerfile lsst stack centos6.5 dockerfile systematic generation docker image centos6.5 base image contain lsst stack v9_2 moment library dependency,Create Docker Image / Dockerfile for LSST Stack for CentOS6.5 Make a Dockerfile for systematic generation of docker images using a Centos6.5 base image containing the LSST Stack (v9_2 at the moment) and library dependencies.
"Ensure that the partition package is C++11 clean and compiles on OSX 10.9 The LSST buildbot infrastructure recently changed to building everything with --std=c++0x, which broke the partition package, and hence automated Qserv builds. While debugging this, I discovered that the partition package does not build on OSX 10.9, and considering how minimal its dependencies are, it really should. The OSX issue can be fixed by avoiding {{using boost::make_shared}}.  The partition package should be cleaned up to avoid all use of {{using}}. If we decide to use C++11 in Qserv, then the codebase should also be modernized (in particular, there are use-cases for static_assert, nullptr, etc... ). ",2,DM-1376,datamanagement,ensure partition package c++11 clean compile osx 10.9 lsst buildbot infrastructure recently change build --std c++0x break partition package automate qserv build debug discover partition package build osx 10.9 consider minimal dependency osx issue fix avoid boost::make_shared partition package clean avoid use decide use c++11 qserv codebase modernize particular use case static_assert nullptr etc ...,"Ensure that the partition package is C++11 clean and compiles on OSX 10.9 The LSST buildbot infrastructure recently changed to building everything with --std=c++0x, which broke the partition package, and hence automated Qserv builds. While debugging this, I discovered that the partition package does not build on OSX 10.9, and considering how minimal its dependencies are, it really should. The OSX issue can be fixed by avoiding {{using boost::make_shared}}. The partition package should be cleaned up to avoid all use of {{using}}. If we decide to use C++11 in Qserv, then the codebase should also be modernized (in particular, there are use-cases for static_assert, nullptr, etc... )."
"Merge Footprints from different bands/epochs The current concept of the deblender assumes that the inputs are  - A merged set of Footprints that define which pixels are part of the blend  - A merged set of Peaks within that merged Footprint  Please generate these merged Footprints (which will be defined in (x, y) coordinates in the tract/patch coordinate system). ",5,DM-1386,datamanagement,merge footprints different band epoch current concept deblender assume input merged set footprints define pixel blend merged set peaks merge footprint generate merge footprints define coordinate tract patch coordinate system,"Merge Footprints from different bands/epochs The current concept of the deblender assumes that the inputs are - A merged set of Footprints that define which pixels are part of the blend - A merged set of Peaks within that merged Footprint Please generate these merged Footprints (which will be defined in (x, y) coordinates in the tract/patch coordinate system)."
"Generate a master list of Objects given detections in multiple bands/epochs Once we've detected Sources in multiple bands we need to merge the positions to generate Objects.  This is a little complicated (or at least messy):  - The positions have errors  - If the seeing is different in different visits, objects may be blended in some but not all exposures  - If we use more than one detection pass (e.g. smoothing when looking for faint objects, not smoothing for bright) this has similar-but-different consequences (but we should probably deal with in the per-band processing)  - Objects move, so even if the positions are within the errors the motion may still be detectable ",5,DM-1387,datamanagement,generate master list object give detection multiple band epoch detect source multiple band need merge position generate object little complicated messy position error seeing different different visit object blend exposure use detection pass e.g. smooth look faint object smooth bright similar different consequence probably deal band processing object position error motion detectable,"Generate a master list of Objects given detections in multiple bands/epochs Once we've detected Sources in multiple bands we need to merge the positions to generate Objects. This is a little complicated (or at least messy): - The positions have errors - If the seeing is different in different visits, objects may be blended in some but not all exposures - If we use more than one detection pass (e.g. smoothing when looking for faint objects, not smoothing for bright) this has similar-but-different consequences (but we should probably deal with in the per-band processing) - Objects move, so even if the positions are within the errors the motion may still be detectable"
"Submit LCR for LSE-68 Create an LCR, including a summary of changes, for LSE-68.",2,DM-1388,datamanagement,submit lcr lse-68 create lcr include summary change lse-68,"Submit LCR for LSE-68 Create an LCR, including a summary of changes, for LSE-68."
"Improve output from integration tests Currently the integration tests print pages and pages of output, it is hard to see what failed and how. It'd be nice to clean that out, and instead, print some short summary, perhaps in a form of a summary table what queries has run, what succeeded, what failed etc. HTML format might be a reasonable way to display it.",6,DM-1390,datamanagement,improve output integration test currently integration test print page page output hard fail nice clean instead print short summary form summary table query run succeed fail etc html format reasonable way display,"Improve output from integration tests Currently the integration tests print pages and pages of output, it is hard to see what failed and how. It'd be nice to clean that out, and instead, print some short summary, perhaps in a form of a summary table what queries has run, what succeeded, what failed etc. HTML format might be a reasonable way to display it."
"Eups 1.5.4 requires each new shell to source the eups setups.sh eups v 1.5.4 requires each new shell to source ...eups/../bin/setups.sh.  This requires the buildbot scripts: runManifestDemo.sh, create_xlinkdocs.sh, be updated to individually  do that task.  Add  demo2012: bin/demo.sh . ",2,DM-1394,datamanagement,eup 1.5.4 require new shell source eup setups.sh eup 1.5.4 require new shell source eups/ /bin setups.sh require buildbot script runmanifestdemo.sh create_xlinkdocs.sh update individually task add demo2012 bin demo.sh,"Eups 1.5.4 requires each new shell to source the eups setups.sh eups v 1.5.4 requires each new shell to source ...eups/../bin/setups.sh. This requires the buildbot scripts: runManifestDemo.sh, create_xlinkdocs.sh, be updated to individually do that task. Add demo2012: bin/demo.sh ."
"Design CSS schema to support database deletion Need to implement deleting databases. Deliverable: a design of the system that will be capable of deleting a distributed database including all copies of that database on all workers, all replicas of all chunks are deleted. It should be possible to ""create database x"" at any time later.",2,DM-1396,datamanagement,design css schema support database deletion need implement delete database deliverable design system capable delete distribute database include copy database worker replica chunk delete possible create database time later,"Design CSS schema to support database deletion Need to implement deleting databases. Deliverable: a design of the system that will be capable of deleting a distributed database including all copies of that database on all workers, all replicas of all chunks are deleted. It should be possible to ""create database x"" at any time later."
"Improve documentation of pixel systems in obs_lsstSim There is not much documentation of the coordinate systems in use by CameraGeom.  This is by nature documentation that is instrument specific, so should go in the obs_ package for each instrument.",1,DM-1400,datamanagement,improve documentation pixel system obs_lsstsim documentation coordinate system use camerageom nature documentation instrument specific obs package instrument,"Improve documentation of pixel systems in obs_lsstSim There is not much documentation of the coordinate systems in use by CameraGeom. This is by nature documentation that is instrument specific, so should go in the obs_ package for each instrument."
"Create suite of Dockerfiles / docker images for LSST Stack for ubuntu, CentOS Building on issues DM-1317 and DM-1375 where initial images and Dockerfile's were constructed, we can now use these Dockerfile's as prototypes to extend the set of Dockerfiles & images.  We observe that by making  simple (scriptable) edits to the initial Dockerfile, we can run 'docker build' to make docker images for several combinations of OS and base compiler gcc version.",2,DM-1404,datamanagement,create suite dockerfiles docker image lsst stack ubuntu centos building issue dm-1317 dm-1375 initial image dockerfile construct use dockerfile prototype extend set dockerfiles image observe make simple scriptable edit initial dockerfile run docker build docker image combination os base compiler gcc version,"Create suite of Dockerfiles / docker images for LSST Stack for ubuntu, CentOS Building on issues DM-1317 and DM-1375 where initial images and Dockerfile's were constructed, we can now use these Dockerfile's as prototypes to extend the set of Dockerfiles & images. We observe that by making simple (scriptable) edits to the initial Dockerfile, we can run 'docker build' to make docker images for several combinations of OS and base compiler gcc version."
Prepare a SL6x openstack image for with Qserv development environment Qserv packaging procedure requires to often rebuild Qserv and relaunch integration tests.  IN2P3 Openstack platform offer next virtual machines :  {code:bash} [fjammes@ccage030 ~]$ nova flavor-list {code} | ID | Name              | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | | 1  | m1.tiny           | 512       | 0    | 0         |      | 1     | 1.0         | True      | | 15 | cc.windows.small  | 4096      | 20   | 0         |      | 2     | 1.0         | True      | | 16 | cc.windows.xlarge | 8192      | 50   | 0         |      | 4     | 1.0         | True      | | 2  | m1.small          | 2048      | 10   | 20        |      | 1     | 1.0         | True      | | 3  | m1.medium         | 4096      | 10   | 40        |      | 2     | 1.0         | True      | | 4  | m1.large          | 8192      | 10   | 80        |      | 4     | 1.0         | True      | | 5  | m1.xlarge         | 16384     | 10   | 160       |      | 8     | 1.0         | True      | | 6  | cc.lsst.medium    | 4096      | 20   | 40        |      | 2     | 1.0         | False     | | 7  | cc.lsst.large     | 16384     | 20   | 160       |      | 8     | 1.0         | False     | | 9  | cc.lsst.xlarge    | 40000     | 20   | 160       |      | 20    | 1.0         | False     |  cc.lsst.xlarge would allow a quick build/test of new Qserv release.,5,DM-1405,datamanagement,prepare sl6x openstack image qserv development environment qserv packaging procedure require rebuild qserv relaunch integration test in2p3 openstack platform offer virtual machine code bash fjammes@ccage030 nova flavor list code id memory_mb disk ephemeral swap vcpu rxtx_factor m1.tiny 512 1.0 true 15 cc.windows.small 4096 20 1.0 true 16 8192 50 1.0 true m1.small 2048 10 20 1.0 true m1.medium 4096 10 40 1.0 true m1.large 8192 10 80 1.0 true m1.xlarge 16384 10 160 1.0 true 4096 20 40 1.0 false cc.lsst.large 16384 20 160 1.0 false 40000 20 160 20 1.0 false allow quick build test new qserv release,Prepare a SL6x openstack image for with Qserv development environment Qserv packaging procedure requires to often rebuild Qserv and relaunch integration tests. IN2P3 Openstack platform offer next virtual machines : {code:bash} [fjammes@ccage030 ~]$ nova flavor-list {code} | ID | Name | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | | 1 | m1.tiny | 512 | 0 | 0 | | 1 | 1.0 | True | | 15 | cc.windows.small | 4096 | 20 | 0 | | 2 | 1.0 | True | | 16 | cc.windows.xlarge | 8192 | 50 | 0 | | 4 | 1.0 | True | | 2 | m1.small | 2048 | 10 | 20 | | 1 | 1.0 | True | | 3 | m1.medium | 4096 | 10 | 40 | | 2 | 1.0 | True | | 4 | m1.large | 8192 | 10 | 80 | | 4 | 1.0 | True | | 5 | m1.xlarge | 16384 | 10 | 160 | | 8 | 1.0 | True | | 6 | cc.lsst.medium | 4096 | 20 | 40 | | 2 | 1.0 | False | | 7 | cc.lsst.large | 16384 | 20 | 160 | | 8 | 1.0 | False | | 9 | cc.lsst.xlarge | 40000 | 20 | 160 | | 20 | 1.0 | False | cc.lsst.xlarge would allow a quick build/test of new Qserv release.
"Add return code for integration tests Integration test have to returns non-zero when failing. This will ease use of CI or debugging tools ({{git bisect}}, buildbot).",1,DM-1407,datamanagement,add return code integration test integration test return non zero fail ease use ci debugging tool git bisect buildbot,"Add return code for integration tests Integration test have to returns non-zero when failing. This will ease use of CI or debugging tools ({{git bisect}}, buildbot)."
"Create persistent volume of Cinder block storage and attach to instance We create a persistent volume of Cinder block storage and attach to working instance.  When it was created, the instance does have a specified amount of ephemeral disk, but this disk will be destroyed with the instance.  We want to test that we can create a persistent volume of block storage, attach it to an instance, format the storage with a file system, and mount the volume for use with processing, where data/output can be retained after the instance is destroyed. ",1,DM-1424,datamanagement,create persistent volume cinder block storage attach instance create persistent volume cinder block storage attach work instance create instance specify ephemeral disk disk destroy instance want test create persistent volume block storage attach instance format storage file system mount volume use processing datum output retain instance destroy,"Create persistent volume of Cinder block storage and attach to instance We create a persistent volume of Cinder block storage and attach to working instance. When it was created, the instance does have a specified amount of ephemeral disk, but this disk will be destroyed with the instance. We want to test that we can create a persistent volume of block storage, attach it to an instance, format the storage with a file system, and mount the volume for use with processing, where data/output can be retained after the instance is destroyed."
"Document how to switch Qserv or dependency in eups DM-1338 may allow to switch Qserv or dependency version using eups, without having to re-configure Qserv (i.e. without any change in QSERV_RUN_DIR).  There may have conditions for this to work. That's why it's reasonable to get some user feedback on DM-1338 before documenting this feature.",2,DM-1426,datamanagement,document switch qserv dependency eup dm-1338 allow switch qserv dependency version eup have configure qserv i.e. change qserv_run_dir condition work reasonable user feedback dm-1338 document feature,"Document how to switch Qserv or dependency in eups DM-1338 may allow to switch Qserv or dependency version using eups, without having to re-configure Qserv (i.e. without any change in QSERV_RUN_DIR). There may have conditions for this to work. That's why it's reasonable to get some user feedback on DM-1338 before documenting this feature."
"Improve Qserv Configuration Procedure Based on input from review of DM-1338, and discussions at hangout https://confluence.lsstcorp.org/display/DM/LSST+Database+Hangout+2014-10-29: implement -keepdata option.  * --keepdata will preserve qserv_meta.conf and my.cnf. Regarding the latter, this is necessary because user might have customized data_dir. * The structure of qserv_meta.conf and my.cnf must remain the same between existing version of Qserv and the to-be-configured version of Qserv. It is users's responsibility to ensure the structure did not change. Note in particular, the template files (with the exception of my.cnf) should not be customized",7,DM-1429,datamanagement,improve qserv configuration procedure base input review dm-1338 discussion hangout https://confluence.lsstcorp.org/display/dm/lsst+database+hangout+2014-10-29 implement -keepdata option preserve my.cnf necessary user customize data_dir structure qserv_meta.conf my.cnf remain exist version qserv configure version qserv user responsibility ensure structure change note particular template file exception my.cnf customize,"Improve Qserv Configuration Procedure Based on input from review of DM-1338, and discussions at hangout https://confluence.lsstcorp.org/display/DM/LSST+Database+Hangout+2014-10-29: implement -keepdata option. * --keepdata will preserve qserv_meta.conf and my.cnf. Regarding the latter, this is necessary because user might have customized data_dir. * The structure of qserv_meta.conf and my.cnf must remain the same between existing version of Qserv and the to-be-configured version of Qserv. It is users's responsibility to ensure the structure did not change. Note in particular, the template files (with the exception of my.cnf) should not be customized"
"Process sample sdss data with LSST Stack in a Docker container in OpenStack Process sample sdss data, starting with the lsst_dm_stack_demo, with LSST Stack in a Docker container in an OpenStack instance. ",2,DM-1431,datamanagement,process sample sdss datum lsst stack docker container openstack process sample sdss datum start lsst_dm_stack_demo lsst stack docker container openstack instance,"Process sample sdss data with LSST Stack in a Docker container in OpenStack Process sample sdss data, starting with the lsst_dm_stack_demo, with LSST Stack in a Docker container in an OpenStack instance."
"SUI install Qserv and test DB, enable access to catalogs for SUI team members Install Qserv and test DB in local hosts.  Enable SUI team members to access the catalogs. ",6,DM-1433,datamanagement,sui install qserv test db enable access catalog sui team member install qserv test db local host enable sui team member access catalog,"SUI install Qserv and test DB, enable access to catalogs for SUI team members Install Qserv and test DB in local hosts. Enable SUI team members to access the catalogs."
"afw tests use the same name for a dummy file: test.fits The afw package  uses a file named: test.fits in multiple testers.  If the user sets up the build to use multiple CPU (-j #), then there is the risk that the shared filename will be affected by more than one tester at a time.   In the case which provoked this Issue, the tester: testSimpleTable.py, reported that the file was missing.  A simple rerun managed to get past the error.  I recommend that the different testers use uniquely named demo files.",1,DM-1439,datamanagement,afw test use dummy file test.fit afw package use file name test.fit multiple tester user set build use multiple cpu risk share filename affect tester time case provoke issue tester testsimpletable.py report file miss simple rerun manage past error recommend different tester use uniquely name demo file,"afw tests use the same name for a dummy file: test.fits The afw package uses a file named: test.fits in multiple testers. If the user sets up the build to use multiple CPU (-j #), then there is the risk that the shared filename will be affected by more than one tester at a time. In the case which provoked this Issue, the tester: testSimpleTable.py, reported that the file was missing. A simple rerun managed to get past the error. I recommend that the different testers use uniquely named demo files."
"Github Transition: Naming conventions for repositories The Simulations team has requested that repos in general and the numerous DM repos in particular are prefixed in a way that would make fitering them out of searches and display easy (for example, their repos are prefixed sims_*)  This would be an evident useability aid to DM developers and outside contributors too.   Obtain a decision on how to allow users to quickly isolate repositories they are interested in. ",3,DM-1440,datamanagement,github transition naming convention repository simulations team request repos general numerous dm repos particular prefix way fitere search display easy example repos prefix sim evident useability aid dm developer outside contributor obtain decision allow user quickly isolate repository interested,"Github Transition: Naming conventions for repositories The Simulations team has requested that repos in general and the numerous DM repos in particular are prefixed in a way that would make fitering them out of searches and display easy (for example, their repos are prefixed sims_*) This would be an evident useability aid to DM developers and outside contributors too. Obtain a decision on how to allow users to quickly isolate repositories they are interested in."
Github Transition: Storage of large data files Github currently has fixed 100MB per blob or 1GB per repo limits. Sims has at least one file in its repo whose history exceeds this (sims_meas) and this has been raised as an issue before.  Obtain a decision on a suitable way forward for the time being that would allow the upload of the repositories on Github. Experienced to be reviewed in a few months time to consider whether has proved satsifactory.,4,DM-1441,datamanagement,github transition storage large data file github currently fix 100 mb blob gb repo limit sim file repo history exceed sims_mea raise issue obtain decision suitable way forward time allow upload repository github experience review month time consider prove satsifactory,Github Transition: Storage of large data files Github currently has fixed 100MB per blob or 1GB per repo limits. Sims has at least one file in its repo whose history exceeds this (sims_meas) and this has been raised as an issue before. Obtain a decision on a suitable way forward for the time being that would allow the upload of the repositories on Github. Experienced to be reviewed in a few months time to consider whether has proved satsifactory.
"Github Transition: Stash-stored pull requests, extraction Extract comments from Atlassian Stash made during pull requests /code reviews and their associated SHA1s if it all possible.   ",4,DM-1442,datamanagement,github transition stash store pull request extraction extract comment atlassian stash pull request review associate sha1s possible,"Github Transition: Stash-stored pull requests, extraction Extract comments from Atlassian Stash made during pull requests /code reviews and their associated SHA1s if it all possible."
"Github Transition: pull request discussion, retention - proof of concept Store review comments / PR discussion into relevant git repositories  Code & process to do this on a continuous basis post-transition will be a different issue, ",1,DM-1443,datamanagement,github transition pull request discussion retention proof concept store review comment pr discussion relevant git repository code process continuous basis post transition different issue,"Github Transition: pull request discussion, retention - proof of concept Store review comments / PR discussion into relevant git repositories Code & process to do this on a continuous basis post-transition will be a different issue,"
"Test absence of individual components Test absence of individual components of the AP simulator.  Bring each down, run the system, and restart just those components to see if the system still operates as expected",2,DM-1444,datamanagement,test absence individual component test absence individual component ap simulator bring run system restart component system operate expect,"Test absence of individual components Test absence of individual components of the AP simulator. Bring each down, run the system, and restart just those components to see if the system still operates as expected"
"Fine tune czar and worker database initialization For now, Qserv databases are the same on the master and on the worker. It means that czar tables are created on the worker and vice-versa.  This ticket aims at creating the right tables at the right places, and also at sharpening permissions on these tables.  This is done by splitting configuration script qserv-czar.sh, which configure a mono-node instance, in two scripts: - qserv-czar.sh, for configuring the czar - qserv-worker.sh, for configuring workers.  see https://confluence.lsstcorp.org/display/DM/LSST+Database+Hangout+2014-10-29  Please note that this code can't be validated with mono-node integration tests (whereas it doesn't break them): indeed it requires a mono-node installation with 2 MySQL instance, one for the worker and one for the master. Updating the mono-node configuration and integration tests with such a feature would make them far more complex.  A quick and dirty, hard-coded, testbed is available in u/fjammes/DM-1446-test. It has been used successfully to test this ticket.  This ticket will also be validated during next Qserv install on in2p3 clusters.",4,DM-1446,datamanagement,fine tune czar worker database initialization qserv database master worker mean czar table create worker vice versa ticket aim create right table right place sharpen permission table split configuration script qserv-czar.sh configure mono node instance script qserv-czar.sh configure czar qserv-worker.sh configure worker https://confluence.lsstcorp.org/display/dm/lsst+database+hangout+2014-10-29 note code validate mono node integration test break require mono node installation mysql instance worker master update mono node configuration integration test feature far complex quick dirty hard code testbe available fjamme dm-1446 test successfully test ticket ticket validate qserv install in2p3 cluster,"Fine tune czar and worker database initialization For now, Qserv databases are the same on the master and on the worker. It means that czar tables are created on the worker and vice-versa. This ticket aims at creating the right tables at the right places, and also at sharpening permissions on these tables. This is done by splitting configuration script qserv-czar.sh, which configure a mono-node instance, in two scripts: - qserv-czar.sh, for configuring the czar - qserv-worker.sh, for configuring workers. see https://confluence.lsstcorp.org/display/DM/LSST+Database+Hangout+2014-10-29 Please note that this code can't be validated with mono-node integration tests (whereas it doesn't break them): indeed it requires a mono-node installation with 2 MySQL instance, one for the worker and one for the master. Updating the mono-node configuration and integration tests with such a feature would make them far more complex. A quick and dirty, hard-coded, testbed is available in u/fjammes/DM-1446-test. It has been used successfully to test this ticket. This ticket will also be validated during next Qserv install on in2p3 clusters."
"Move code for mock images into afw so it reusable. There is some code in the exampleUtils in $IP_ISR_DIR/examples that could be of wider use.  Specifically there is code to generate mock darks, flats, and raw data from a mock camera.  There is also code to generate a mock dataRef.  It could be used more widely if moved someplace else.  Russell suggested afw.cameraGeom.utils.",1,DM-1448,datamanagement,code mock image afw reusable code exampleutil ip_isr_dir example wide use specifically code generate mock dark flat raw datum mock camera code generate mock dataref widely move someplace russell suggest afw.camerageom.utils,"Move code for mock images into afw so it reusable. There is some code in the exampleUtils in $IP_ISR_DIR/examples that could be of wider use. Specifically there is code to generate mock darks, flats, and raw data from a mock camera. There is also code to generate a mock dataRef. It could be used more widely if moved someplace else. Russell suggested afw.cameraGeom.utils."
newinstall.sh should check that python2 is available The standards now suggest using: {code} #!/usr/bin/env python2 {code} instead of {code} #!/usr/bin/env python {code} This doesn't work with (at least) Mac OSX 10.9 system python.  newinstall.sh should check if this works and suggest a fix (creating a symlink).,1,DM-1449,datamanagement,newinstall.sh check python2 available standard suggest code /usr bin env python2 code instead code /usr bin env python code work mac osx 10.9 system python newinstall.sh check work suggest fix create symlink,newinstall.sh should check that python2 is available The standards now suggest using: {code} #!/usr/bin/env python2 {code} instead of {code} #!/usr/bin/env python {code} This doesn't work with (at least) Mac OSX 10.9 system python. newinstall.sh should check if this works and suggest a fix (creating a symlink).
"Add data versions to Zookeeper Track versions of data inside zookeeper, and detect from Qserv code if Qserv code is compatible with given format of data.",2,DM-1453,datamanagement,add datum version zookeeper track version datum inside zookeeper detect qserv code qserv code compatible give format datum,"Add data versions to Zookeeper Track versions of data inside zookeeper, and detect from Qserv code if Qserv code is compatible with given format of data."
C++ Redesign -- Result definition for custom algorithms Additions to Jim's redesign to make it easier to define custom results.,3,DM-1461,datamanagement,c++ redesign result definition custom algorithm addition jim redesign easy define custom result,C++ Redesign -- Result definition for custom algorithms Additions to Jim's redesign to make it easier to define custom results.
Add NaN check to PixelFlags The test of PixelFlags in measureSources.py (from measAlg) requires a check to be sure that the center inputs are not NaN.,1,DM-1462,datamanagement,add nan check pixelflags test pixelflags measuresources.py measalg require check sure center input nan.,Add NaN check to PixelFlags The test of PixelFlags in measureSources.py (from measAlg) requires a check to be sure that the center inputs are not NaN.
SdssShape shiftMax config item is being ignored The code we ported from meas_algorithms sets the maxShift to 2 without regard to the config item which is supposed to set that value.,1,DM-1463,datamanagement,sdssshape shiftmax config item ignore code port meas_algorithm set maxshift regard config item suppose set value,SdssShape shiftMax config item is being ignored The code we ported from meas_algorithms sets the maxShift to 2 without regard to the config item which is supposed to set that value.
Design Review prep for C++ redesign Write up the design developed on DM-829 and push it through review.,1,DM-1464,datamanagement,design review prep c++ redesign write design develop dm-829 push review,Design Review prep for C++ redesign Write up the design developed on DM-829 and push it through review.
"Github Transition Plan: Reverse mirror for beta-tester repositories Test the reverse mirror for beta-testers.  Straw man:  1. Break the mirror for anybody beta-testing github workflow 2. Mirror back to new gitolite area: ""mirror"")  Method:  https://help.github.com/articles/duplicating-a-repository/  ",1,DM-1469,datamanagement,github transition plan reverse mirror beta tester repository test reverse mirror beta tester straw man break mirror anybody beta testing github workflow mirror new gitolite area mirror method https://help.github.com/articles/duplicating-a-repository/,"Github Transition Plan: Reverse mirror for beta-tester repositories Test the reverse mirror for beta-testers. Straw man: 1. Break the mirror for anybody beta-testing github workflow 2. Mirror back to new gitolite area: ""mirror"") Method: https://help.github.com/articles/duplicating-a-repository/"
"Execute multi-platform lsst_dm_stack_demo test with Fig orchestration In DM-1431 we demonstrated processing of sample data within Docker containers within an OpenStack instance.  Data was processed for containers based on CentOS6.5, Ubuntu 13.10, Ubuntu 14.10.  We consider a multi-platform ""testing"" scenario, where we process the sample data in numerous containers based on various platforms/OSs all simultaneously on an OpenStack instance.   This scenario entails starting up and managing multiple Docker containers.  Fig (http://www.fig.sh) is a new tool for starting up and managing services via Docker containers.   It is a common use case with Docker to have individual components of an application each run separately in a container (resulting in numerous  containers running on a node or cloud env, i.e., a 'Docker stack'), with the containers having linkages/dependencies to be managed. Fig may be used to encode the relationship between the containers in the Docker stack, and to start up such a set of containers.  We install and examine Fig, and apply Fig to our multi-platform ""testing"" scenario.   ",4,DM-1473,datamanagement,execute multi platform lsst_dm_stack_demo test fig orchestration dm-1431 demonstrate processing sample datum docker container openstack instance datum process container base centos6.5 ubuntu 13.10 ubuntu 14.10 consider multi platform testing scenario process sample datum numerous container base platform oss simultaneously openstack instance scenario entail start manage multiple docker container fig http://www.fig.sh new tool start manage service docker container common use case docker individual component application run separately container result numerous container run node cloud env i.e. docker stack container have linkage dependency manage fig encode relationship container docker stack start set container install examine fig apply fig multi platform testing scenario,"Execute multi-platform lsst_dm_stack_demo test with Fig orchestration In DM-1431 we demonstrated processing of sample data within Docker containers within an OpenStack instance. Data was processed for containers based on CentOS6.5, Ubuntu 13.10, Ubuntu 14.10. We consider a multi-platform ""testing"" scenario, where we process the sample data in numerous containers based on various platforms/OSs all simultaneously on an OpenStack instance. This scenario entails starting up and managing multiple Docker containers. Fig (http://www.fig.sh) is a new tool for starting up and managing services via Docker containers. It is a common use case with Docker to have individual components of an application each run separately in a container (resulting in numerous containers running on a node or cloud env, i.e., a 'Docker stack'), with the containers having linkages/dependencies to be managed. Fig may be used to encode the relationship between the containers in the Docker stack, and to start up such a set of containers. We install and examine Fig, and apply Fig to our multi-platform ""testing"" scenario."
Fix 2014_09 documentation Replace {{NEWINSTALL_URL=http://sw.lsstcorp.org/pkgs/}} with: {{NEWINSTALL_URL=http://sw.lsstcorp.org/eupspkg/}}  and {{eups distrib install qserv_distrib 2014_10.0}} with:  {{eups distrib install qserv_distrib -t 2014_10}},1,DM-1475,datamanagement,fix 2014_09 documentation replace newinstall_url http://sw.lsstcorp.org pkgs/ newinstall_url http://sw.lsstcorp.org eupspkg/ eup distrib install qserv_distrib 2014_10.0 eup distrib install qserv_distrib -t 2014_10,Fix 2014_09 documentation Replace {{NEWINSTALL_URL=http://sw.lsstcorp.org/pkgs/}} with: {{NEWINSTALL_URL=http://sw.lsstcorp.org/eupspkg/}} and {{eups distrib install qserv_distrib 2014_10.0}} with: {{eups distrib install qserv_distrib -t 2014_10}}
"Secure MySQL root password in configuration templates MySQL password in written in multiple file during configuration procedure. One single file (QSERV_RUN_DIR/tmp/my.cnf) should be used, and removed at the end of configuration procedure. qserv-meta.conf also contains MySQL password and should be also secured (move password to qserv-configure.py cmd line?).",4,DM-1476,datamanagement,secure mysql root password configuration template mysql password write multiple file configuration procedure single file qserv_run_dir tmp my.cnf remove end configuration procedure qserv-meta.conf contain mysql password secure password qserv-configure.py cmd line,"Secure MySQL root password in configuration templates MySQL password in written in multiple file during configuration procedure. One single file (QSERV_RUN_DIR/tmp/my.cnf) should be used, and removed at the end of configuration procedure. qserv-meta.conf also contains MySQL password and should be also secured (move password to qserv-configure.py cmd line?)."
Study fig to manage Qserv cluster http://www.fig.sh/  coordinate with @GregDaues who is also investigating fig.,5,DM-1477,datamanagement,study fig manage qserv cluster http://www.fig.sh/ coordinate @gregdaue investigate fig,Study fig to manage Qserv cluster http://www.fig.sh/ coordinate with @GregDaues who is also investigating fig.
Buildbot master takes exception when exiting from mail notifier after dynamic email sent. Buildbot master exits without posting the required statically-addressed email notification if a dynamically-addressed was sent.   This fix needs to ensure that the required (by buildbot specification) static email is sent even if it has to be directed to a dead-letter box (which it is).,3,DM-1480,datamanagement,buildbot master take exception exit mail notifier dynamic email send buildbot master exit post required statically address email notification dynamically address send fix need ensure require buildbot specification static email send direct dead letter box,Buildbot master takes exception when exiting from mail notifier after dynamic email sent. Buildbot master exits without posting the required statically-addressed email notification if a dynamically-addressed was sent. This fix needs to ensure that the required (by buildbot specification) static email is sent even if it has to be directed to a dead-letter box (which it is).
Finalize the Design of Query Metadata Fine-tune the design outlined in DM-1251 (metadata for capturing information about running queries in Qserv.).,6,DM-1488,datamanagement,finalize design query metadata fine tune design outline dm-1251 metadata capture information run query qserv,Finalize the Design of Query Metadata Fine-tune the design outlined in DM-1251 (metadata for capturing information about running queries in Qserv.).
"Modify czar to interact with query metadata Modify czar code: it should interact with the query metadata (store information for long running queries, retrieving).",6,DM-1489,datamanagement,modify czar interact query metadata modify czar code interact query metadata store information long running query retrieve,"Modify czar to interact with query metadata Modify czar code: it should interact with the query metadata (store information for long running queries, retrieving)."
"Add queries on partitioned table for Qserv integration test dataset #3 Qserv test dataset #3 runs only queries on non-partitioned tables. Adding queries on partitioned table would increase drastically the coverage of these tests.  Please note that a long-term solution could be to launch all the *.FIXME queries and to have a more detailed report. For example define queries which must pass for the integration test to succeed, and test queries which may pass and log their results in a report.",4,DM-1493,datamanagement,add query partition table qserv integration test dataset qserv test dataset run query non partitioned table add query partition table increase drastically coverage test note long term solution launch .fixme query detailed report example define query pass integration test succeed test query pass log result report,"Add queries on partitioned table for Qserv integration test dataset #3 Qserv test dataset #3 runs only queries on non-partitioned tables. Adding queries on partitioned table would increase drastically the coverage of these tests. Please note that a long-term solution could be to launch all the *.FIXME queries and to have a more detailed report. For example define queries which must pass for the integration test to succeed, and test queries which may pass and log their results in a report."
"Allow newinstall.sh to run in batch mode without installing Anaconda Installing Qserv on a cluster without internet access requires newinstall.sh to run in batch mode without installing Anaconda (whose install requires internet access). So Anaconda should have a {{-anaconda=yes|no}}. Usefulness of git {{-git=yes|no}} option, against full batch option answering ""yes"" everywhere could also be studied.",2,DM-1495,datamanagement,allow newinstall.sh run batch mode instal anaconda installing qserv cluster internet access require newinstall.sh run batch mode instal anaconda install require internet access anaconda -anaconda yes|no usefulness git -git yes|no option batch option answer yes study,"Allow newinstall.sh to run in batch mode without installing Anaconda Installing Qserv on a cluster without internet access requires newinstall.sh to run in batch mode without installing Anaconda (whose install requires internet access). So Anaconda should have a {{-anaconda=yes|no}}. Usefulness of git {{-git=yes|no}} option, against full batch option answering ""yes"" everywhere could also be studied."
Package Qserv mono-node instance in Docker Learn Docker basics and then package a Qserv mono-node instance.,5,DM-1497,datamanagement,package qserv mono node instance docker learn docker basic package qserv mono node instance,Package Qserv mono-node instance in Docker Learn Docker basics and then package a Qserv mono-node instance.
Package Qserv master and worker instance in Docker Learn Docker basics and then package a Qserv mono-node instance.,5,DM-1498,datamanagement,package qserv master worker instance docker learn docker basic package qserv mono node instance,Package Qserv master and worker instance in Docker Learn Docker basics and then package a Qserv mono-node instance.
"confusing error message when enabling unregistered items in RegistryField pex_config seems to split out this confusing error message when trying to enable (i.e. append to .names) a registry item that doesn't exist: {code:hide-linenum}   File ""/home/lam3/tigress/LSST/obs_subaru/config/processCcd.py"", line 51, in <module>     root.measurement.algorithms.names |= [""jacobian"", ""focalplane""]   File ""/tigress/HSC/LSST/lsstsw/anaconda/lib/python2.7/_abcoll.py"", line 330, in __ior__     self.add(value)   File ""/tigress/HSC/LSST/lsstsw/stack/Linux64/pex_config/9.0+26/python/lsst/pex/config/configChoiceField.py"", line 72, in add     r = self.__getitem__(value, at=at) AttributeError: 'SelectionSet' object has no attribute '__getitem__' {code}",1,DM-1505,datamanagement,confuse error message enable unregistered item registryfield pex_config split confusing error message try enable i.e. append registry item exist code hide linenum file /home lam3 tigress lsst obs_subaru config processccd.py line 51 root.measurement.algorithms.name |= jacobian focalplane file /tigress hsc lsst lsstsw anaconda lib python2.7/_abcoll.py line 330 file /tigress hsc lsst lsstsw stack linux64 pex_config/9.0 26 python lsst pex config configchoicefield.py line 72 add self.__getitem__(value attributeerror selectionset object attribute getitem code,"confusing error message when enabling unregistered items in RegistryField pex_config seems to split out this confusing error message when trying to enable (i.e. append to .names) a registry item that doesn't exist: {code:hide-linenum} File ""/home/lam3/tigress/LSST/obs_subaru/config/processCcd.py"", line 51, in  root.measurement.algorithms.names |= [""jacobian"", ""focalplane""] File ""/tigress/HSC/LSST/lsstsw/anaconda/lib/python2.7/_abcoll.py"", line 330, in __ior__ self.add(value) File ""/tigress/HSC/LSST/lsstsw/stack/Linux64/pex_config/9.0+26/python/lsst/pex/config/configChoiceField.py"", line 72, in add r = self.__getitem__(value, at=at) AttributeError: 'SelectionSet' object has no attribute '__getitem__' {code}"
Support new version of newinstall.sh newinstall.sh now creates loadLSST.bash instead of loadLSST.sh. This has to be taken in account in Qserv automated install script: qserv-install.sh and in Qserv documentation.,1,DM-1506,datamanagement,support new version newinstall.sh newinstall.sh create loadlsst.bash instead loadlsst.sh take account qserv automate install script qserv-install.sh qserv documentation,Support new version of newinstall.sh newinstall.sh now creates loadLSST.bash instead of loadLSST.sh. This has to be taken in account in Qserv automated install script: qserv-install.sh and in Qserv documentation.
"Reverse image slicing doesn't work as expected The lsst.afw.image image-like classes support slicing, but reverse slicing does not work as expected. Here are some examples: {code} from lsst.afw.image import ImageF im = ImageF(10, 10) im[:,:].getDimensions() # is (10,10) as expected im[0:10, 0:10].getDimensions() # is (10,10) as expected im[::-1, ::-1] # should be the data with x and y reversed, but fails with: Box2I(Point2I(-2,-2),Extent2I(12,12)) doesn't fit in image 10x10 im[9:1:-1, :].getBBox() # starts at 0,0, as expected, but is 10x10 instead of 9x10 {code}",1,DM-1508,datamanagement,"reverse image slicing work expect lsst.afw.image image like class support slicing reverse slicing work expect example code lsst.afw.image import imagef imagef(10 10 im[:,:].getdimension 10,10 expect im[0:10 0:10].getdimension 10,10 expect im[::-1 -1 datum reverse fail box2i(point2i(-2,-2),extent2i(12,12 fit image 10x10 im[9:1:-1 .getbbox start 0,0 expect 10x10 instead 9x10 code","Reverse image slicing doesn't work as expected The lsst.afw.image image-like classes support slicing, but reverse slicing does not work as expected. Here are some examples: {code} from lsst.afw.image import ImageF im = ImageF(10, 10) im[:,:].getDimensions() # is (10,10) as expected im[0:10, 0:10].getDimensions() # is (10,10) as expected im[::-1, ::-1] # should be the data with x and y reversed, but fails with: Box2I(Point2I(-2,-2),Extent2I(12,12)) doesn't fit in image 10x10 im[9:1:-1, :].getBBox() # starts at 0,0, as expected, but is 10x10 instead of 9x10 {code}"
"Remove unnecessary config/install steps Some existing installation configuration steps are not needed any more. For instance, we don't need $RUN/q or $RUN/result any more-- these paths are handled directly by qserv code, so the filesystem lookup shouldn't ever take place.  Completion of this ticket should remove unnecessary steps and concepts from installation, simplifying future maintenance. ",2,DM-1509,datamanagement,remove unnecessary config install step exist installation configuration step need instance need run run result more-- path handle directly qserv code filesystem lookup place completion ticket remove unnecessary step concept installation simplify future maintenance,"Remove unnecessary config/install steps Some existing installation configuration steps are not needed any more. For instance, we don't need $RUN/q or $RUN/result any more-- these paths are handled directly by qserv code, so the filesystem lookup shouldn't ever take place. Completion of this ticket should remove unnecessary steps and concepts from installation, simplifying future maintenance."
"C++ Standards Rule 3-9 needs rewrite Jim Bosch reviewed Section 3 of the C++ Standard.  He noted this change which I felt warranted SAT clarification:  3-9: This is a confusing conflation of two entirely different concepts: it seems to discourage all global variables, regardless of whether they're in a namespace, but only to discourage free functions when they aren't in namespace. If that reading is correct, I think both are sensible recommendations, but they need to be clarified, and probably split up. If the reading should be that all free functions are discouraged, that'd be a terrible rule we violate all over the place. If the reading should be that global variables are only discouraged when not in a namespace, that needs to be clarified (and IMO it could be bumped up to a complete prohibition).  Refer to: https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685 Rule 3.9.   The current Rule states: 3-9. Global variables and functions SHOULD be avoided and if used MUST always be referred to using the '::' operator. _________________________________________ ::mainWindow.open(), ::applicationContext.getName(), ::erf(1.0) __________________________________________ In general, the use of global variables should be avoided. Consider using singleton objects instead. Only use where required (i.e. reusing a framework that requires it.) See Rule 5-7 ( .https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908706#C++Statements-5-7 ).  Global functions in the root namespace that are defined by standard libraries can often be avoided by using the C++ versions of the include files (e.g. ""#include <cmath>"" instead of ""#include <math.h>""). Since the C++ include files place functions in the std namespace, ""using namespace std;"", which is permitted by Rule 5-41, will allow these functions to be called without using the '::' operator. In cases where functions are only available in the C include files, the '::' operator must be used to call them. This requirement is intended to highlight that these functions are in the root namespace and are different from class methods or other namespaced free functions. ",1,DM-1510,datamanagement,c++ standards rule need rewrite jim bosch review section c++ standard note change feel warrant sat clarification confusing conflation entirely different concept discourage global variable regardless namespace discourage free function namespace reading correct think sensible recommendation need clarify probably split reading free function discourage terrible rule violate place reading global variable discourage namespace need clarify imo bump complete prohibition refer https://confluence.lsstcorp.org/pages/viewpage.action?pageid=16908685 rule 3.9 current rule state global variable function avoid refer operator mainwindow.open applicationcontext.getname erf(1.0 general use global variable avoid consider singleton object instead use require i.e. reuse framework require rule .https://confluence.lsstcorp.org/pages/viewpage.action?pageid=16908706#c++statements-5-7 global function root namespace define standard library avoid c++ version include file e.g. include instead include c++ include file place function std namespace namespace std permit rule 41 allow function call operator case function available include file operator requirement intend highlight function root namespace different class method namespaced free function,"C++ Standards Rule 3-9 needs rewrite Jim Bosch reviewed Section 3 of the C++ Standard. He noted this change which I felt warranted SAT clarification: 3-9: This is a confusing conflation of two entirely different concepts: it seems to discourage all global variables, regardless of whether they're in a namespace, but only to discourage free functions when they aren't in namespace. If that reading is correct, I think both are sensible recommendations, but they need to be clarified, and probably split up. If the reading should be that all free functions are discouraged, that'd be a terrible rule we violate all over the place. If the reading should be that global variables are only discouraged when not in a namespace, that needs to be clarified (and IMO it could be bumped up to a complete prohibition). Refer to: https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685 Rule 3.9. The current Rule states: 3-9. Global variables and functions SHOULD be avoided and if used MUST always be referred to using the '::' operator. _________________________________________ ::mainWindow.open(), ::applicationContext.getName(), ::erf(1.0) __________________________________________ In general, the use of global variables should be avoided. Consider using singleton objects instead. Only use where required (i.e. reusing a framework that requires it.) See Rule 5-7 ( .https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908706#C++Statements-5-7 ). Global functions in the root namespace that are defined by standard libraries can often be avoided by using the C++ versions of the include files (e.g. ""#include "" instead of ""#include ""). Since the C++ include files place functions in the std namespace, ""using namespace std;"", which is permitted by Rule 5-41, will allow these functions to be called without using the '::' operator. In cases where functions are only available in the C include files, the '::' operator must be used to call them. This requirement is intended to highlight that these functions are in the root namespace and are different from class methods or other namespaced free functions."
"calling extend with a SchemaMapper should support positional arguments Calling {{catalog.extend(other, mapper)}} isn't equivalent to {{catalog.extend(other, mapper=mapper)}} because the second argument is the boolean {{deep}}.  When a SchemaMapper is passed as the second argument, we should recognize it for what it is.",1,DM-1514,datamanagement,call extend schemamapper support positional argument call catalog.extend(other mapper equivalent catalog.extend(other mapper mapper second argument boolean deep schemamapper pass second argument recognize,"calling extend with a SchemaMapper should support positional arguments Calling {{catalog.extend(other, mapper)}} isn't equivalent to {{catalog.extend(other, mapper=mapper)}} because the second argument is the boolean {{deep}}. When a SchemaMapper is passed as the second argument, we should recognize it for what it is."
"sconsUtils fails to identify Ubuntu's gcc On Ubuntu 12.04, gcc --version says: {code:hide-linenum} gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3 Copyright (C) 2011 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. {code}  This apparently isn't quite what sconsUtils expected, because it says: {code:hide-linenum} scons: Reading SConscript files ... Checking who built the CC compiler...(cached) error: no result CC is unknown version unknown {code}  Happily, everything seems to work anyway, as the fall-back options for the unknown compiler work fine with this one.",1,DM-1515,datamanagement,sconsutil fail identify ubuntu gcc ubuntu 12.04 gcc say code hide linenum gcc ubuntu linaro 4.6.3 1ubuntu5 4.6.3 copyright 2011 free software foundation inc. free software source copy condition warranty merchantability fitness particular purpose code apparently sconsutil expect say code hide linenum scon read sconscript file check build cc compiler (cached error result cc unknown version unknown code happily work fall option unknown compiler work fine,"sconsUtils fails to identify Ubuntu's gcc On Ubuntu 12.04, gcc --version says: {code:hide-linenum} gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3 Copyright (C) 2011 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. {code} This apparently isn't quite what sconsUtils expected, because it says: {code:hide-linenum} scons: Reading SConscript files ... Checking who built the CC compiler...(cached) error: no result CC is unknown version unknown {code} Happily, everything seems to work anyway, as the fall-back options for the unknown compiler work fine with this one."
"Use eups swig instead of system swig during Qserv build Qserv build system use system swig, it should be fixed to use eups swig. ",3,DM-1516,datamanagement,use eup swig instead system swig qserv build qserv build system use system swig fix use eup swig,"Use eups swig instead of system swig during Qserv build Qserv build system use system swig, it should be fixed to use eups swig."
"Add support for ""SET @@session.autocommit"" The SUI team is using JDBC connector and queries like  ""SET @@session.autocommit = {0}"".format(switch)  are derailing everything. We should add support for these queries. ",2,DM-1518,datamanagement,"add support set @@session.autocommit sui team jdbc connector query like set 0}"".format(switch derail add support query","Add support for ""SET @@session.autocommit"" The SUI team is using JDBC connector and queries like ""SET @@session.autocommit = {0}"".format(switch) are derailing everything. We should add support for these queries."
"Fix confusing error message Selecting from a table that does not exist, e.g. something like:  select count(*) from whdfd  produces a strange error:  ERROR 4110 (Proxy): Qserv error: 'Unknown error setting QuerySession' ",1,DM-1520,datamanagement,fix confuse error message selecting table exist e.g. like select count whdfd produce strange error error 4110 proxy qserv error unknown error set querysession,"Fix confusing error message Selecting from a table that does not exist, e.g. something like: select count(*) from whdfd produces a strange error: ERROR 4110 (Proxy): Qserv error: 'Unknown error setting QuerySession'"
"Fix ""stripes and substripes must be natural numbers"" bug on user side:  {code} ERROR 4110 (Proxy) at line 1: Qserv error: Unexpected error: (<class 'lsst.qserv.czar.config.ConfigError'>, ConfigError(), <traceback object at 0x26b8b90>) {code}   in czar log:  {code} 1114 22:30:10.155 [0x7fafcefab700] ERROR root (app.py:370) - Unexpected error: (<class 'lsst.qserv.czar.config.ConfigError'>, ConfigError(), <traceback object at 0x26b8b90>) Traceback (most recent call last):   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 360, in __init__     self._prepareForExec()   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 437, in _prepareForExec     self._addChunks()   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 535, in _addChunks     self._computeConstraintsAsHints()   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 529, in _computeConstraintsAsHints     self.pmap = self._makePmap(self.dominantDb, self.dbStriping)   File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 478, in _makePmap     raise lsst.qserv.czar.config.ConfigError(msg) ConfigError: ""Partitioner's stripes and substripes must be natural numbers.""  {code}  To reproduce:  {code} qserv-check-integration.py --case-no 01 --load {code}  then {code} mysql --port 4040 LSST -e ""select count(*) from Object"" {code}   ",2,DM-1521,datamanagement,fix stripe substripe natural number bug user code error 4110 proxy line qserv error unexpected error configerror code czar log code 1114 22:30:10.155 0x7fafcefab700 error root app.py:370 unexpected error configerror traceback recent file /usr local home becla qservdev linux64 qserv/2014_09.0 lib python lsst qserv czar app.py line 360 init self._prepareforexec file /usr local home becla qservdev linux64 qserv/2014_09.0 lib python lsst qserv czar app.py line 437 prepareforexec self._addchunk file /usr local home becla qservdev linux64 qserv/2014_09.0 lib python lsst qserv czar app.py line 535 addchunks self._computeconstraintsashints file /usr local home becla qservdev linux64 qserv/2014_09.0 lib python lsst qserv czar app.py line 529 computeconstraintsashint self.pmap self._makepmap(self.dominantdb self.dbstriping file /usr local home becla qservdev linux64 qserv/2014_09.0 lib python lsst qserv czar app.py line 478 makepmap raise lsst.qserv.czar.config configerror(msg configerror partitioner stripe substripe natural number code reproduce code qserv-check-integration.py --case 01 code code mysql 4040 lsst select count object code,"Fix ""stripes and substripes must be natural numbers"" bug on user side: {code} ERROR 4110 (Proxy) at line 1: Qserv error: Unexpected error: (, ConfigError(), ) {code} in czar log: {code} 1114 22:30:10.155 [0x7fafcefab700] ERROR root (app.py:370) - Unexpected error: (, ConfigError(), ) Traceback (most recent call last): File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 360, in __init__ self._prepareForExec() File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 437, in _prepareForExec self._addChunks() File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 535, in _addChunks self._computeConstraintsAsHints() File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 529, in _computeConstraintsAsHints self.pmap = self._makePmap(self.dominantDb, self.dbStriping) File ""/usr/local/home/becla/qservDev/Linux64/qserv/2014_09.0/lib/python/lsst/qserv/czar/app.py"", line 478, in _makePmap raise lsst.qserv.czar.config.ConfigError(msg) ConfigError: ""Partitioner's stripes and substripes must be natural numbers."" {code} To reproduce: {code} qserv-check-integration.py --case-no 01 --load {code} then {code} mysql --port 4040 LSST -e ""select count(*) from Object"" {code}"
"Switch readMetadata' return value from PropertySet to PropertyList It'd be useful if readMetadata would return PropertyList instead of PropertySet, because in many situations order matters. For details, see discussion in DM-1517",2,DM-1524,datamanagement,switch readmetadata return value propertyset propertylist useful readmetadata return propertylist instead propertyset situation order matter detail discussion dm-1517,"Switch readMetadata' return value from PropertySet to PropertyList It'd be useful if readMetadata would return PropertyList instead of PropertySet, because in many situations order matters. For details, see discussion in DM-1517"
"Investigate halos around stars Andrew Becker reports on dm-users: {quote} We have been using the LSST stack to reduce CFHT data at UW, and have come across a potential bug in the DM software.  I don't see how this could be something intrinsic to the data, instead it seems like it could be a bug in the software triggered by its application to new data.  I have a how to reproduce at NCSA at /lsst/home/becker/CFHT.  Follow commands in the SETUP file to get an up-to-date version of the stack (b449) and a modified version of ip_isr (a hack to avoid overscan correction) and a private branch of obs_cfht (u/krughoff/ossos_support).  If you run the commands in SOURCE_ME it will run the processCcd on a single CCD, in 2 modes, yielding 2 output directories.  The second call merely sets the bin sizes to be the same amongst all the various background subtractions, but is sufficient to trigger the bug.  A basic script included in the directory differences the 2 output calexps (a straight -=) and ds9 is used to view the diff.  If you pan to e.g. 1845,546 you'll see a feature like attached in the difference (out1 on the left, out2 in the middle, diff on the right).  This can be seen in various other parts of the image, as irregular annuli around bright blended objects.  It suggests to me that during measurement some substamp manipulations are leaking into the original image.  But you can kind of see in the center image that the second run seems to trigger the issue. {quote}  And then: {quote} So to summarize:     export LSSTSW=~lsstsw    source ~lsstsw/bin/setup.sh    setup lsst_apps -t b449    setup -k -r /nfs/lsst/home/becker/LSST/DMS/obs_cfht    setup -k -r /nfs/lsst/home/becker/LSST/DMS/ip_isr    setup -k -r /nfs/lsst/home/becker/LSST/DMS/processFile    cd /nfs/lsst/home/becker/CFHT   will set up your environment.  The following is sufficient to recreate the bug, and create postIsrCCD images for processFile:     processCcd.py input/ --id visit=1612606 ccd=8 --output out1/ --config isr.doAssembleDetrends=False isr.fringeAfterFlat=False isr.fringe.pedestal=False isr.doBias=False isr.doFlat=False isr.doFringe=False isr.doWrite=True calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True     processCcd.py input/ --id visit=1612606 ccd=8 --output out2/ --config isr.doAssembleDetrends=False isr.fringeAfterFlat=False isr.fringe.pedestal=False isr.doBias=False isr.doFlat=False isr.doFringe=False isr.doWrite=True calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True detection.background.binSize=512 calibrate.detection.background.binSize=512 calibrate.background.binSize=512   Following are the calls I made to processFile.py (using my modified version of the processFile.git package), and which return images whose difference is exactly 0.  So I don't really get whats happening with processFile, but it doesn't seem to be accepting my command-line config changes.     processFile.py out1/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp1.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True  calibrate.repair.doCosmicRay=False     processFile.py out2/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp2.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True  calibrate.repair.doCosmicRay=False  detection.background.binSize=512 calibrate.detection.background.binSize=512 calibrate.background.binSize=512     processFile.py out2/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp3.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True  calibrate.repair.doCosmicRay=False  detection.background.binSize=1024 calibrate.detection.background.binSize=1024 calibrate.background.binSize=1024 {quote}",2,DM-1525,datamanagement,"investigate halo star andrew becker report dm user quote lsst stack reduce cfht datum uw come potential bug dm software intrinsic datum instead like bug software trigger application new datum reproduce ncsa /lsst home becker cfht follow command setup file date version stack b449 modify version ip_isr hack avoid overscan correction private branch obs_cfht krughoff ossos_support run command source_me run processccd single ccd mode yield output directory second merely set bin size background subtraction sufficient trigger bug basic script include directory difference output calexps straight -= ds9 view diff pan e.g. 1845,546 feature like attach difference out1 left out2 middle diff right see part image irregular annulus bright blend object suggest measurement substamp manipulation leak original image kind center image second run trigger issue quote quote summarize export lsstsw=~lsstsw source ~lsstsw bin setup.sh setup lsst_apps b449 setup -r /nfs lsst home becker lsst dms obs_cfht setup -r /nfs lsst home becker lsst dms ip_isr setup -r /nfs lsst home becker lsst dms processfile cd /nfs lsst home becker cfht set environment follow sufficient recreate bug create postisrccd image processfile processccd.py input/ --id visit=1612606 ccd=8 out1/ isr.doassembledetrends false isr.fringeafterflat false isr.fringe.pedestal false isr.dobias false isr.doflat false isr.dofringe false isr.dowrite true calibrate.dophotocal false calibrate.doastrometry false dodeblend true processccd.py input/ --id visit=1612606 ccd=8 out2/ isr.doassembledetrends false isr.fringeafterflat false isr.fringe.pedestal false isr.dobias false isr.doflat false isr.dofringe false isr.dowrite true calibrate.dophotocal false calibrate.doastrometry false dodeblend true detection.background.binsize=512 calibrate.detection.background.binsize=512 calibrate.background.binsize=512 follow call processfile.py modify version processfile.git package return image difference exactly happen processfile accept command line config change processfile.py out1 postisrccd/13ap06 e+0 0/2013 03 11 postisrccd-1612606 --outputcalexp calexp1.fit calibrate.dophotocal false calibrate.doastrometry false dodeblend true calibrate.repair.docosmicray false processfile.py out2 postisrccd/13ap06 e+0 0/2013 03 11 postisrccd-1612606 --outputcalexp calexp2.fits calibrate.dophotocal false calibrate.doastrometry false dodeblend true calibrate.repair.docosmicray false detection.background.binsize=512 calibrate.detection.background.binsize=512 calibrate.background.binsize=512 processfile.py out2 postisrccd/13ap06 e+0 0/2013 03 11 postisrccd-1612606 --outputcalexp calexp3.fit calibrate.dophotocal false calibrate.doastrometry false dodeblend true calibrate.repair.docosmicray false detection.background.binsize=1024 calibrate.detection.background.binsize=1024 calibrate.background.binsize=1024 quote","Investigate halos around stars Andrew Becker reports on dm-users: {quote} We have been using the LSST stack to reduce CFHT data at UW, and have come across a potential bug in the DM software. I don't see how this could be something intrinsic to the data, instead it seems like it could be a bug in the software triggered by its application to new data. I have a how to reproduce at NCSA at /lsst/home/becker/CFHT. Follow commands in the SETUP file to get an up-to-date version of the stack (b449) and a modified version of ip_isr (a hack to avoid overscan correction) and a private branch of obs_cfht (u/krughoff/ossos_support). If you run the commands in SOURCE_ME it will run the processCcd on a single CCD, in 2 modes, yielding 2 output directories. The second call merely sets the bin sizes to be the same amongst all the various background subtractions, but is sufficient to trigger the bug. A basic script included in the directory differences the 2 output calexps (a straight -=) and ds9 is used to view the diff. If you pan to e.g. 1845,546 you'll see a feature like attached in the difference (out1 on the left, out2 in the middle, diff on the right). This can be seen in various other parts of the image, as irregular annuli around bright blended objects. It suggests to me that during measurement some substamp manipulations are leaking into the original image. But you can kind of see in the center image that the second run seems to trigger the issue. {quote} And then: {quote} So to summarize: export LSSTSW=~lsstsw source ~lsstsw/bin/setup.sh setup lsst_apps -t b449 setup -k -r /nfs/lsst/home/becker/LSST/DMS/obs_cfht setup -k -r /nfs/lsst/home/becker/LSST/DMS/ip_isr setup -k -r /nfs/lsst/home/becker/LSST/DMS/processFile cd /nfs/lsst/home/becker/CFHT will set up your environment. The following is sufficient to recreate the bug, and create postIsrCCD images for processFile: processCcd.py input/ --id visit=1612606 ccd=8 --output out1/ --config isr.doAssembleDetrends=False isr.fringeAfterFlat=False isr.fringe.pedestal=False isr.doBias=False isr.doFlat=False isr.doFringe=False isr.doWrite=True calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True processCcd.py input/ --id visit=1612606 ccd=8 --output out2/ --config isr.doAssembleDetrends=False isr.fringeAfterFlat=False isr.fringe.pedestal=False isr.doBias=False isr.doFlat=False isr.doFringe=False isr.doWrite=True calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True detection.background.binSize=512 calibrate.detection.background.binSize=512 calibrate.background.binSize=512 Following are the calls I made to processFile.py (using my modified version of the processFile.git package), and which return images whose difference is exactly 0. So I don't really get whats happening with processFile, but it doesn't seem to be accepting my command-line config changes. processFile.py out1/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp1.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True calibrate.repair.doCosmicRay=False processFile.py out2/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp2.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True calibrate.repair.doCosmicRay=False detection.background.binSize=512 calibrate.detection.background.binSize=512 calibrate.background.binSize=512 processFile.py out2/postISRCCD/13AP06/E+0+0/2013-03-11/r/postISRCCD-1612606-08.fits --outputCalexp calexp3.fits -c calibrate.doPhotoCal=False calibrate.doAstrometry=False doDeblend=True calibrate.repair.doCosmicRay=False detection.background.binSize=1024 calibrate.detection.background.binSize=1024 calibrate.background.binSize=1024 {quote}"
"Update processFile to use new measurement framework processFile.py uses the old measurement framework in meas_algorithms, but all the other components expect it to be using the new measurement framework in meas_base.",4,DM-1526,datamanagement,update processfile use new measurement framework processfile.py use old measurement framework meas_algorithm component expect new measurement framework meas_base,"Update processFile to use new measurement framework processFile.py uses the old measurement framework in meas_algorithms, but all the other components expect it to be using the new measurement framework in meas_base."
"Draft security risks into the Center's template The Cyber security center has provided a risk template consistent with their templates,  The Center attempted to populate their templates with material from LSE 99  to the templates,   The impedance mis match was too large, The way forward is seem as  attempting a high level decomposition of risk into the templates.",4,DM-1527,datamanagement,draft security risk center template cyber security center provide risk template consistent template center attempt populate template material lse 99 template impedance mis match large way forward attempt high level decomposition risk template,"Draft security risks into the Center's template The Cyber security center has provided a risk template consistent with their templates, The Center attempted to populate their templates with material from LSE 99 to the templates, The impedance mis match was too large, The way forward is seem as attempting a high level decomposition of risk into the templates."
Identify potential KVM hardware Identify potential KVM hardware that would meet our needs.  e.g. a current version of the Avocent or Dell KVMs used at NCSA.,1,DM-1528,datamanagement,identify potential kvm hardware identify potential kvm hardware meet need e.g. current version avocent dell kvms ncsa,Identify potential KVM hardware Identify potential KVM hardware that would meet our needs. e.g. a current version of the Avocent or Dell KVMs used at NCSA.
"Reorganize docker image repositories and align with github A heterogeneous collection of docker images have been accumulating within the public docker repository  daues/lsstdistrib . Such a heterogeneous collection prevents the assignment of a ""latest"" tag to allow users to easily obtain the most recent image for a particular item (detailed version numbers, labels currently required.)     Thus we should break out the single repository into multiple repositories where are ""latest"" tag will be effective.  We also make  github repositories of matching names to hold the Dockerfiles which produced images (a common pattern for github/dockerhub usage, especially with automated builds; so we start this practice.) ",2,DM-1529,datamanagement,reorganize docker image repository align github heterogeneous collection docker image accumulate public docker repository daue lsstdistrib heterogeneous collection prevent assignment late tag allow user easily obtain recent image particular item detailed version number label currently require break single repository multiple repository late tag effective github repository match name hold dockerfile produce image common pattern github dockerhub usage especially automate build start practice,"Reorganize docker image repositories and align with github A heterogeneous collection of docker images have been accumulating within the public docker repository daues/lsstdistrib . Such a heterogeneous collection prevents the assignment of a ""latest"" tag to allow users to easily obtain the most recent image for a particular item (detailed version numbers, labels currently required.) Thus we should break out the single repository into multiple repositories where are ""latest"" tag will be effective. We also make github repositories of matching names to hold the Dockerfiles which produced images (a common pattern for github/dockerhub usage, especially with automated builds; so we start this practice.)"
Test if IPMI can replace KVM Can we use IPMI in place of a KVM?  Can we reliably do the following across our various server vendors?  - power cycle hung systems  - view and use text console  - view and use remote gui console  - boot from a remote ISO,3,DM-1530,datamanagement,test ipmi replace kvm use ipmi place kvm reliably following server vendor power cycle hung system view use text console view use remote gui console boot remote iso,Test if IPMI can replace KVM Can we use IPMI in place of a KVM? Can we reliably do the following across our various server vendors? - power cycle hung systems - view and use text console - view and use remote gui console - boot from a remote ISO
"Fix qserv_testdata documentation qserv_testdata relies on sconsUtils, and its build procedure has to be clearly documented.",1,DM-1538,datamanagement,fix qserv_testdata documentation qserv_testdata rely sconsutil build procedure clearly document,"Fix qserv_testdata documentation qserv_testdata relies on sconsUtils, and its build procedure has to be clearly documented."
"Add support for mysql JDBC driver SUI which rely on JDBC fail because they internally issue some queries that are not yet supported by Qserv. Need to patch it (in the short term), and add proper support (in the long term). This story covers the patching only. The queries that upset Qserv are listed below.   {code} SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect'  SELECT @@session.auto_increment_increment  SET NAMES latin1  SET character_set_results = NULL  SET autocommit=1  SET sql_mode='STRICT_TRANS_TABLES' {code}",2,DM-1539,datamanagement,add support mysql jdbc driver sui rely jdbc fail internally issue query support qserv need patch short term add proper support long term story cover patching query upset qserv list code variables variable_name language net_write_timeout interactive_timeout wait_timeout character_set_client character_set_connection character_set character_set_server tx_isolation transaction_isolation character_set_result timezone time_zone system_time_zone lower_case_table_name max_allowed_packet net_buffer_length sql_mode query_cache_type query_cache_size license init_connect select @@session.auto_increment_increment set names latin1 set character_set_results null set set sql_mode='strict_trans_tables code,"Add support for mysql JDBC driver SUI which rely on JDBC fail because they internally issue some queries that are not yet supported by Qserv. Need to patch it (in the short term), and add proper support (in the long term). This story covers the patching only. The queries that upset Qserv are listed below. {code} SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect' SELECT @@session.auto_increment_increment SET NAMES latin1 SET character_set_results = NULL SET autocommit=1 SET sql_mode='STRICT_TRANS_TABLES' {code}"
"Add support for BIT columns The mysql/SchemFactory code has incomplete logic for dealing with columns that have BIT type - in particular, it doesn't properly handle the length of a bit field, and it sets the SQL type to ""BIT?"", which causes table creation failures later on. Fixing this requires modifying SchemaFactory, and making sure that bit values are  transmitted properly.",4,DM-1540,datamanagement,add support bit column mysql schemfactory code incomplete logic deal column bit type particular properly handle length bit field set sql type bit cause table creation failure later fix require modify schemafactory make sure bit value transmit properly,"Add support for BIT columns The mysql/SchemFactory code has incomplete logic for dealing with columns that have BIT type - in particular, it doesn't properly handle the length of a bit field, and it sets the SQL type to ""BIT?"", which causes table creation failures later on. Fixing this requires modifying SchemaFactory, and making sure that bit values are transmitted properly."
"Add support for transmitting [VAR]BINARY column data The code that pulls data out of a {{MYSQL_ROW}} and puts it into a protobuf {{RowBundle}} does not handle binary data correctly. See [_fillRows|https://github.com/LSST/qserv/blob/master/core/modules/wdb/QueryAction.cc#L217] for the relevant code.  The issue is that the generated {{add_column(const char*)}} member function of {{RowBundle}} treats the input as C-style NULL-terminated strings. But in the case of {{BINARY}} column data (and {{VARBINARY}}/{{BLOB}} variants, maybe also {{BIT\(n\)}}), the contents can contain embedded NULLs. We are currently using such columns for user defined types in MySQL (e.g. image bounding polygons), so it's important to get this right. On the protobuf side the fix is as simple as calling {{add_column(const char* value, int size)}} instead. I'm not a MySQL C API expert, but the size will presumably have to be obtained/derived from the corresponding {{MYSQL_FIELD}}.  ",8,DM-1541,datamanagement,add support transmit var]binary column data code pull datum mysql_row put protobuf rowbundle handle binary datum correctly fillrows|https://github.com lsst qserv blob master core module wdb queryaction.cc#l217 relevant code issue generate add_column(const char member function rowbundle treat input style null terminate string case binary column datum varbinary}}/{{blob variant maybe bit\(n\ content contain embed null currently column user define type mysql e.g. image bounding polygon important right protobuf fix simple call add_column(const char value int size instead mysql api expert size presumably obtain derive correspond mysql_field,"Add support for transmitting [VAR]BINARY column data The code that pulls data out of a {{MYSQL_ROW}} and puts it into a protobuf {{RowBundle}} does not handle binary data correctly. See [_fillRows|https://github.com/LSST/qserv/blob/master/core/modules/wdb/QueryAction.cc#L217] for the relevant code. The issue is that the generated {{add_column(const char*)}} member function of {{RowBundle}} treats the input as C-style NULL-terminated strings. But in the case of {{BINARY}} column data (and {{VARBINARY}}/{{BLOB}} variants, maybe also {{BIT\(n\)}}), the contents can contain embedded NULLs. We are currently using such columns for user defined types in MySQL (e.g. image bounding polygons), so it's important to get this right. On the protobuf side the fix is as simple as calling {{add_column(const char* value, int size)}} instead. I'm not a MySQL C API expert, but the size will presumably have to be obtained/derived from the corresponding {{MYSQL_FIELD}}."
"Span-based shrink operations for Footprint Analogous to DM-1128, but shrinking rather than growing footprints.",5,DM-1545,datamanagement,span base shrink operation footprint analogous dm-1128 shrink grow footprint,"Span-based shrink operations for Footprint Analogous to DM-1128, but shrinking rather than growing footprints."
"Improve management of integration test datasets description Currently data set description (i.e. data file extension, compressed extension, schema file extension) is hard-coded in python/lsst/qserv/tests/datareader.py  This could be improve (standard format for test dataset, meta service, meta-configuration file, ...)",5,DM-1563,datamanagement,improve management integration test dataset description currently data set description i.e. datum file extension compress extension schema file extension hard code python lsst qserv test datareader.py improve standard format test dataset meta service meta configuration file,"Improve management of integration test datasets description Currently data set description (i.e. data file extension, compressed extension, schema file extension) is hard-coded in python/lsst/qserv/tests/datareader.py This could be improve (standard format for test dataset, meta service, meta-configuration file, ...)"
Create integration test case using data duplicator Integration tests should provide a new test case which use sph-duplicate in partition package.,6,DM-1570,datamanagement,create integration test case data duplicator integration test provide new test case use sph duplicate partition package,Create integration test case using data duplicator Integration tests should provide a new test case which use sph-duplicate in partition package.
Setup Qserv for SUI tests Setup Qserv on lsst-db2 with and load some reasonable data set (perhaps PT 1.2). One potential caveat: we need to setup access for some accounts that are ideally other than our internal qsmaster.,2,DM-1571,datamanagement,setup qserv sui test setup qserv lsst db2 load reasonable datum set pt 1.2 potential caveat need setup access account ideally internal qsmaster,Setup Qserv for SUI tests Setup Qserv on lsst-db2 with and load some reasonable data set (perhaps PT 1.2). One potential caveat: we need to setup access for some accounts that are ideally other than our internal qsmaster.
"Test deblended CModel colors Using HSC data, examine color-color and color-magnitude diagrams with deblended CModel magnitudes.  Investigate outliers by looking at images and deblended HeavyFootprints.",8,DM-1572,datamanagement,test deblende cmodel color hsc datum examine color color color magnitude diagram deblende cmodel magnitude investigate outlier look image deblende heavyfootprints,"Test deblended CModel colors Using HSC data, examine color-color and color-magnitude diagrams with deblended CModel magnitudes. Investigate outliers by looking at images and deblended HeavyFootprints."
Basic validation of LSST pipeline on HSC data Get the pipeline running on HSC data to the point where nothing is obviously wrong.,8,DM-1573,datamanagement,basic validation lsst pipeline hsc datum pipeline run hsc datum point obviously wrong,Basic validation of LSST pipeline on HSC data Get the pipeline running on HSC data to the point where nothing is obviously wrong.
"add support for ""freeze-drying"" measurement failures We should make it easy for users to inspect and capture problems in measurement algorithms into an on-disk format that allows them to be reproduced later with minimal setup (ideally, the package would require no access to the original data or configuration).  While this issue is mostly concerned with capturing problems in measurement algorithms, an extension to capture deblender failures should be considered as a future extension.",6,DM-1574,datamanagement,add support freeze dry measurement failure easy user inspect capture problem measurement algorithm disk format allow reproduce later minimal setup ideally package require access original datum configuration issue concerned capturing problem measurement algorithm extension capture deblender failure consider future extension,"add support for ""freeze-drying"" measurement failures We should make it easy for users to inspect and capture problems in measurement algorithms into an on-disk format that allows them to be reproduced later with minimal setup (ideally, the package would require no access to the original data or configuration). While this issue is mostly concerned with capturing problems in measurement algorithms, an extension to capture deblender failures should be considered as a future extension."
Support Mac OS in scisql-deploy.sh cf. Andy Connolly message: {quote} Just as an FYI on my mac scisql-deploy.py was looking for 0.3.4/lib/libscisql-scisql_0.3.so but there is only 0.3.4/lib/libscisql-scisql_0.3.dylib {quote},1,DM-1575,datamanagement,support mac os scisql-deploy.sh cf andy connolly message quote fyi mac scisql-deploy.py look 0.3.4 lib libscisql scisql_0.3.so 0.3.4 lib libscisql scisql_0.3.dylib quote,Support Mac OS in scisql-deploy.sh cf. Andy Connolly message: {quote} Just as an FYI on my mac scisql-deploy.py was looking for 0.3.4/lib/libscisql-scisql_0.3.so but there is only 0.3.4/lib/libscisql-scisql_0.3.dylib {quote}
"Rework Astrometry class The Astrometry class shares information upstream with AstrometryTask.  This should be factored out so that the Astrometry class can be configured and called via a single well known method (solve?).  One thing the Astrometry class that is needed by down stream processing (PhotoCal) is to match sources.  This is currently a private method, but should be made public so that  it can be used without running AstrometryTask.",4,DM-1577,datamanagement,rework astrometry class astrometry class share information upstream astrometrytask factor astrometry class configure call single know method solve thing astrometry class need stream processing photocal match source currently private method public run astrometrytask,"Rework Astrometry class The Astrometry class shares information upstream with AstrometryTask. This should be factored out so that the Astrometry class can be configured and called via a single well known method (solve?). One thing the Astrometry class that is needed by down stream processing (PhotoCal) is to match sources. This is currently a private method, but should be made public so that it can be used without running AstrometryTask."
Move photocal out of meas_astrom It is confusing that photocal is in meas_astrom.  I assume that is historical.  I think it could probably live in pipe_tasks.,2,DM-1578,datamanagement,photocal meas_astrom confuse photocal meas_astrom assume historical think probably live pipe_task,Move photocal out of meas_astrom It is confusing that photocal is in meas_astrom. I assume that is historical. I think it could probably live in pipe_tasks.
"Implement replacement for A.net index files Astrometry.net index files are hard to generate and hard to read.  We need another, more flexible, more standard system for storing reference files.  We should also be able to read FITS files and other formats, but having a standard format with the utilities to create and query them is a must.  Coming up with a format that satisfies astrometry.net's solver may be too hard, because a.net requires quads, which a non-blind solver may not need. However, a format that we can convert to something suitable for a.net would probably suffice (conversion would presumably be a separate task that is run once).  It will be easier to identify a suitable format once we have identified at least one solver other than than a.net that we wish to implement or adapt. hscAstrom appears to use a catalog of star positions, which is nice and simple.",5,DM-1579,datamanagement,implement replacement a.net index file astrometry.net index file hard generate hard read need flexible standard system store reference file able read fits file format have standard format utility create query come format satisfy astrometry.net solver hard a.net require quad non blind solver need format convert suitable a.net probably suffice conversion presumably separate task run easy identify suitable format identify solver a.net wish implement adapt hscastrom appear use catalog star position nice simple,"Implement replacement for A.net index files Astrometry.net index files are hard to generate and hard to read. We need another, more flexible, more standard system for storing reference files. We should also be able to read FITS files and other formats, but having a standard format with the utilities to create and query them is a must. Coming up with a format that satisfies astrometry.net's solver may be too hard, because a.net requires quads, which a non-blind solver may not need. However, a format that we can convert to something suitable for a.net would probably suffice (conversion would presumably be a separate task that is run once). It will be easier to identify a suitable format once we have identified at least one solver other than than a.net that we wish to implement or adapt. hscAstrom appears to use a catalog of star positions, which is nice and simple."
"Qserv spatial restrictor names are case sensitive The SQL grammar treats Qserv spatial restrictor names case insensitively, but {{qana/QservRestrictorPlugin.cc}} does not, which means that one must use e.g. {{qserv_areaspec_box}} rather than {{qserv_areaSpec_box}}. We are loose with case in a lot of our wiki pages, so we really should fix this to avoid confusing users. Also, case insensitivity is consistent with MySQL behavior for UDF names.",1,DM-1582,datamanagement,qserv spatial restrictor name case sensitive sql grammar treat qserv spatial restrictor name case insensitively qana qservrestrictorplugin.cc mean use e.g. qserv_areaspec_box qserv_areaspec_box loose case lot wiki page fix avoid confusing user case insensitivity consistent mysql behavior udf name,"Qserv spatial restrictor names are case sensitive The SQL grammar treats Qserv spatial restrictor names case insensitively, but {{qana/QservRestrictorPlugin.cc}} does not, which means that one must use e.g. {{qserv_areaspec_box}} rather than {{qserv_areaSpec_box}}. We are loose with case in a lot of our wiki pages, so we really should fix this to avoid confusing users. Also, case insensitivity is consistent with MySQL behavior for UDF names."
"Research how to integrate different image metadata stores with DataCat Data Release Production will generate highly structured image metadata (exposure* tables). If we decide to use DataCat (e.g., for keeping more ad hoc metadata), the question arises if/how to integrate all this together: * should we integrate all exposure* tables from all releases into DataCat? (eg via foreign tables) * should we keep them distinct, and integrate at higher level (e.g., Metadata Service)",4,DM-1584,datamanagement,research integrate different image metadata store datacat data release production generate highly structured image metadata exposure table decide use datacat e.g. keep ad hoc metadata question arise integrate integrate exposure table release datacat eg foreign table distinct integrate high level e.g. metadata service,"Research how to integrate different image metadata stores with DataCat Data Release Production will generate highly structured image metadata (exposure* tables). If we decide to use DataCat (e.g., for keeping more ad hoc metadata), the question arises if/how to integrate all this together: * should we integrate all exposure* tables from all releases into DataCat? (eg via foreign tables) * should we keep them distinct, and integrate at higher level (e.g., Metadata Service)"
"Design system for tracking existing images/files We have a lot of files/images already brought in or generated through Data Challenges done to date. We need a system for cataloging them. This story will define such system, eg, sketch of UI, underlying technology used (DataCat, plain mysql, schema etc).",6,DM-1585,datamanagement,design system track exist image file lot file image bring generate data challenges date need system catalog story define system eg sketch ui underlying technology datacat plain mysql schema etc,"Design system for tracking existing images/files We have a lot of files/images already brought in or generated through Data Challenges done to date. We need a system for cataloging them. This story will define such system, eg, sketch of UI, underlying technology used (DataCat, plain mysql, schema etc)."
Define structure of web form for collecting metadata about existing data sets Web alpha version of the form (using django or Fermi Java webservices code) that collects input from users about data repositories. Authentication not covered in this version.,2,DM-1587,datamanagement,define structure web form collect metadata exist datum set web alpha version form django fermi java webservice code collect input user datum repository authentication cover version,Define structure of web form for collecting metadata about existing data sets Web alpha version of the form (using django or Fermi Java webservices code) that collects input from users about data repositories. Authentication not covered in this version.
Implement FITS header crawler and integrate it with the form Implement crawler that walks through registered repos (through the form) and loads metadata from fits headers into the mysql backend,7,DM-1588,datamanagement,implement fit header crawler integrate form implement crawler walk register repos form load metadata fit header mysql backend,Implement FITS header crawler and integrate it with the form Implement crawler that walks through registered repos (through the form) and loads metadata from fits headers into the mysql backend
"Research and experiment with building form for capturing user input Need to build a form that will be used to capture user input about existing image repositories (users will be registering their files/repositories). Options to consider: python-based django, java-based system that is part of Fermi DataCat. ",4,DM-1589,datamanagement,research experiment building form capture user input need build form capture user input exist image repository user register file repository option consider python base django java base system fermi datacat,"Research and experiment with building form for capturing user input Need to build a form that will be used to capture user input about existing image repositories (users will be registering their files/repositories). Options to consider: python-based django, java-based system that is part of Fermi DataCat."
"Break down & discuss DM-1074 I will take the lead on DM-1074. First step will be to sit with [~jbosch], get a feeling for what needs to be done, and sketch out a set of stories.",1,DM-1590,datamanagement,break discuss dm-1074 lead dm-1074 step sit ~jbosch feeling need sketch set story,"Break down & discuss DM-1074 I will take the lead on DM-1074. First step will be to sit with [~jbosch], get a feeling for what needs to be done, and sketch out a set of stories."
"Convert test_qservAdmin.py into a real unit test Need to turn ./admin/tests/test_qservAdmin.py into a real unit test. In the past it was broken and it went unnoticed, see DM-1395",1,DM-1591,datamanagement,convert test_qservadmin.py real unit test need turn test test_qservadmin.py real unit test past break go unnoticed dm-1395,"Convert test_qservAdmin.py into a real unit test Need to turn ./admin/tests/test_qservAdmin.py into a real unit test. In the past it was broken and it went unnoticed, see DM-1395"
"Remove check for stack dir write access in qserv-configure.py qserv-configure.py checks for write access to stack dir, this should be replace by check for read access, or removed.  fjammes@qserv-build-server-xlarge:~/src/qserv$ qserv-configure.py --all INFO: Qserv configuration tool =======================================================================  WARNING : Do you want to erase all configuration data in /home/fjammes/qserv-run/2014_10.0 ? [y/n] y INFO: Copying template configuration from /home/fjammes/stack/Linux64/qserv/2014_10.0/cfg/templates to /home/fjammes/qserv-run/2014_10.0 INFO: Creating meta-configuration file: /home/fjammes/qserv-run/2014_10.0/qserv-meta.conf INFO: Reading meta-configuration file /home/fjammes/qserv-run/2014_10.0/qserv-meta.conf INFO: Defining main directory structure INFO: No write access to dir /home/fjammes/stack/Linux64/qserv/2014_10.0 : [Errno 13] Permission denied: '/home/fjammes/stack/Linux64/qserv/2014_10.0/write_tester' ",2,DM-1594,datamanagement,remove check stack dir write access qserv-configure.py qserv-configure.py check write access stack dir replace check read access remove fjammes@qserv build server xlarge:~/src qserv$ qserv-configure.py --all info qserv configuration tool warning want erase configuration datum /home fjamme qserv run/2014_10.0 info copy template configuration /home fjamme stack linux64 qserv/2014_10.0 cfg template /home fjamme qserv run/2014_10.0 info create meta configuration file /home fjamme qserv run/2014_10.0 qserv meta.conf info read meta configuration file fjamme qserv run/2014_10.0 qserv meta.conf info define main directory structure info write access dir /home fjamme stack linux64 qserv/2014_10.0 errno 13 permission deny /home fjamme stack linux64 qserv/2014_10.0 write_tester,"Remove check for stack dir write access in qserv-configure.py qserv-configure.py checks for write access to stack dir, this should be replace by check for read access, or removed. fjammes@qserv-build-server-xlarge:~/src/qserv$ qserv-configure.py --all INFO: Qserv configuration tool ======================================================================= WARNING : Do you want to erase all configuration data in /home/fjammes/qserv-run/2014_10.0 ? [y/n] y INFO: Copying template configuration from /home/fjammes/stack/Linux64/qserv/2014_10.0/cfg/templates to /home/fjammes/qserv-run/2014_10.0 INFO: Creating meta-configuration file: /home/fjammes/qserv-run/2014_10.0/qserv-meta.conf INFO: Reading meta-configuration file /home/fjammes/qserv-run/2014_10.0/qserv-meta.conf INFO: Defining main directory structure INFO: No write access to dir /home/fjammes/stack/Linux64/qserv/2014_10.0 : [Errno 13] Permission denied: '/home/fjammes/stack/Linux64/qserv/2014_10.0/write_tester'"
"Clean up multi-component Footprints Following the landing of DM-1545, it's possible for an erosion operation on a footprint to cause it to split into multiple components and for peaks which were previously inside the footprint to not fall inside any of those components.  Here, we should provide a ""clean up"" operation that takes a multiple-component footprint and splits it into a set of contiguous footprints with appropriate peak lists.",4,DM-1596,datamanagement,clean multi component footprint follow landing dm-1545 possible erosion operation footprint cause split multiple component peak previously inside footprint fall inside component provide clean operation take multiple component footprint split set contiguous footprint appropriate peak list,"Clean up multi-component Footprints Following the landing of DM-1545, it's possible for an erosion operation on a footprint to cause it to split into multiple components and for peaks which were previously inside the footprint to not fall inside any of those components. Here, we should provide a ""clean up"" operation that takes a multiple-component footprint and splits it into a set of contiguous footprints with appropriate peak lists."
"init script fails to start xrootd after crash I think we saw this issue in the past, not sure it was actually fixed back then or just was to reintroduced one more time.  After crash of xrootd the regular {{etc/init.d/xrootd start}} fails to start it: {code} [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd status xrootd is dead but PID file exists                         [FAILED]   see /usr/local/home/salnikov/qserv-run/dm-621/var/run/worker/xrootd.pid [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd start Starting xrootd: (already up)                              [  OK  ] [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd status xrootd is dead but PID file exists                         [FAILED]   see /usr/local/home/salnikov/qserv-run/dm-621/var/run/worker/xrootd.pid {code} I can start it with {{restart}} but I think that {{start}} should detect that xrootd is dead ({{status}} does that) and start service correctly.  This issue may exist for other services but I did not check. ",1,DM-1597,datamanagement,init script fail start xrootd crash think see issue past sure actually fix reintroduce time crash xrootd regular etc init.d xrootd start fail start code salnikov@lsst dbdev2 dm-621]$ ~/qserv run dm-621 etc init.d xrootd status xrootd dead pid file exist failed /usr local home salnikov qserv run dm-621 var run worker xrootd.pid salnikov@lsst dbdev2 dm-621]$ ~/qserv run dm-621 etc init.d xrootd start start xrootd ok salnikov@lsst dbdev2 dm-621]$ ~/qserv run dm-621 etc init.d xrootd status xrootd dead pid file exist failed /usr local home salnikov qserv run dm-621 var run worker xrootd.pid code start restart think start detect xrootd dead status start service correctly issue exist service check,"init script fails to start xrootd after crash I think we saw this issue in the past, not sure it was actually fixed back then or just was to reintroduced one more time. After crash of xrootd the regular {{etc/init.d/xrootd start}} fails to start it: {code} [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd status xrootd is dead but PID file exists [FAILED] see /usr/local/home/salnikov/qserv-run/dm-621/var/run/worker/xrootd.pid [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd start Starting xrootd: (already up) [ OK ] [salnikov@lsst-dbdev2 dm-621]$ ~/qserv-run/dm-621/etc/init.d/xrootd status xrootd is dead but PID file exists [FAILED] see /usr/local/home/salnikov/qserv-run/dm-621/var/run/worker/xrootd.pid {code} I can start it with {{restart}} but I think that {{start}} should detect that xrootd is dead ({{status}} does that) and start service correctly. This issue may exist for other services but I did not check."
"Determine if Astrometry class is desired The question is whether the Astrometry class is the thing that is overridden or if the AstrometryTask has component configurables that are overridden.  Also, determine location default implementation.",2,DM-1600,datamanagement,determine astrometry class desire question astrometry class thing overridden astrometrytask component configurable overridden determine location default implementation,"Determine if Astrometry class is desired The question is whether the Astrometry class is the thing that is overridden or if the AstrometryTask has component configurables that are overridden. Also, determine location default implementation."
"Add support for c-style comments in front of queries sent to qserv Connecting to qserv from java fails because the jdbc driver inserts comments. ""/* ... */"" in front of queries (example pasted below). The fix involves removing the comments at the proxy level   {code} SQLException: Qserv error: 'ParseException:ANTLR parse error:unexpected token: /:'  Query being executed when exception was thrown: /* mysql-connector-java-5.1.34 ( Revision: jess.balint@oracle.com-20141014163213-wqbwpf1ok2kvo1om ) */SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect' {code}",1,DM-1601,datamanagement,add support style comment query send qserv connect qserv java fail jdbc driver insert comment query example paste fix involve remove comment proxy level code sqlexception qserv error parseexception antlr parse error unexpected token query execute exception throw mysql connector java-5.1.34 revision jess.balint@oracle.com-20141014163213 wqbwpf1ok2kvo1om variables variable_name language net_write_timeout interactive_timeout wait_timeout character_set_client character_set_connection character_set character_set_server tx_isolation transaction_isolation character_set_result timezone time_zone system_time_zone lower_case_table_name max_allowed_packet net_buffer_length sql_mode query_cache_type query_cache_size license init_connect code,"Add support for c-style comments in front of queries sent to qserv Connecting to qserv from java fails because the jdbc driver inserts comments. ""/* ... */"" in front of queries (example pasted below). The fix involves removing the comments at the proxy level {code} SQLException: Qserv error: 'ParseException:ANTLR parse error:unexpected token: /:' Query being executed when exception was thrown: /* mysql-connector-java-5.1.34 ( Revision: jess.balint@oracle.com-20141014163213-wqbwpf1ok2kvo1om ) */SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect' {code}"
Prevent collisions in /tmp related to scisql deployment Deploying scisql involves creating a file in  /tmp. The file never gets removed. This can cause the following error when Qserv is installed later on the same machine: {code} ERROR: failed to open output file /tmp/scisql_demo_htmid10.tsv for writing {code}  We should switch to using a temporary file instead of file with fixed name.,2,DM-1602,datamanagement,prevent collision /tmp relate scisql deployment deploying scisql involve create file file get remove cause follow error qserv instal later machine code error fail open output file scisql_demo_htmid10.tsv write code switch temporary file instead file fix,Prevent collisions in /tmp related to scisql deployment Deploying scisql involves creating a file in /tmp. The file never gets removed. This can cause the following error when Qserv is installed later on the same machine: {code} ERROR: failed to open output file /tmp/scisql_demo_htmid10.tsv for writing {code} We should switch to using a temporary file instead of file with fixed name.
"Qserv scons scripts do not pick up the version of swig provided by eups See the summary. The consequence is that the Qserv integration tests fail on systems that provide swig 2.x+ - there seems to be some implicit dependency on SWIG 1.x. The reason may be that swig is getting confused about shared_ptr to objects that are defined in one module, but used in another (recent swig reorganization split czar and css into two separate swig modules).",2,DM-1603,datamanagement,qserv scon script pick version swig provide eup summary consequence qserv integration test fail system provide swig 2.x+ implicit dependency swig 1.x reason swig getting confuse shared_ptr object define module recent swig reorganization split czar css separate swig module,"Qserv scons scripts do not pick up the version of swig provided by eups See the summary. The consequence is that the Qserv integration tests fail on systems that provide swig 2.x+ - there seems to be some implicit dependency on SWIG 1.x. The reason may be that swig is getting confused about shared_ptr to objects that are defined in one module, but used in another (recent swig reorganization split czar and css into two separate swig modules)."
"Sanitize configs Some solver specific information is stored in the AstrometryTask config.  Further, the solver config is accessed inside the AstrometryTask run method.  This mixing of information make it hard to make the framework pluggable.  Solver configuration should be confined completely within the solver class (whether it be part of the Astrometry class or a configurable of its own).",2,DM-1604,datamanagement,sanitize config solver specific information store astrometrytask config solver config access inside astrometrytask run method mixing information hard framework pluggable solver configuration confine completely solver class astrometry class configurable,"Sanitize configs Some solver specific information is stored in the AstrometryTask config. Further, the solver config is accessed inside the AstrometryTask run method. This mixing of information make it hard to make the framework pluggable. Solver configuration should be confined completely within the solver class (whether it be part of the Astrometry class or a configurable of its own)."
Add unit tests to test c-style comments in/around queries I should have thought about it when doing DM-1601 but I didn't... it'd be good to add test queries that test comments before/after/inside query.,1,DM-1607,datamanagement,add unit test test style comment query think dm-1601 good add test query test comment inside query,Add unit tests to test c-style comments in/around queries I should have thought about it when doing DM-1601 but I didn't... it'd be good to add test queries that test comments before/after/inside query.
Move meas_algorithms unit tests to meas_base framework The following tests in meas_algorithms need to be ported to the meas_base framework:  measure.py psfSelectTest.py testPsfDetermination.py ticket2019.py testCorrectFluxes.py (though this cannot be done until the algorithm exists),2,DM-1608,datamanagement,meas_algorithm unit test meas_base framework follow test meas_algorithm need port meas_base framework measure.py psfselecttest.py testpsfdetermination.py ticket2019.py testcorrectfluxes.py algorithm exist,Move meas_algorithms unit tests to meas_base framework The following tests in meas_algorithms need to be ported to the meas_base framework: measure.py psfSelectTest.py testPsfDetermination.py ticket2019.py testCorrectFluxes.py (though this cannot be done until the algorithm exists)
Research off-the-shelf solutions for harvesting metadata Relevant links:  * http://www.ivoa.net/documents/SIA/20141024/index.html - there’s a list of metadata in the document  * CAOM (Common Archive Object Model): http://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/caom2/,4,DM-1609,datamanagement,research shelf solution harvesting metadata relevant link http://www.ivoa.net/documents/sia/20141024/index.html list metadata document caom common archive object model,Research off-the-shelf solutions for harvesting metadata Relevant links: * http://www.ivoa.net/documents/SIA/20141024/index.html - there s a list of metadata in the document * CAOM (Common Archive Object Model): http://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/caom2/
"Integrate Metadata Store prototype v1 with cat and db   Integrate prototype developed through DM-1255 with cat and db as appropriate (don't hardcode schema, don't hardcode credentials, etc).",4,DM-1610,datamanagement,integrate metadata store prototype v1 cat db integrate prototype develop dm-1255 cat db appropriate hardcode schema hardcode credential etc,"Integrate Metadata Store prototype v1 with cat and db Integrate prototype developed through DM-1255 with cat and db as appropriate (don't hardcode schema, don't hardcode credentials, etc)."
"Add support for mysql JDBC driver (v2) MySQL client 4.1 and higher is stripping out comments before sending them to server, so the fixes done in DM-1539 are not sufficient.",1,DM-1614,datamanagement,add support mysql jdbc driver v2 mysql client 4.1 high strip comment send server fix dm-1539 sufficient,"Add support for mysql JDBC driver (v2) MySQL client 4.1 and higher is stripping out comments before sending them to server, so the fixes done in DM-1539 are not sufficient."
Design and implement CSS structure for distributed Qserv setup For management of the distributed databases/tables we need info in CSS about all workers and tables. The info will be created by data loader and updated by replicator which do not exist yet. For this issue we need to provide python API which can fill the same information in CSS so that we can build and test other pieces needed for this epic.,5,DM-1615,datamanagement,design implement css structure distribute qserv setup management distribute database table need info css worker table info create datum loader update replicator exist issue need provide python api fill information css build test piece need epic,Design and implement CSS structure for distributed Qserv setup For management of the distributed databases/tables we need info in CSS about all workers and tables. The info will be created by data loader and updated by replicator which do not exist yet. For this issue we need to provide python API which can fill the same information in CSS so that we can build and test other pieces needed for this epic.
Implement remote host access for management framework To manage remote workers we need a way to access services on remote machines that run workers. Services may be something like mysql (which we would prefer to run without TCP port open) and optionally xrootd. This ticket will implement Python module which will hide a complexity of doing things like ssh/port forwarding/authentication from the client.,5,DM-1616,datamanagement,implement remote host access management framework manage remote worker need way access service remote machine run worker service like mysql prefer run tcp port open optionally xrootd ticket implement python module hide complexity thing like ssh port forwarding authentication client,Implement remote host access for management framework To manage remote workers we need a way to access services on remote machines that run workers. Services may be something like mysql (which we would prefer to run without TCP port open) and optionally xrootd. This ticket will implement Python module which will hide a complexity of doing things like ssh/port forwarding/authentication from the client.
"Client library for accessing distributed database/table information from CSS Provide Python interface for accessing information in CSS which is relevant to distributed management, such as database/table/node data. This interface can be used also by data loader and replicator.",8,DM-1617,datamanagement,client library access distribute database table information css provide python interface access information css relevant distribute management database table node datum interface datum loader replicator,"Client library for accessing distributed database/table information from CSS Provide Python interface for accessing information in CSS which is relevant to distributed management, such as database/table/node data. This interface can be used also by data loader and replicator."
"Move CSS documentation close to code  CSS documentation about the structure is currently in trac (which is readonly), at https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS. We need to move it close to the source code, e.g., to a doc file.",2,DM-1620,datamanagement,css documentation close code css documentation structure currently trac readonly https://dev.lsstcorp.org/trac/wiki/db/qserv/css need close source code e.g. doc file,"Move CSS documentation close to code CSS documentation about the structure is currently in trac (which is readonly), at https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS. We need to move it close to the source code, e.g., to a doc file."
"Add unit test to to verify zookeeper versioning works This is a follow u pto DM-1453, we need to add a unit test that will prove that mismatched versions in zookeeper and software are properly handled.",1,DM-1621,datamanagement,add unit test verify zookeeper versioning work follow pto dm-1453 need add unit test prove mismatch version zookeeper software properly handle,"Add unit test to to verify zookeeper versioning works This is a follow u pto DM-1453, we need to add a unit test that will prove that mismatched versions in zookeeper and software are properly handled."
Add unit test for queries from DM-1539 Need to add unit test for queries listed in DM-1539,1,DM-1622,datamanagement,add unit test query dm-1539 need add unit test query list dm-1539,Add unit test for queries from DM-1539 Need to add unit test for queries listed in DM-1539
"Qserv should report it's version It should be possible to determine which version of qserv we are running by looking at information from log files. So, in practice, we probably should generate in scons a unique id (from sha from git?) and compile it into the code.",1,DM-1624,datamanagement,qserv report version possible determine version qserv run look information log file practice probably generate scon unique sha git compile code,"Qserv should report it's version It should be possible to determine which version of qserv we are running by looking at information from log files. So, in practice, we probably should generate in scons a unique id (from sha from git?) and compile it into the code."
SciSQL should report its version It should be possible to determine which version of scisql we are running.,1,DM-1625,datamanagement,scisql report version possible determine version scisql run,SciSQL should report its version It should be possible to determine which version of scisql we are running.
Build 2014_12 Qserv release The title says it all. Please open ticket for next release when closing this one.,1,DM-1626,datamanagement,build 2014_12 qserv release title say open ticket release close,Build 2014_12 Qserv release The title says it all. Please open ticket for next release when closing this one.
"Adopted/Retired RFCs are not counted as resolved Also, marking an RFC as Adopted brings up a box with a message applicable to the Closed status.",1,DM-1629,datamanagement,adopt retired rfc count resolve mark rfc adopted bring box message applicable closed status,"Adopted/Retired RFCs are not counted as resolved Also, marking an RFC as Adopted brings up a box with a message applicable to the Closed status."
New RFCs should result in dm-devel E-mails and HipChat postings Email notices of new RFCs filed with a DM component go to dm-devel Email notices of new RFCs filed with a DM component go to Data Management chat room Email notices of all new RFCs go to Bot: RFC room ,1,DM-1630,datamanagement,new rfc result dm devel mail hipchat posting email notice new rfc file dm component dm devel email notice new rfc file dm component data management chat room email notice new rfc bot rfc room,New RFCs should result in dm-devel E-mails and HipChat postings Email notices of new RFCs filed with a DM component go to dm-devel Email notices of new RFCs filed with a DM component go to Data Management chat room Email notices of all new RFCs go to Bot: RFC room
"Build 2014_11 Qserv release The title says it all. Please open ticket for next release when closing this one.  Tasks to do:  - publish last buildbot build under a temporary eups-tag (""qserv-dev"")and test it, if it works fine: - create git-tags for Qserv and dependencies - publish the release with eups-tags ""qserv"" and ""YYYY_MM"" - generate and publish the doc for release ""YYYY_MM"" - update release number in Qserv code and set ""YYYY_MM+1"" as release in dev and ""YYYY_MM"" as stable release (update {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}}) - generate and publish the doc for release ""YYYY_MM+1""  - look at the doc - commit  {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}} with current ticket number   This procedure should be validated and documented.",1,DM-1632,datamanagement,"build 2014_11 qserv release title say open ticket release close task publish buildbot build temporary eup tag qserv dev"")and test work fine create git tag qserv dependency publish release eup tag qserv yyyy_mm generate publish doc release yyyy_mm update release number qserv code set yyyy_mm+1 release dev yyyy_mm stable release update admin bin qserv version.sh doc source conf.py doc source toplevel.rst generate publish doc release yyyy_mm+1 look doc commit admin bin qserv version.sh doc source conf.py doc source toplevel.rst current ticket number procedure validate document","Build 2014_11 Qserv release The title says it all. Please open ticket for next release when closing this one. Tasks to do: - publish last buildbot build under a temporary eups-tag (""qserv-dev"")and test it, if it works fine: - create git-tags for Qserv and dependencies - publish the release with eups-tags ""qserv"" and ""YYYY_MM"" - generate and publish the doc for release ""YYYY_MM"" - update release number in Qserv code and set ""YYYY_MM+1"" as release in dev and ""YYYY_MM"" as stable release (update {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}}) - generate and publish the doc for release ""YYYY_MM+1"" - look at the doc - commit {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}} with current ticket number This procedure should be validated and documented."
Update build process for Firefly opensource Update build and deploy related scripts to reflects the changes affected by open sourcing of Firefly.,7,DM-1633,datamanagement,update build process firefly opensource update build deploy related script reflect change affect open sourcing firefly,Update build process for Firefly opensource Update build and deploy related scripts to reflects the changes affected by open sourcing of Firefly.
Remove redundant CORS headers from Firefly's http response Make sure CORS related headers are not sent when the Origin header is missing.  Firefox does not like it.,1,DM-1635,datamanagement,remove redundant cors header firefly http response sure cors relate header send origin header miss firefox like,Remove redundant CORS headers from Firefly's http response Make sure CORS related headers are not sent when the Origin header is missing. Firefox does not like it.
Research popular web development technologies Research popular web development technologies to prepare Firefly for the future with the focus on front-end framework. ,6,DM-1636,datamanagement,research popular web development technology research popular web development technology prepare firefly future focus end framework,Research popular web development technologies Research popular web development technologies to prepare Firefly for the future with the focus on front-end framework.
"Document ""getting started"" procedure for new stack developers Document a procedure for building the stack on a new system in a way that is appropriate for both project members and external contributors.  This can be linked as a ""getting started"" guide from http://dm.lsst.org/.",5,DM-1641,datamanagement,document getting start procedure new stack developer document procedure build stack new system way appropriate project member external contributor link getting start guide http://dm.lsst.org/.,"Document ""getting started"" procedure for new stack developers Document a procedure for building the stack on a new system in a way that is appropriate for both project members and external contributors. This can be linked as a ""getting started"" guide from http://dm.lsst.org/."
"preparation work for FIrefly open source 1. discuss with IPAC director and managers to open source Firefly 2. study the major open source license,Apache, GPL, BSD 3-clause, MIT.  3. final decision: BSD-3 clause",6,DM-1645,datamanagement,preparation work firefly open source discuss ipac director manager open source firefly study major open source license apache gpl bsd clause mit final decision bsd-3 clause,"preparation work for FIrefly open source 1. discuss with IPAC director and managers to open source Firefly 2. study the major open source license,Apache, GPL, BSD 3-clause, MIT. 3. final decision: BSD-3 clause"
Use an OpenStack instance to run an HTCondor Central manager Use an OpenStack instance to run an HTCondor Central manager,4,DM-1647,datamanagement,use openstack instance run htcondor central manager use openstack instance run htcondor central manager,Use an OpenStack instance to run an HTCondor Central manager Use an OpenStack instance to run an HTCondor Central manager
"Extend data loading script to support multi-node setup Implement simplest use case for data loading with one or more worker database separate from czar database. Simplest means minimal functionality in what concerns access to workers, just assume for now that we can connect to every worker directly using regular TCP connection. It should be possible to just add a list of worker nodes as an option to loader script and send the chunks in some random order to the workers in that list. Of course chunks for different tables should end up on the same host, so some form of chunk management is needed (use for now whatever is defined in CSS doc on trac).",8,DM-1653,datamanagement,extend datum loading script support multi node setup implement simple use case datum load worker database separate czar database simplest mean minimal functionality concern access worker assume connect worker directly regular tcp connection possible add list worker node option loader script send chunk random order worker list course chunk different table end host form chunk management need use define css doc trac,"Extend data loading script to support multi-node setup Implement simplest use case for data loading with one or more worker database separate from czar database. Simplest means minimal functionality in what concerns access to workers, just assume for now that we can connect to every worker directly using regular TCP connection. It should be possible to just add a list of worker nodes as an option to loader script and send the chunks in some random order to the workers in that list. Of course chunks for different tables should end up on the same host, so some form of chunk management is needed (use for now whatever is defined in CSS doc on trac)."
"Working with SLAC on definition of metadata store Follow up metadata store schema development to ensure SUI will be able to use it. Define the fields that should go into Data Definition table. Define the fields that must be present in the image metadata table, which SUI will be searching.",2,DM-1655,datamanagement,work slac definition metadata store follow metadata store schema development ensure sui able use define field data definition table define field present image metadata table sui search,"Working with SLAC on definition of metadata store Follow up metadata store schema development to ensure SUI will be able to use it. Define the fields that should go into Data Definition table. Define the fields that must be present in the image metadata table, which SUI will be searching."
"hipchat support for maigically urlifying docushare documents It would be generally use full if references to official documents in hipchat (or it's successor) automagically generated urls for official document handles.  Eg: Document-1234, LSE-123",1,DM-1656,datamanagement,hipchat support maigically urlifying docushare document generally use reference official document hipchat successor automagically generate url official document handle eg document-1234 lse-123,"hipchat support for maigically urlifying docushare documents It would be generally use full if references to official documents in hipchat (or it's successor) automagically generated urls for official document handles. Eg: Document-1234, LSE-123"
"Add git bisect tool for Qserv repos {code:bash} fjammes@clrlsst-dbmaster-vm:~/src/qserv (u/fjammes/DM-627 $%) $ qserv-test-head.sh -h  Usage: qserv-test-head.sh [options]    Available options:     -h          this message     -q          quick: only rebuild/install new Qserv code,                 and perform test case #01    Rebuild from scratch, configure and run integration tests against   a Qserv git repository.   Pre-requisite:     source loadLSST.bash     setup qserv_distrib -t qserv     setup -k -r ${QSERV_SRC_DIR}    Can be used with 'git bisect' :     cd ${QSERV_SRC_DIR}     git bisect start     git bisect bad     git bisect good git-commit-id     git bisect run /home/fjammes/src/qserv_testdata/bin/qserv-test-head.sh {code}  Code is in DM-627 ticket branch.",2,DM-1658,datamanagement,add git bisect tool qserv repos code bash fjammes@clrlsst dbmaster vm:~/src qserv fjamme dm-627 qserv-test-head.sh usage qserv-test-head.sh option available option message quick rebuild install new qserv code perform test case 01 rebuild scratch configure run integration test qserv git repository pre requisite source loadlsst.bash setup qserv_distrib qserv setup -r qserv_src_dir git bisect cd qserv_src_dir git bisect start git bisect bad git bisect good git commit id git bisect run fjamme src qserv_testdata bin qserv test head.sh code code dm-627 ticket branch,"Add git bisect tool for Qserv repos {code:bash} fjammes@clrlsst-dbmaster-vm:~/src/qserv (u/fjammes/DM-627 $%) $ qserv-test-head.sh -h Usage: qserv-test-head.sh [options] Available options: -h this message -q quick: only rebuild/install new Qserv code, and perform test case #01 Rebuild from scratch, configure and run integration tests against a Qserv git repository. Pre-requisite: source loadLSST.bash setup qserv_distrib -t qserv setup -k -r ${QSERV_SRC_DIR} Can be used with 'git bisect' : cd ${QSERV_SRC_DIR} git bisect start git bisect bad git bisect good git-commit-id git bisect run /home/fjammes/src/qserv_testdata/bin/qserv-test-head.sh {code} Code is in DM-627 ticket branch."
"Aperture Flux back into an abstract class The last change to ApertureFlux to make it work with the new C++ design changed ApertureFluxAlgorithm into an instantiatable class.  However, I have now figured out how to make this work with SWIG while still allowing measure and fail to be defined by default at the ApertureFlux level..  So this issue is to put things back in order.",1,DM-1659,datamanagement,aperture flux abstract class change apertureflux work new c++ design change aperturefluxalgorithm instantiatable class figure work swig allow measure fail define default apertureflux level issue thing order,"Aperture Flux back into an abstract class The last change to ApertureFlux to make it work with the new C++ design changed ApertureFluxAlgorithm into an instantiatable class. However, I have now figured out how to make this work with SWIG while still allowing measure and fail to be defined by default at the ApertureFlux level.. So this issue is to put things back in order."
Statistics tests use a constant image I just noticed that the tests for Statistics (clipped mean etc.) use a constant image.  We should be testing a Gaussian field (although that makes the tests a little trickier) ,2,DM-1660,datamanagement,statistic test use constant image notice test statistics clip mean etc use constant image test gaussian field make test little tricky,Statistics tests use a constant image I just noticed that the tests for Statistics (clipped mean etc.) use a constant image. We should be testing a Gaussian field (although that makes the tests a little trickier)
"czar log4cxx link/load bug Under ubuntu 14.04 (at least), the czar falls over at load time with an unresolved sym for typeinfo for log4cxx::helpers::ObjectPtrBase while loading the css python wrapper shared lib.",2,DM-1661,datamanagement,czar log4cxx link load bug ubuntu 14.04 czar fall load time unresolved sym typeinfo log4cxx::helpers::objectptrbase load css python wrapper shared lib,"czar log4cxx link/load bug Under ubuntu 14.04 (at least), the czar falls over at load time with an unresolved sym for typeinfo for log4cxx::helpers::ObjectPtrBase while loading the css python wrapper shared lib."
Make qserv dependencies build on OS X with clang Fix anything necessary for qserv dependencies to build on OS X with clang.  Note -- making qserv itself build is more complicated and may require a separate ticket.,4,DM-1662,datamanagement,qserv dependency build os clang fix necessary qserv dependency build os clang note make qserv build complicated require separate ticket,Make qserv dependencies build on OS X with clang Fix anything necessary for qserv dependencies to build on OS X with clang. Note -- making qserv itself build is more complicated and may require a separate ticket.
"fix dependency problems in obs_subaru scons scripts When building obs_subaru with -j4, it often tries to build files related to the defects before the main Python module is built, resulting in import errors (because the scripts it invokes depend on the main Python module).    We need to rewrite the SCons scripts to ensure this dependency is captured.    A preliminary look indicated that this is not entirely trivial, and I'll have to remind myself a bit of how some things in SCons work to get it done, so I'm putting this off for a future sprint.    In the meantime, the workaround is to build obs_subaru with no parallelization.",2,DM-1663,datamanagement,fix dependency problem obs_subaru scon script build obs_subaru -j4 try build file relate defect main python module build result import error script invoke depend main python module need rewrite scons script ensure dependency capture preliminary look indicate entirely trivial remind bit thing scons work put future sprint meantime workaround build obs_subaru parallelization,"fix dependency problems in obs_subaru scons scripts When building obs_subaru with -j4, it often tries to build files related to the defects before the main Python module is built, resulting in import errors (because the scripts it invokes depend on the main Python module). We need to rewrite the SCons scripts to ensure this dependency is captured. A preliminary look indicated that this is not entirely trivial, and I'll have to remind myself a bit of how some things in SCons work to get it done, so I'm putting this off for a future sprint. In the meantime, the workaround is to build obs_subaru with no parallelization."
"Understand historical written docushare materials dealing with the operational security environment. researched the docushare traversing the plans for materials to be embedded in the OCS and similar systems, as well as extant operational plans. The goal was to understand how to separate the the security responsibilities of development, what security constraints ought to be but on items that are delivered, and what to tell the camera and telecscope teams  to mender ea smooth integration with IT security systems upon delivey and integration of their sub systems in Chile.",3,DM-1664,datamanagement,understand historical write docushare material deal operational security environment research docushare traverse plan material embed ocs similar system extant operational plan goal understand separate security responsibility development security constraint ought item deliver tell camera telecscope team mender ea smooth integration security system delivey integration sub system chile,"Understand historical written docushare materials dealing with the operational security environment. researched the docushare traversing the plans for materials to be embedded in the OCS and similar systems, as well as extant operational plans. The goal was to understand how to separate the the security responsibilities of development, what security constraints ought to be but on items that are delivered, and what to tell the camera and telecscope teams to mender ea smooth integration with IT security systems upon delivey and integration of their sub systems in Chile."
"Install PgMySQL and use to connect to local Qserv. Used ""pip"" to install it. ""Conda"" should work as well. Therefore, it should be easy to make it part of the delivered system: VM, container, tar file, after the fact download, etc. It has documentation, uses the MIT license, under active development and available from PyPI. DB connection is straight forward and requires little experience to get meaningful work done.",2,DM-1667,datamanagement,install pgmysql use connect local qserv pip install conda work easy deliver system vm container tar file fact download etc documentation use mit license active development available pypi db connection straight forward require little experience meaningful work,"Install PgMySQL and use to connect to local Qserv. Used ""pip"" to install it. ""Conda"" should work as well. Therefore, it should be easy to make it part of the delivered system: VM, container, tar file, after the fact download, etc. It has documentation, uses the MIT license, under active development and available from PyPI. DB connection is straight forward and requires little experience to get meaningful work done."
"Create SQL code to read Qserv into Python Pandas data frame This works well, at least for a simple case. You can move directly from a query statement to a Pandas data frame for analysis in just a few lines of code. Here is the start of an iPython Qserv session showing how easy it is.  In [6]: import pandas as pd In [7]: import pymysql as db  In [8]: conn = db.connect(host='lsst-db1.ipac.caltech.edu',port=4040, user='qsmaster', passwd='', db='LSST')  In [11]: df = pd.read_sql(""select deepCoaddId, tract, patch, ra, decl from DeepCoadd"", conn)  In [12]: df Out[12]:     deepCoaddId  tract   patch        ra      decl 0      26607706      0  406,11  0.669945  1.152218 1      26673242      0  407,11  0.449945  1.152218 2      26804242      0   409,2  0.011595 -0.734160 3      26673154      0   407,0  0.449945 -1.152108 … ",2,DM-1668,datamanagement,"create sql code read qserv python pandas datum frame work simple case directly query statement pandas data frame analysis line code start ipython qserv session show easy import panda pd import pymysql db conn db.connect(host='lsst db1.ipac.caltech.edu',port=4040 user='qsmaster passwd= db='lsst 11 df pd.read_sql(""select deepcoaddid tract patch ra decl deepcoadd conn 12 df out[12 deepcoaddid tract patch ra decl 26607706 406,11 0.669945 1.152218 26673242 407,11 0.449945 1.152218 26804242 409,2 0.011595 -0.734160 26673154 407,0 0.449945 -1.152108","Create SQL code to read Qserv into Python Pandas data frame This works well, at least for a simple case. You can move directly from a query statement to a Pandas data frame for analysis in just a few lines of code. Here is the start of an iPython Qserv session showing how easy it is. In [6]: import pandas as pd In [7]: import pymysql as db In [8]: conn = db.connect(host='lsst-db1.ipac.caltech.edu',port=4040, user='qsmaster', passwd='', db='LSST') In [11]: df = pd.read_sql(""select deepCoaddId, tract, patch, ra, decl from DeepCoadd"", conn) In [12]: df Out[12]: deepCoaddId tract patch ra decl 0 26607706 0 406,11 0.669945 1.152218 1 26673242 0 407,11 0.449945 1.152218 2 26804242 0 409,2 0.011595 -0.734160 3 26673154 0 407,0 0.449945 -1.152108"
"Explore queries for Qserv database. Use a local database here at IPAC with 5 Qserv tables in it. Looked at several Python query interfaces. Used the pymysql interface for testing because it's pure Python and because I found reports suggesting it was almost as fast as the mysqldb interface that requires C language libraries.  Ad hoc queries can be constructed in three lines of code, so useable in a science environment.  Found a couple of bugs in Qserv that were reported.",6,DM-1669,datamanagement,explore query qserv database use local database ipac qserv table look python query interface pymysql interface testing pure python find report suggest fast mysqldb interface require language library ad hoc query construct line code useable science environment find couple bug qserv report,"Explore queries for Qserv database. Use a local database here at IPAC with 5 Qserv tables in it. Looked at several Python query interfaces. Used the pymysql interface for testing because it's pure Python and because I found reports suggesting it was almost as fast as the mysqldb interface that requires C language libraries. Ad hoc queries can be constructed in three lines of code, so useable in a science environment. Found a couple of bugs in Qserv that were reported."
"Begin looking at how Python Pandas can be used for LSST data analysis. Pandas is well integrated with the other parts of SciPy: numpy, matlibpy, etc.  It’s a good candidate for data analysis, especially where time series are involved. However, there are no multidimensional columns, poor metadata support for FITS files and a need to use masks instead of NaN values. These may, or may not, be problems.  There is a 400 page book about Pandas, so it will take some further time to learn its value, especially with astronomical data in different situations.",5,DM-1670,datamanagement,begin look python panda lsst data analysis pandas integrate part scipy numpy matlibpy etc good candidate datum analysis especially time series involve multidimensional column poor metadata support fits file need use mask instead nan value problem 400 page book pandas time learn value especially astronomical datum different situation,"Begin looking at how Python Pandas can be used for LSST data analysis. Pandas is well integrated with the other parts of SciPy: numpy, matlibpy, etc. It s a good candidate for data analysis, especially where time series are involved. However, there are no multidimensional columns, poor metadata support for FITS files and a need to use masks instead of NaN values. These may, or may not, be problems. There is a 400 page book about Pandas, so it will take some further time to learn its value, especially with astronomical data in different situations."
"Allow SWIG override for broken SWIG installations Dependency on SWIG 2.0+ was introduced into Qserv, and this broke Qserv building on systems relying on SWIG 1.3.x.  This ticket introduces basic code to override SWIG_LIB on those systems to allow use of the broken installation (some SWIG search paths are fixed during its build process otherwise).",1,DM-1673,datamanagement,allow swig override broken swig installation dependency swig 2.0 introduce qserv break qserv building system rely swig 1.3.x ticket introduce basic code override swig_lib system allow use break installation swig search path fix build process,"Allow SWIG override for broken SWIG installations Dependency on SWIG 2.0+ was introduced into Qserv, and this broke Qserv building on systems relying on SWIG 1.3.x. This ticket introduces basic code to override SWIG_LIB on those systems to allow use of the broken installation (some SWIG search paths are fixed during its build process otherwise)."
"Minor bug in a test tests/centroid.py has a bug in testMeasureCentroid: ""c"" is undefined in the following bit of code: {code} if display:     ds9.dot(""x"", c.getX(), c.getY(), ctype=ds9.GREEN) {code}",1,DM-1685,datamanagement,"minor bug test test centroid.py bug testmeasurecentroid undefined following bit code code display ds9.dot(""x c.getx c.gety ctype ds9.green code","Minor bug in a test tests/centroid.py has a bug in testMeasureCentroid: ""c"" is undefined in the following bit of code: {code} if display: ds9.dot(""x"", c.getX(), c.getY(), ctype=ds9.GREEN) {code}"
"Implement interfaces for Data Access Services Implement proof of concept, skeleton of the prototype. The work will continue in follow up stories in February and in S15.",8,DM-1695,datamanagement,implement interface data access services implement proof concept skeleton prototype work continue follow story february s15,"Implement interfaces for Data Access Services Implement proof of concept, skeleton of the prototype. The work will continue in follow up stories in February and in S15."
Finish metadata store prototype * A quick proof-of-concept prototype of loading tool for loading database-information into metadata store.   * Handling mysql credentials through auth file in home dir instead of hardcoded values.,4,DM-1698,datamanagement,finish metadata store prototype quick proof concept prototype loading tool load database information metadata store handle mysql credential auth file home dir instead hardcode value,Finish metadata store prototype * A quick proof-of-concept prototype of loading tool for loading database-information into metadata store. * Handling mysql credentials through auth file in home dir instead of hardcoded values.
Create read only OpenStack volume and execute processing scenario Create read only OpenStack volume and execute processing scenario,4,DM-1700,datamanagement,create read openstack volume execute processing scenario create read openstack volume execute processing scenario,Create read only OpenStack volume and execute processing scenario Create read only OpenStack volume and execute processing scenario
Examine fqdn/hostname assignment for OpenStack instance Examine fqdn/hostname assignment for OpenStack instance,4,DM-1702,datamanagement,examine fqdn hostname assignment openstack instance examine fqdn hostname assignment openstack instance,Examine fqdn/hostname assignment for OpenStack instance Examine fqdn/hostname assignment for OpenStack instance
"S15 Analyze Qserv Performance Final analysis of Qserv performance, measure KPIs. Based on LDM-240, we are aiming to demonstrate:  * 50 simultaneous low volume queries, 18 sec/query  * 5 simultaneous high-volume queries, 24 h/query  * data size: 10% of DR1 level.  * Continuous running for 24 h with no software failures.  ",5,DM-1706,datamanagement,s15 analyze qserv performance final analysis qserv performance measure kpi base ldm-240 aim demonstrate 50 simultaneous low volume query 18 sec query simultaneous high volume query 24 query datum size 10 dr1 level continuous run 24 software failure,"S15 Analyze Qserv Performance Final analysis of Qserv performance, measure KPIs. Based on LDM-240, we are aiming to demonstrate: * 50 simultaneous low volume queries, 18 sec/query * 5 simultaneous high-volume queries, 24 h/query * data size: 10% of DR1 level. * Continuous running for 24 h with no software failures."
"Implement result sorting for integration tests We need to be able to sort results, because we can't always rely on ORDER BY. So we need a formatting per query in the integration tests (sort result for some, don't sort for others etc.)    The following queries have been disabled because we don't have result sorting, so once it is implemented, we will need to re-enabled them prior to closing this ticket:  {code}  case02/queries/0003_selectMetadataForOneGalaxy_withUSING.sql  case02/queries/3001_query_035.sql  case02/queries/3008_selectObjectWithColorMagnitudeGreaterThan.sql  case02/queries/3011_selectObjectWithMagnitudes.sql  case02/queries/3011_selectObjectWithMagnitudes_noalias.sql  {code}",8,DM-1709,datamanagement,implement result sort integration test need able sort result rely order need formatting query integration test sort result sort etc follow query disabled result sort implement need enable prior close ticket code case02 queries/0003_selectmetadataforonegalaxy_withusing.sql case02 queries/3001_query_035.sql case02 queries/3008_selectobjectwithcolormagnitudegreaterthan.sql case02 queries/3011_selectobjectwithmagnitudes.sql case02 queries/3011_selectobjectwithmagnitudes_noalias.sql code,"Implement result sorting for integration tests We need to be able to sort results, because we can't always rely on ORDER BY. So we need a formatting per query in the integration tests (sort result for some, don't sort for others etc.) The following queries have been disabled because we don't have result sorting, so once it is implemented, we will need to re-enabled them prior to closing this ticket: {code} case02/queries/0003_selectMetadataForOneGalaxy_withUSING.sql case02/queries/3001_query_035.sql case02/queries/3008_selectObjectWithColorMagnitudeGreaterThan.sql case02/queries/3011_selectObjectWithMagnitudes.sql case02/queries/3011_selectObjectWithMagnitudes_noalias.sql {code}"
"ValueError in lsst.afw.table.Catalog.extend() {code} from lsst.afw.table import BaseCatalog, Schema  s = Schema() c1 = BaseCatalog(s) c2 = BaseCatalog(s)  c1.extend(c2) {code}  The above fails, saying:  {code} Traceback (most recent call last):   File ""test.py"", line 7, in <module>     c1.extend(c2)   File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/afw/10.0+3/python/lsst/afw/table/tableLib.py"", line 6909, in extend     _tableLib.BaseCatalog_extend(self, iterable, deep) ValueError: invalid null reference in method 'BaseCatalog_extend', argument 3 of type 'lsst::afw::table::SchemaMapper const &' {code}",1,DM-1710,datamanagement,valueerror lsst.afw.table.catalog.extend code lsst.afw.table import basecatalog schema schema c1 basecatalog(s c2 basecatalog(s c1.extend(c2 code fail say code traceback recent file test.py line c1.extend(c2 file /users jds projects astronomy lsst stack darwinx86 afw/10.0 python lsst afw table tablelib.py line 6909 extend basecatalog_extend(self iterable deep valueerror invalid null reference method basecatalog_extend argument type lsst::afw::table::schemamapper const code,"ValueError in lsst.afw.table.Catalog.extend() {code} from lsst.afw.table import BaseCatalog, Schema s = Schema() c1 = BaseCatalog(s) c2 = BaseCatalog(s) c1.extend(c2) {code} The above fails, saying: {code} Traceback (most recent call last): File ""test.py"", line 7, in  c1.extend(c2) File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/afw/10.0+3/python/lsst/afw/table/tableLib.py"", line 6909, in extend _tableLib.BaseCatalog_extend(self, iterable, deep) ValueError: invalid null reference in method 'BaseCatalog_extend', argument 3 of type 'lsst::afw::table::SchemaMapper const &' {code}"
S15 Image & File Archive v2 System for tracking existing image data sets integrated with metadata services.,5,DM-1713,datamanagement,s15 image file archive v2 system track exist image datum set integrate metadata service,S15 Image & File Archive v2 System for tracking existing image data sets integrated with metadata services.
"Integrate MetaServ with Schema Browser Schema browser displays detailed info about schema, including custom fields like UCD, units etc. This information is stored as comments embedded in the master version of the schema (in ""cat"" repo). Currently we are generating ascii from the master schema for schema browser, and we load it into mysql, then schema browser reads it from mysql. This story involves changing schema browser such that it will read the schema information directly from MetaServ.",6,DM-1714,datamanagement,integrate metaserv schema browser schema browser display detailed info schema include custom field like ucd unit etc information store comment embed master version schema cat repo currently generate ascii master schema schema browser load mysql schema browser read mysql story involve change schema browser read schema information directly metaserv,"Integrate MetaServ with Schema Browser Schema browser displays detailed info about schema, including custom fields like UCD, units etc. This information is stored as comments embedded in the master version of the schema (in ""cat"" repo). Currently we are generating ascii from the master schema for schema browser, and we load it into mysql, then schema browser reads it from mysql. This story involves changing schema browser such that it will read the schema information directly from MetaServ."
Disable query killing Apparently killing a query through Ctrl-C is confusing xrootd. Disable query killing (which seems to be only partly implemented).,1,DM-1715,datamanagement,disable query kill apparently kill query ctrl confuse xrootd disable query kill partly implement,Disable query killing Apparently killing a query through Ctrl-C is confusing xrootd. Disable query killing (which seems to be only partly implemented).
"Make secondary index for director table only Following discussion on qserv-l, we only need to generate ""secondary"" index for director table, no other table is supposed to have it. Need to modify data loader to recognize which table is director table and generate index only for that table. ",2,DM-1720,datamanagement,secondary index director table follow discussion qserv need generate secondary index director table table suppose need modify datum loader recognize table director table generate index table,"Make secondary index for director table only Following discussion on qserv-l, we only need to generate ""secondary"" index for director table, no other table is supposed to have it. Need to modify data loader to recognize which table is director table and generate index only for that table."
"stack build fails on gcc 4.8 with opt=3 The stack fails to build with gcc 4.8, with a test failure in meas_base (though a similar problem on the HSC fork suggests the problem is actually in afw).  [~darko] reports that on OpenSuse 13.1, the failure goes away when compiling with opt=1 instead of the default opt=3, indicating that the problem is overly aggressive optimization.  This, and the fact that a traceback of the HSC-side failure implicates MaskedImage::getXY0, leads me to guess that something is going wrong with the alignment of the Eigen data members in afw::geom::Point.  Until that theory is disproven, this probably belongs in my court, though I'd be happy to let someone steal it from me if they're interested in working on it.",4,DM-1725,datamanagement,stack build fail gcc 4.8 opt=3 stack fail build gcc 4.8 test failure meas_base similar problem hsc fork suggest problem actually afw ~darko report opensuse 13.1 failure go away compile opt=1 instead default opt=3 indicate problem overly aggressive optimization fact traceback hsc failure implicate maskedimage::getxy0 lead guess go wrong alignment eigen datum member afw::geom::point theory disproven probably belong court happy let steal interested work,"stack build fails on gcc 4.8 with opt=3 The stack fails to build with gcc 4.8, with a test failure in meas_base (though a similar problem on the HSC fork suggests the problem is actually in afw). [~darko] reports that on OpenSuse 13.1, the failure goes away when compiling with opt=1 instead of the default opt=3, indicating that the problem is overly aggressive optimization. This, and the fact that a traceback of the HSC-side failure implicates MaskedImage::getXY0, leads me to guess that something is going wrong with the alignment of the Eigen data members in afw::geom::Point. Until that theory is disproven, this probably belongs in my court, though I'd be happy to let someone steal it from me if they're interested in working on it."
"fix table file handling of MANPATH in dependencies As discussed on DM-1220, the table files for:  - mysqlproxy  - protobuf  - lua  - expat should have the MANPATH entry removed entirely, while:  - xrootd should have "":"" added to the end of its MANPATH value, to allow the default paths to be searched as well.",1,DM-1731,datamanagement,fix table file handling manpath dependency discuss dm-1220 table file mysqlproxy protobuf lua expat manpath entry remove entirely xrootd add end manpath value allow default path search,"fix table file handling of MANPATH in dependencies As discussed on DM-1220, the table files for: - mysqlproxy - protobuf - lua - expat should have the MANPATH entry removed entirely, while: - xrootd should have "":"" added to the end of its MANPATH value, to allow the default paths to be searched as well."
"Fix error on duplicate result_id_m table while launching qserv integration tests Next command:  {code} qserv-check-integration.py --case=01 --load;  qserv-check-integration.py --case=02 --load {code} Fails, most of the time, with next error in logs: {code} qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-627 *+)⟫ cat ~/qserv-run/2014_12/var/log/qserv-czar.log | grep result_1211906_m 0103 16:38:18.576 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:432) - InfileMerger create table:CREATE TABLE qservResult.result_1211906_m (`ra` DOUBLE) 0103 16:38:18.661 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:354) - InfileMerger sql success: CREATE TABLE qservResult.result_1211906_m (`ra` DOUBLE) 0103 16:38:18.661 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:445) - InfileMerger table qservResult.result_1211906_m is ready 0103 16:38:18.686 [0x7f23c77fe700] INFO  root (build/rproc/InfileMerger.cc:299) - Merging w/CREATE TABLE qservResult.result_1211906 SELECT ra AS ra FROM qservResult.result_1211906_m ORDER BY ra 0103 16:38:18.720 [0x7f23c77fe700] DEBUG root (build/rproc/InfileMerger.cc:354) - InfileMerger sql success: CREATE TABLE qservResult.result_1211906 SELECT ra AS ra FROM qservResult.result_1211906_m ORDER BY ra 0103 16:38:18.720 [0x7f23c77fe700] INFO  root (build/rproc/InfileMerger.cc:305) - Cleaning up qservResult.result_1211906_m 0103 16:38:18.721 [0x7f23c77fe700] INFO  root (build/rproc/InfileMerger.cc:317) - Merged qservResult.result_1211906_m into qservResult.result_1211906 0103 16:38:24.716 [0x7fe3cf7fe700] DEBUG root (build/rproc/InfileMerger.cc:432) - InfileMerger create table:CREATE TABLE qservResult.result_1211906_m (`QS1_COUNT` BIGINT(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] ERROR root (build/rproc/InfileMerger.cc:351) - InfileMerger sql error: Error applying sql. Error 1050: Table 'result_1211906_m' already exists Unable to execute query: CREATE TABLE qservResult.result_1211906_m (`QS1_COUNT` BIGINT(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] ERROR root (build/rproc/InfileMerger.cc:438) - InfileMerger error: Error creating table (qservResult.result_1211906_m) 0103 16:38:34.672 [0x7fe3deffd700] INFO  root (build/rproc/InfileMerger.cc:299) - Merging w/CREATE TABLE qservResult.result_1211906 SELECT SUM(QS1_COUNT) AS OBJ_COUNT FROM qservResult.result_1211906_m 0103 16:38:34.673 [0x7fe3deffd700] ERROR root (build/rproc/InfileMerger.cc:351) - InfileMerger sql error: Error applying sql. Error 1054: Unknown column 'QS1_COUNT' in 'field list' Unable to execute query: CREATE TABLE qservResult.result_1211906 SELECT SUM(QS1_COUNT) AS OBJ_COUNT FROM qservResult.result_1211906_m 0103 16:38:34.673 [0x7fe3deffd700] INFO  root (build/rproc/InfileMerger.cc:305) - Cleaning up qservResult.result_1211906_m 0103 16:38:34.674 [0x7fe3deffd700] INFO  root (build/rproc/InfileMerger.cc:317) - Merged qservResult.result_1211906_m into qservResult.result_1211906 {code}  So it seems test case #01 create table .result_1211906_m, and then test case #02 try to re-use this name for an other query.  Qserv result tables aren't cleaned (here Qserv has be stopped): {code} ls  ~/qserv-run/2014_12/var/lib/mysql/qservResult/ db.opt                result_1211336_m.MYI  result_1211340_m.MYI  result_1211343_m.MYI  result_1211346_m.MYI  result_1211382_m.MYI  result_1211385_m.MYI  result_1211902_m.MYI  result_1211905_m.MYI result_1211334_m.frm  result_1211337_m.frm  result_1211341_m.frm  result_1211344_m.frm  result_1211347_m.frm  result_1211383_m.frm  result_1211386_m.frm  result_1211903_m.frm  result_1211906_m.frm result_1211334_m.MYD  result_1211337_m.MYD  result_1211341_m.MYD  result_1211344_m.MYD  result_1211347_m.MYD  result_1211383_m.MYD  result_1211386_m.MYD  result_1211903_m.MYD  result_1211906_m.MYD result_1211334_m.MYI  result_1211337_m.MYI  result_1211341_m.MYI  result_1211344_m.MYI  result_1211347_m.MYI  result_1211383_m.MYI  result_1211386_m.MYI  result_1211903_m.MYI  result_1211906_m.MYI result_1211335_m.frm  result_1211339_m.frm  result_1211342_m.frm  result_1211345_m.frm  result_1211381_m.frm  result_1211384_m.frm  result_1211901_m.frm  result_1211904_m.frm result_1211335_m.MYD  result_1211339_m.MYD  result_1211342_m.MYD  result_1211345_m.MYD  result_1211381_m.MYD  result_1211384_m.MYD  result_1211901_m.MYD  result_1211904_m.MYD result_1211335_m.MYI  result_1211339_m.MYI  result_1211342_m.MYI  result_1211345_m.MYI  result_1211381_m.MYI  result_1211384_m.MYI  result_1211901_m.MYI  result_1211904_m.MYI result_1211336_m.frm  result_1211340_m.frm  result_1211343_m.frm  result_1211346_m.frm  result_1211382_m.frm  result_1211385_m.frm  result_1211902_m.frm  result_1211905_m.frm result_1211336_m.MYD  result_1211340_m.MYD  result_1211343_m.MYD  result_1211346_m.MYD  result_1211382_m.MYD  result_1211385_m.MYD  result_1211902_m.MYD  result_1211905_m.MYD {code}  Changing _idCounter to next value in appInterface.py: {code:python} self._idCounter = int(time.time() % (60*60*24*365) * 10) {code} solves the problem, but a deeper explanation will allow to bring a more robust fix.",8,DM-1732,datamanagement,fix error duplicate result_id_m table launch qserv integration test command code qserv-check-integration.py qserv-check-integration.py code fail time error log code qserv@clrinfoport09:~/src qserv fjamme dm-627 cat ~/qserv run/2014_12 var log qserv czar.log grep result_1211906_m 0103 16:38:18.576 0x7f23c4ff9700 debug root build rproc infilemerger.cc:432 infilemerger create table create table qservresult.result_1211906_m ra double 0103 16:38:18.661 0x7f23c4ff9700 debug root build rproc infilemerger.cc:354 infilemerger sql success create table qservresult.result_1211906_m ra double 0103 16:38:18.661 0x7f23c4ff9700 debug root build rproc infilemerger.cc:445 infilemerger table qservresult.result_1211906_m ready 0103 16:38:18.686 0x7f23c77fe700 info root build rproc infilemerger.cc:299 merging create table qservresult.result_1211906 select ra ra qservresult.result_1211906_m order ra 0103 16:38:18.720 0x7f23c77fe700 debug root build rproc infilemerger.cc:354 infilemerger sql success create table qservresult.result_1211906 select ra ra qservresult.result_1211906_m order ra 0103 16:38:18.720 0x7f23c77fe700 info root build rproc infilemerger.cc:305 clean qservresult.result_1211906_m 0103 16:38:18.721 0x7f23c77fe700 info root build rproc infilemerger.cc:317 merge qservresult.result_1211906_m qservresult.result_1211906 0103 16:38:24.716 0x7fe3cf7fe700 debug root build rproc infilemerger.cc:432 infilemerger create table create table qservresult.result_1211906_m qs1_count bigint(21 0103 16:38:24.717 0x7fe3cf7fe700 error root build rproc infilemerger.cc:351 infilemerger sql error error apply sql error 1050 table result_1211906_m exist unable execute query create table qservresult.result_1211906_m qs1_count bigint(21 0103 16:38:24.717 0x7fe3cf7fe700 error root build rproc infilemerger.cc:438 infilemerger error error create table qservresult.result_1211906_m 0103 16:38:34.672 0x7fe3deffd700 info root build rproc infilemerger.cc:299 merging create table qservresult.result_1211906 select sum(qs1_count obj_count qservresult.result_1211906_m 0103 16:38:34.673 0x7fe3deffd700 error root build rproc infilemerger.cc:351 infilemerger sql error error apply sql error 1054 unknown column qs1_count field list unable execute query create table qservresult.result_1211906 select sum(qs1_count obj_count qservresult.result_1211906_m 0103 16:38:34.673 0x7fe3deffd700 info root build rproc infilemerger.cc:305 clean qservresult.result_1211906_m 0103 16:38:34.674 0x7fe3deffd700 info root build rproc infilemerger.cc:317 merge qservresult.result_1211906_m qservresult.result_1211906 code test case 01 create table test case 02 try use query qserv result table clean qserv stop code ls ~/qserv run/2014_12 var lib mysql qservresult/ db.opt result_1211336_m myi result_1211340_m myi result_1211343_m myi result_1211346_m myi result_1211382_m myi result_1211385_m myi result_1211902_m myi result_1211905_m myi result_1211334_m.frm result_1211337_m.frm result_1211341_m.frm result_1211344_m.frm result_1211347_m.frm result_1211383_m.frm result_1211386_m.frm result_1211903_m.frm result_1211906_m.frm result_1211334_m myd result_1211337_m myd result_1211341_m myd result_1211344_m myd result_1211347_m myd result_1211383_m myd result_1211386_m myd result_1211903_m myd result_1211906_m myd result_1211334_m myi result_1211337_m myi result_1211341_m myi result_1211344_m myi result_1211347_m myi result_1211383_m myi result_1211386_m myi result_1211903_m myi result_1211906_m myi result_1211335_m.frm result_1211339_m.frm result_1211342_m.frm result_1211345_m.frm result_1211381_m.frm result_1211384_m.frm result_1211901_m.frm result_1211904_m.frm result_1211335_m myd result_1211339_m myd result_1211342_m myd result_1211345_m myd result_1211381_m myd result_1211384_m myd result_1211901_m myd result_1211904_m myd result_1211335_m myi result_1211339_m myi result_1211342_m myi result_1211345_m myi result_1211381_m myi result_1211384_m myi result_1211901_m myi result_1211904_m myi result_1211336_m.frm result_1211340_m.frm result_1211343_m.frm result_1211346_m.frm result_1211382_m.frm result_1211385_m.frm result_1211902_m.frm result_1211905_m.frm result_1211336_m myd result_1211340_m myd result_1211343_m myd result_1211346_m myd result_1211382_m myd result_1211385_m myd result_1211902_m myd result_1211905_m myd code change idcounter value appinterface.py code python self._idcounter int(time.time 60 60 24 365 10 code solve problem deep explanation allow bring robust fix,"Fix error on duplicate result_id_m table while launching qserv integration tests Next command: {code} qserv-check-integration.py --case=01 --load; qserv-check-integration.py --case=02 --load {code} Fails, most of the time, with next error in logs: {code} qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-627 *+) cat ~/qserv-run/2014_12/var/log/qserv-czar.log | grep result_1211906_m 0103 16:38:18.576 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:432) - InfileMerger create table:CREATE TABLE qservResult.result_1211906_m (`ra` DOUBLE) 0103 16:38:18.661 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:354) - InfileMerger sql success: CREATE TABLE qservResult.result_1211906_m (`ra` DOUBLE) 0103 16:38:18.661 [0x7f23c4ff9700] DEBUG root (build/rproc/InfileMerger.cc:445) - InfileMerger table qservResult.result_1211906_m is ready 0103 16:38:18.686 [0x7f23c77fe700] INFO root (build/rproc/InfileMerger.cc:299) - Merging w/CREATE TABLE qservResult.result_1211906 SELECT ra AS ra FROM qservResult.result_1211906_m ORDER BY ra 0103 16:38:18.720 [0x7f23c77fe700] DEBUG root (build/rproc/InfileMerger.cc:354) - InfileMerger sql success: CREATE TABLE qservResult.result_1211906 SELECT ra AS ra FROM qservResult.result_1211906_m ORDER BY ra 0103 16:38:18.720 [0x7f23c77fe700] INFO root (build/rproc/InfileMerger.cc:305) - Cleaning up qservResult.result_1211906_m 0103 16:38:18.721 [0x7f23c77fe700] INFO root (build/rproc/InfileMerger.cc:317) - Merged qservResult.result_1211906_m into qservResult.result_1211906 0103 16:38:24.716 [0x7fe3cf7fe700] DEBUG root (build/rproc/InfileMerger.cc:432) - InfileMerger create table:CREATE TABLE qservResult.result_1211906_m (`QS1_COUNT` BIGINT(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] ERROR root (build/rproc/InfileMerger.cc:351) - InfileMerger sql error: Error applying sql. Error 1050: Table 'result_1211906_m' already exists Unable to execute query: CREATE TABLE qservResult.result_1211906_m (`QS1_COUNT` BIGINT(21)) 0103 16:38:24.717 [0x7fe3cf7fe700] ERROR root (build/rproc/InfileMerger.cc:438) - InfileMerger error: Error creating table (qservResult.result_1211906_m) 0103 16:38:34.672 [0x7fe3deffd700] INFO root (build/rproc/InfileMerger.cc:299) - Merging w/CREATE TABLE qservResult.result_1211906 SELECT SUM(QS1_COUNT) AS OBJ_COUNT FROM qservResult.result_1211906_m 0103 16:38:34.673 [0x7fe3deffd700] ERROR root (build/rproc/InfileMerger.cc:351) - InfileMerger sql error: Error applying sql. Error 1054: Unknown column 'QS1_COUNT' in 'field list' Unable to execute query: CREATE TABLE qservResult.result_1211906 SELECT SUM(QS1_COUNT) AS OBJ_COUNT FROM qservResult.result_1211906_m 0103 16:38:34.673 [0x7fe3deffd700] INFO root (build/rproc/InfileMerger.cc:305) - Cleaning up qservResult.result_1211906_m 0103 16:38:34.674 [0x7fe3deffd700] INFO root (build/rproc/InfileMerger.cc:317) - Merged qservResult.result_1211906_m into qservResult.result_1211906 {code} So it seems test case #01 create table .result_1211906_m, and then test case #02 try to re-use this name for an other query. Qserv result tables aren't cleaned (here Qserv has be stopped): {code} ls ~/qserv-run/2014_12/var/lib/mysql/qservResult/ db.opt result_1211336_m.MYI result_1211340_m.MYI result_1211343_m.MYI result_1211346_m.MYI result_1211382_m.MYI result_1211385_m.MYI result_1211902_m.MYI result_1211905_m.MYI result_1211334_m.frm result_1211337_m.frm result_1211341_m.frm result_1211344_m.frm result_1211347_m.frm result_1211383_m.frm result_1211386_m.frm result_1211903_m.frm result_1211906_m.frm result_1211334_m.MYD result_1211337_m.MYD result_1211341_m.MYD result_1211344_m.MYD result_1211347_m.MYD result_1211383_m.MYD result_1211386_m.MYD result_1211903_m.MYD result_1211906_m.MYD result_1211334_m.MYI result_1211337_m.MYI result_1211341_m.MYI result_1211344_m.MYI result_1211347_m.MYI result_1211383_m.MYI result_1211386_m.MYI result_1211903_m.MYI result_1211906_m.MYI result_1211335_m.frm result_1211339_m.frm result_1211342_m.frm result_1211345_m.frm result_1211381_m.frm result_1211384_m.frm result_1211901_m.frm result_1211904_m.frm result_1211335_m.MYD result_1211339_m.MYD result_1211342_m.MYD result_1211345_m.MYD result_1211381_m.MYD result_1211384_m.MYD result_1211901_m.MYD result_1211904_m.MYD result_1211335_m.MYI result_1211339_m.MYI result_1211342_m.MYI result_1211345_m.MYI result_1211381_m.MYI result_1211384_m.MYI result_1211901_m.MYI result_1211904_m.MYI result_1211336_m.frm result_1211340_m.frm result_1211343_m.frm result_1211346_m.frm result_1211382_m.frm result_1211385_m.frm result_1211902_m.frm result_1211905_m.frm result_1211336_m.MYD result_1211340_m.MYD result_1211343_m.MYD result_1211346_m.MYD result_1211382_m.MYD result_1211385_m.MYD result_1211902_m.MYD result_1211905_m.MYD {code} Changing _idCounter to next value in appInterface.py: {code:python} self._idCounter = int(time.time() % (60*60*24*365) * 10) {code} solves the problem, but a deeper explanation will allow to bring a more robust fix."
Build 2015_01 Qserv release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,1,DM-1733,datamanagement,build 2015_01 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build 2015_01 Qserv release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
Have newinstall.sh check itself against distrib version We want to alert people who are just using a newinstall.sh they have lying around (old or hacked up or...) that they are not using the official server version.  ,1,DM-1735,datamanagement,newinstall.sh check distrib version want alert people newinstall.sh lie old hack official server version,Have newinstall.sh check itself against distrib version We want to alert people who are just using a newinstall.sh they have lying around (old or hacked up or...) that they are not using the official server version.
"deblender artifacts in noise-replaced images We still see noise artifacts in some deblended images on the LSST side when running the M31 HSC data.  They look like the result of running NoiseReplacer on HeavyFootprints in which the children can extend beyond the parents.  This was fixed on the HSC side on DM-340 (before the HSC JIRA split off), and I *think* we just need to transfer the fix to LSST.",1,DM-1738,datamanagement,deblender artifact noise replace image noise artifact deblende image lsst run m31 hsc datum look like result run noisereplacer heavyfootprints child extend parent fix hsc dm-340 hsc jira split think need transfer fix lsst,"deblender artifacts in noise-replaced images We still see noise artifacts in some deblended images on the LSST side when running the M31 HSC data. They look like the result of running NoiseReplacer on HeavyFootprints in which the children can extend beyond the parents. This was fixed on the HSC side on DM-340 (before the HSC JIRA split off), and I *think* we just need to transfer the fix to LSST."
"CSV reader for Qserv partitioner doesn't handle no-escape and no-quote options properly Both the no-quote and no-escape CSV formatting command line options should not have a default value, as specifying any value turns off field escaping and quoting. Furthermore, when quoting is turned off, the reader incorrectly treats embedded NUL characters as a quote character.",1,DM-1743,datamanagement,csv reader qserv partitioner handle escape quote option properly quote escape csv format command line option default value specify value turn field escape quoting furthermore quote turn reader incorrectly treat embed nul character quote character,"CSV reader for Qserv partitioner doesn't handle no-escape and no-quote options properly Both the no-quote and no-escape CSV formatting command line options should not have a default value, as specifying any value turns off field escaping and quoting. Furthermore, when quoting is turned off, the reader incorrectly treats embedded NUL characters as a quote character."
"Fix SWIG_SWIG_LIB empty list default value See Serge message to Qserv-l ""xrootd premature death"": {quote} However, there are bigger problems. First of all, master doesn’t build for me. I get this error:    File ""/home/lsstadm/qserv/SConstruct"", line 104:     env.Alias(""dist-core"", get_install_targets())   File ""/home/lsstadm/qserv/SConstruct"", line 90:     exports=['env', 'ARGUMENTS'])   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 609:     return method(*args, **kw)   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 546:     return _SConscript(self.fs, *files, **subst_kw)   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 260:     exec _file_ in call_stack[-1].globals   File ""/home/lsstadm/qserv/build/SConscript"", line 39:     canBuild = detect.checkMySql(env) and detect.setXrootd(env) and detect.checkXrootdLink(env)   File ""/home/lsstadm/qserv/site_scons/detect.py"", line 225:     xrdLibPath = findXrootdLibPath(""XrdCl"", env[""LIBPATH""])   File ""/home/lsstadm/qserv/site_scons/detect.py"", line 213:     if os.access(os.path.join(path, fName), os.R_OK):   File ""/home/lsstadm/stack/Linux64/anaconda/2.1.0/lib/python2.7/posixpath.py"", line 77:     elif path == '' or path.endswith('/'):  which is caused by the fact that env[“LIBPATH”] looks like:  [[], '/home/lsstadm/stack/Linux64/antlr/2.7.7/lib', '/home/lsstadm/stack/Linux64/boost/1.55.0.1.lsst2/lib', '/home/lsstadm/stack/Linux64/log4cxx/0.10.0.lsst1+2/lib', '/home/lsstadm/stack/Linux64/xrootd/4.0.0rc4-qsClient2/lib', '/home/lsstadm/stack/Linux64/zookeeper/3.4.6/c-binding/lib', '/home/lsstadm/stack/Linux64/mysql/5.1.65.lsst1/lib', '/home/lsstadm/stack/Linux64/protobuf/2.4.1/lib', '/home/lsstadm/stack/Linux64/log/10.0+3/lib']  The first element is [], which comes from https://github.com/LSST/qserv/blob/master/site_scons/state.py#L173 where a PathVariable called SWIG_SWIG_LIB is given a default value of []. I can fix the build by changing the default to an empty string… but I don’t know enough scons to say whether that’s the right thing to do. Can one of the scons gurus confirm that’s the right fix? {quote}",1,DM-1744,datamanagement,"fix swig_swig_lib list default value serge message qserv xrootd premature death quote big problem master doesn build error file /home lsstadm qserv sconstruct line 104 env alias(""dist core get_install_target file /home lsstadm qserv sconstruct line 90 exports=['env argument file /home lsstadm stack linux64 scons/2.3.0 lib scon scons script sconscript.py line 609 return method(*args kw file /home lsstadm stack linux64 scons/2.3.0 lib scon scons script sconscript.py line 546 return sconscript(self.fs file subst_kw file /home lsstadm stack linux64 scons/2.3.0 lib scon scons script sconscript.py line 260 exec file call_stack[-1].global file /home lsstadm qserv build sconscript line 39 canbuild detect.checkmysql(env detect.setxrootd(env detect.checkxrootdlink(env file /home lsstadm qserv site_scon detect.py line 225 xrdlibpath findxrootdlibpath(""xrdcl env[""libpath file /home lsstadm qserv site_scon detect.py line 213 os.access(os.path.join(path fname os r_ok file /home lsstadm stack linux64 anaconda/2.1.0 lib python2.7 posixpath.py line 77 elif path path.endswith('/ cause fact env libpath look like /home lsstadm stack linux64 antlr/2.7.7 lib /home lsstadm stack linux64 boost/1.55.0.1.lsst2 lib /home lsstadm stack linux64 log4cxx/0.10.0.lsst1 lib /home lsstadm stack linux64 xrootd/4.0.0rc4 qsclient2 lib /home lsstadm stack linux64 zookeeper/3.4.6 bind lib /home lsstadm stack linux64 mysql/5.1.65.lsst1 lib /home lsstadm stack linux64 protobuf/2.4.1 lib /home lsstadm stack linux64 log/10.0 lib element come https://github.com/lsst/qserv/blob/master/site_scons/state.py#l173 pathvariable call swig_swig_lib give default value fix build change default string don know scon right thing scon guru confirm right fix quote","Fix SWIG_SWIG_LIB empty list default value See Serge message to Qserv-l ""xrootd premature death"": {quote} However, there are bigger problems. First of all, master doesn t build for me. I get this error: File ""/home/lsstadm/qserv/SConstruct"", line 104: env.Alias(""dist-core"", get_install_targets()) File ""/home/lsstadm/qserv/SConstruct"", line 90: exports=['env', 'ARGUMENTS']) File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 609: return method(*args, **kw) File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 546: return _SConscript(self.fs, *files, **subst_kw) File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 260: exec _file_ in call_stack[-1].globals File ""/home/lsstadm/qserv/build/SConscript"", line 39: canBuild = detect.checkMySql(env) and detect.setXrootd(env) and detect.checkXrootdLink(env) File ""/home/lsstadm/qserv/site_scons/detect.py"", line 225: xrdLibPath = findXrootdLibPath(""XrdCl"", env[""LIBPATH""]) File ""/home/lsstadm/qserv/site_scons/detect.py"", line 213: if os.access(os.path.join(path, fName), os.R_OK): File ""/home/lsstadm/stack/Linux64/anaconda/2.1.0/lib/python2.7/posixpath.py"", line 77: elif path == '' or path.endswith('/'): which is caused by the fact that env[ LIBPATH ] looks like: [[], '/home/lsstadm/stack/Linux64/antlr/2.7.7/lib', '/home/lsstadm/stack/Linux64/boost/1.55.0.1.lsst2/lib', '/home/lsstadm/stack/Linux64/log4cxx/0.10.0.lsst1+2/lib', '/home/lsstadm/stack/Linux64/xrootd/4.0.0rc4-qsClient2/lib', '/home/lsstadm/stack/Linux64/zookeeper/3.4.6/c-binding/lib', '/home/lsstadm/stack/Linux64/mysql/5.1.65.lsst1/lib', '/home/lsstadm/stack/Linux64/protobuf/2.4.1/lib', '/home/lsstadm/stack/Linux64/log/10.0+3/lib'] The first element is [], which comes from https://github.com/LSST/qserv/blob/master/site_scons/state.py#L173 where a PathVariable called SWIG_SWIG_LIB is given a default value of []. I can fix the build by changing the default to an empty string but I don t know enough scons to say whether that s the right thing to do. Can one of the scons gurus confirm that s the right fix? {quote}"
"Update auto build tool to work with new split repositories  After the repository split, changes are required to get the auto build tool to work properly. Firefly and Firefly based applications are built using Gradle system.  ",8,DM-1754,datamanagement,update auto build tool work new split repository repository split change require auto build tool work properly firefly firefly base application build gradle system,"Update auto build tool to work with new split repositories After the repository split, changes are required to get the auto build tool to work properly. Firefly and Firefly based applications are built using Gradle system."
"Create an integration test case with GB-sized data It's difficult to load manually data in Qserv, so a way to do that is to use integration test framework to automatically do this.  Big data file won't be stored in git, but the user wil lhave to retrieve them manually, and the test case won't be executed by integration tests.",4,DM-1755,datamanagement,create integration test case gb sized datum difficult load manually datum qserv way use integration test framework automatically big data file will store git user wil lhave retrieve manually test case will execute integration test,"Create an integration test case with GB-sized data It's difficult to load manually data in Qserv, so a way to do that is to use integration test framework to automatically do this. Big data file won't be stored in git, but the user wil lhave to retrieve them manually, and the test case won't be executed by integration tests."
"Provide input data for exampleCmdLineTask.py {{pipe_tasks/examples/exampleCmdLineTask.py}} reads data from a repository. The comments in {{pipe_tasks/python/lsst/pipe/tasks/exampleCmdLineTask.py}} suggest that  {code} # The following will work on an NCSA lsst* computer: examples/exampleCmdLineTask.py /lsst8/krughoff/diffim_data/sparse_diffim_output_v7_2 --id visit=6866601 {code}  There are a few problems with that:  * External contributors don't have access to {{lsst*}}; * Even though that data exists now, it's unclear how long it will remain there, or what steps are being taken to preserve it; * The mention of this data is fairly well buried -- it does appear in the documentation, but it's certainly not the first thing a new user will stumble upon.  At least the first two points could be addressed by referring to a publicly available data repository. For example, the following works once {{afwdata}} has been set up:  {code} examples/exampleCmdLineTask.py ${AFWDATA_DIR}/ImSim --id visit=85408556 {code}  Although this has the downside of only providing a single image.",1,DM-1761,datamanagement,provide input datum examplecmdlinetask.py pipe_tasks example examplecmdlinetask.py read datum repository comment pipe_tasks python lsst pipe task examplecmdlinetask.py suggest code following work ncsa lsst computer example examplecmdlinetask.py /lsst8 krughoff diffim_data sparse_diffim_output_v7_2 --id visit=6866601 code problem external contributor access lsst datum exist unclear long remain step take preserve mention datum fairly bury appear documentation certainly thing new user stumble point address refer publicly available datum repository example follow work afwdata set code example examplecmdlinetask.py afwdata_dir}/imsim --id visit=85408556 code downside provide single image,"Provide input data for exampleCmdLineTask.py {{pipe_tasks/examples/exampleCmdLineTask.py}} reads data from a repository. The comments in {{pipe_tasks/python/lsst/pipe/tasks/exampleCmdLineTask.py}} suggest that {code} # The following will work on an NCSA lsst* computer: examples/exampleCmdLineTask.py /lsst8/krughoff/diffim_data/sparse_diffim_output_v7_2 --id visit=6866601 {code} There are a few problems with that: * External contributors don't have access to {{lsst*}}; * Even though that data exists now, it's unclear how long it will remain there, or what steps are being taken to preserve it; * The mention of this data is fairly well buried -- it does appear in the documentation, but it's certainly not the first thing a new user will stumble upon. At least the first two points could be addressed by referring to a publicly available data repository. For example, the following works once {{afwdata}} has been set up: {code} examples/exampleCmdLineTask.py ${AFWDATA_DIR}/ImSim --id visit=85408556 {code} Although this has the downside of only providing a single image."
"Export SUI data (DC_W13_Stripe82_subset) - import sui.sql.bzip2.out (produced by Serge) into MySQL for DeepSource and DeepForcedSource tables: - remove columns chunkId and subChunkId for each chunk table - merge all chunk table into the main table - join DeepSource and DeepForcedSource to add coordinates of DeepSource (director) object in DeepForcedSource table. then dump  DeepSource and DeepForcedSource  to files DeepSource.csv and DeepForcedSource.csv {code:sql} SELECT f.*, COALESCE(s.ra, f.ra), COALESCE(s.decl, f.decl) FROM DeepForcedSource f LEFT JOIN DeepSource s ON (f.deepSourceId = s.deepSourceId) INTO OUTFILE '/db1/dump/DeepForcedSource.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\n'; {code} - Load this file using Qserv loader.  A sample should be made and tested first to validate this procedure. This sample could be added in qserv_testdata",3,DM-1762,datamanagement,export sui datum dc_w13_stripe82_subset import produce serge mysql deepsource deepforcedsource table remove column chunkid subchunkid chunk table merge chunk table main table join deepsource deepforcedsource add coordinate deepsource director object deepforcedsource table dump deepsource deepforcedsource file deepsource.csv deepforcedsource.csv code sql select f. coalesce(s.ra f.ra coalesce(s.decl f.decl deepforcedsource left join deepsource f.deepsourceid s.deepsourceid outfile /db1 dump deepforcedsource.csv fields terminate optionally enclosed lines terminate \n code load file qserv loader sample test validate procedure sample add qserv_testdata,"Export SUI data (DC_W13_Stripe82_subset) - import sui.sql.bzip2.out (produced by Serge) into MySQL for DeepSource and DeepForcedSource tables: - remove columns chunkId and subChunkId for each chunk table - merge all chunk table into the main table - join DeepSource and DeepForcedSource to add coordinates of DeepSource (director) object in DeepForcedSource table. then dump DeepSource and DeepForcedSource to files DeepSource.csv and DeepForcedSource.csv {code:sql} SELECT f.*, COALESCE(s.ra, f.ra), COALESCE(s.decl, f.decl) FROM DeepForcedSource f LEFT JOIN DeepSource s ON (f.deepSourceId = s.deepSourceId) INTO OUTFILE '/db1/dump/DeepForcedSource.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\n'; {code} - Load this file using Qserv loader. A sample should be made and tested first to validate this procedure. This sample could be added in qserv_testdata"
"overhaul slot and alias system While working on DM-1218 and DM-464, I've grown quite dissatisfied with the current state of the slot and alias mechanisms, and we now have a concrete proposal for larger-scale changes on RFC-11.  Unfortunately, I don't think we'll be in a good position to do much about this until we've completed the transition to meas_base and removed the old measurement framework in meas_algorithms.",6,DM-1764,datamanagement,overhaul slot alia system work dm-1218 dm-464 grow dissatisfied current state slot alia mechanism concrete proposal large scale change rfc-11 unfortunately think good position complete transition meas_base remove old measurement framework meas_algorithm,"overhaul slot and alias system While working on DM-1218 and DM-464, I've grown quite dissatisfied with the current state of the slot and alias mechanisms, and we now have a concrete proposal for larger-scale changes on RFC-11. Unfortunately, I don't think we'll be in a good position to do much about this until we've completed the transition to meas_base and removed the old measurement framework in meas_algorithms."
"move SourceRecord/Table/Catalog to meas_base We can make address a lot of dependency issues if we move the Source classes to meas_base, because we'll no longer have low-level code (e.g. afw::table persistence) in the same module as very high-level code (e.g. slots).   It will also put all the slot code in the same place, instead of spreading it across two pacakges.  This should be straightforward, except that we'll have a lot of downstream code to (trivially) change, and there's a good chance Swig will get confused somewhere along the way.",2,DM-1765,datamanagement,sourcerecord table catalog meas_base address lot dependency issue source class meas_base long low level code e.g. afw::table persistence module high level code e.g. slot slot code place instead spread pacakge straightforward lot downstream code trivially change good chance swig confuse way,"move SourceRecord/Table/Catalog to meas_base We can make address a lot of dependency issues if we move the Source classes to meas_base, because we'll no longer have low-level code (e.g. afw::table persistence) in the same module as very high-level code (e.g. slots). It will also put all the slot code in the same place, instead of spreading it across two pacakges. This should be straightforward, except that we'll have a lot of downstream code to (trivially) change, and there's a good chance Swig will get confused somewhere along the way."
"Support DDL in MetaServ - design DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves designing the procedure involving loading DDL information into MetaServ. We need to be ready to support a variety of scenarios: * we are getting already preloaded database, need to just load metadata about it to metaserv (we might have the original ascii file with extra information, or not) * we are starting from scratch, need to initialize database (including loading schema), and need to load the information to the metaserv * we already have the database and metadata in metaserv, but we want to change something (eg. alter table, or delete table, or delete database).",2,DM-1770,datamanagement,support ddl metaserv design ddl information embed comment master version schema cat repo currently schema browser story involve design procedure involve load ddl information metaserv need ready support variety scenario get preloade database need load metadata metaserv original ascii file extra information start scratch need initialize database include loading schema need load information metaserv database metadata metaserv want change eg alter table delete table delete database,"Support DDL in MetaServ - design DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves designing the procedure involving loading DDL information into MetaServ. We need to be ready to support a variety of scenarios: * we are getting already preloaded database, need to just load metadata about it to metaserv (we might have the original ascii file with extra information, or not) * we are starting from scratch, need to initialize database (including loading schema), and need to load the information to the metaserv * we already have the database and metadata in metaserv, but we want to change something (eg. alter table, or delete table, or delete database)."
"move executionOrder from plugin config class to plugin class We originally put the executionOrder parameter (which determines when a plugin is run, relative to others), in the config object, simply because that's where it was in the old framework.  But it's really not something that should be configurable, as it depends only on the inputs the algorithm needs, which don't change.",1,DM-1771,datamanagement,executionorder plugin config class plugin class originally executionorder parameter determine plugin run relative config object simply old framework configurable depend input algorithm needs change,"move executionOrder from plugin config class to plugin class We originally put the executionOrder parameter (which determines when a plugin is run, relative to others), in the config object, simply because that's where it was in the old framework. But it's really not something that should be configurable, as it depends only on the inputs the algorithm needs, which don't change."
"Read SUI requirements, send a list of questions to the group scientist Read the requirements document on Archive Browser and Query Tools, make a record of unclear items, send questions to our scientist (D. Ciardi) ",5,DM-1773,datamanagement,read sui requirement send list question group scientist read requirement document archive browser query tools record unclear item send question scientist d. ciardi,"Read SUI requirements, send a list of questions to the group scientist Read the requirements document on Archive Browser and Query Tools, make a record of unclear items, send questions to our scientist (D. Ciardi)"
"Add support for running unit tests in qserv/admin This came up during review of DM-370: ""We do not run tests scripts in admin/ during regular build, SConscript in admin/ does not support that unfortunately."" This story involves tweaking SConscript to enable running unit tests automatically.",1,DM-1774,datamanagement,add support run unit test qserv admin come review dm-370 run test script admin/ regular build sconscript admin/ support unfortunately story involve tweak sconscript enable run unit test automatically,"Add support for running unit tests in qserv/admin This came up during review of DM-370: ""We do not run tests scripts in admin/ during regular build, SConscript in admin/ does not support that unfortunately."" This story involves tweaking SConscript to enable running unit tests automatically."
"Setup work environment / test builds for refactored repositories After the repository was split into firefly and new-ife, it was necessary to understand the changes, check for inconsistencies, test builds, set up new IDEA project, etc. ",3,DM-1775,datamanagement,setup work environment test build refactored repository repository split firefly new ife necessary understand change check inconsistency test build set new idea project etc,"Setup work environment / test builds for refactored repositories After the repository was split into firefly and new-ife, it was necessary to understand the changes, check for inconsistencies, test builds, set up new IDEA project, etc."
"XYPlotter should be caching and restoring plot metadata For efficiency, XYPlotter is designed to create up to 4 cards. When the card limit is exceeded, a previously created card is reused to plot catalog data for the current catalog. When card is reused the previous plot metadata (like column selections, grid option, etc.) are lost.",4,DM-1777,datamanagement,xyplotter cache restore plot metadata efficiency xyplotter design create card card limit exceed previously create card reuse plot catalog datum current catalog card reuse previous plot metadata like column selection grid option etc lose,"XYPlotter should be caching and restoring plot metadata For efficiency, XYPlotter is designed to create up to 4 cards. When the card limit is exceeded, a previously created card is reused to plot catalog data for the current catalog. When card is reused the previous plot metadata (like column selections, grid option, etc.) are lost."
"Remove optimisation flag management in partition SConstruct - eups build with -g and -O3 by default.  - Developpers can build their sources with next options:  scons debug=false opt=3  - Nevetheless partition SConstruct add -O2 option if debug is disabled, which is orthogonal. ",2,DM-1778,datamanagement,remove optimisation flag management partition sconstruct eup build -o3 default developper build source option scon debug false opt=3 nevetheless partition sconstruct add -o2 option debug disable orthogonal,"Remove optimisation flag management in partition SConstruct - eups build with -g and -O3 by default. - Developpers can build their sources with next options: scons debug=false opt=3 - Nevetheless partition SConstruct add -O2 option if debug is disabled, which is orthogonal."
"fix faint source and minimum-radius problems in Kron photometry This transfers some improvements to the Kron photometry from the HSC side:  - HSC-983: address failures on faint sources  - HSC-989: fix the minimum radius  - HSC-865: switch to determinant radius instead of semimajor axis  - HSC-962: bad radius flag was not being used  - HSC-121: fix scaling in forced photometry  The story points estimate here is 50% of the actual effort, as the work (already done) also benefited HSC.",5,DM-1783,datamanagement,fix faint source minimum radius problem kron photometry transfer improvement kron photometry hsc hsc-983 address failure faint source hsc-989 fix minimum radius hsc-865 switch determinant radius instead semimajor axis hsc-962 bad radius flag hsc-121 fix scale force photometry story point estimate 50 actual effort work benefit hsc,"fix faint source and minimum-radius problems in Kron photometry This transfers some improvements to the Kron photometry from the HSC side: - HSC-983: address failures on faint sources - HSC-989: fix the minimum radius - HSC-865: switch to determinant radius instead of semimajor axis - HSC-962: bad radius flag was not being used - HSC-121: fix scaling in forced photometry The story points estimate here is 50% of the actual effort, as the work (already done) also benefited HSC."
Fix errors in parsing or rendering nested expressions Qserv has errors rendering nested expressions in predicates of the WHERE clause. It is unclear whether the problem is in constructing the predicate representation or in rendering the representation (or both).  Example: {code} SELECT  o1.objectId FROM Object o1  WHERE ABS( (scisql_fluxToAbMag(o1.gFlux_PS)-scisql_fluxToAbMag(o1.rFlux_PS)) -              (scisql_fluxToAbMag(o1.gFlux_PS)-scisql_fluxToAbMag(o1.rFlux_PS)) ) < 1 {code} Yields: {code} SELECT o1.objectId FROM LSST.Object_100 AS o1 WHERE ABS((VALUE_EXP FACTOR FUNCTION_SPEC scisql_fluxToAbMag(VALUE_EXP FACTOR COLUMN_REF o1.gFlux_PS)-FACTOR FUNCTION_SPEC scisql_fluxToAbMag(VALUE_EXP FACTOR COLUMN_REF o1.rFlux_PS)))<1 {code}  I probably left out a more general implementation one/both of those parts of query parsing/analysis.,4,DM-1784,datamanagement,fix error parse render nested expression qserv error render nested expression predicate clause unclear problem construct predicate representation render representation example code select o1.objectid object o1 abs scisql_fluxtoabmag(o1.gflux_ps)-scisql_fluxtoabmag(o1.rflux_ps scisql_fluxtoabmag(o1.gflux_ps)-scisql_fluxtoabmag(o1.rflux_ps code yield code select o1.objectid lsst.object_100 o1 abs((value_exp factor function_spec scisql_fluxtoabmag(value_exp factor column_ref o1.gflux_ps)-factor scisql_fluxtoabmag(value_exp factor column_ref o1.rflux_ps)))<1 code probably leave general implementation part query parse analysis,Fix errors in parsing or rendering nested expressions Qserv has errors rendering nested expressions in predicates of the WHERE clause. It is unclear whether the problem is in constructing the predicate representation or in rendering the representation (or both). Example: {code} SELECT o1.objectId FROM Object o1 WHERE ABS( (scisql_fluxToAbMag(o1.gFlux_PS)-scisql_fluxToAbMag(o1.rFlux_PS)) - (scisql_fluxToAbMag(o1.gFlux_PS)-scisql_fluxToAbMag(o1.rFlux_PS)) ) < 1 {code} Yields: {code} SELECT o1.objectId FROM LSST.Object_100 AS o1 WHERE ABS((VALUE_EXP FACTOR FUNCTION_SPEC scisql_fluxToAbMag(VALUE_EXP FACTOR COLUMN_REF o1.gFlux_PS)-FACTOR FUNCTION_SPEC scisql_fluxToAbMag(VALUE_EXP FACTOR COLUMN_REF o1.rFlux_PS)))<1 {code} I probably left out a more general implementation one/both of those parts of query parsing/analysis.
"Add rotAngle to baseline schema Add ""rotAngle DOUBLE"" to every table that has image ra/decl.  ",1,DM-1785,datamanagement,add rotangle baseline schema add rotangle double table image ra decl,"Add rotAngle to baseline schema Add ""rotAngle DOUBLE"" to every table that has image ra/decl."
"OpenStack automation via Python scripts : Launch an Instance In our introductory work with OpenStack we have been utilizing the Horizon GUI interface for first steps, followed by the use of command line tools (the 'CLI') (e.g., nova, cinder, etc) as shown in DM-1334  DM-1700 , DM-1701. While it is possible to write automation scripts that utilize the CLI, an approach based on 'pure' Python scripting would fit more seamlessly into the LSST software development process. Enabling OpenStack automation via Python offers the opportunity to integrate provisioning of resources into the overall flow of LSST workflow & processing (e.g., DRP.)  The OpenStack services expose native Python APIs that expose the same feature set as the command-line tools.  The required python packages (python-keystoneclient, python-novaclient, ..) are installed on the head node 'vlad-mgmt' of the NCSA ISL OpenStack, and so initial Python scripting can be executed/tested there.  A first Python script will perform required authentication and launch an instance. ",4,DM-1787,datamanagement,openstack automation python script launch instance introductory work openstack utilize horizon gui interface step follow use command line tool cli e.g. nova cinder etc show dm-1334 dm-1700 dm-1701 possible write automation script utilize cli approach base pure python scripting fit seamlessly lsst software development process enable openstack automation python offer opportunity integrate provisioning resource overall flow lsst workflow processing e.g. drp openstack service expose native python api expose feature set command line tool required python package python keystoneclient python novaclient instal head node vlad mgmt ncsa isl openstack initial python scripting execute test python script perform require authentication launch instance,"OpenStack automation via Python scripts : Launch an Instance In our introductory work with OpenStack we have been utilizing the Horizon GUI interface for first steps, followed by the use of command line tools (the 'CLI') (e.g., nova, cinder, etc) as shown in DM-1334 DM-1700 , DM-1701. While it is possible to write automation scripts that utilize the CLI, an approach based on 'pure' Python scripting would fit more seamlessly into the LSST software development process. Enabling OpenStack automation via Python offers the opportunity to integrate provisioning of resources into the overall flow of LSST workflow & processing (e.g., DRP.) The OpenStack services expose native Python APIs that expose the same feature set as the command-line tools. The required python packages (python-keystoneclient, python-novaclient, ..) are installed on the head node 'vlad-mgmt' of the NCSA ISL OpenStack, and so initial Python scripting can be executed/tested there. A first Python script will perform required authentication and launch an instance."
"OpenStack automation via Python scripts : Software installation/test on an LSST node The set of packages that will enable us to write against the native Python APIs of the OpenStack services is  {code} python-keystoneclient python-glanceclient python-novaclient python-quantumclient python-cinderclient python-swiftclient {code}  We begin testing these in DM-1787  on the NCSA OpenStack head node, but eventual use within LSST orchestrating workflow would entail these being installed on LSST nodes in the LSST stack.  In this issue we perform a basic installation of these packages into the system space on an LSST node/VM for testing.  These are managed in github, and we install these via 'pip install' onto an LSST VM for initial tests.",4,DM-1788,datamanagement,openstack automation python script software installation test lsst node set package enable write native python api openstack service code python keystoneclient python glanceclient python novaclient python quantumclient python cinderclient python swiftclient code begin test dm-1787 ncsa openstack head node eventual use lsst orchestrate workflow entail instal lsst node lsst stack issue perform basic installation package system space lsst node vm testing manage github install pip install lsst vm initial test,"OpenStack automation via Python scripts : Software installation/test on an LSST node The set of packages that will enable us to write against the native Python APIs of the OpenStack services is {code} python-keystoneclient python-glanceclient python-novaclient python-quantumclient python-cinderclient python-swiftclient {code} We begin testing these in DM-1787 on the NCSA OpenStack head node, but eventual use within LSST orchestrating workflow would entail these being installed on LSST nodes in the LSST stack. In this issue we perform a basic installation of these packages into the system space on an LSST node/VM for testing. These are managed in github, and we install these via 'pip install' onto an LSST VM for initial tests."
Update documentation and automatic install script w.r.t. new newinstall.sh script newinstall.sh script has evolved and breaks Qserv install procedure.,1,DM-1792,datamanagement,update documentation automatic install script w.r.t new newinstall.sh script newinstall.sh script evolve break qserv install procedure,Update documentation and automatic install script w.r.t. new newinstall.sh script newinstall.sh script has evolved and breaks Qserv install procedure.
"Pull distEst package into obs_subaru Reducing HSC data requires an estimate of the distortion, which is provided by the HSC package distEst.  This can be pulled into obs_subaru to consolidate code and reduce dependencies.  I propose to treat distEst as legacy code, which means I will pull it into obs_subaru without major changes to the code style.",6,DM-1794,datamanagement,pull distest package obs_subaru reduce hsc datum require estimate distortion provide hsc package distest pull obs_subaru consolidate code reduce dependency propose treat distest legacy code mean pull obs_subaru major change code style,"Pull distEst package into obs_subaru Reducing HSC data requires an estimate of the distortion, which is provided by the HSC package distEst. This can be pulled into obs_subaru to consolidate code and reduce dependencies. I propose to treat distEst as legacy code, which means I will pull it into obs_subaru without major changes to the code style."
"Use anonymous NCSA rsync server to distribute large test datafiles. DM-1755 has been done before this feature was available. It uses rsync over ssh which require use to have an ssh-key on lsst-dev.  NSCA rsync server can now be accessed with next syntax: {code:bash} rsync -av lsst-rsync.ncsa.illinois.edu::qserv/qserv_testdata/datasets/case04/data/DeepSource.csv.gz . # LIST FILES AVAILABLE IN THE MODULE NAMED ""qserv"" rsync lsst-rsync.ncsa.illinois.edu::qserv  # To add content to this module/group, you can copy files into the following path:   /lsst/rsync/qserv/ {code}  rsync over ssh feature should be kept, in order to distribute private data are distributed.",4,DM-1795,datamanagement,use anonymous ncsa rsync server distribute large test datafile dm-1755 feature available use rsync ssh require use ssh key lsst dev nsca rsync server access syntax code bash rsync -av lsst rsync.ncsa.illinois.edu::qserv qserv_testdata dataset case04 data deepsource.csv.gz list files available module named qserv rsync lsst rsync.ncsa.illinois.edu::qserv add content module group copy file follow path /lsst rsync qserv/ code rsync ssh feature keep order distribute private datum distribute,"Use anonymous NCSA rsync server to distribute large test datafiles. DM-1755 has been done before this feature was available. It uses rsync over ssh which require use to have an ssh-key on lsst-dev. NSCA rsync server can now be accessed with next syntax: {code:bash} rsync -av lsst-rsync.ncsa.illinois.edu::qserv/qserv_testdata/datasets/case04/data/DeepSource.csv.gz . # LIST FILES AVAILABLE IN THE MODULE NAMED ""qserv"" rsync lsst-rsync.ncsa.illinois.edu::qserv # To add content to this module/group, you can copy files into the following path: /lsst/rsync/qserv/ {code} rsync over ssh feature should be kept, in order to distribute private data are distributed."
"Improve test coverage for case04 Most of queries used in GB-sized case04 return empty results.  These queries should be added: {code:bash} fjammes@lsst-db2:~/src/qserv_testdata (u/fjammes/DM-1755) $ mysql --host 127.0.0.1 --port 4040 --user qsmaster qservTest_caseSUI_qserv -e ""SELECT count(*) FROM DeepForcedSource"" +----------------+ | SUM(QS1_COUNT) | +----------------+ |       33349940 | +----------------+ fjammes@lsst-db2:~/src/qserv_testdata (u/fjammes/DM-1755) $ mysql --host 127.0.0.1 --port 4040 --user qsmaster qservTest_caseSUI_qserv -e ""SELECT * FROM DeepForcedSource LIMIT 1"" {code}  But, even better, SUI team could provide some more interesting query to Qserv team in order to improve case04 quality.",3,DM-1796,datamanagement,improve test coverage case04 query gb sized case04 return result query add code bash fjammes@lsst db2:~/src qserv_testdata fjamme dm-1755 mysql 127.0.0.1 --port 4040 --user qsmaster qservtest_casesui_qserv select count deepforcedsource ----------------+ sum(qs1_count ----------------+ 33349940 ----------------+ fjammes@lsst db2:~/src qserv_testdata fjamme dm-1755 mysql 127.0.0.1 --port 4040 --user qsmaster qservtest_casesui_qserv select deepforcedsource limit code well sui team provide interesting query qserv team order improve case04 quality,"Improve test coverage for case04 Most of queries used in GB-sized case04 return empty results. These queries should be added: {code:bash} fjammes@lsst-db2:~/src/qserv_testdata (u/fjammes/DM-1755) $ mysql --host 127.0.0.1 --port 4040 --user qsmaster qservTest_caseSUI_qserv -e ""SELECT count(*) FROM DeepForcedSource"" +----------------+ | SUM(QS1_COUNT) | +----------------+ | 33349940 | +----------------+ fjammes@lsst-db2:~/src/qserv_testdata (u/fjammes/DM-1755) $ mysql --host 127.0.0.1 --port 4040 --user qsmaster qservTest_caseSUI_qserv -e ""SELECT * FROM DeepForcedSource LIMIT 1"" {code} But, even better, SUI team could provide some more interesting query to Qserv team in order to improve case04 quality."
"Package flask The Data Access Webservice APIs are relying on flask, so we need to package flask according to the LSST standards. For my initial testing, I just run ""sudo aptitude install python-flask"".  ",1,DM-1797,datamanagement,package flask data access webservice api rely flask need package flask accord lsst standard initial testing run sudo aptitude install python flask,"Package flask The Data Access Webservice APIs are relying on flask, so we need to package flask according to the LSST standards. For my initial testing, I just run ""sudo aptitude install python-flask""."
Regression testing of AP Simulator Run the AP simulator to make sure that none of the changes to the ctrl_events package break anything.,1,DM-1798,datamanagement,regression testing ap simulator run ap simulator sure change ctrl_event package break,Regression testing of AP Simulator Run the AP simulator to make sure that none of the changes to the ctrl_events package break anything.
Regression testing of Orca Do some test runs using Orca to make sure Orca still works after the the changes to ctrl_events.,1,DM-1799,datamanagement,regression testing orca test run orca sure orca work change ctrl_event,Regression testing of Orca Do some test runs using Orca to make sure Orca still works after the the changes to ctrl_events.
remove unused local typedefs gcc 4.8 now warns about locally-defined typedefs that aren't used.  We have a few of these in ndarray and afw::gpu that should be removed.,1,DM-1802,datamanagement,remove unused local typedef gcc 4.8 warn locally define typedef ndarray afw::gpu remove,remove unused local typedefs gcc 4.8 now warns about locally-defined typedefs that aren't used. We have a few of these in ndarray and afw::gpu that should be removed.
"S15 Explore Qserv Authorization Explore authorization centrally: use information generated by parser. Either generate dummy query and run on mysql that runs near czar, or use info produced by parser to determine if user is authorized.  Note, we want to limit this to ~1 week, just to reveal potential problems, or do a quick proof of concept.",8,DM-1803,datamanagement,s15 explore qserv authorization explore authorization centrally use information generate parser generate dummy query run mysql run near czar use info produce parser determine user authorize note want limit ~1 week reveal potential problem quick proof concept,"S15 Explore Qserv Authorization Explore authorization centrally: use information generated by parser. Either generate dummy query and run on mysql that runs near czar, or use info produced by parser to determine if user is authorized. Note, we want to limit this to ~1 week, just to reveal potential problems, or do a quick proof of concept."
Study the current SUI requirement  Study the current requirement carefully to make sure they all make sense and we can do it.,4,DM-1804,datamanagement,study current sui requirement study current requirement carefully sure sense,Study the current SUI requirement Study the current requirement carefully to make sure they all make sense and we can do it.
"segfaults in ip_diffim on gcc 4.8 I'm seeing test segfaults in ip_diffim on gcc 4.8, similar to those resolved on DM-1725, but with no similar smoking gun yet.  Preliminary indication is that the problem is actually in meas_algorithms.",2,DM-1810,datamanagement,segfault ip_diffim gcc 4.8 see test segfault ip_diffim gcc 4.8 similar resolve dm-1725 similar smoking gun preliminary indication problem actually meas_algorithm,"segfaults in ip_diffim on gcc 4.8 I'm seeing test segfaults in ip_diffim on gcc 4.8, similar to those resolved on DM-1725, but with no similar smoking gun yet. Preliminary indication is that the problem is actually in meas_algorithms."
Prepare initial content Prepare LSE-130 content as far as possible without input from the new collimated-projector calibration plan.,4,DM-1811,datamanagement,prepare initial content prepare lse-130 content far possible input new collimate projector calibration plan,Prepare initial content Prepare LSE-130 content as far as possible without input from the new collimated-projector calibration plan.
"Determine LSE-130 impact of collimated projector calibration plan During a working meeting with Robert Lupton and Chris Stubbs, determine the impact on LSE-130 of the introduction of the collimated projector for calibration.",8,DM-1812,datamanagement,determine lse-130 impact collimate projector calibration plan working meeting robert lupton chris stubbs determine impact lse-130 introduction collimate projector calibration,"Determine LSE-130 impact of collimated projector calibration plan During a working meeting with Robert Lupton and Chris Stubbs, determine the impact on LSE-130 of the introduction of the collimated projector for calibration."
Prepare draft of LSE-130 for Camera and CCB review Produce a reviewable draft of LSE-130 based on decisions on calibration operations,4,DM-1813,datamanagement,prepare draft lse-130 camera ccb review produce reviewable draft lse-130 base decision calibration operation,Prepare draft of LSE-130 for Camera and CCB review Produce a reviewable draft of LSE-130 based on decisions on calibration operations
"Support Camera CD-2 (mainly re: LSE-130) Provide slides and other information needed for CD-2, mainly relative to the open questions around LSE-130",2,DM-1814,datamanagement,support camera cd-2 mainly lse-130 provide slide information need cd-2 mainly relative open question lse-130,"Support Camera CD-2 (mainly re: LSE-130) Provide slides and other information needed for CD-2, mainly relative to the open questions around LSE-130"
"Support LSE-130 review by CCB (mainly Camera) Respond to comments, perform revisions to LSE-130 as necessary based on feedback from CCB review of the document",4,DM-1815,datamanagement,support lse-130 review ccb mainly camera respond comment perform revision lse-130 necessary base feedback ccb review document,"Support LSE-130 review by CCB (mainly Camera) Respond to comments, perform revisions to LSE-130 as necessary based on feedback from CCB review of the document"
"Convert LSE-130 to SysML Following CCB recommendation of approval of LSE-130 draft, convert Word draft to SysML and provide a docgen to Robert McKercher for final posting. ",2,DM-1816,datamanagement,convert lse-130 sysml follow ccb recommendation approval lse-130 draft convert word draft sysml provide docgen robert mckercher final posting,"Convert LSE-130 to SysML Following CCB recommendation of approval of LSE-130 draft, convert Word draft to SysML and provide a docgen to Robert McKercher for final posting."
"Create and post docgen of LSE-68 To support discussions with the Camera, post a provisional docgen of LSE-68 to the appropriate Confluence page.  Use knowledge from EA training to improve template.",4,DM-1817,datamanagement,create post docgen lse-68 support discussion camera post provisional docgen lse-68 appropriate confluence page use knowledge ea training improve template,"Create and post docgen of LSE-68 To support discussions with the Camera, post a provisional docgen of LSE-68 to the appropriate Confluence page. Use knowledge from EA training to improve template."
"Support completion of final document Based on CCB approval of LSE-72 on 10 October, support the completion of the final copy of the document for posting on Docushare.",1,DM-1818,datamanagement,support completion final document base ccb approval lse-72 10 october support completion final copy document post docushare,"Support completion of final document Based on CCB approval of LSE-72 on 10 October, support the completion of the final copy of the document for posting on Docushare."
Complete LSE-140 work as needed to produce final document Complete any review-driven revisions of LSE-140 and support the CCB meeting and following final document preparation.,2,DM-1819,datamanagement,complete work need produce final document complete review drive revision lse-140 support ccb meeting follow final document preparation,Complete LSE-140 work as needed to produce final document Complete any review-driven revisions of LSE-140 and support the CCB meeting and following final document preparation.
"LSE-140: Collect desired changes for future release Prepare for a future revision (Phase 3) of LSE-140.  Collect issues to be addressed in the revision.  Determine if any affect Phase 2 scope (which would require a prompt revision).  It is not anticipated that there will be an actual revision of LSE-140 during the Winter 2015 cycle, because additional detail on calibration requirements will not be available in time.",1,DM-1820,datamanagement,lse-140 collect desire change future release prepare future revision phase lse-140 collect issue address revision determine affect phase scope require prompt revision anticipate actual revision lse-140 winter 2015 cycle additional detail calibration requirement available time,"LSE-140: Collect desired changes for future release Prepare for a future revision (Phase 3) of LSE-140. Collect issues to be addressed in the revision. Determine if any affect Phase 2 scope (which would require a prompt revision). It is not anticipated that there will be an actual revision of LSE-140 during the Winter 2015 cycle, because additional detail on calibration requirements will not be available in time."
"Fix czar assertion failure Reported by Tatiana: I am encountering this once in a while.   qserv-czar.log  python: build/rproc/ProtoRowBuffer.cc:69: int lsst::qserv::rproc::escapeString(Iter, CIter, CIter) [with Iter = __gnu_cxx::__normal_iterator<char*, std::vector<char, std::allocator<char> > >, CIter = __gnu_cxx::__normal_iterator<const char*, std::basic_string<char, std::char_traits<char>, std::allocator<char> > >]: Assertion `srcBegin != srcEnd' failed.  Czar is dead and qserv stops responding after that.  ----  For more details, search in qserv-l archives mails with ""czar assertion failure"" subject: https://listserv.slac.stanford.edu/cgi-bin/wa for complete description.",4,DM-1822,datamanagement,fix czar assertion failure report tatiana encounter qserv-czar.log python build rproc protorowbuffer.cc:69 int lsst::qserv::rproc::escapestring(iter citer citer iter gnu_cxx::__normal_iterator citer gnu_cxx::__normal_iterator std::allocator assertion srcbegin srcend fail czar dead qserv stop respond detail search qserv archive mail czar assertion failure subject https://listserv.slac.stanford.edu/cgi-bin/wa complete description,"Fix czar assertion failure Reported by Tatiana: I am encountering this once in a while. qserv-czar.log python: build/rproc/ProtoRowBuffer.cc:69: int lsst::qserv::rproc::escapeString(Iter, CIter, CIter) [with Iter = __gnu_cxx::__normal_iterator > >, CIter = __gnu_cxx::__normal_iterator, std::allocator > >]: Assertion `srcBegin != srcEnd' failed. Czar is dead and qserv stops responding after that. ---- For more details, search in qserv-l archives mails with ""czar assertion failure"" subject: https://listserv.slac.stanford.edu/cgi-bin/wa for complete description."
"Define issues to be addressed Work with TCS contacts (Jacques Sebag, Paul Lotz, etc.) to define the principal issues",1,DM-1824,datamanagement,define issue address work tcs contact jacques sebag paul lotz etc define principal issue,"Define issues to be addressed Work with TCS contacts (Jacques Sebag, Paul Lotz, etc.) to define the principal issues"
"Produce draft of LSE-75 with agreed revisions Produce a draft of LSE-75 with the following agreed revisions: * remove reference to advance notice of pointing, now in LSE-72 * add reference to PSF reporting",1,DM-1825,datamanagement,produce draft lse-75 agree revision produce draft lse-75 following agree revision remove reference advance notice pointing lse-72 add reference psf report,"Produce draft of LSE-75 with agreed revisions Produce a draft of LSE-75 with the following agreed revisions: * remove reference to advance notice of pointing, now in LSE-72 * add reference to PSF reporting"
Develop General Acceptable Use Policy Don Petravick and Lee LeClair,6,DM-1829,datamanagement,develop general acceptable use policy don petravick lee leclair,Develop General Acceptable Use Policy Don Petravick and Lee LeClair
"Develop PMO Sub-Project Plan and Risk Table Lee LeClair, Iain Goodenow",6,DM-1833,datamanagement,develop pmo sub project plan risk table lee leclair iain goodenow,"Develop PMO Sub-Project Plan and Risk Table Lee LeClair, Iain Goodenow"
"Develop Camera Sub-Project Plan and Risk Table Don Petravick, Richard Dubois",6,DM-1835,datamanagement,develop camera sub project plan risk table don petravick richard dubois,"Develop Camera Sub-Project Plan and Risk Table Don Petravick, Richard Dubois"
"Fix query error on case03: ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata""  Xrootd prevents the worker to return more than 2MB data.  On GB-sized data: {code} mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch  -e ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata""                                                                                                                                                                     ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case03_qserv/1234567890): 20150123-16:27:45, Error merging result, 1420, Result message MD5 mismatch (-1) {code}  On integration test case 04: {code} qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-1841 *)⟫ mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case04_qserv  -e ""SELECT * FROM DeepForcedSource""   ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case04_qserv/6970): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=2 Resource(/chk/qservTest_case04_qserv/7138): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=3 (-1) {code}",5,DM-1841,datamanagement,fix query error case03 select scienceccdexposureid science_ccd_exposure_metadata xrootd prevent worker return mb datum gb sized datum code mysql --port=4040 qsmaster select scienceccdexposureid science_ccd_exposure_metadata error 4120 proxy line error execution ref=1 resource(/chk qservtest_case03_qserv/1234567890 ): 20150123 16:27:45 error merging result 1420 result message md5 mismatch -1 code integration test case 04 code qserv@clrinfoport09:~/src qserv fjamme dm-1841 mysql --port=4040 qsmaster qservtest_case04_qserv select deepforcedsource error 4120 proxy line error execution ref=1 resource(/chk qservtest_case04_qserv/6970 20150204 16:23:43 error merging result 1420 result message md5 mismatch ref=2 resource(/chk qservtest_case04_qserv/7138 20150204 16:23:43 error merging result 1420 result message md5 mismatch ref=3 -1 code,"Fix query error on case03: ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata"" Xrootd prevents the worker to return more than 2MB data. On GB-sized data: {code} mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch -e ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata"" ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case03_qserv/1234567890): 20150123-16:27:45, Error merging result, 1420, Result message MD5 mismatch (-1) {code} On integration test case 04: {code} qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-1841 *) mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case04_qserv -e ""SELECT * FROM DeepForcedSource"" ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case04_qserv/6970): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=2 Resource(/chk/qservTest_case04_qserv/7138): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=3 (-1) {code}"
"Permit PropertySets to be represented in event payloads In the old marshalling code, property sets were representable within the payload of the event.   This was removed in the new marshalling scheme.   There are things (ctrl_orca) that still used this, so this needs to be added to the new marshaling code.  At the same time, new new filtering code can not allow this to be added, because the JMS headers only take simple data types.",2,DM-1843,datamanagement,permit propertyset represent event payload old marshal code property set representable payload event remove new marshalling scheme thing ctrl_orca need add new marshal code time new new filtering code allow add jms header simple datum type,"Permit PropertySets to be represented in event payloads In the old marshalling code, property sets were representable within the payload of the event. This was removed in the new marshalling scheme. There are things (ctrl_orca) that still used this, so this needs to be added to the new marshaling code. At the same time, new new filtering code can not allow this to be added, because the JMS headers only take simple data types."
Test Qserv on SL7 Needed to run Qserv on CC-IN2P3 cluster.,2,DM-1844,datamanagement,test qserv sl7 needed run qserv cc in2p3 cluster,Test Qserv on SL7 Needed to run Qserv on CC-IN2P3 cluster.
"Coordinate implementation of web form for collecting data about existing data sets The form is being implemented by the DataCat team. Need to coordinate (including with the NCSA team which parts are covered by which team), test, fine tune etc.",4,DM-1845,datamanagement,coordinate implementation web form collect datum exist datum set form implement datacat team need coordinate include ncsa team part cover team test fine tune etc,"Coordinate implementation of web form for collecting data about existing data sets The form is being implemented by the DataCat team. Need to coordinate (including with the NCSA team which parts are covered by which team), test, fine tune etc."
"Implement nightly/weekly release automatic distribution - Part I This ticket covers code in sqre-codekit to do migrate as much of the process of special machines, and git tag repos on the basis of eupspkg manifests.",4,DM-1856,datamanagement,implement nightly weekly release automatic distribution ticket cover code sqre codekit migrate process special machine git tag repos basis eupspkg manifest,"Implement nightly/weekly release automatic distribution - Part I This ticket covers code in sqre-codekit to do migrate as much of the process of special machines, and git tag repos on the basis of eupspkg manifests."
Update documentation for v10_0 release All done bar obtaining some release notes. ,2,DM-1860,datamanagement,update documentation v10_0 release bar obtain release note,Update documentation for v10_0 release All done bar obtaining some release notes.
Review existing Level 3 documentation Review existing requirements in this area.  Find all relevant existing project-controlled and other key documents.,4,DM-1865,datamanagement,review exist level documentation review exist requirement area find relevant exist project control key document,Review existing Level 3 documentation Review existing requirements in this area. Find all relevant existing project-controlled and other key documents.
"Document as-is Level 3 requirements and conceptual design Produce a single jumping-off point for documentation on all aspects of Level 3, on Confluence.  Ensure that flowdown for existing Level 3 requirements in SysML is modeled.  Describe the high-level conceptual design.",4,DM-1866,datamanagement,document level requirement conceptual design produce single jumping point documentation aspect level confluence ensure flowdown exist level requirement sysml model describe high level conceptual design,"Document as-is Level 3 requirements and conceptual design Produce a single jumping-off point for documentation on all aspects of Level 3, on Confluence. Ensure that flowdown for existing Level 3 requirements in SysML is modeled. Describe the high-level conceptual design."
"tighten control over heterogeneous DictFields DM-1218 added support for DictFields with heterogeneous item types, which probably allows a bit too much freedom (the rest of pex_config is much more strongly-typed).  Instead of passing None to allow any type to be used, we should pass a tuple of supported types.",1,DM-1867,datamanagement,tighten control heterogeneous dictfields dm-1218 add support dictfields heterogeneous item type probably allow bit freedom rest pex_config strongly type instead pass allow type pass tuple support type,"tighten control over heterogeneous DictFields DM-1218 added support for DictFields with heterogeneous item types, which probably allows a bit too much freedom (the rest of pex_config is much more strongly-typed). Instead of passing None to allow any type to be used, we should pass a tuple of supported types."
"Define JSON Results for Data Access Services As discussed at [Data Access Hangout 2015-02-23|https://confluence.lsstcorp.org/display/DM/Data+Access+Hangout+2015-02-23], we should support json format. This story covers defining structure of JSON results for Data Access Services (dbserv, imgserv, metaserv) ",3,DM-1868,datamanagement,define json result data access services discuss data access hangout 2015 02 23|https://confluence.lsstcorp.org display dm data+access+hangout+2015 02 23 support json format story cover define structure json result data access services dbserv imgserv metaserv,"Define JSON Results for Data Access Services As discussed at [Data Access Hangout 2015-02-23|https://confluence.lsstcorp.org/display/DM/Data+Access+Hangout+2015-02-23], we should support json format. This story covers defining structure of JSON results for Data Access Services (dbserv, imgserv, metaserv)"
"Implement RESTful interfaces for Database (GET) Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""GET"" type requests only, ""POST"" will be handled separately.",5,DM-1880,datamanagement,implement restful interface database implement restful interface database https://confluence.lsstcorp.org/display/dm/api base prototype develop dm-1695 work include add support return appropriately format result support common format cover type request post handle separately,"Implement RESTful interfaces for Database (GET) Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""GET"" type requests only, ""POST"" will be handled separately."
"summarize WEBDAV capabilities and past experience using it WEBDAV could be a candidate for managing the user workspace.  summarize its capabilities and past experience, collect some use cases will help us to make a better decision.",4,DM-1884,datamanagement,summarize webdav capability past experience webdav candidate manage user workspace summarize capability past experience collect use case help well decision,"summarize WEBDAV capabilities and past experience using it WEBDAV could be a candidate for managing the user workspace. summarize its capabilities and past experience, collect some use cases will help us to make a better decision."
"Contribute to the workspace capability discussion  This include past experience, collection of use cases. ",2,DM-1885,datamanagement,contribute workspace capability discussion include past experience collection use case,"Contribute to the workspace capability discussion This include past experience, collection of use cases."
"HDF5 file format study Xiquin, Loi, Trey, and myself discussed HDF5 as a default format to return result set and metadata from lower-level database services vs. traditional IPAC table. Here is the summary:  Advantages of IPAC Table format  - Simple and human-readable, contains a single table - Fixed length rows (easy to page through) - Supported by many astronomical tools  - Provides a way to pass data type, units, and null values in the header - More metadata can be added through keywords (attributes)  Disadvantages of IPAC table format   - Steaming can not be started before all data are received – need to know column width before the table can be written (csv is better alternative) - Only alpha-numeric and '_' characters are allowed in column names (small subset of available characters) - Only predefined datatypes and one attribute type (string) - ASCII representation requires about twice as much storage to represent floating-point number data than the binary equivalent.  Advantages of HDF5  - Can represent complex data and metadata (according to LOFAR, good to represent time series) - Structured data, arbitrary attribute types, datatypes can be combined to create structured datatypes - Flexible datatypes: can be enumerations, bit strings, pointers, composite datatypes, custom atomic datatypes - Access time and storage space optimizations - Partial I/O: “Chunked” data for faster access - Supports parallel I/O (reading and writing) - Built-in compression (GNU zlib, but can be replaced with others) - Existing inspection and visualization tools (HDFView, MATLAB, etc.)  Disadvantages of HDF5  - Complex - Tuned to do efficient I/O and storage for ""big"" data (hundreds of megabytes and more), not efficient for small reads/writes. - Requires native libraries (available in prepackaged jars, see below) - Not human readable - (?) Not yet widely supported by astronomical tools (counter-examples: AstroPy, IDL, more at hdfgroup site)  Tools and Java wrappers:  * JHI5 - the low level JNI wrappers: very flexible, but also quite tedious to use. * Java HDF object package - a high-level interface based on JHI5. * HDFView - a Java-based viewer application based on the Java HDF object package.  * JHDF5 - a high-level interface building on the JHI5 layer which provides most of the functionality of HDF5 to Java. The API has a shallow learning curve and hides most of the house-keeping work from the developer. You can run the Java HDF object package (and HDFView) on the JHI5 interface that is part of JHDF5, so the two APIs can co-exist within one Java program. (from StackOverflow answer, 2012)  * NetCDF-Java is a Pure Java Library, that reads HDF5. However, it's hard to keep pure java version up-to-date with the standard, does not support all the features.  A way to set up native libraries (3rd option from JHDF5 FAQ):      ""Use a library packaged in a jar file and provided as a resource (by putting the jar file on the class path). Internally this uses the same directory structure as method 2., but packaged in a jar file so you don't have to care about it. Jar files with the appropriate structure are cisd-jhdf5-batteries_included.jar and lib/nativejar/.jar (one file for each platform). This is the simplest way to use the library.""       ",1,DM-1887,datamanagement,hdf5 file format study xiquin loi trey discuss hdf5 default format return result set metadata low level database service vs. traditional ipac table summary advantage ipac table format simple human readable contain single table fix length row easy page support astronomical tool provide way pass data type unit null value header metadata add keyword attribute disadvantages ipac table format steaming start datum receive need know column width table write csv well alternative alpha numeric character allow column name small subset available character predefine datatype attribute type string ascii representation require twice storage represent float point number datum binary equivalent advantage hdf5 represent complex datum metadata accord lofar good represent time series structured datum arbitrary attribute type datatype combine create structured datatype flexible datatype enumeration bit string pointer composite datatype custom atomic datatype access time storage space optimization partial chunk datum fast access support parallel reading writing build compression gnu zlib replace exist inspection visualization tool hdfview matlab etc disadvantage hdf5 complex tune efficient storage big datum hundred megabyte efficient small read write require native library available prepackage jar human readable widely support astronomical tool counter example astropy idl hdfgroup site tools java wrapper jhi5 low level jni wrapper flexible tedious use java hdf object package high level interface base jhi5 hdfview java base viewer application base java hdf object package jhdf5 high level interface building jhi5 layer provide functionality hdf5 java api shallow learning curve hide house keep work developer run java hdf object package hdfview jhi5 interface jhdf5 api co exist java program stackoverflow answer 2012 netcdf java pure java library read hdf5 hard pure java version date standard support feature way set native library 3rd option jhdf5 faq use library package jar file provide resource put jar file class path internally use directory structure method package jar file care jar file appropriate structure cisd-jhdf5-batteries_included.jar lib nativejar/.jar file platform simple way use library,"HDF5 file format study Xiquin, Loi, Trey, and myself discussed HDF5 as a default format to return result set and metadata from lower-level database services vs. traditional IPAC table. Here is the summary: Advantages of IPAC Table format - Simple and human-readable, contains a single table - Fixed length rows (easy to page through) - Supported by many astronomical tools - Provides a way to pass data type, units, and null values in the header - More metadata can be added through keywords (attributes) Disadvantages of IPAC table format - Steaming can not be started before all data are received need to know column width before the table can be written (csv is better alternative) - Only alpha-numeric and '_' characters are allowed in column names (small subset of available characters) - Only predefined datatypes and one attribute type (string) - ASCII representation requires about twice as much storage to represent floating-point number data than the binary equivalent. Advantages of HDF5 - Can represent complex data and metadata (according to LOFAR, good to represent time series) - Structured data, arbitrary attribute types, datatypes can be combined to create structured datatypes - Flexible datatypes: can be enumerations, bit strings, pointers, composite datatypes, custom atomic datatypes - Access time and storage space optimizations - Partial I/O: Chunked data for faster access - Supports parallel I/O (reading and writing) - Built-in compression (GNU zlib, but can be replaced with others) - Existing inspection and visualization tools (HDFView, MATLAB, etc.) Disadvantages of HDF5 - Complex - Tuned to do efficient I/O and storage for ""big"" data (hundreds of megabytes and more), not efficient for small reads/writes. - Requires native libraries (available in prepackaged jars, see below) - Not human readable - (?) Not yet widely supported by astronomical tools (counter-examples: AstroPy, IDL, more at hdfgroup site) Tools and Java wrappers: * JHI5 - the low level JNI wrappers: very flexible, but also quite tedious to use. * Java HDF object package - a high-level interface based on JHI5. * HDFView - a Java-based viewer application based on the Java HDF object package. * JHDF5 - a high-level interface building on the JHI5 layer which provides most of the functionality of HDF5 to Java. The API has a shallow learning curve and hides most of the house-keeping work from the developer. You can run the Java HDF object package (and HDFView) on the JHI5 interface that is part of JHDF5, so the two APIs can co-exist within one Java program. (from StackOverflow answer, 2012) * NetCDF-Java is a Pure Java Library, that reads HDF5. However, it's hard to keep pure java version up-to-date with the standard, does not support all the features. A way to set up native libraries (3rd option from JHDF5 FAQ): ""Use a library packaged in a jar file and provided as a resource (by putting the jar file on the class path). Internally this uses the same directory structure as method 2., but packaged in a jar file so you don't have to care about it. Jar files with the appropriate structure are cisd-jhdf5-batteries_included.jar and lib/nativejar/.jar (one file for each platform). This is the simplest way to use the library."""
Add support for IPAC table format Implement support for result formatting in IPAC table format.,6,DM-1891,datamanagement,add support ipac table format implement support result format ipac table format,Add support for IPAC table format Implement support for result formatting in IPAC table format.
"Design CSS schema to support table deletion Table/chunk deletion can be an extended process as some worker nodes may be temporarily down. We need to define a process and its supporting structures in CSS to allow gradual deletion of individual chunks and full tables.  Deliverable: a design of a system capable of deleting a distributed table (all chunks, all replicas). It should be possible to create a table with the same name after deletion.",4,DM-1896,datamanagement,design css schema support table deletion table chunk deletion extended process worker node temporarily need define process support structure css allow gradual deletion individual chunk table deliverable design system capable delete distribute table chunk replicas possible create table deletion,"Design CSS schema to support table deletion Table/chunk deletion can be an extended process as some worker nodes may be temporarily down. We need to define a process and its supporting structures in CSS to allow gradual deletion of individual chunks and full tables. Deliverable: a design of a system capable of deleting a distributed table (all chunks, all replicas). It should be possible to create a table with the same name after deletion."
"Modify CSS structure to support table deletion Modify CSS structures to support DROP TABLE, as defined in DM-1896.",2,DM-1897,datamanagement,modify css structure support table deletion modify css structure support drop table define dm-1896,"Modify CSS structure to support table deletion Modify CSS structures to support DROP TABLE, as defined in DM-1896."
"Consistency checking for table data CSS  CSS data on tables/chunks/nodes is supposed to be consistent at all times. Would be nice to have a tool that verifies consistency, probably including checking actual worker state.",4,DM-1898,datamanagement,consistency check table datum css css datum table chunk nodes suppose consistent time nice tool verifie consistency probably include check actual worker state,"Consistency checking for table data CSS CSS data on tables/chunks/nodes is supposed to be consistent at all times. Would be nice to have a tool that verifies consistency, probably including checking actual worker state."
Tool to dump CSS information CSS information tree may become large and it would be nice to have a tool to examine that tree or parts of it. Something that dumps the tree in user-friendly way and allows filtering or summarizing.,2,DM-1899,datamanagement,tool dump css information css information tree large nice tool examine tree part dump tree user friendly way allow filtering summarizing,Tool to dump CSS information CSS information tree may become large and it would be nice to have a tool to examine that tree or parts of it. Something that dumps the tree in user-friendly way and allows filtering or summarizing.
"Worker management service - design We need to replace direct worker-mysql communication and other administrative channels with a special service which will control all worker communication. Some light-weight service running alongside other worker  servers, probably HTTP-based. Data loading, start/stop should be handled by this service.",5,DM-1900,datamanagement,worker management service design need replace direct worker mysql communication administrative channel special service control worker communication light weight service run alongside worker server probably http base datum loading start stop handle service,"Worker management service - design We need to replace direct worker-mysql communication and other administrative channels with a special service which will control all worker communication. Some light-weight service running alongside other worker servers, probably HTTP-based. Data loading, start/stop should be handled by this service."
Re-implement data loading scripts based on new worker control service Once we have new service that controls worker communication we'll need to reimplement WorkerAdmin class based on that.,8,DM-1901,datamanagement,implement data loading script base new worker control service new service control worker communication need reimplement workeradmin class base,Re-implement data loading scripts based on new worker control service Once we have new service that controls worker communication we'll need to reimplement WorkerAdmin class based on that.
"Implementation of calibration transformation framework Following DM-1598 there will be a detailed design and prototype implementation for the calibration & ingest system. This issue covers cleaning up that code, documenting it, having it reviewed, and merging to master.",2,DM-1903,datamanagement,implementation calibration transformation framework follow dm-1598 detailed design prototype implementation calibration ingest system issue cover clean code document have review merge master,"Implementation of calibration transformation framework Following DM-1598 there will be a detailed design and prototype implementation for the calibration & ingest system. This issue covers cleaning up that code, documenting it, having it reviewed, and merging to master."
Continued footprint improvements A redesigned API and support for topological operations within the Footprint class.  This continues the work started in DM-1107 in W15.  Breakdown: jbosch 15%; swinbank 85%,8,DM-1904,datamanagement,continue footprint improvement redesign api support topological operation footprint class continue work start dm-1107 w15 breakdown jbosch 15 swinbank 85,Continued footprint improvements A redesigned API and support for topological operations within the Footprint class. This continues the work started in DM-1107 in W15. Breakdown: jbosch 15%; swinbank 85%
"QSERV issues when working with scisql_s2PtInCircle This problem has been adressed un u/fjammes/DM-1841.  Here's Tatiana report:  {quote} This error happens on all DeepForcedSource queries. (It happens on DeepSource too, but not always.)  [2015-01-15 11:02:29] [Proxy][4120] Error during execution: -1 Ref=1 Resource(/chk/LSST/6970): 20150115-11:01:05, Complete (success), 0, Ref=2 Resource(/chk/LSST/7138): 20150115-11:02:19, Complete (success), 0, Ref=3 Resource(/chk/LSST/7140): 20150115-11:02:18, Error merging result, 0, Ref=4 Resource(/chk/LSST/730 (-1)  Query examples:  select * from DeepForcedSource where ra>0.4 and ra<0.6 and decl>0.9 and decl<1.1;  select * from DeepForcedSource where scisql_s2PtInCircle(ra, decl, 0.5, 1.1, 0.138) = 1; {quote}",2,DM-1905,datamanagement,qserv issue work scisql_s2ptincircle problem adresse un fjamme dm-1841 tatiana report quote error happen deepforcedsource query happen deepsource 2015 01 15 11:02:29 proxy][4120 error execution ref=1 resource(/chk lsst/6970 20150115 11:01:05 complete success ref=2 resource(/chk lsst/7138 20150115 11:02:19 complete success ref=3 resource(/chk lsst/7140 20150115 11:02:18 error merging result ref=4 resource(/chk lsst/730 -1 query example select deepforcedsource ra>0.4 ra<0.6 decl>0.9 decl<1.1 select deepforcedsource scisql_s2ptincircle(ra decl 0.5 1.1 0.138 quote,"QSERV issues when working with scisql_s2PtInCircle This problem has been adressed un u/fjammes/DM-1841. Here's Tatiana report: {quote} This error happens on all DeepForcedSource queries. (It happens on DeepSource too, but not always.) [2015-01-15 11:02:29] [Proxy][4120] Error during execution: -1 Ref=1 Resource(/chk/LSST/6970): 20150115-11:01:05, Complete (success), 0, Ref=2 Resource(/chk/LSST/7138): 20150115-11:02:19, Complete (success), 0, Ref=3 Resource(/chk/LSST/7140): 20150115-11:02:18, Error merging result, 0, Ref=4 Resource(/chk/LSST/730 (-1) Query examples: select * from DeepForcedSource where ra>0.4 and ra<0.6 and decl>0.9 and decl<1.1; select * from DeepForcedSource where scisql_s2PtInCircle(ra, decl, 0.5, 1.1, 0.138) = 1; {quote}"
Fix missing virtual destructors The compiler is warning about some derived class hierarchies that are lacking virtual destructors.  We should add at least empty implementations to the base classes of these hierarchies.,1,DM-1917,datamanagement,fix miss virtual destructor compiler warn derive class hierarchy lack virtual destructor add implementation base class hierarchy,Fix missing virtual destructors The compiler is warning about some derived class hierarchies that are lacking virtual destructors. We should add at least empty implementations to the base classes of these hierarchies.
"Address misc. compiler warnings Fix places where compiler is warning about some things we are doing on purpose and which we don't intend to change.  This helps keep compiler noise down so its easier to notice ""real"" warnings.",1,DM-1919,datamanagement,address misc compiler warning fix place compiler warn thing purpose intend change help compiler noise easy notice real warning,"Address misc. compiler warnings Fix places where compiler is warning about some things we are doing on purpose and which we don't intend to change. This helps keep compiler noise down so its easier to notice ""real"" warnings."
"update shapeHSM wrappers to latest external version The HSM shape code has undergone many improvements and bug fixes as part of being included in the GalSim package, and we've recently included those in the HSC fork of meas_extensions_shapeHSM (HSC-129, HSC-1093).  We should transfer those changes to the LSST side before tackling DM-981 (or at least before finishing it).  The story point estimate here is for the work already done on the HSC side (with the usual 50% factor for shared work).  The transfer to the LSST side should be essentially no effort.  To the extent that EVM cares about this, the credit should go to [~price], even though I ([~jbosch]) am doing the transfer.",6,DM-1920,datamanagement,update shapehsm wrapper late external version hsm shape code undergo improvement bug fix include galsim package recently include hsc fork meas_extensions_shapehsm hsc-129 hsc-1093 transfer change lsst tackle dm-981 finish story point estimate work hsc usual 50 factor shared work transfer lsst essentially effort extent evm care credit ~price ~jbosch transfer,"update shapeHSM wrappers to latest external version The HSM shape code has undergone many improvements and bug fixes as part of being included in the GalSim package, and we've recently included those in the HSC fork of meas_extensions_shapeHSM (HSC-129, HSC-1093). We should transfer those changes to the LSST side before tackling DM-981 (or at least before finishing it). The story point estimate here is for the work already done on the HSC side (with the usual 50% factor for shared work). The transfer to the LSST side should be essentially no effort. To the extent that EVM cares about this, the credit should go to [~price], even though I ([~jbosch]) am doing the transfer."
"Make unit tests use shared libraries Many (all?) unit tests are currently built as static executables which include all needed object files. This has several issues associated with it: - many files are compiled twice, once as *.os files for shared libraries, second time as *.o file for unit tests - unit tests do not test actual code in the shared libraries but instead separately-built copy of the same code  We should change our procedure and make unit test to link against shared libraries to avoid these problems.",4,DM-1921,datamanagement,unit test use share library unit test currently build static executable include need object file issue associate file compile twice .os file share library second time .o file unit test unit test test actual code share library instead separately build copy code change procedure unit test link share library avoid problem,"Make unit tests use shared libraries Many (all?) unit tests are currently built as static executables which include all needed object files. This has several issues associated with it: - many files are compiled twice, once as *.os files for shared libraries, second time as *.o file for unit tests - unit tests do not test actual code in the shared libraries but instead separately-built copy of the same code We should change our procedure and make unit test to link against shared libraries to avoid these problems."
Base configuration of NFS servers install and configure OS,3,DM-1926,datamanagement,base configuration nfs server install configure os,Base configuration of NFS servers install and configure OS
"HSC backport: convert Peak to PeakRecord This issue covers transferring all changesets from [HSC-1074|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1074] and its subtasks, as well as:  - An RFC to propose the API change, and any requested modifications generated by the RFC.  - Additional fixes to downstream code that's broken by this change (HSC-side changesets should be present for most of downstream fixes, but perhaps not all).",8,DM-1943,datamanagement,hsc backport convert peak peakrecord issue cover transfer changeset hsc-1074|https://hsc jira.astro.princeton.edu jira browse hsc-1074 subtask rfc propose api change request modification generate rfc additional fix downstream code break change hsc changeset present downstream fix,"HSC backport: convert Peak to PeakRecord This issue covers transferring all changesets from [HSC-1074|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1074] and its subtasks, as well as: - An RFC to propose the API change, and any requested modifications generated by the RFC. - Additional fixes to downstream code that's broken by this change (HSC-side changesets should be present for most of downstream fixes, but perhaps not all)."
HSC backport: guarantee consistent handling of peaks in deblender This issue covers transferring changesets from:  - [HSC-134|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-134]  - [HSC-1109|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1109]  - [HSC-1083|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1083]  ,4,DM-1944,datamanagement,hsc backport guarantee consistent handling peak deblender issue cover transfer changeset hsc-134|https://hsc jira.astro.princeton.edu jira browse hsc-134 hsc-1109|https://hsc jira.astro.princeton.edu jira browse hsc-1109 hsc-1083|https://hsc jira.astro.princeton.edu jira browse hsc-1083,HSC backport: guarantee consistent handling of peaks in deblender This issue covers transferring changesets from: - [HSC-134|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-134] - [HSC-1109|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1109] - [HSC-1083|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1083]
"HSC backport: multiband processing for coadds This issue includes transferring changesets from many HSC issues:  - HSC-1060  - HSC-1064  - HSC-1065  - HSC-1061  Most of this is in multiBand.py in pipe_tasks, but there are scattered changes elsewhere (including updates to camera mappers to include the new datasets, for which we'll need to modify more than just obs_subaru).  However, before we make these changes, we'll need to open an RFC to gather comments on the design of this task.  We should qualify there that this is not a long-term plan for consistent multiband processing (which we'll be starting to design on DM-1908), but a step towards better processing in the interim.  Note: while I've assigned this to [~lauren], as I think it will be very helpful for her to get familiar with this code by doing the transfers, the RFC will have to involve a collaboration with [~jbosch], [~price], and Bob Armstrong, as we can't expect someone who wasn't involved in the design to be able to write a document justifying it.",8,DM-1945,datamanagement,hsc backport multiband process coadd issue include transfer changeset hsc issue hsc-1060 hsc-1064 hsc-1065 hsc-1061 multiband.py pipe_task scatter change include update camera mapper include new dataset need modify obs_subaru change need open rfc gather comment design task qualify long term plan consistent multiband processing start design dm-1908 step well processing interim note assign ~lauren think helpful familiar code transfer rfc involve collaboration ~jbosch ~price bob armstrong expect involve design able write document justify,"HSC backport: multiband processing for coadds This issue includes transferring changesets from many HSC issues: - HSC-1060 - HSC-1064 - HSC-1065 - HSC-1061 Most of this is in multiBand.py in pipe_tasks, but there are scattered changes elsewhere (including updates to camera mappers to include the new datasets, for which we'll need to modify more than just obs_subaru). However, before we make these changes, we'll need to open an RFC to gather comments on the design of this task. We should qualify there that this is not a long-term plan for consistent multiband processing (which we'll be starting to design on DM-1908), but a step towards better processing in the interim. Note: while I've assigned this to [~lauren], as I think it will be very helpful for her to get familiar with this code by doing the transfers, the RFC will have to involve a collaboration with [~jbosch], [~price], and Bob Armstrong, as we can't expect someone who wasn't involved in the design to be able to write a document justifying it."
"HSC backport: low-level Footprint merge code This is a transfer of changesets from the follow epics:  - [HSC-1020|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1020]  - [HSC-1075|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1075]: only the afw changes  Because this is purely an addition to the interface, and we're planning to redesign that interface in DM-1904, I don't think we need an RFC here.",4,DM-1946,datamanagement,hsc backport low level footprint merge code transfer changeset follow epic hsc-1020|https://hsc jira.astro.princeton.edu jira browse hsc-1020 hsc-1075|https://hsc jira.astro.princeton.edu jira browse hsc-1075 afw change purely addition interface plan redesign interface dm-1904 think need rfc,"HSC backport: low-level Footprint merge code This is a transfer of changesets from the follow epics: - [HSC-1020|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1020] - [HSC-1075|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1075]: only the afw changes Because this is purely an addition to the interface, and we're planning to redesign that interface in DM-1904, I don't think we need an RFC here."
Add abstraction in czar for unit tests Add hooks in czar that will let us build unit tests.,8,DM-1950,datamanagement,add abstraction czar unit test add hook czar let build unit test,Add abstraction in czar for unit tests Add hooks in czar that will let us build unit tests.
"Change log priority for message ""Unknown column 'whatever' in 'field list'""   Next message should be logged with ERROR priority:  {code} 0204 15:08:03.748 [0x7f1f4b4f4700] INFO  Foreman (build/wdb/QueryAction.cc:250) - [1054] Unknown column 'whatever' in 'field list'   {code}",1,DM-1952,datamanagement,change log priority message unknown column field list message log error priority code 0204 15:08:03.748 0x7f1f4b4f4700 info foreman build wdb queryaction.cc:250 1054 unknown column field list code,"Change log priority for message ""Unknown column 'whatever' in 'field list'"" Next message should be logged with ERROR priority: {code} 0204 15:08:03.748 [0x7f1f4b4f4700] INFO Foreman (build/wdb/QueryAction.cc:250) - [1054] Unknown column 'whatever' in 'field list' {code}"
"Post meas_base move changes to Kron These are to note leftovers from DM-982.  They could be done in a single issue. 1.  I commented code out referring to correctfluxes, but it will need to be restored once it is available in the new framework.  2.  Jim asked me to replace the computeSincFlux which is currently in PsfImage.cc in meas_algorithms with a similar call in meas_base/ApertureFlux.cc.  I did not do this because it became rather complicated, and can just as easily be done when the meas_algorithms routine is moved or removed.  Basically, the templating in ApertureFlux is on Pixel type, whereas in meas_algorithms it is on ImageT (where ImageT is not necessarily a single class hierarchy -- e.g., Image and MaskedImage).  So I left this for now.",1,DM-1953,datamanagement,post meas_base change kron note leftover dm-982 single issue comment code refer correctfluxe need restore available new framework jim ask replace computesincflux currently psfimage.cc meas_algorithm similar meas_base apertureflux.cc complicated easily meas_algorithm routine move remove basically templating apertureflux pixel type meas_algorithm imaget imaget necessarily single class hierarchy e.g. image maskedimage leave,"Post meas_base move changes to Kron These are to note leftovers from DM-982. They could be done in a single issue. 1. I commented code out referring to correctfluxes, but it will need to be restored once it is available in the new framework. 2. Jim asked me to replace the computeSincFlux which is currently in PsfImage.cc in meas_algorithms with a similar call in meas_base/ApertureFlux.cc. I did not do this because it became rather complicated, and can just as easily be done when the meas_algorithms routine is moved or removed. Basically, the templating in ApertureFlux is on Pixel type, whereas in meas_algorithms it is on ImageT (where ImageT is not necessarily a single class hierarchy -- e.g., Image and MaskedImage). So I left this for now."
Run large scale tests Coordinate running large scale tests,6,DM-1962,datamanagement,run large scale test coordinate run large scale test,Run large scale tests Coordinate running large scale tests
"Parallelization requirements for PSF estimation We need to gather algorithmic ideas for how full focal plane PSF estimation will work from a parallelization and data flow standpoint, and discuss with the middleware team how these should be handled from an interface standpoint.  Questions include:  - Will we need to do significant cross-CCD image processing or require significant memory for these tasks?  If so, should we structure this via message passing between CCD-level processes, or scatter-gather?  - Assuming a scatter-gather approach, will we need multiple scatter/gather iterations when processing a single visit?  - How much data will be passed between threads/processes?  Would this include complex serializable objects, or just POD arrays?  - Will different PSF estimation plugins will have different parallelization requirements?  Or, can we define the plugin interfaces at a low-enough level that parallelization can be handled by the framework?  If plugins do need to control their own parallelization, how do we make parallelization interfaces accessible to the plugins?",8,DM-1964,datamanagement,parallelization requirement psf estimation need gather algorithmic idea focal plane psf estimation work parallelization datum flow standpoint discuss middleware team handle interface standpoint question include need significant cross ccd image processing require significant memory task structure message pass ccd level process scatter gather assume scatter gather approach need multiple scatter gather iteration process single visit datum pass thread process include complex serializable object pod array different psf estimation plugin different parallelization requirement define plugin interface low level parallelization handle framework plugin need control parallelization parallelization interface accessible plugin,"Parallelization requirements for PSF estimation We need to gather algorithmic ideas for how full focal plane PSF estimation will work from a parallelization and data flow standpoint, and discuss with the middleware team how these should be handled from an interface standpoint. Questions include: - Will we need to do significant cross-CCD image processing or require significant memory for these tasks? If so, should we structure this via message passing between CCD-level processes, or scatter-gather? - Assuming a scatter-gather approach, will we need multiple scatter/gather iterations when processing a single visit? - How much data will be passed between threads/processes? Would this include complex serializable objects, or just POD arrays? - Will different PSF estimation plugins will have different parallelization requirements? Or, can we define the plugin interfaces at a low-enough level that parallelization can be handled by the framework? If plugins do need to control their own parallelization, how do we make parallelization interfaces accessible to the plugins?"
"Python interface for full-visit PSF estimation Create a Python interface for a pluggable PSF estimation system that supports algorithms that will operate over full images.  This should include a sketch of how a calling command-line task, and placeholders for parallelization interfaces that may not yet be finalized.  The output of this issue is a completed RFC on the design.",6,DM-1965,datamanagement,python interface visit psf estimation create python interface pluggable psf estimation system support algorithm operate image include sketch call command line task placeholder parallelization interface finalize output issue complete rfc design,"Python interface for full-visit PSF estimation Create a Python interface for a pluggable PSF estimation system that supports algorithms that will operate over full images. This should include a sketch of how a calling command-line task, and placeholders for parallelization interfaces that may not yet be finalized. The output of this issue is a completed RFC on the design."
"Command-line driver and placeholder implementation for PSF estimation Create a command-line task that makes use of the new PSF estimation interface, duplicating as much of ProcessCcdTask's functionality as necessary to provide the inputs to PSF estimation (I expect this new task to ultimately replace ProcessCcdTask).  This may have to include workarounds or temporary implementations for parallelization features that are not yet available.  Create a simple PSF estimation placeholder that simply uses existing single-CCD PSF-determiners.",6,DM-1966,datamanagement,command line driver placeholder implementation psf estimation create command line task make use new psf estimation interface duplicate processccdtask functionality necessary provide input psf estimation expect new task ultimately replace processccdtask include workaround temporary implementation parallelization feature available create simple psf estimation placeholder simply use exist single ccd psf determiner,"Command-line driver and placeholder implementation for PSF estimation Create a command-line task that makes use of the new PSF estimation interface, duplicating as much of ProcessCcdTask's functionality as necessary to provide the inputs to PSF estimation (I expect this new task to ultimately replace ProcessCcdTask). This may have to include workarounds or temporary implementations for parallelization features that are not yet available. Create a simple PSF estimation placeholder that simply uses existing single-CCD PSF-determiners."
Create a kind of Wcs that encapsulates a TAN WCS and a distortion model We can simplify the astrometry solver if we have a Wcs that encapsulates a pure tangent-plane WCS and a distortion model that takes converts between PIXELS and TAN_PIXELS. This is useful because at the early stages of processing raw data we have a TAN WCS from the telescope control system and a pretty good estimate of distortion from the optical model (represented in the camera geometry).  The result will be a Wcs whose sky<->pixel transformation is a reasonable approximation of reality (and a much better approximation than the tangent-plane WCS that is currently available. This will potentially eliminate a significant amount of confusing code that attempts to correct for distortion by manually applying the optical distortion model.,4,DM-1969,datamanagement,create kind wcs encapsulate tan wcs distortion model simplify astrometry solver wcs encapsulate pure tangent plane wcs distortion model take convert pixels tan_pixels useful early stage process raw datum tan wcs telescope control system pretty good estimate distortion optical model represent camera geometry result wcs sky<->pixel transformation reasonable approximation reality well approximation tangent plane wcs currently available potentially eliminate significant confusing code attempt correct distortion manually apply optical distortion model,Create a kind of Wcs that encapsulates a TAN WCS and a distortion model We can simplify the astrometry solver if we have a Wcs that encapsulates a pure tangent-plane WCS and a distortion model that takes converts between PIXELS and TAN_PIXELS. This is useful because at the early stages of processing raw data we have a TAN WCS from the telescope control system and a pretty good estimate of distortion from the optical model (represented in the camera geometry). The result will be a Wcs whose sky<->pixel transformation is a reasonable approximation of reality (and a much better approximation than the tangent-plane WCS that is currently available. This will potentially eliminate a significant amount of confusing code that attempts to correct for distortion by manually applying the optical distortion model.
Build 2015_02 Qserv release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,1,DM-1973,datamanagement,build 2015_02 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build 2015_02 Qserv release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
"Fix enclose, escape, and line termination characters in qserv-data-loader Add this string to mysql loader 'LOAD DATA INFILE' command:   {code}  q += ""ENCLOSED BY '%s' ESCAPED BY '%s' LINES TERMINATED BY '%s'"" % (enclose, escape, newline) {code} and add params in cfg file.",2,DM-1974,datamanagement,fix enclose escape line termination character qserv data loader add string mysql loader load data infile command code enclosed escaped lines terminate enclose escape newline code add param cfg file,"Fix enclose, escape, and line termination characters in qserv-data-loader Add this string to mysql loader 'LOAD DATA INFILE' command: {code} q += ""ENCLOSED BY '%s' ESCAPED BY '%s' LINES TERMINATED BY '%s'"" % (enclose, escape, newline) {code} and add params in cfg file."
"Research and Document API Versioning Research and document versioning of the RESTful API (through flask blueprints). In particular, need to understand how to avoid code duplication between different versions of API.",4,DM-1978,datamanagement,research document api versioning research document versioning restful api flask blueprints particular need understand avoid code duplication different version api,"Research and Document API Versioning Research and document versioning of the RESTful API (through flask blueprints). In particular, need to understand how to avoid code duplication between different versions of API."
"Add SQLite-based v0.1 unit testing for metaserv Add unit tests for the RESTful flask based API. I think it'd be most useful if we could  a) load some test data into underlying metaserv  b) run programmatically things like ""curl -H accept:text/html http://127.0.0.1:5000/meta/v0/db/L2/DC_W13_Stripe82/tables"" etc (more examples in dax_*serv/README.txt) and verify that we got what we expected. Do it for both json and html.",4,DM-1979,datamanagement,add sqlite base v0.1 unit test metaserv add unit test restful flask base api think useful load test datum underlying metaserv run programmatically thing like curl -h accept text html http://127.0.0.1:5000 meta v0 db l2 dc_w13_stripe82 table etc example dax_*serv readme.txt verify get expect json html,"Add SQLite-based v0.1 unit testing for metaserv Add unit tests for the RESTful flask based API. I think it'd be most useful if we could a) load some test data into underlying metaserv b) run programmatically things like ""curl -H accept:text/html http://127.0.0.1:5000/meta/v0/db/L2/DC_W13_Stripe82/tables"" etc (more examples in dax_*serv/README.txt) and verify that we got what we expected. Do it for both json and html."
Add error handling for webserv Add basic error handling for the RESTful flask-based API,6,DM-1980,datamanagement,add error handling webserv add basic error handling restful flask base api,Add error handling for webserv Add basic error handling for the RESTful flask-based API
"Improve security for mysql in python Revisit all code that talks to mysql from python to use parameter bindings instead of direct string substitutions. In practice: {code} conn.execute(""SELECT * FROM t WHERE name=%s"" % theName) {code} should be replaced with {code} conn.execute(""SELECT * FROM t WHERE name=%s"", (theName,)) {code}  For details, see http://mysql-python.sourceforge.net/MySQLdb.html#some-examples ",5,DM-1981,datamanagement,"improve security mysql python revisit code talk mysql python use parameter binding instead direct string substitution practice code conn.execute(""select name=%s thename code replace code conn.execute(""select name=%s thename code detail http://mysql-python.sourceforge.net/mysqldb.html#some-example","Improve security for mysql in python Revisit all code that talks to mysql from python to use parameter bindings instead of direct string substitutions. In practice: {code} conn.execute(""SELECT * FROM t WHERE name=%s"" % theName) {code} should be replaced with {code} conn.execute(""SELECT * FROM t WHERE name=%s"", (theName,)) {code} For details, see http://mysql-python.sourceforge.net/MySQLdb.html#some-examples"
Fix JDBC timestamp error JDBC driver returns an error on next query:  {code:sql} sql> select * from Science_Ccd_Exposure [2015-02-06 13:39:37] 1 row(s) retrieved starting from 0 in 927/970 ms [2015-02-06 13:39:37] [S1009] Cannot convert value '0000-00-00 00:00:00' from column 32 to TIMESTAMP. [2015-02-06 13:39:37] [S1009] Value '[B@548997d1' can not be represented as java.sql.Timestamp {code},1,DM-1982,datamanagement,fix jdbc timestamp error jdbc driver return error query code sql sql select science_ccd_exposure 2015 02 06 13:39:37 row(s retrieve start 927/970 ms 2015 02 06 13:39:37 s1009 convert value 0000 00 00 00:00:00 column 32 timestamp 2015 02 06 13:39:37 s1009 value b@548997d1 represent java.sql timestamp code,Fix JDBC timestamp error JDBC driver returns an error on next query: {code:sql} sql> select * from Science_Ccd_Exposure [2015-02-06 13:39:37] 1 row(s) retrieved starting from 0 in 927/970 ms [2015-02-06 13:39:37] [S1009] Cannot convert value '0000-00-00 00:00:00' from column 32 to TIMESTAMP. [2015-02-06 13:39:37] [S1009] Value '[B@548997d1' can not be represented as java.sql.Timestamp {code}
"Story point display and roll-up in epic display I understand that there is a pending request to display the story points for individual story issues in the mini-table in which they are displayed for an epic.  It would also be useful to see a rolled-up total of the story points for the defined set of stories - so that, among other things, this could be compared to the story point value for the epic.  Ideally the story points for the roll-up might be displayed as ""nn (mm)"" where nn is the total points and mm is the number of points remaining to do (or done already - I don't care which as long as the definition is clear).",1,DM-1994,datamanagement,story point display roll epic display understand pende request display story point individual story issue mini table display epic useful roll total story point define set story thing compare story point value epic ideally story point roll display nn mm nn total point mm number point remain care long definition clear,"Story point display and roll-up in epic display I understand that there is a pending request to display the story points for individual story issues in the mini-table in which they are displayed for an epic. It would also be useful to see a rolled-up total of the story points for the defined set of stories - so that, among other things, this could be compared to the story point value for the epic. Ideally the story points for the roll-up might be displayed as ""nn (mm)"" where nn is the total points and mm is the number of points remaining to do (or done already - I don't care which as long as the definition is clear)."
"Define data distribution/replication testing strategy Once we decide on a design for data distribution / replication, we should come up with a test plan.",4,DM-2002,datamanagement,define data distribution replication testing strategy decide design datum distribution replication come test plan,"Define data distribution/replication testing strategy Once we decide on a design for data distribution / replication, we should come up with a test plan."
switch ndarray to external package There is already an external ndarray project on GitHub (we've been using a fork of that).  We should merge the forks and switch to using the external package. ,2,DM-2005,datamanagement,switch ndarray external package external ndarray project github fork merge fork switch external package,switch ndarray to external package There is already an external ndarray project on GitHub (we've been using a fork of that). We should merge the forks and switch to using the external package.
"merge ""basics"" packages Create detailed RFC and implement merge of base, utils, and daf_base.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",2,DM-2006,datamanagement,merge basic package create detailed rfc implement merge base util daf_base https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,"merge ""basics"" packages Create detailed RFC and implement merge of base, utils, and daf_base. See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
"separate pex_exceptions from base and rename Remove dependency on base from pex_exceptions and rename to just ""exceptions"" (after RFC).  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1,DM-2007,datamanagement,separate pex_exception base rename remove dependency base pex_exception rename exception rfc https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,"separate pex_exceptions from base and rename Remove dependency on base from pex_exceptions and rename to just ""exceptions"" (after RFC). See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
Move Wcs from afw::image to afw::coord Create RFC and implement move of Wcs from afw::image to afw::coord.  See https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning,1,DM-2008,datamanagement,wcs afw::image afw::coord create rfc implement wcs afw::image afw::coord https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,Move Wcs from afw::image to afw::coord Create RFC and implement move of Wcs from afw::image to afw::coord. See https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning
"move Jarvis/shapelet code to legacy package Create RFC and remove Jarvis/shapelet package from meas_algorithms, into new legacy sci package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1,DM-2010,datamanagement,jarvis shapelet code legacy package create rfc remove jarvis shapelet package meas_algorithm new legacy sci package https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,"move Jarvis/shapelet code to legacy package Create RFC and remove Jarvis/shapelet package from meas_algorithms, into new legacy sci package. See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
"move Psf, Kernel code to new afw::convolution subpackage Create detailed RFC and implement move for these packages.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",2,DM-2011,datamanagement,psf kernel code new afw::convolution subpackage create detailed rfc implement package https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,"move Psf, Kernel code to new afw::convolution subpackage Create detailed RFC and implement move for these packages. See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
"split pipe_base into command-line and non-command-line components Create detailed RFC and implement package split, to separate basic Tasks (to be used as e.g. subtasks) from CmdLineTask and ArgumentParser.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1,DM-2012,datamanagement,split pipe_base command line non command line component create detailed rfc implement package split separate basic task e.g. subtask cmdlinetask argumentparser https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,"split pipe_base into command-line and non-command-line components Create detailed RFC and implement package split, to separate basic Tasks (to be used as e.g. subtasks) from CmdLineTask and ArgumentParser. See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
"Implement image stitching This story involves implementing code that stitches images, simple case that does not involve tract boundaries. More advanced case in covered in separate ticket. We will need to determine WCS information for the target images.",6,DM-2013,datamanagement,implement image stitch story involve implement code stitch image simple case involve tract boundary advanced case cover separate ticket need determine wcs information target image,"Implement image stitching This story involves implementing code that stitches images, simple case that does not involve tract boundaries. More advanced case in covered in separate ticket. We will need to determine WCS information for the target images."
Create interface and utility package for single-frame/forced processing Create detailed RFC and implement move of interface and utility code from multiple existing packages to new package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning,2,DM-2014,datamanagement,create interface utility package single frame force processing create detailed rfc implement interface utility code multiple exist package new package https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,Create interface and utility package for single-frame/forced processing Create detailed RFC and implement move of interface and utility code from multiple existing packages to new package. See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning
Split PSF estimation and PSF model code into separate package Create detailed RFC and implement move of concrete PSF estimation code and Psf subclasses from meas_algorithms to separate package.  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning,1,DM-2016,datamanagement,split psf estimation psf model code separate package create detailed rfc implement concrete psf estimation code psf subclass meas_algorithm separate package https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,Split PSF estimation and PSF model code into separate package Create detailed RFC and implement move of concrete PSF estimation code and Psf subclasses from meas_algorithms to separate package. See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning
"rename packages with minimal reorganization meas_deblender, ip_isr, meas_astrom, meas_modelfit, and ip_diffim do not require major refactoring to fit into the new package reorganization, but they should be renamed (with sci_prefixes).  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",1,DM-2017,datamanagement,rename package minimal reorganization meas_deblender ip_isr meas_astrom meas_modelfit ip_diffim require major refactoring fit new package reorganization rename sci_prefixes https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,"rename packages with minimal reorganization meas_deblender, ip_isr, meas_astrom, meas_modelfit, and ip_diffim do not require major refactoring to fit into the new package reorganization, but they should be renamed (with sci_prefixes). See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
"Split measurement plugins into separate packages Create detailed RFCs and implement splitting measurement plugins into separate package.  May want one package for extremely basic plugins, always-used plugins (PixelFlags, TransformedCentroid, etc).  See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning",2,DM-2018,datamanagement,split measurement plugin separate package create detailed rfc implement splitting measurement plugin separate package want package extremely basic plugin plugin pixelflags transformedcentroid etc https://confluence.lsstcorp.org/display/dm/summer2015+package+reorganization+planning,"Split measurement plugins into separate packages Create detailed RFCs and implement splitting measurement plugins into separate package. May want one package for extremely basic plugins, always-used plugins (PixelFlags, TransformedCentroid, etc). See also https://confluence.lsstcorp.org/display/DM/Summer2015+Package+Reorganization+Planning"
Split coaddition code and single-frame/forced command-line drivers This should split all content in pipe_tasks into two packages (aside from what may have been removed in previous issues).,2,DM-2019,datamanagement,split coaddition code single frame force command line driver split content pipe_task package aside remove previous issue,Split coaddition code and single-frame/forced command-line drivers This should split all content in pipe_tasks into two packages (aside from what may have been removed in previous issues).
"Architecture for supporting small non-partitioned tables Some tables, like Exposure, provenance, will not be partitioned, and the current plan is to either replicate them on each node, or store on shared file system. Need to decide how it will be dealt with.",6,DM-2021,datamanagement,architecture support small non partitioned table table like exposure provenance partition current plan replicate node store share file system need decide deal,"Architecture for supporting small non-partitioned tables Some tables, like Exposure, provenance, will not be partitioned, and the current plan is to either replicate them on each node, or store on shared file system. Need to decide how it will be dealt with."
Investigate procedures for package reorganization e.g. develop script to handle bulk namespace changes.,4,DM-2023,datamanagement,investigate procedure package reorganization e.g. develop script handle bulk namespace change,Investigate procedures for package reorganization e.g. develop script to handle bulk namespace changes.
"Add image-query related KPIs to the plan Existing plan in LDM-240 does not mention image related KPIs. Need to come up with a road map, and propose KPIs. This should be synchronized what realistically NCSA cluster can deliver in any given FY.",5,DM-2031,datamanagement,add image query relate kpi plan exist plan ldm-240 mention image relate kpi need come road map propose kpi synchronize realistically ncsa cluster deliver give fy,"Add image-query related KPIs to the plan Existing plan in LDM-240 does not mention image related KPIs. Need to come up with a road map, and propose KPIs. This should be synchronized what realistically NCSA cluster can deliver in any given FY."
server side preparation for  histogram plot (1) Convert necessary code to make it possible for a JavaScript component to place a JSON request to the server and to parse the resulting RawDataSet.  ,6,DM-2045,datamanagement,server preparation histogram plot convert necessary code possible javascript component place json request server parse result rawdataset,server side preparation for histogram plot (1) Convert necessary code to make it possible for a JavaScript component to place a JSON request to the server and to parse the resulting RawDataSet.
"Attend Scale 13x conference Attend database talks, in particular the MaxScale proxy talk (http://www.socallinuxexpo.org/scale/13x/presentations/advanced-query-routing-and-proxying-maxscale?utm_campaign=north-american-trade-shows&utm_source=hs_email&utm_medium=email&utm_content=16099082&_hsenc=p2ANqtz-_MFjfxvpCdmV_Ax2RKDdOGypHPQ85UL-UMuy0eRs_MrlJ2qJVp-MXx-g7_-dAQsq0trpA61hkZrzO-3gp6bKVkpK52fQ&_hsmi=16099082).  If anyone has questions they would like me to ask, please post them here as well.  I will post notes to this issue. ",2,DM-2057,datamanagement,attend scale 13x conference attend database talk particular maxscale proxy talk http://www.socallinuxexpo.org/scale/13x/presentations/advanced-query-routing-and-proxying-maxscale?utm_campaign=north-american-trade-shows&utm_source=hs_email&utm_medium=email&utm_content=16099082&_hsenc=p2anqtz-_mfjfxvpcdmv_ax2rkddogyphpq85ul-umuy0ers_mrlj2qjvp-mxx-g7_-daqsq0trpa61hkzrzo-3gp6bkvkpk52fq&_hsmi=16099082 question like ask post post note issue,"Attend Scale 13x conference Attend database talks, in particular the MaxScale proxy talk (http://www.socallinuxexpo.org/scale/13x/presentations/advanced-query-routing-and-proxying-maxscale?utm_campaign=north-american-trade-shows&utm_source=hs_email&utm_medium=email&utm_content=16099082&_hsenc=p2ANqtz-_MFjfxvpCdmV_Ax2RKDdOGypHPQ85UL-UMuy0eRs_MrlJ2qJVp-MXx-g7_-dAQsq0trpA61hkZrzO-3gp6bKVkpK52fQ&_hsmi=16099082). If anyone has questions they would like me to ask, please post them here as well. I will post notes to this issue."
"Data loader should always create overlap tables  We have discovered that some overlap tables that are supposed to exist were not actually created. It looks like partitioner is not creating overlap files when there is no overlap data and loader is not creating overlap table if there is no input file. Situation is actually symmetric, there could be non-empty overlap table but empty/missing chunk table. When we create one table we should always make another as well. ",2,DM-2058,datamanagement,datum loader create overlap table discover overlap table suppose exist actually create look like partitioner create overlap file overlap datum loader create overlap table input file situation actually symmetric non overlap table miss chunk table create table,"Data loader should always create overlap tables We have discovered that some overlap tables that are supposed to exist were not actually created. It looks like partitioner is not creating overlap files when there is no overlap data and loader is not creating overlap table if there is no input file. Situation is actually symmetric, there could be non-empty overlap table but empty/missing chunk table. When we create one table we should always make another as well."
"Clean up QuerySession-related code in czar (created in response to DM-211) This ticket should address the following inelegancies in the qserv-czar.   * QuerySession->QueryPipeline. The ""session"" abstraction has moved to a better place. The iterator portion should be shifted into its own separate class, though perhaps still associated with QueryPipeline. The iterator portion's new home should be amenable to eventually moving the actual query materialization to the worker, though we shouldn't introduce new abstractions until we are actually ready to move the substitution/materialization to the worker.  * QueryContext needs to be split into incoming external QueryContext and a sort of QueryClipboard for passing information between analysis/manipulation plugins. Eventually, I imagine a chain/tree of them attached to the select statements themselves in order to represent subquery scope nesting (which is complicated to represent and to reason about--nesting and the resulting namespace resolution is tricky), but I don't think we should try doing the chaining in the first phase. For this ticket, create QueryClipboard to hold the portion for interchange between analysis plugins. Query analysis plugins would then pass this object (which points at an immutable? QueryContext) between themselves. QueryClipboard probably should live in qana, QueryContext in query (unless there is a good reason to move it).   ",8,DM-2059,datamanagement,clean querysession relate code czar create response dm-211 ticket address follow inelegancie qserv czar querysession->querypipeline session abstraction move well place iterator portion shift separate class associate querypipeline iterator portion new home amenable eventually move actual query materialization worker introduce new abstraction actually ready substitution materialization worker querycontext need split incoming external querycontext sort queryclipboard pass information analysis manipulation plugin eventually imagine chain tree attach select statement order represent subquery scope nesting complicated represent reason nest result namespace resolution tricky think try chaining phase ticket create queryclipboard hold portion interchange analysis plugin query analysis plugin pass object point immutable querycontext queryclipboard probably live qana querycontext query good reason,"Clean up QuerySession-related code in czar (created in response to DM-211) This ticket should address the following inelegancies in the qserv-czar. * QuerySession->QueryPipeline. The ""session"" abstraction has moved to a better place. The iterator portion should be shifted into its own separate class, though perhaps still associated with QueryPipeline. The iterator portion's new home should be amenable to eventually moving the actual query materialization to the worker, though we shouldn't introduce new abstractions until we are actually ready to move the substitution/materialization to the worker. * QueryContext needs to be split into incoming external QueryContext and a sort of QueryClipboard for passing information between analysis/manipulation plugins. Eventually, I imagine a chain/tree of them attached to the select statements themselves in order to represent subquery scope nesting (which is complicated to represent and to reason about--nesting and the resulting namespace resolution is tricky), but I don't think we should try doing the chaining in the first phase. For this ticket, create QueryClipboard to hold the portion for interchange between analysis plugins. Query analysis plugins would then pass this object (which points at an immutable? QueryContext) between themselves. QueryClipboard probably should live in qana, QueryContext in query (unless there is a good reason to move it)."
Rename TaskMsgFactory2 Rename TaskMsgFactory2 to TaskMsgFactory.    Please see DM-211 for more information.,1,DM-2060,datamanagement,rename taskmsgfactory2 rename taskmsgfactory2 taskmsgfactory dm-211 information,Rename TaskMsgFactory2 Rename TaskMsgFactory2 to TaskMsgFactory. Please see DM-211 for more information.
"Creates overlap tables even if empty while loading data Query execution expects all chunk and overlap tables to exist, even if they are empty. In short term, that means loader should: * look at all chunks and add corresponding overlap chunks, * look at overlap chunks and add missing empty chunk table ",2,DM-2063,datamanagement,create overlap table load data query execution expect chunk overlap table exist short term mean loader look chunk add corresponding overlap chunk look overlap chunk add miss chunk table,"Creates overlap tables even if empty while loading data Query execution expects all chunk and overlap tables to exist, even if they are empty. In short term, that means loader should: * look at all chunks and add corresponding overlap chunks, * look at overlap chunks and add missing empty chunk table"
Add test case to catch missing empty chunks or overlaps Discussed at db hangout 2015-02-18.   We need a use case to test for missing empty overlap chunk tables and/or empty chunk tables.,2,DM-2066,datamanagement,add test case catch miss chunk overlap discuss db hangout 2015 02 18 need use case test miss overlap chunk table and/or chunk table,Add test case to catch missing empty chunks or overlaps Discussed at db hangout 2015-02-18. We need a use case to test for missing empty overlap chunk tables and/or empty chunk tables.
"Port metaREST.py to db metaREST_v0.py in metaserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it. ",1,DM-2094,datamanagement,port metarest.py db metarest_v0.py metaserv currently mysqldb instead go db api need use parameter bind security reason switch db db interface support,"Port metaREST.py to db metaREST_v0.py in metaserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it."
"Port dbREST.py to db dbREST_v0.py in dbserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it. ",1,DM-2095,datamanagement,port dbrest.py db dbrest_v0.py dbserv currently mysqldb instead go db api need use parameter bind security reason switch db db interface support,"Port dbREST.py to db dbREST_v0.py in dbserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it."
"Package andyH xssi fixed version (>2MB answer pb) in eups See DM-1847 - Andy made a patch, it'd be good to the xrootd we use for our stack.",1,DM-2097,datamanagement,package andyh xssi fix version mb answer pb eup dm-1847 andy patch good xrootd use stack,"Package andyH xssi fixed version (>2MB answer pb) in eups See DM-1847 - Andy made a patch, it'd be good to the xrootd we use for our stack."
"Resolve compiler warnings in new measurement framework When building {{meas_base}}, or any other measurement plugins which follow the same interface, with clang, I see a bunch of warnings along the lines of:  {code} In file included from src/ApertureFlux.cc:34: include/lsst/meas/base/ApertureFlux.h:197:18: warning: 'lsst::meas::base::ApertureFluxAlgorithm::measure' hides overloaded virtual function       [-Woverloaded-virtual]     virtual void measure(                  ^ include/lsst/meas/base/Algorithm.h:183:18: note: hidden overloaded virtual function 'lsst::meas::base::SimpleAlgorithm::measure' declared here:       different number of parameters (4 vs 2)     virtual void measure( {code}  This is an artefact of a [workaround for SWIG issues|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284390]; the warnings aren't indicative of a fundamental problem, but if we can avoid them we should.  While we're at it, we should also fix:  {code} include/lsst/meas/base/ApertureFlux.h:233:1: warning: 'ApertureFluxResult' defined as a struct here but previously declared as a class       [-Wmismatched-tags] struct ApertureFluxResult : public FluxResult { ^ include/lsst/meas/base/ApertureFlux.h:65:1: note: did you mean struct here? class ApertureFluxResult; ^~~~~ struct {code}",1,DM-2131,datamanagement,resolve compiler warning new measurement framework build meas_base measurement plugin follow interface clang bunch warning line code file include src apertureflux.cc:34 include lsst meas base apertureflux.h:197:18 warning lsst::meas::base::aperturefluxalgorithm::measure hide overload virtual function -woverloade virtual virtual void measure include lsst meas base algorithm.h:183:18 note hide overload virtual function lsst::meas::base::simplealgorithm::measure declare different number parameter vs virtual void measure code artefact workaround swig issues|https://confluence.lsstcorp.org page viewpage.action?pageid=20284390 warning indicative fundamental problem avoid fix code include lsst meas base apertureflux.h:233:1 warning aperturefluxresult define struct previously declare class -wmismatched tag struct aperturefluxresult public fluxresult include lsst meas base apertureflux.h:65:1 note mean struct class aperturefluxresult ^~~~~ struct code,"Resolve compiler warnings in new measurement framework When building {{meas_base}}, or any other measurement plugins which follow the same interface, with clang, I see a bunch of warnings along the lines of: {code} In file included from src/ApertureFlux.cc:34: include/lsst/meas/base/ApertureFlux.h:197:18: warning: 'lsst::meas::base::ApertureFluxAlgorithm::measure' hides overloaded virtual function [-Woverloaded-virtual] virtual void measure( ^ include/lsst/meas/base/Algorithm.h:183:18: note: hidden overloaded virtual function 'lsst::meas::base::SimpleAlgorithm::measure' declared here: different number of parameters (4 vs 2) virtual void measure( {code} This is an artefact of a [workaround for SWIG issues|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284390]; the warnings aren't indicative of a fundamental problem, but if we can avoid them we should. While we're at it, we should also fix: {code} include/lsst/meas/base/ApertureFlux.h:233:1: warning: 'ApertureFluxResult' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct ApertureFluxResult : public FluxResult { ^ include/lsst/meas/base/ApertureFlux.h:65:1: note: did you mean struct here? class ApertureFluxResult; ^~~~~ struct {code}"
Validate user query in dbREST Need to validate query (from security standpoint that user enters through rest api.,1,DM-2138,datamanagement,validate user query dbrest need validate query security standpoint user enter rest api,Validate user query in dbREST Need to validate query (from security standpoint that user enters through rest api.
"Support DDL in MetaServ - implementation DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves building tools that will load the DDL schema into MetaServ. Design aspects are covered in DM-1770.",8,DM-2139,datamanagement,support ddl metaserv implementation ddl information embed comment master version schema cat repo currently schema browser story involve building tool load ddl schema metaserv design aspect cover dm-1770,"Support DDL in MetaServ - implementation DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves building tools that will load the DDL schema into MetaServ. Design aspects are covered in DM-1770."
"Add meas_extensions_shapeHSM to lsstsw, lsst_distrib meas_extensions_shapeHSM has just been resurrected from bitrot, and should be included in our distribution.    Contrary to DM-2140, it should probably not be included in lsst_apps, as it's not clear we want to add a dependency on tmv and GalSim there.",1,DM-2141,datamanagement,add lsstsw lsst_distrib meas_extensions_shapehsm resurrect bitrot include distribution contrary dm-2140 probably include lsst_apps clear want add dependency tmv galsim,"Add meas_extensions_shapeHSM to lsstsw, lsst_distrib meas_extensions_shapeHSM has just been resurrected from bitrot, and should be included in our distribution. Contrary to DM-2140, it should probably not be included in lsst_apps, as it's not clear we want to add a dependency on tmv and GalSim there."
"Log fails on uniccode string Log is currently failing if we pass unicode string, it is easy to reproduce by doing: log.info(u""hello""). It fails with:  {code}   File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/log.py"", line 103, in info     log("""", INFO, fmt, *args, depth=2)   File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/log.py"", line 94, in log     _getFuncName(depth), frame.f_lineno, fmt % args)   File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/logLib.py"", line 648, in forcedLog_iface     return _logLib.forcedLog_iface(*args) TypeError: in method 'forcedLog_iface', argument 6 of type 'std::string const &' {code}",1,DM-2155,datamanagement,"log fail uniccode string log currently fail pass unicode string easy reproduce log.info(u""hello fail code file /home becla dataarchdev linux64 log master gfab0203bd3 python lsst log log.py line 103 info log info fmt args depth=2 file /home becla dataarchdev linux64 log master gfab0203bd3 python lsst log log.py line 94 log getfuncname(depth frame.f_lineno fmt args file /home becla dataarchdev linux64 log master gfab0203bd3 python lsst log loglib.py line 648 forcedlog_iface return typeerror method forcedlog_iface argument type std::string const code","Log fails on uniccode string Log is currently failing if we pass unicode string, it is easy to reproduce by doing: log.info(u""hello""). It fails with: {code} File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/log.py"", line 103, in info log("""", INFO, fmt, *args, depth=2) File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/log.py"", line 94, in log _getFuncName(depth), frame.f_lineno, fmt % args) File ""/home/becla/dataArchDev/Linux64/log/master-gfab0203bd3+3/python/lsst/log/logLib.py"", line 648, in forcedLog_iface return _logLib.forcedLog_iface(*args) TypeError: in method 'forcedLog_iface', argument 6 of type 'std::string const &' {code}"
"Data loader crashes on uncompressed data. Vaikunth just mentioned to me that the is a crash in data loader when it tries to load uncompressed data: {noformat} root - CRITICAL - Exception occured: local variable 'outfile' referenced before assignment Traceback (most recent call last): File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 312, in <module> sys.exit(loader.run()) File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 248, in run self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data) File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 168, in load return self._run(d atabase, table, schema, data)   File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 192, in _run     files = self._gunzip(data)   File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 388, in _gunzip     result.append(outfile) UnboundLocalError: local variable 'outfile' referenced before assignment {noformat}  It looks like we never tested loader on uncompressed data and there is a bug in handling uncompressed data. ",1,DM-2157,datamanagement,datum loader crash uncompressed datum vaikunth mention crash datum loader try load uncompressed datum noformat root critical exception occur local variable outfile reference assignment traceback recent file /home vaikunth src qserv bin qserv data loader.py line 312 sys.exit(loader.run file /home vaikunth src qserv bin qserv data loader.py line 248 run self.loader.load(self.args.database self.args.table self.args.schema self.args.data file /home vaikunth src qserv lib python lsst qserv admin dataloader.py line 168 load return self._run(d atabase table schema datum file /home vaikunth src qserv lib python lsst qserv admin dataloader.py line 192 run file self._gunzip(data file /home vaikunth src qserv lib python lsst qserv admin dataloader.py line 388 gunzip result.append(outfile unboundlocalerror local variable outfile reference assignment noformat look like test loader uncompressed datum bug handle uncompressed datum,"Data loader crashes on uncompressed data. Vaikunth just mentioned to me that the is a crash in data loader when it tries to load uncompressed data: {noformat} root - CRITICAL - Exception occured: local variable 'outfile' referenced before assignment Traceback (most recent call last): File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 312, in  sys.exit(loader.run()) File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 248, in run self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data) File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 168, in load return self._run(d atabase, table, schema, data) File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 192, in _run files = self._gunzip(data) File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 388, in _gunzip result.append(outfile) UnboundLocalError: local variable 'outfile' referenced before assignment {noformat} It looks like we never tested loader on uncompressed data and there is a bug in handling uncompressed data."
"Add support for JSON - define structure As discussed at [Data Access Hangout 2015-02-23|https://confluence.lsstcorp.org/display/DM/Data+Access+Hangout+2015-02-23], we should support json format. This includes defining the exact format, and implementing it. This story covers defining the format.",2,DM-2158,datamanagement,add support json define structure discuss data access hangout 2015 02 23|https://confluence.lsstcorp.org display dm data+access+hangout+2015 02 23 support json format include define exact format implement story cover define format,"Add support for JSON - define structure As discussed at [Data Access Hangout 2015-02-23|https://confluence.lsstcorp.org/display/DM/Data+Access+Hangout+2015-02-23], we should support json format. This includes defining the exact format, and implementing it. This story covers defining the format."
"Implement Image Response for ImgServ This story covers implementing proper response, and the header metadata for the fits image response.",3,DM-2159,datamanagement,implement image response imgserv story cover implement proper response header metadata fit image response,"Implement Image Response for ImgServ This story covers implementing proper response, and the header metadata for the fits image response."
"Setup webserv for SUI tests We need to setup a service (eg on lsst-dev) that can be used by the IPAC team to play with our webserv/metaserv/dbserv/imgserv.  The server runs on lsst-dev machine, port 5000. To ssh-tunnel, try: {code} ssh -L 5000:localhost:5000 lsst-dev.ncsa.illinois.edu {code}  An example usage:  {code}   curl 'http://localhost:5000/db/v0/query?sql=SHOW+DATABASES+LIKE+""%Stripe%""'   curl 'http://localhost:5000/db/v0/query?sql=SHOW+TABLES+IN+DC_W13_Stripe82'   curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.DeepForcedSource'   curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.Science_Ccd_Exposure'   curl 'http://localhost:5000/db/v0/query?sql=SELECT+deepForcedSourceId,scienceCcdExposureId+FROM+DC_W13_Stripe82.DeepForcedSource+LIMIT+10'   curl 'http://localhost:5000/db/v0/query?sql=SELECT+ra,decl,filterName+FROM+DC_W13_Stripe82.Science_Ccd_Exposure+WHERE+scienceCcdExposureId=125230127'   curl 'http://localhost:5000/image/v0/raw/cutout?ra=7.90481567257&dec=-0.299951669961&filter=r&width=30.0&height=45.0' {code} ",2,DM-2161,datamanagement,"setup webserv sui test need setup service eg lsst dev ipac team play webserv metaserv dbserv imgserv server run lsst dev machine port 5000 ssh tunnel try code ssh -l 5000 localhost:5000 lsst-dev.ncsa.illinois.edu code example usage code curl http://localhost:5000 db v0 query?sql show+databases+like+""%stripe% curl http://localhost:5000 db v0 query?sql show+tables+in+dc_w13_stripe82 curl http://localhost:5000 db v0 query?sql describe+dc_w13_stripe82.deepforcedsource curl http://localhost:5000 db v0 query?sql describe+dc_w13_stripe82.science_ccd_exposure curl http://localhost:5000 db v0 query?sql select+deepforcedsourceid scienceccdexposureid+from+dc_w13_stripe82.deepforcedsource+limit+10 curl http://localhost:5000 db v0 query?sql select+ra decl filtername+from+dc_w13_stripe82.science_ccd_exposure+where+scienceccdexposureid=125230127 curl http://localhost:5000 image v0 raw cutout?ra=7.90481567257&dec=-0.299951669961&filter r&width=30.0&height=45.0 code","Setup webserv for SUI tests We need to setup a service (eg on lsst-dev) that can be used by the IPAC team to play with our webserv/metaserv/dbserv/imgserv. The server runs on lsst-dev machine, port 5000. To ssh-tunnel, try: {code} ssh -L 5000:localhost:5000 lsst-dev.ncsa.illinois.edu {code} An example usage: {code} curl 'http://localhost:5000/db/v0/query?sql=SHOW+DATABASES+LIKE+""%Stripe%""' curl 'http://localhost:5000/db/v0/query?sql=SHOW+TABLES+IN+DC_W13_Stripe82' curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.DeepForcedSource' curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.Science_Ccd_Exposure' curl 'http://localhost:5000/db/v0/query?sql=SELECT+deepForcedSourceId,scienceCcdExposureId+FROM+DC_W13_Stripe82.DeepForcedSource+LIMIT+10' curl 'http://localhost:5000/db/v0/query?sql=SELECT+ra,decl,filterName+FROM+DC_W13_Stripe82.Science_Ccd_Exposure+WHERE+scienceCcdExposureId=125230127' curl 'http://localhost:5000/image/v0/raw/cutout?ra=7.90481567257&dec=-0.299951669961&filter=r&width=30.0&height=45.0' {code}"
Refactor Geom class in Firefly The Geom class was ported from C code 20 years ago.  It needs to refactor to comply with Java OO design.  ,8,DM-2163,datamanagement,refactor geom class firefly geom class port code 20 year ago need refactor comply java oo design,Refactor Geom class in Firefly The Geom class was ported from C code 20 years ago. It needs to refactor to comply with Java OO design.
"Review at DM leadership team meeting review document  with Kantor, KT, Hobblit, and Lambert,  including prep time ",3,DM-2164,datamanagement,review dm leadership team meeting review document kantor kt hobblit lambert include prep time,"Review at DM leadership team meeting review document with Kantor, KT, Hobblit, and Lambert, including prep time"
Refactor document for that specifications are clearer 1) Have one basic definition of racks and other components in the specifications.  2) Fully write up first full draft  specification for the supporting material  handing area.,3,DM-2165,datamanagement,refactor document specification clear basic definition rack component specification fully write draft specification support material handing area,Refactor document for that specifications are clearer 1) Have one basic definition of racks and other components in the specifications. 2) Fully write up first full draft specification for the supporting material handing area.
"receive / process comments  from Jeff Barr a receive edits from Jeff Barr,  accept the formatting and mechanical l edits. Compose separate email list issues related to non LSST tenants in the  room.  ",1,DM-2166,datamanagement,receive process comment jeff barr receive edit jeff barr accept formatting mechanical edit compose separate email list issue relate non lsst tenant room,"receive / process comments from Jeff Barr a receive edits from Jeff Barr, accept the formatting and mechanical l edits. Compose separate email list issues related to non LSST tenants in the room."
Investigate  Commerical  vendor to deal with comments on requirements.  process email discussion about the need to liaison with the putative Chilean design contractor.    Kantor suggests a contractor to support requirements may be apropos. ,1,DM-2167,datamanagement,investigate commerical vendor deal comment requirement process email discussion need liaison putative chilean design contractor kantor suggest contractor support requirement apropo,Investigate Commerical vendor to deal with comments on requirements. process email discussion about the need to liaison with the putative Chilean design contractor. Kantor suggests a contractor to support requirements may be apropos.
"Work inside NCSA to connect procurement contract Modification to OSPRA contract officet work Jeff's proposal until it reached university contract officer. -- January meeting  -- clarify  purchasing rules -  Internal discussion of property management, -  General work within  contract modification process. ",6,DM-2168,datamanagement,work inside ncsa connect procurement contract modification ospra contract officet work jeff proposal reach university contract officer january meeting clarify purchase rule internal discussion property management general work contract modification process,"Work inside NCSA to connect procurement contract Modification to OSPRA contract officet work Jeff's proposal until it reached university contract officer. -- January meeting -- clarify purchasing rules - Internal discussion of property management, - General work within contract modification process."
"Implement JSON Results for MetaServ and DbServ Implement JSON results for Metadata Service (see all M* in https://confluence.lsstcorp.org/display/DM/API),  and Database Service (see all D*) as defined in DM-1868",3,DM-2171,datamanagement,implement json result metaserv dbserv implement json result metadata service https://confluence.lsstcorp.org/display/dm/api database service define dm-1868,"Implement JSON Results for MetaServ and DbServ Implement JSON results for Metadata Service (see all M* in https://confluence.lsstcorp.org/display/DM/API), and Database Service (see all D*) as defined in DM-1868"
Disable testDbLocal.py in db if auth file not found tests/testDbLocal.py can easily fail if required mysql authorization file is not found in user home dir. Skip the test instead of failing in such case.,1,DM-2173,datamanagement,disable testdblocal.py db auth file find test testdblocal.py easily fail require mysql authorization file find user home dir skip test instead fail case,Disable testDbLocal.py in db if auth file not found tests/testDbLocal.py can easily fail if required mysql authorization file is not found in user home dir. Skip the test instead of failing in such case.
"Migrate Qserv to external sphgeom Migrating qserv to the new c++ geometry API required porting a fair amount of code from the python layer and updating the plumbing in the czar. During implementation, the sphgeom was in the process of finding a home, so the sg code was temporarily placed under core/modules.  This ticket covers: * removing core/modules/sg * updating code to point at the external sphgeom * updating build-logic to properly depend on and link with external sphgeom.",4,DM-2178,datamanagement,migrate qserv external sphgeom migrating qserv new c++ geometry api require port fair code python layer update plumbing czar implementation sphgeom process find home sg code temporarily place core module ticket cover remove core module sg update code point external sphgeom update build logic properly depend link external sphgeom,"Migrate Qserv to external sphgeom Migrating qserv to the new c++ geometry API required porting a fair amount of code from the python layer and updating the plumbing in the czar. During implementation, the sphgeom was in the process of finding a home, so the sg code was temporarily placed under core/modules. This ticket covers: * removing core/modules/sg * updating code to point at the external sphgeom * updating build-logic to properly depend on and link with external sphgeom."
Move astrometry_net wrapper code from meas_astrom to meas_astrometry_net We would like to remove all astrometry.net wrapper code from meas_astrom and put it in a new package with a name such as meas_astrometry_net.  This will also require moving any abstract task base classes into a lower-level package such as meas_astrom.,6,DM-2186,datamanagement,astrometry_net wrapper code meas_astrom meas_astrometry_net like remove astrometry.net wrapper code meas_astrom new package meas_astrometry_net require move abstract task base class low level package meas_astrom,Move astrometry_net wrapper code from meas_astrom to meas_astrometry_net We would like to remove all astrometry.net wrapper code from meas_astrom and put it in a new package with a name such as meas_astrometry_net. This will also require moving any abstract task base classes into a lower-level package such as meas_astrom.
"Update the astrometry.net astrometry solver to use the new standard schema DM-1576 provides a new astrometry solver and a new schema for reference objects. However, the old astrometry.net astrometry solver still uses the old schema. It would be wise to convert the old solver to the new schema so that the match list returned by it is in standard format.",4,DM-2188,datamanagement,update astrometry.net astrometry solver use new standard schema dm-1576 provide new astrometry solver new schema reference object old astrometry.net astrometry solver use old schema wise convert old solver new schema match list return standard format,"Update the astrometry.net astrometry solver to use the new standard schema DM-1576 provides a new astrometry solver and a new schema for reference objects. However, the old astrometry.net astrometry solver still uses the old schema. It would be wise to convert the old solver to the new schema so that the match list returned by it is in standard format."
"Documentation for data loader Vaikunth had some ""expected"" troubles playing with data loader options for his DM-1570 ticket. Main issue I believe is the absence of the documented use cases and their corresponding data loader options. I'll try to add a bunch of common use cases to RST documentation and also verify that all options behave as expected.",2,DM-2190,datamanagement,documentation datum loader vaikunth expect trouble play datum loader option dm-1570 ticket main issue believe absence document use case corresponding datum loader option try add bunch common use case rst documentation verify option behave expect,"Documentation for data loader Vaikunth had some ""expected"" troubles playing with data loader options for his DM-1570 ticket. Main issue I believe is the absence of the documented use cases and their corresponding data loader options. I'll try to add a bunch of common use cases to RST documentation and also verify that all options behave as expected."
"Define command line tasks for pre-ingest transformation DM-1903 provided a command line task which would transform a {{src}} catalogue into calibrated form. Here, we build on that to provide command line tasks for all source catalogues which will need to be ingested; will include at least {{deepCoadd_src}}, {{goodSeeingCoadd_src}}, {{chiSquaredCoadd_src}}.",6,DM-2191,datamanagement,define command line task pre ingest transformation dm-1903 provide command line task transform src catalogue calibrate form build provide command line task source catalogue need ingest include deepcoadd_src goodseeingcoadd_src chisquaredcoadd_src,"Define command line tasks for pre-ingest transformation DM-1903 provided a command line task which would transform a {{src}} catalogue into calibrated form. Here, we build on that to provide command line tasks for all source catalogues which will need to be ingested; will include at least {{deepCoadd_src}}, {{goodSeeingCoadd_src}}, {{chiSquaredCoadd_src}}."
"Add assertXNearlyEqual to afw We often want to compare two WCS for approximate equality. afw/image/testUtils has similar functions to compare images and masks and I would like to add one for WCS    This ended up being expanded to adding functions for many afw classes (not yet including image-like classes, though existing functions in image/testUtils for that purpose should probably be wrapped or rewritten on a different ticket)",5,DM-2193,datamanagement,add assertxnearlyequal afw want compare wcs approximate equality afw image testutil similar function compare image mask like add wcs end expand add function afw class include image like class exist function image testutil purpose probably wrap rewrite different ticket,"Add assertXNearlyEqual to afw We often want to compare two WCS for approximate equality. afw/image/testUtils has similar functions to compare images and masks and I would like to add one for WCS This ended up being expanded to adding functions for many afw classes (not yet including image-like classes, though existing functions in image/testUtils for that purpose should probably be wrapped or rewritten on a different ticket)"
Ensure proper functioning of HSC distortion correction within obs_subaru There may be some discrepancy between the pixel units being passed to distest.cc compared to what it is expecting (units of pixels).  This needs to be investigated further and remedied in such a way that all other representations (e.g. in camera.py) are consistent with the other obs_XXX representations.,6,DM-2194,datamanagement,ensure proper functioning hsc distortion correction obs_subaru discrepancy pixel unit pass distest.cc compare expect unit pixel need investigate remedie way representation e.g. camera.py consistent obs_xxx representation,Ensure proper functioning of HSC distortion correction within obs_subaru There may be some discrepancy between the pixel units being passed to distest.cc compared to what it is expecting (units of pixels). This needs to be investigated further and remedied in such a way that all other representations (e.g. in camera.py) are consistent with the other obs_XXX representations.
Prototype HTM-based spatial binning to visualize large number of catalog sources See story DM-1551.,8,DM-2197,datamanagement,prototype htm base spatial bin visualize large number catalog source story dm-1551,Prototype HTM-based spatial binning to visualize large number of catalog sources See story DM-1551.
Build 2015_03 Qserv release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe. ,1,DM-2199,datamanagement,build 2015_03 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build 2015_03 Qserv release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
"Add typemaps for numpy scalars Add typemaps so that we can use numpy scalars to call C++ functions that take plain old scalar types (e.g. float, double or int). At present attempting to pass numpy scalars will fail unless the type is one of a restricted subset, e.g. float and numpy.float64 succeed but numpy.float32 is rejected as being an incompatible type, and similarly for integer types.",4,DM-2201,datamanagement,add typemap numpy scalar add typemap use numpy scalar c++ function plain old scalar type e.g. float double int present attempt pass numpy scalar fail type restricted subset e.g. float numpy.float64 succeed numpy.float32 reject incompatible type similarly integer type,"Add typemaps for numpy scalars Add typemaps so that we can use numpy scalars to call C++ functions that take plain old scalar types (e.g. float, double or int). At present attempting to pass numpy scalars will fail unless the type is one of a restricted subset, e.g. float and numpy.float64 succeed but numpy.float32 is rejected as being an incompatible type, and similarly for integer types."
"Acquire development data We'll need a reference set of data to work against.  This could be SDSS, CFHT, or simulated.  Should be 10? epochs with realistic atmospheric conditions taken at similar airmass and hour angle.  Single band is fine for now.",4,DM-2202,datamanagement,acquire development datum need reference set datum work sdss cfht simulate 10 epoch realistic atmospheric condition take similar airmass hour angle single band fine,"Acquire development data We'll need a reference set of data to work against. This could be SDSS, CFHT, or simulated. Should be 10? epochs with realistic atmospheric conditions taken at similar airmass and hour angle. Single band is fine for now."
"Produce task API This will require a new task, so will require a new interface and associated RFCs.  The interface should take an arbitrarily large stack of catalogs with or without a reference catalog.  It should return a stack ow WCSs that map from the individual coordinate systems to the reference.",6,DM-2203,datamanagement,produce task api require new task require new interface associated rfc interface arbitrarily large stack catalog reference catalog return stack ow wcs map individual coordinate system reference,"Produce task API This will require a new task, so will require a new interface and associated RFCs. The interface should take an arbitrarily large stack of catalogs with or without a reference catalog. It should return a stack ow WCSs that map from the individual coordinate systems to the reference."
"Break down monster DM-1108 stories [~pgee] -- After finishing the measurement work, your next priority is to get started on DM-1108. However, the stories you have been assigned there are currently too big for useful scheduling (20-30 SPs is a mini-epic; we're looking for less than 10 SPs per story). The first task therefore is to work with [~jbosch], and others if required, to break them down and come up with a set of stories which usefully reflect the work which needs to be done.",4,DM-2205,datamanagement,break monster dm-1108 story ~pgee finish measurement work priority start dm-1108 story assign currently big useful scheduling 20 30 sp mini epic look 10 sp story task work ~jbosch require break come set story usefully reflect work need,"Break down monster DM-1108 stories [~pgee] -- After finishing the measurement work, your next priority is to get started on DM-1108. However, the stories you have been assigned there are currently too big for useful scheduling (20-30 SPs is a mini-epic; we're looking for less than 10 SPs per story). The first task therefore is to work with [~jbosch], and others if required, to break them down and come up with a set of stories which usefully reflect the work which needs to be done."
"Understand GPFS and commercial filesystems between data centers Start May 2015, finish June 2015 Petravick D - 50%, TBD from SET group",6,DM-2216,datamanagement,understand gpfs commercial filesystem datum center start 2015 finish june 2015 petravick 50 tbd set group,"Understand GPFS and commercial filesystems between data centers Start May 2015, finish June 2015 Petravick D - 50%, TBD from SET group"
"Start understanding inheritability and reusability of dataset types In order to allow for on-the-fly Task creation of dataset types, the essentials of each type need to be encapsulated in code.  That code should be reused across all similar dataset types, and there are opportunities for inheritance and specialization, particularly in cases like simple file-oriented mappers.  Investigate this by prototyping a number of possibilities.",4,DM-2221,datamanagement,start understand inheritability reusability dataset type order allow fly task creation dataset type essential type need encapsulate code code reuse similar dataset type opportunity inheritance specialization particularly case like simple file orient mapper investigate prototype number possibility,"Start understanding inheritability and reusability of dataset types In order to allow for on-the-fly Task creation of dataset types, the essentials of each type need to be encapsulated in code. That code should be reused across all similar dataset types, and there are opportunities for inheritance and specialization, particularly in cases like simple file-oriented mappers. Investigate this by prototyping a number of possibilities."
LOE - Week ending 3/6/15 - backup issues with lsst-stor141 (https://jira.ncsa.illinois.edu/browse/LSST-632) - setup jumbo frames on lsst-xfer (https://jira.ncsa.illinois.edu/browse/LSST-628) - crashplan reconfig on lsst-xfer (https://jira.ncsa.illinois.edu/browse/LSST-629),8,DM-2228,datamanagement,loe week end 3/6/15 backup issue lsst stor141 https://jira.ncsa.illinois.edu/browse/lsst-632 setup jumbo frame lsst xfer https://jira.ncsa.illinois.edu/browse/lsst-628 crashplan reconfig lsst xfer https://jira.ncsa.illinois.edu/browse/lsst-629,LOE - Week ending 3/6/15 - backup issues with lsst-stor141 (https://jira.ncsa.illinois.edu/browse/LSST-632) - setup jumbo frames on lsst-xfer (https://jira.ncsa.illinois.edu/browse/LSST-628) - crashplan reconfig on lsst-xfer (https://jira.ncsa.illinois.edu/browse/LSST-629)
LOE - Week ending 3/13/15 - crashplan issue with lsst-netem (https://jira.ncsa.illinois.edu/browse/LSST-633) - yum/glibc issue with lsst-dbdev1 (https://jira.ncsa.illinois.edu/browse/LSST-631) - account for Jacques Sebag (https://jira.ncsa.illinois.edu/browse/LSST-624),8,DM-2229,datamanagement,loe week end 3/13/15 crashplan issue lsst netem https://jira.ncsa.illinois.edu/browse/lsst-633 yum glibc issue lsst dbdev1 https://jira.ncsa.illinois.edu/browse/lsst-631 account jacques sebag https://jira.ncsa.illinois.edu/browse/lsst-624,LOE - Week ending 3/13/15 - crashplan issue with lsst-netem (https://jira.ncsa.illinois.edu/browse/LSST-633) - yum/glibc issue with lsst-dbdev1 (https://jira.ncsa.illinois.edu/browse/LSST-631) - account for Jacques Sebag (https://jira.ncsa.illinois.edu/browse/LSST-624)
Extend API: expose cursor Extend API to expose cursor. This was brought up by Andy in DM-2137. ,1,DM-2243,datamanagement,extend api expose cursor extend api expose cursor bring andy dm-2137,Extend API: expose cursor Extend API to expose cursor. This was brought up by Andy in DM-2137.
Define ntermediate plan for MacOSX builds  We have  1. Obtain a dedicated colo OSX server  2. Have done some testing using the SQuaRE vagrant-sandbox harness  It is therefore a plausible avenue forward to do at least a nightly build/deploy/intgeration-test on OSX pending more extensive arrangements requiring purchase of hardware.  ,4,DM-2245,datamanagement,define ntermediate plan macosx build obtain dedicated colo osx server testing square vagrant sandbox harness plausible avenue forward nightly build deploy intgeration test osx pende extensive arrangement require purchase hardware,Define ntermediate plan for MacOSX builds We have 1. Obtain a dedicated colo OSX server 2. Have done some testing using the SQuaRE vagrant-sandbox harness It is therefore a plausible avenue forward to do at least a nightly build/deploy/intgeration-test on OSX pending more extensive arrangements requiring purchase of hardware.
Workflow improvements for Sims / PST projects New wokflow for Sims Merge of Opsim and CATsim New workflow for PST ,5,DM-2247,datamanagement,workflow improvement sims pst project new wokflow sims merge opsim catsim new workflow pst,Workflow improvements for Sims / PST projects New wokflow for Sims Merge of Opsim and CATsim New workflow for PST
"Implement SExtractor's SPREAD_MODEL The new SExtractor star/galaxy classifier, SPREAD_MODEL, is popular with everyone who has tried it, and should be simple to implement by building on code in meas_modelfit.  See definition and discussion here: http://arxiv.org/abs/1306.4446",2,DM-2251,datamanagement,implement sextractor new sextractor star galaxy classifier spread_model popular try simple implement build code meas_modelfit definition discussion,"Implement SExtractor's SPREAD_MODEL The new SExtractor star/galaxy classifier, SPREAD_MODEL, is popular with everyone who has tried it, and should be simple to implement by building on code in meas_modelfit. See definition and discussion here: http://arxiv.org/abs/1306.4446"
"Define common interface for star/galaxy classifiers We need some common fields for star/galaxy classifiers so they can participate in a slots-like mechanism once we have several of them.  Most of these can produce a floating point number between 0 and 1 (but sometimes it's not limited to that range), and it's rarely a true probability.  We may want to make a boolean that results from a threshold on these be the common interface, but we don't necessarily want to hard-code such a threshold into the processing either - especially when we could also use a FunctorKey to get a boolean from the floating point value.",2,DM-2252,datamanagement,define common interface star galaxy classifier need common field star galaxy classifier participate slot like mechanism produce float point number limit range rarely true probability want boolean result threshold common interface necessarily want hard code threshold processing especially use functorkey boolean float point value,"Define common interface for star/galaxy classifiers We need some common fields for star/galaxy classifiers so they can participate in a slots-like mechanism once we have several of them. Most of these can produce a floating point number between 0 and 1 (but sometimes it's not limited to that range), and it's rarely a true probability. We may want to make a boolean that results from a threshold on these be the common interface, but we don't necessarily want to hard-code such a threshold into the processing either - especially when we could also use a FunctorKey to get a boolean from the floating point value."
"add third-party package builds for ngmix dependencies In addition to numpy and scipy, ngmix depends on the emcee and statsmodel packages.  While it can build without them, we probably want the full functionality.  I also see some undeclared dependencies on the ""esutil"" and ""fitsio"" packages (all from esheldon's GitHub), and there may be a few more dependencies on some of his own packages.    This issue includes creating a third-party build for ngmix itself.",6,DM-2253,datamanagement,add party package build ngmix dependency addition numpy scipy ngmix depend emcee statsmodel package build probably want functionality undeclared dependency esutil fitsio package esheldon github dependency package issue include create party build ngmix,"add third-party package builds for ngmix dependencies In addition to numpy and scipy, ngmix depends on the emcee and statsmodel packages. While it can build without them, we probably want the full functionality. I also see some undeclared dependencies on the ""esutil"" and ""fitsio"" packages (all from esheldon's GitHub), and there may be a few more dependencies on some of his own packages. This issue includes creating a third-party build for ngmix itself."
make a simple build for Firefly package We want to have a out of box build for users of Firefly package. It will include a simple Firefly viewer. ,6,DM-2256,datamanagement,simple build firefly package want box build user firefly package include simple firefly viewer,make a simple build for Firefly package We want to have a out of box build for users of Firefly package. It will include a simple Firefly viewer.
"Allow eups xrootd install script to be relocatable xrootd lib/ directory should be s relative symlink to lib64, no a full path link.",1,DM-2257,datamanagement,allow eup xrootd install script relocatable xrootd lib/ directory relative symlink lib64 path link,"Allow eups xrootd install script to be relocatable xrootd lib/ directory should be s relative symlink to lib64, no a full path link."
Setup in2p3 cluster for Qserv team - create accounts - update umask on stack  to each account - provide easy ssh config if possible - setup up build procedure (each developer can build Qserv using tag git and 'git' version is set up by default on all the Qserv if ti exists),2,DM-2258,datamanagement,setup in2p3 cluster qserv team create account update umask stack account provide easy ssh config possible setup build procedure developer build qserv tag git git version set default qserv ti exist,Setup in2p3 cluster for Qserv team - create accounts - update umask on stack to each account - provide easy ssh config if possible - setup up build procedure (each developer can build Qserv using tag git and 'git' version is set up by default on all the Qserv if ti exists)
"remove PSFAttributes PSFAttributes has long been deprecated, and we just need a little more work to remove it:  - Add an effective area accessor to the Psf interface, and implement it in ImagePsf.  - Replace usage of PSFAttributes with usage of Psf accessors.  This may require a little work if code depends on the details of how the shape was calculated, as PSFAttributes provided support for more algorithms than we will going forward.",2,DM-2259,datamanagement,remove psfattributes psfattributes long deprecate need little work remove add effective area accessor psf interface implement imagepsf replace usage psfattributes usage psf accessor require little work code depend detail shape calculate psfattribute provide support algorithm go forward,"remove PSFAttributes PSFAttributes has long been deprecated, and we just need a little more work to remove it: - Add an effective area accessor to the Psf interface, and implement it in ImagePsf. - Replace usage of PSFAttributes with usage of Psf accessors. This may require a little work if code depends on the details of how the shape was calculated, as PSFAttributes provided support for more algorithms than we will going forward."
Implement task switching between work job machines AP requires that jobs are handed off to different worker job clusters as the previous set of images is being worked on.,4,DM-2264,datamanagement,implement task switching work job machine ap require job hand different worker job cluster previous set image work,Implement task switching between work job machines AP requires that jobs are handed off to different worker job clusters as the previous set of images is being worked on.
Implement file transfer API  Create file transfer API so we can easily test different types of file transfer mechanisms to/from the AP.,4,DM-2269,datamanagement,implement file transfer api create file transfer api easily test different type file transfer mechanism ap,Implement file transfer API Create file transfer API so we can easily test different types of file transfer mechanisms to/from the AP.
Move VMs to Docker containers We anticipate being able to move from the VMs that we currently use to using docker.  This will require some coordination with Greg Daues to see how HTCondor is configured.  ,2,DM-2270,datamanagement,vms docker container anticipate able vms currently use docker require coordination greg daues htcondor configure,Move VMs to Docker containers We anticipate being able to move from the VMs that we currently use to using docker. This will require some coordination with Greg Daues to see how HTCondor is configured.
"Unify logging strategy for python scripts - add -vvv option  - remove default value for configuration file in logger, provide it at each script level (i.e. integration test, data loader).   - if it exists, provide configuration file option explicitly to all called submodules which uses it.    See admin/python/lsst/qserv/admin/logger.py  {code:python}   14 def get_default_log_conf():                                                                                                                                                                  15     default_log_conf = ""{0}/.lsst/logging.ini"".format(os.path.expanduser('~'))                                                                                                               16     return default_log_conf                                                                                                                                                                  17                                                                                                                                                                                              18 def add_logfile_opt(parser):                                                                                                                                                                 19     """"""                                                                                                                                                                                      20     Add option to command line interface in order to set path to standar                                                                                                                     21     configuration file for python logger                                                                                                                                                     22     """"""                                                                                                                                                                                      23                                                                                                                                                                                              24     parser.add_argument(""-V"", ""--log-cfg"", dest=""log_conf"",                                                                                                                                  25                         default=get_default_log_conf(),                                                                                                                                      26                         help=""Absolute path to yaml file containing python"" +                                                                                                                27                         ""logger standard configuration file"")                                                                                                                                28     return parser                                                                                                                                                                            29                                                                                                                                                                                              30                                                                                                                                                                                              31 def setup_logging(path='logging.ini',                                                                                                                                                        32                   default_level=logging.INFO):                                                                                                                                               33     """"""                                                                                                                                                                                      34     Setup logging configuration from yaml file                                                                                                                                               35     if the yaml file doesn't exists:                                                                                                                                                         36     - return false                                                                                                                                                                           37     - configure logging to default_level                                                                                                                                                     38     """"""                                                                                                                                                                                      39     if os.path.exists(path):                                                                                                                                                                 40         with open(path, 'r') as f:                                                                                                                                                           41             logging.config.fileConfig(f)                                                                                                                                                     42         return True                                                                                                                                                                          43     else:                                                                                                                                                                                    44         logging.basicConfig(level=default_level)                                                                                                                                  45         return False    {code}",6,DM-2272,datamanagement,"unify log strategy python script add -vvv option remove default value configuration file logger provide script level i.e. integration test datum loader exist provide configuration file option explicitly call submodule use admin python lsst qserv admin logger.py code python 14 def get_default_log_conf 15 default_log_conf 0}/.lsst logging.ini"".format(os.path.expanduser('~ 16 return default_log_conf 17 18 def add_logfile_opt(parser 19 20 add option command line interface order set path standar 21 configuration file python logger 22 23 24 parser.add_argument(""-v --log cfg dest=""log_conf 25 default get_default_log_conf 26 help=""absolute path yaml file contain python 27 logger standard configuration file 28 return parser 29 30 31 def setup_logging(path='logging.ini 32 default_level log info 33 34 setup log configuration yaml file 35 yaml file exist 36 return false 37 configure logging default_level 38 39 os.path.exists(path 40 open(path 41 logging.config.fileconfig(f 42 return true 43 44 logging.basicconfig(level default_level 45 return false code","Unify logging strategy for python scripts - add -vvv option - remove default value for configuration file in logger, provide it at each script level (i.e. integration test, data loader). - if it exists, provide configuration file option explicitly to all called submodules which uses it. See admin/python/lsst/qserv/admin/logger.py {code:python} 14 def get_default_log_conf(): 15 default_log_conf = ""{0}/.lsst/logging.ini"".format(os.path.expanduser('~')) 16 return default_log_conf 17 18 def add_logfile_opt(parser): 19 """""" 20 Add option to command line interface in order to set path to standar 21 configuration file for python logger 22 """""" 23 24 parser.add_argument(""-V"", ""--log-cfg"", dest=""log_conf"", 25 default=get_default_log_conf(), 26 help=""Absolute path to yaml file containing python"" + 27 ""logger standard configuration file"") 28 return parser 29 30 31 def setup_logging(path='logging.ini', 32 default_level=logging.INFO): 33 """""" 34 Setup logging configuration from yaml file 35 if the yaml file doesn't exists: 36 - return false 37 - configure logging to default_level 38 """""" 39 if os.path.exists(path): 40 with open(path, 'r') as f: 41 logging.config.fileConfig(f) 42 return True 43 else: 44 logging.basicConfig(level=default_level) 45 return False {code}"
Document HOW-TO setup-up krb5 for easy cluster access {code:bash} su aptitude install krb5-user # edit /etc/krb5.conf w.r.t ccage one # then as desktop user kinit ssh ccqservxxx {code}  /etc/krb5.conf {code:bash} [libdefaults] 	default_realm = IN2P3.FR  ... 	allow_weak_crypto = true   ... [realms] 	IN2P3.FR = { 		kdc = kerberos-1.in2p3.fr:88 		kdc = kerberos-2.in2p3.fr:88 		kdc = kerberos-3.in2p3.fr:88     		master_kdc = kerberos-admin.in2p3.fr:88     		admin_server = kerberos-admin.in2p3.fr     		kpasswd_server = kerberos-admin.in2p3.fr     		default_domain = in2p3.fr {code}  sshconfig: {code:bash} Host ccqservbuild GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName ccqservbuild.in2p3.fr #ProxyCommand ssh -W %h:%p cc   Host ccqserv1* GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName %h.in2p3.fr ProxyCommand ssh -W %h:%p ccqservbuild {code},2,DM-2277,datamanagement,document setup krb5 easy cluster access code bash su aptitude install krb5 user edit krb5.conf w.r.t ccage desktop user kinit ssh ccqservxxx code krb5.conf code bash libdefault default_realm in2p3.fr allow_weak_crypto true realm in2p3.fr kdc kerberos-1.in2p3.fr:88 kdc kerberos-2.in2p3.fr:88 kdc kerberos-3.in2p3.fr:88 master_kdc kerberos-admin.in2p3.fr:88 admin_server kerberos-admin.in2p3.fr kpasswd_server kerberos-admin.in2p3.fr default_domain in2p3.fr code sshconfig code bash host ccqservbuild gssapiauthentication yes gssapidelegatecredential yes forwardx11 yes hostname ccqservbuild.in2p3.fr proxycommand ssh -w h:%p cc host ccqserv1 gssapiauthentication yes gssapidelegatecredential yes forwardx11 yes hostname h.in2p3.fr proxycommand ssh -w h:%p ccqservbuild code,Document HOW-TO setup-up krb5 for easy cluster access {code:bash} su aptitude install krb5-user # edit /etc/krb5.conf w.r.t ccage one # then as desktop user kinit ssh ccqservxxx {code} /etc/krb5.conf {code:bash} [libdefaults] default_realm = IN2P3.FR ... allow_weak_crypto = true ... [realms] IN2P3.FR = { kdc = kerberos-1.in2p3.fr:88 kdc = kerberos-2.in2p3.fr:88 kdc = kerberos-3.in2p3.fr:88 master_kdc = kerberos-admin.in2p3.fr:88 admin_server = kerberos-admin.in2p3.fr kpasswd_server = kerberos-admin.in2p3.fr default_domain = in2p3.fr {code} sshconfig: {code:bash} Host ccqservbuild GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName ccqservbuild.in2p3.fr #ProxyCommand ssh -W %h:%p cc Host ccqserv1* GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName %h.in2p3.fr ProxyCommand ssh -W %h:%p ccqservbuild {code}
Fix problems with mysql timeout We added some code for supporting reconnecting (see https://dev.lsstcorp.org/trac/ticket/3042) but clearly not enough to recover from connection timeouts. This needs to be addressed.,1,DM-2279,datamanagement,fix problem mysql timeout add code support reconnecte https://dev.lsstcorp.org/trac/ticket/3042 clearly recover connection timeout need address,Fix problems with mysql timeout We added some code for supporting reconnecting (see https://dev.lsstcorp.org/trac/ticket/3042) but clearly not enough to recover from connection timeouts. This needs to be addressed.
"The TAN_PIXELS cameraGeom coordinate system should be with respect to the center of the focal plane The TAN_PIXELS cameraGeom coordinate system (the position on a detector if there is no optical distortion) is presently defined with respect to the center of the detector -- i.e. a star at the center of the detector will have the same position in PIXELS and TAN_PIXELS coordinates. That is a mistake. TAN_PIXELS should be defined with respect to the center of the focal plane, since it then reflects the effects of having optical distortion or not.  Fixing this will help meas_astrom match stars. The effects of not fixing it are making the matcher search farther for a fit. As long as we allow sufficient offset in the matcher config the current system will work, but it is not ideal.",2,DM-2280,datamanagement,tan_pixels camerageom coordinate system respect center focal plane tan_pixels camerageom coordinate system position detector optical distortion presently define respect center detector i.e. star center detector position pixels tan_pixels coordinate mistake tan_pixel define respect center focal plane reflect effect have optical distortion fix help meas_astrom match star effect fix make matcher search far fit long allow sufficient offset matcher config current system work ideal,"The TAN_PIXELS cameraGeom coordinate system should be with respect to the center of the focal plane The TAN_PIXELS cameraGeom coordinate system (the position on a detector if there is no optical distortion) is presently defined with respect to the center of the detector -- i.e. a star at the center of the detector will have the same position in PIXELS and TAN_PIXELS coordinates. That is a mistake. TAN_PIXELS should be defined with respect to the center of the focal plane, since it then reflects the effects of having optical distortion or not. Fixing this will help meas_astrom match stars. The effects of not fixing it are making the matcher search farther for a fit. As long as we allow sufficient offset in the matcher config the current system will work, but it is not ideal."
"Implement connection pool Implement a class that manages a connection pool, and optionally, if configured, restarts connection as needed in case of timeout.",1,DM-2281,datamanagement,implement connection pool implement class manage connection pool optionally configure restart connection need case timeout,"Implement connection pool Implement a class that manages a connection pool, and optionally, if configured, restarts connection as needed in case of timeout."
"Switch to using db connection pool Switch to using the db connection pool. Note, in addition to getting auto-reconnect, in metaserv that would handy if we need to talk to multiple database servers simultaneously.",1,DM-2282,datamanagement,switch db connection pool switch db connection pool note addition get auto reconnect metaserv handy need talk multiple database server simultaneously,"Switch to using db connection pool Switch to using the db connection pool. Note, in addition to getting auto-reconnect, in metaserv that would handy if we need to talk to multiple database servers simultaneously."
"Personnell requisitions  Work though recruiting for software effort.  Investigated and filled the ""kenton"" recruiting pattern at NCSA -- (few explicit requirements, many desirable)  Began discussion to break down hires for ""2nd"" floor  work -- to be in the LSST group v.s support groups -- ADS and Doug's group",3,DM-2289,datamanagement,personnell requisition work recruit software effort investigate fill kenton recruiting pattern ncsa explicit requirement desirable begin discussion break hire 2nd floor work lsst group v.s support group ads doug group,"Personnell requisitions Work though recruiting for software effort. Investigated and filled the ""kenton"" recruiting pattern at NCSA -- (few explicit requirements, many desirable) Began discussion to break down hires for ""2nd"" floor work -- to be in the LSST group v.s support groups -- ADS and Doug's group"
"Arrange for commercial object store presentation arrage for presentations next week w.r.t commercial object store.  The vendor in question is know to NCSA and has claims to have produced a commercial object store having both NFS,  GPFS  and swift interfaces. ",1,DM-2290,datamanagement,arrange commercial object store presentation arrage presentation week w.r.t commercial object store vendor question know ncsa claim produce commercial object store have nfs gpfs swift interface,"Arrange for commercial object store presentation arrage for presentations next week w.r.t commercial object store. The vendor in question is know to NCSA and has claims to have produced a commercial object store having both NFS, GPFS and swift interfaces."
Begin WBS review  Begin  comprehensive review of the WBS.   Forced on overall framework and begin  workflow systems  ,1,DM-2291,datamanagement,begin wbs review begin comprehensive review wbs force overall framework begin workflow system,Begin WBS review Begin comprehensive review of the WBS. Forced on overall framework and begin workflow systems
Security officer orientation begin orientation of LSST ISO ALEX Withers. ,1,DM-2292,datamanagement,security officer orientation begin orientation lsst iso alex withers,Security officer orientation begin orientation of LSST ISO ALEX Withers.
"Unable to start cmsd on Qserv worker node Some build issues have qlready been fixed in commit: 9dd378829e8751a6852356967411c20580e2a1c3  Here's the log:  {code:bash} [fjammes@ccqserv101 ~]$ cat /qserv/qserv-run/var/log/worker/cmsd.log 150309 21:19:46 9794 Starting on Linux 3.10.0-123.8.1.el7.x86_64 Copr.  2004-2012 Stanford University, xrd version v20140617-203cf45 ++++++ cmsd worker@ccqserv101.in2p3.fr initialization started. Config using configuration file /qserv/qserv-run/etc/lsp.cf =====> all.adminpath /qserv/qserv-run/tmp =====> xrd.port 1094 =====> xrd.network nodnr Config maximum number of connections restricted to 4096 Config maximum number of threads restricted to 2048 Copr.  2007 Stanford University/SLAC cmsd. ++++++ worker@ccqserv101.in2p3.fr phase 1 initialization started. =====> all.role server =====> ofs.osslib libxrdoss.so  =====> oss.localroot /qserv/qserv-run/xrootd-run =====> cms.space linger 0 recalc 15 min 10m 11m =====> all.pidpath /qserv/qserv-run/var/run =====> all.adminpath /qserv/qserv-run/tmp =====> all.manager ccqserv100.in2p3.fr:2131 =====> all.export / nolock The following paths are available to the redirector: w  /   ------ worker@ccqserv101.in2p3.fr phase 1 server initialization completed. ++++++ worker@ccqserv101.in2p3.fr phase 2 server initialization started. Plugin Unable to find  required version information for XrdOssGetStorageSystem in osslib libxrdoss.so ------ worker@ccqserv101.in2p3.fr phase 2 server initialization failed. 150309 21:19:46 9794 XrdProtocol: Protocol cmsd could not be loaded ------ cmsd worker@ccqserv101.in2p3.fr:1094 initialization failed {code}",2,DM-2294,datamanagement,unable start cmsd qserv worker node build issue qlready fix commit 9dd378829e8751a6852356967411c20580e2a1c3 log code bash fjammes@ccqserv101 cat qserv run var log worker cmsd.log 150309 21:19:46 9794 start linux 3.10.0 123.8.1.el7.x86_64 copr 2004 2012 stanford university xrd version v20140617 203cf45 cmsd worker@ccqserv101.in2p3.fr initialization start config configuration file qserv run etc lsp.cf all.adminpath /qserv qserv run tmp xrd.port 1094 xrd.network nodnr config maximum number connection restrict 4096 config maximum number thread restrict 2048 copr 2007 stanford university slac cmsd worker@ccqserv101.in2p3.fr phase initialization start all.role server ofs.osslib libxrdoss.so oss.localroot qserv run xrootd run cms.space linger recalc 15 min 10 11 all.pidpath qserv run var run all.adminpath /qserv qserv run tmp all.manag ccqserv100.in2p3.fr:2131 all.export nolock follow path available redirector ------ worker@ccqserv101.in2p3.fr phase server initialization complete worker@ccqserv101.in2p3.fr phase server initialization start plugin unable find required version information xrdossgetstoragesystem osslib libxrdoss.so worker@ccqserv101.in2p3.fr phase server initialization fail 150309 21:19:46 9794 xrdprotocol protocol cmsd load cmsd worker@ccqserv101.in2p3.fr:1094 initialization fail code,"Unable to start cmsd on Qserv worker node Some build issues have qlready been fixed in commit: 9dd378829e8751a6852356967411c20580e2a1c3 Here's the log: {code:bash} [fjammes@ccqserv101 ~]$ cat /qserv/qserv-run/var/log/worker/cmsd.log 150309 21:19:46 9794 Starting on Linux 3.10.0-123.8.1.el7.x86_64 Copr. 2004-2012 Stanford University, xrd version v20140617-203cf45 ++++++ cmsd worker@ccqserv101.in2p3.fr initialization started. Config using configuration file /qserv/qserv-run/etc/lsp.cf =====> all.adminpath /qserv/qserv-run/tmp =====> xrd.port 1094 =====> xrd.network nodnr Config maximum number of connections restricted to 4096 Config maximum number of threads restricted to 2048 Copr. 2007 Stanford University/SLAC cmsd. ++++++ worker@ccqserv101.in2p3.fr phase 1 initialization started. =====> all.role server =====> ofs.osslib libxrdoss.so =====> oss.localroot /qserv/qserv-run/xrootd-run =====> cms.space linger 0 recalc 15 min 10m 11m =====> all.pidpath /qserv/qserv-run/var/run =====> all.adminpath /qserv/qserv-run/tmp =====> all.manager ccqserv100.in2p3.fr:2131 =====> all.export / nolock The following paths are available to the redirector: w / ------ worker@ccqserv101.in2p3.fr phase 1 server initialization completed. ++++++ worker@ccqserv101.in2p3.fr phase 2 server initialization started. Plugin Unable to find required version information for XrdOssGetStorageSystem in osslib libxrdoss.so ------ worker@ccqserv101.in2p3.fr phase 2 server initialization failed. 150309 21:19:46 9794 XrdProtocol: Protocol cmsd could not be loaded ------ cmsd worker@ccqserv101.in2p3.fr:1094 initialization failed {code}"
"Support metadata for databases without description The metaserv should be able to support databases for which we don't have the ascii schema with descriptions and special tokens (ucd, units etc). This story involves implementing it. In practice, the metaserv/bin/metaBackend will need to be extended to implement ""ADD DB""",4,DM-2301,datamanagement,support metadata database description metaserv able support database ascii schema description special token ucd unit etc story involve implement practice metaserv bin metabackend need extend implement add db,"Support metadata for databases without description The metaserv should be able to support databases for which we don't have the ascii schema with descriptions and special tokens (ucd, units etc). This story involves implementing it. In practice, the metaserv/bin/metaBackend will need to be extended to implement ""ADD DB"""
Measurement transforms for centroids Provide calibration transforms for all algorithms measuring centroids.,5,DM-2306,datamanagement,measurement transform centroid provide calibration transform algorithm measure centroid,Measurement transforms for centroids Provide calibration transforms for all algorithms measuring centroids.
Measurement transforms for shapes Provide calibration transforms for algorithms measuring shapes.,2,DM-2307,datamanagement,measurement transform shape provide calibration transform algorithm measure shape,Measurement transforms for shapes Provide calibration transforms for algorithms measuring shapes.
Update dev quick-start guide to new git repositories The quick-start documentation for developers still points to the old git repositories. The RST document needs to be updated to the GitHub repos.,1,DM-2309,datamanagement,update dev quick start guide new git repository quick start documentation developer point old git repository rst document need update github repos,Update dev quick-start guide to new git repositories The quick-start documentation for developers still points to the old git repositories. The RST document needs to be updated to the GitHub repos.
"obs_test's table file is out of date obs_test's table file is somewhat out of date. Problems include:  - afw is required but missing  - meas_algorithms and skypix are used by bin/genInputRegistry.py, which is only used to create the input repo so these can be optional  - daf_persistence is not used  - daf_base is only used by bin/genInputRegistry.py, so it can be optional (though it is presumably setup by daf_butlerUtils in any case)",1,DM-2312,datamanagement,obs_t table file date obs_test table file somewhat date problem include afw require miss meas_algorithm skypix bin geninputregistry.py create input repo optional daf_persistence daf_base bin geninputregistry.py optional presumably setup daf_butlerutil case,"obs_test's table file is out of date obs_test's table file is somewhat out of date. Problems include: - afw is required but missing - meas_algorithms and skypix are used by bin/genInputRegistry.py, which is only used to create the input repo so these can be optional - daf_persistence is not used - daf_base is only used by bin/genInputRegistry.py, so it can be optional (though it is presumably setup by daf_butlerUtils in any case)"
"Improve xssi API to send a few bytes with the message informing the client that a response is available on  the server This would allow Qserv no to send the first protobuf header as a xrootd in-band message, and save some resources (network and CPU due to xrootd/TCP/IP encapsulation)",6,DM-2314,datamanagement,improve xssi api send byte message inform client response available server allow qserv send protobuf header xrootd band message save resource network cpu xrootd tcp ip encapsulation,"Improve xssi API to send a few bytes with the message informing the client that a response is available on the server This would allow Qserv no to send the first protobuf header as a xrootd in-band message, and save some resources (network and CPU due to xrootd/TCP/IP encapsulation)"
"Clarify expectations for unauthenticated user data access h4. Short version:  Clarify what existing community practices, notably including VO interfaces, appear to rely on the availability of unauthenticated access to information in astronomical archives.  h4. Details:  At the February DM All Hands, [~frossie] raised an objection when it was mentioned that there is a presumption that all user access to LSST data through the DM interfaces (as opposed to through EPO) will be authenticated.  We don't appear to have ever documented an explicit requirement that all access be authenticated.  The basic controlling requirement is OSS-REQ-0176, ""The LSST Data Management System shall provide open access to all LSST Level 1 and Level 2 Data Products, as defined in the LSST System Requirements and herein, in accordance with LSSTC Board approved policies. ..."", which was a carefully crafted indirection at a time when the policy for non-US/Chile access was still being developed.  However, this presumption has been around for a long time.  It is inherent to the project policy that access to the non-Alert data will be limited to individuals who are entitled to it.  No matter what we think the final policy might be, we do have to design a system that can be consistent with this policy.  [~frossie] stated that the astronomical community relies on certain types of data and metadata - she mentioned coverage maps, among others - being available through unauthenticated interfaces.  This ticket is to ask her (and others) to collect documentation of those existing practices, so that we can figure out what the expectations may be and how to respond to them in our design.",2,DM-2316,datamanagement,clarify expectation unauthenticated user datum access h4 short version clarify exist community practice notably include vo interface appear rely availability unauthenticated access information astronomical archive h4 detail february dm hands ~frossie raise objection mention presumption user access lsst datum dm interface oppose epo authenticate appear document explicit requirement access authenticate basic controlling requirement oss req-0176 lsst data management system shall provide open access lsst level level data products define lsst system requirements accordance lsstc board approve policy carefully craft indirection time policy non chile access develop presumption long time inherent project policy access non alert datum limit individual entitle matter think final policy design system consistent policy ~frossie state astronomical community rely certain type datum metadata mention coverage map available unauthenticated interface ticket ask collect documentation exist practice figure expectation respond design,"Clarify expectations for unauthenticated user data access h4. Short version: Clarify what existing community practices, notably including VO interfaces, appear to rely on the availability of unauthenticated access to information in astronomical archives. h4. Details: At the February DM All Hands, [~frossie] raised an objection when it was mentioned that there is a presumption that all user access to LSST data through the DM interfaces (as opposed to through EPO) will be authenticated. We don't appear to have ever documented an explicit requirement that all access be authenticated. The basic controlling requirement is OSS-REQ-0176, ""The LSST Data Management System shall provide open access to all LSST Level 1 and Level 2 Data Products, as defined in the LSST System Requirements and herein, in accordance with LSSTC Board approved policies. ..."", which was a carefully crafted indirection at a time when the policy for non-US/Chile access was still being developed. However, this presumption has been around for a long time. It is inherent to the project policy that access to the non-Alert data will be limited to individuals who are entitled to it. No matter what we think the final policy might be, we do have to design a system that can be consistent with this policy. [~frossie] stated that the astronomical community relies on certain types of data and metadata - she mentioned coverage maps, among others - being available through unauthenticated interfaces. This ticket is to ask her (and others) to collect documentation of those existing practices, so that we can figure out what the expectations may be and how to respond to them in our design."
"Remove deprecated merging code: rproc::TableMerger rproc::TableMerger seems to be replaced with rproc::InfileMerger, so this class could certainly be removed easily. ",2,DM-2320,datamanagement,remove deprecate merge code rproc::tablemerger rproc::tablemerger replace rproc::infilemerger class certainly remove easily,"Remove deprecated merging code: rproc::TableMerger rproc::TableMerger seems to be replaced with rproc::InfileMerger, so this class could certainly be removed easily."
Revisit exceptions in db module Revisit db/python/lsst/db/exception.py. Perhaps get rid of the numbers.,5,DM-2322,datamanagement,revisit exception db module revisit db python lsst db exception.py rid number,Revisit exceptions in db module Revisit db/python/lsst/db/exception.py. Perhaps get rid of the numbers.
"Setup hosts for SUI (2x Tomcat, Apache, and build) Xiuqin's 'short term' version: 1 VM - SUI build server 4GB memory and 200GB hard disk should be good enough. 1 VM - Apache server as a proxy and web front end 4GB memory and 100GB hard disk should be enough port 80 accessible from outside 2 VMs - Tomcat servers each has 16GB memory, access to 1TB of shared hard disk Port 8080 should be open for Apache server to access Port 8009 should be open to each other so they can replicate cache. (First 2 VMs are not absolutely needed. We can always use one of the hosts in number 3 to do build and host Apache server.)  The 2 Tomcat servers are larger than we can currently support as VMs.   We've discussed repurposing 2 of the older LSST ""cluster/condor"" nodes (e.g. lsst14 & lsst15) for this purpose.  But, ideally these could be implemented with the new vSphere hardware if the timeframe works.",4,DM-2327,datamanagement,setup host sui 2x tomcat apache build xiuqin short term version vm sui build server gb memory 200 gb hard disk good vm apache server proxy web end gb memory 100 gb hard disk port 80 accessible outside vms tomcat server 16 gb memory access tb share hard disk port 8080 open apache server access port 8009 open replicate cache vms absolutely need use host number build host apache server tomcat server large currently support vm discuss repurpose old lsst cluster condor node e.g. lsst14 lsst15 purpose ideally implement new vsphere hardware timeframe work,"Setup hosts for SUI (2x Tomcat, Apache, and build) Xiuqin's 'short term' version: 1 VM - SUI build server 4GB memory and 200GB hard disk should be good enough. 1 VM - Apache server as a proxy and web front end 4GB memory and 100GB hard disk should be enough port 80 accessible from outside 2 VMs - Tomcat servers each has 16GB memory, access to 1TB of shared hard disk Port 8080 should be open for Apache server to access Port 8009 should be open to each other so they can replicate cache. (First 2 VMs are not absolutely needed. We can always use one of the hosts in number 3 to do build and host Apache server.) The 2 Tomcat servers are larger than we can currently support as VMs. We've discussed repurposing 2 of the older LSST ""cluster/condor"" nodes (e.g. lsst14 & lsst15) for this purpose. But, ideally these could be implemented with the new vSphere hardware if the timeframe works."
"review """"data center in a box"" mali.  Recover consultant's contact into  reviewed the data center in a  box, recovered consultant's name prior to drafting a SOW.",1,DM-2329,datamanagement,review datum center box mali recover consultant contact review datum center box recover consultant prior draft sow,"review """"data center in a box"" mali. Recover consultant's contact into reviewed the data center in a box, recovered consultant's name prior to drafting a SOW."
" attend DDN WOS briefing, write summary note.  as described above.  Summary note is attached. also looked for use of this product in DOE labs, who would be consumers  of LSST data.  Discovered that it had been investigated for use in HEP a few years earlier, but that is was not adopted because, at that time the hardware and software were coupled.",1,DM-2330,datamanagement,attend ddn wos briefing write summary note describe summary note attach look use product doe lab consumer lsst datum discover investigate use hep year early adopt time hardware software couple,"attend DDN WOS briefing, write summary note. as described above. Summary note is attached. also looked for use of this product in DOE labs, who would be consumers of LSST data. Discovered that it had been investigated for use in HEP a few years earlier, but that is was not adopted because, at that time the hardware and software were coupled."
misc for week of march 9 finalize job descriptions. Meet with kantor additional hour of  orientation for the ISO. group meeting  Misc.,1,DM-2331,datamanagement,misc week march finalize job description meet kantor additional hour orientation iso group meet misc,misc for week of march 9 finalize job descriptions. Meet with kantor additional hour of orientation for the ISO. group meeting Misc.
"Simplify interactions with XrdOss The qserv code is still using the old ssi scheme for the cmsd, this needs to be rewritten. For  details, see  https://listserv.slac.stanford.edu/cgi-bin/wa?A1=ind1503&L=QSERV-L#3",5,DM-2334,datamanagement,simplify interaction xrdos qserv code old ssi scheme cmsd need rewrite detail https://listserv.slac.stanford.edu/cgi-bin/wa?a1=ind1503&l=qserv-l#3,"Simplify interactions with XrdOss The qserv code is still using the old ssi scheme for the cmsd, this needs to be rewritten. For details, see https://listserv.slac.stanford.edu/cgi-bin/wa?A1=ind1503&L=QSERV-L#3"
"Setup IRODS zone on ISL OpenStack We begin an examination of iRODS for managing data collections. We perform initial testing using resources available on NCSA's ISL OpenStack.  To mock up a zone or 'data grid' managed by iRODS, we set up an ICAT server, a resource server (this is a data storage resource that does not run the central database), and a client host. ",4,DM-2335,datamanagement,setup irods zone isl openstack begin examination irods manage datum collection perform initial testing resource available ncsa isl openstack mock zone data grid manage irods set icat server resource server data storage resource run central database client host,"Setup IRODS zone on ISL OpenStack We begin an examination of iRODS for managing data collections. We perform initial testing using resources available on NCSA's ISL OpenStack. To mock up a zone or 'data grid' managed by iRODS, we set up an ICAT server, a resource server (this is a data storage resource that does not run the central database), and a client host."
"Save iRODS installations/servers as docker images We install and configure iRODS servers (an ICAT server, a resource server, a client host) in docker and make images, pushing the results to a docker hub repository. ",4,DM-2336,datamanagement,save irods installation server docker image install configure irods server icat server resource server client host docker image push result docker hub repository,"Save iRODS installations/servers as docker images We install and configure iRODS servers (an ICAT server, a resource server, a client host) in docker and make images, pushing the results to a docker hub repository."
"Reprise SDRP processing metrics In support of an SDRP-based science talk of Yusra AlSayyad, we spent some cycles gathering/summarizing processing middleware results and metrics from the US side of processing of the Split DRP.  This information from notes, logs, databases, etc provided contextual information on the processing campaign that produced the SDRP science results. ",2,DM-2340,datamanagement,reprise sdrp process metric support sdrp base science talk yusra alsayyad spend cycle gathering summarize processing middleware result metric processing split drp information note log database etc provide contextual information processing campaign produce sdrp science result,"Reprise SDRP processing metrics In support of an SDRP-based science talk of Yusra AlSayyad, we spent some cycles gathering/summarizing processing middleware results and metrics from the US side of processing of the Split DRP. This information from notes, logs, databases, etc provided contextual information on the processing campaign that produced the SDRP science results."
Use parallel ssh to manage Qserv on IN2P3 cluster IN2P3 sysadmin won't manage Qserv through puppet. So Qserv team has to provide ssh scripts to do this.  ,5,DM-2341,datamanagement,use parallel ssh manage qserv in2p3 cluster in2p3 sysadmin will manage qserv puppet qserv team provide ssh script,Use parallel ssh to manage Qserv on IN2P3 cluster IN2P3 sysadmin won't manage Qserv through puppet. So Qserv team has to provide ssh scripts to do this.
"(In)equality semantics of Coords are confusing Viz:  {code} In [1]: from lsst.afw.coord import Coord In [2]: c1 = Coord(""11:11:11"", ""22:22:22"") In [3]: c1 == c1, c1 != c1 Out[3]: (True, False) In [4]: c2 = Coord(""33:33:33"", ""44:44:44"") In [5]: c1 == c2, c1 != c2 Out[5]: (False, True) In [6]: c3 = Coord(""11:11:11"", ""22:22:22"") In [7]: c1 == c3, c1 != c3 Out[7]: (True, True) {code}  {{c1}} is simultaneously equal to *and* not equal to {{c3}}!",1,DM-2347,datamanagement,"in)equality semantic coords confuse viz code lsst.afw.coord import coord c1 coord(""11:11:11 22:22:22 c1 c1 c1 c1 out[3 true false c2 coord(""33:33:33 44:44:44 c1 c2 c1 c2 out[5 false true c3 coord(""11:11:11 22:22:22 c1 c3 c1 c3 out[7 true true code c1 simultaneously equal equal c3","(In)equality semantics of Coords are confusing Viz: {code} In [1]: from lsst.afw.coord import Coord In [2]: c1 = Coord(""11:11:11"", ""22:22:22"") In [3]: c1 == c1, c1 != c1 Out[3]: (True, False) In [4]: c2 = Coord(""33:33:33"", ""44:44:44"") In [5]: c1 == c2, c1 != c2 Out[5]: (False, True) In [6]: c3 = Coord(""11:11:11"", ""22:22:22"") In [7]: c1 == c3, c1 != c3 Out[7]: (True, True) {code} {{c1}} is simultaneously equal to *and* not equal to {{c3}}!"
useValueEquality and usePointerEquality fail to fail These SWIG macros return a class instead of raising an exception instance when the equality operation fails.,1,DM-2348,datamanagement,usevalueequality usepointerequality fail fail swig macro return class instead raise exception instance equality operation fail,useValueEquality and usePointerEquality fail to fail These SWIG macros return a class instead of raising an exception instance when the equality operation fails.
"Add unit tests to SchemaToMeta Add unit tests, also improve variable names as suggested by K-T in comments in DM-2139",1,DM-2349,datamanagement,add unit test schematometa add unit test improve variable name suggest comment dm-2139,"Add unit tests to SchemaToMeta Add unit tests, also improve variable names as suggested by K-T in comments in DM-2139"
Identify the hardware resources needed at NCSA for short term development  Supply the hardware resources needed at NCSA for short term development. It is captured in DM-2327  ,1,DM-2356,datamanagement,identify hardware resource need ncsa short term development supply hardware resource need ncsa short term development capture dm-2327,Identify the hardware resources needed at NCSA for short term development Supply the hardware resources needed at NCSA for short term development. It is captured in DM-2327
"make PixelFlagsAlgorithm fully configurable PixelFlagsAlgorithm currently hard-codes the mask planes it considers.  This should be fully configurable instead.  It also overloads the ""edge"" flag to mean both ""EDGE mask plane was set"" and ""centroid was off the edge of the image"".  These should be different flags.  We may also want to have this algorithm use SafeCentroidExtractor.'  Finally, the algorithm is woefully undertested.",2,DM-2357,datamanagement,pixelflagsalgorithm fully configurable pixelflagsalgorithm currently hard code mask plane consider fully configurable instead overload edge flag mean edge mask plane set centroid edge image different flag want algorithm use safecentroidextractor finally algorithm woefully undertested,"make PixelFlagsAlgorithm fully configurable PixelFlagsAlgorithm currently hard-codes the mask planes it considers. This should be fully configurable instead. It also overloads the ""edge"" flag to mean both ""EDGE mask plane was set"" and ""centroid was off the edge of the image"". These should be different flags. We may also want to have this algorithm use SafeCentroidExtractor.' Finally, the algorithm is woefully undertested."
standardize handling of missing peaks in centroiders GaussianCentroid has a NO_PEAK flag that it sets when there is no Peak to use as an input.  SdssCentroid does not.  This behavior should be standardized.  Maybe we should use SafeCentroidExtractor here?,1,DM-2358,datamanagement,standardize handling miss peak centroider gaussiancentroid no_peak flag set peak use input sdsscentroid behavior standardize maybe use safecentroidextractor,standardize handling of missing peaks in centroiders GaussianCentroid has a NO_PEAK flag that it sets when there is no Peak to use as an input. SdssCentroid does not. This behavior should be standardized. Maybe we should use SafeCentroidExtractor here?
"RGB code introduces dependency on matplotlib While the new RGB code looks like it's just calling NumPy, NumPy is actually delegating to matplotlib under the hood when it writes RGB(A) arrays.  It also turns out that code is broken in matplotlib prior to 1.3.1 (though that shouldn't be a problem for anyone but those who - like me - are trying to use slightly older system Python packages).  I think think this means we should add an optional dependency on matplotlib to the afw table file, and condition the running of the test code on matplotlib's presence (and, ideally, having the right version).  I'm happy to do this myself (since I'm probably the only one affected by it right now).",1,DM-2363,datamanagement,rgb code introduce dependency matplotlib new rgb code look like call numpy numpy actually delegate matplotlib hood write array turn code break matplotlib prior 1.3.1 problem like try use slightly old system python package think think mean add optional dependency matplotlib afw table file condition running test code matplotlib presence ideally have right version happy probably affect right,"RGB code introduces dependency on matplotlib While the new RGB code looks like it's just calling NumPy, NumPy is actually delegating to matplotlib under the hood when it writes RGB(A) arrays. It also turns out that code is broken in matplotlib prior to 1.3.1 (though that shouldn't be a problem for anyone but those who - like me - are trying to use slightly older system Python packages). I think think this means we should add an optional dependency on matplotlib to the afw table file, and condition the running of the test code on matplotlib's presence (and, ideally, having the right version). I'm happy to do this myself (since I'm probably the only one affected by it right now)."
"Revisit the choice of using flask We should quickly revisit if flask is the right choice for us.  Related: reportedly, our simple flask-based webserver is using more CPU in an idle state than expected. It might be useful to profile things, and look into that. ",1,DM-2364,datamanagement,revisit choice flask quickly revisit flask right choice relate reportedly simple flask base webserver cpu idle state expect useful profile thing look,"Revisit the choice of using flask We should quickly revisit if flask is the right choice for us. Related: reportedly, our simple flask-based webserver is using more CPU in an idle state than expected. It might be useful to profile things, and look into that."
"run lsstswBuild.sh in a clean sandbox The ""driver"" script, lsstswBuild.sh, used by the buildbot slave on lsst-dev to initiate a ""CI run"" has a number of environment assumptions (binaries in the $PATH, paths to various components, hostnames, etc.).  This prevents it from [easily] being invoked on any other host.  As lsstswBuild.sh builds a number of packages that are not in the lsst_distrib product, the os level dependencies for these other products need to be determined.  In addition, the current version of lsstswBuild.sh and related scripts on lsst-dev are not version controlled.",8,DM-2367,datamanagement,run lsstswbuild.sh clean sandbox driver script lsstswbuild.sh buildbot slave lsst dev initiate ci run number environment assumption binary path path component hostname etc prevent easily invoke host lsstswbuild.sh build number package lsst_distrib product os level dependency product need determine addition current version lsstswbuild.sh related script lsst dev version control,"run lsstswBuild.sh in a clean sandbox The ""driver"" script, lsstswBuild.sh, used by the buildbot slave on lsst-dev to initiate a ""CI run"" has a number of environment assumptions (binaries in the $PATH, paths to various components, hostnames, etc.). This prevents it from [easily] being invoked on any other host. As lsstswBuild.sh builds a number of packages that are not in the lsst_distrib product, the os level dependencies for these other products need to be determined. In addition, the current version of lsstswBuild.sh and related scripts on lsst-dev are not version controlled."
"Move QuerySession::_stmtParallel from query::SelectStmtPtrVector to query::SelectStmtPtr QuerySession::_stmtParallel is a vector but it seems only it's first element is used, so storing it in a vector doesn't seem necessary.  Code can be easily simplified here. This should lead to mode understandable code.  QuerySession public members and method comments could also be improved here.",4,DM-2370,datamanagement,querysession::_stmtparallel query::selectstmtptrvector query::selectstmtptr querysession::_stmtparallel vector element store vector necessary code easily simplify lead mode understandable code querysession public member method comment improve,"Move QuerySession::_stmtParallel from query::SelectStmtPtrVector to query::SelectStmtPtr QuerySession::_stmtParallel is a vector but it seems only it's first element is used, so storing it in a vector doesn't seem necessary. Code can be easily simplified here. This should lead to mode understandable code. QuerySession public members and method comments could also be improved here."
run lsstswBuild.sh under Jenkins on EL6 * Demonstrate lsstswBuild.sh being invoked by jenkins on EL6 (same OS as lsst-dev). * Experiment with a single build slave attached to a jenkins master * Investigate configuration management of jenkins builds.,6,DM-2371,datamanagement,run lsstswbuild.sh jenkins el6 demonstrate lsstswbuild.sh invoke jenkin el6 os lsst dev experiment single build slave attach jenkins master investigate configuration management jenkins build,run lsstswBuild.sh under Jenkins on EL6 * Demonstrate lsstswBuild.sh being invoked by jenkins on EL6 (same OS as lsst-dev). * Experiment with a single build slave attached to a jenkins master * Investigate configuration management of jenkins builds.
"Improve logger use in qserv Qserv logger must be easily configurable. Next technique, based on log4cxx documentation allows to do it easily.  Example:  In QuerySession.cc, initialize a static logger: {code:c++} namespace lsst { namespace qserv { namespace qproc {  LOG_LOGGER QuerySession::_logger = LOG_GET(""lsst.qserv.qproc.QuerySession""); {code}  then use it in Query session member functions:  {code:c++}         if (LOG_CHECK_LVL(_logger, LOG_LVL_DEBUG)) {             std::ostringstream stream;             _showFinal(stream);             LOGF(_logger, LOG_LVL_DEBUG, ""Query Plugins applied:\n %1%"" % stream.str());         } {code}  And use log4cxx.property to easily configure, AT RUNTIME, logging for each Qserv module class:  {code} # logger for all module will inherit this one log4j.logger.lsst.qserv=ERROR # this could be generalized to all Qserv modules: log4j.logger.lsst.qserv.qproc=INFO # can also be done at the class level for advanced debugging #log4j.logger.lsst.qserv.qproc.QuerySession=DEBUG {code}  And then in the log: {code} /home/qserv/qserv-run/2015_03/var/log/qserv-czar.log:0319 17:08:48.786 [0x7f208da93740] DEBUG lsst.qserv.qproc.QuerySession (build/qproc/QuerySession.cc:118) - Query Plugins applied: {code}  This proposal is a draft and should be improved before implementing it.",8,DM-2373,datamanagement,"improve logg use qserv qserv logger easily configurable technique base log4cxx documentation allow easily example querysession.cc initialize static logger code c++ namespace lsst namespace qserv namespace qproc log_logger querysession::_logg log_get(""lsst.qserv.qproc querysession code use query session member function code c++ log_check_lvl(_logger log_lvl_debug std::ostringstream stream showfinal(stream logf(_logg log_lvl_debug query plugins applied:\n stream.str code use log4cxx.property easily configure runtime log qserv module class code logg module inherit log4j.logger.lsst.qserv error generalize qserv module log4j.logger.lsst.qserv.qproc info class level advanced debug log4j.logger.lsst.qserv.qproc querysession debug code log code /home qserv qserv run/2015_03 var log qserv czar.log:0319 17:08:48.786 0x7f208da93740 debug lsst.qserv.qproc querysession build qproc querysession.cc:118 query plugins apply code proposal draft improve implement","Improve logger use in qserv Qserv logger must be easily configurable. Next technique, based on log4cxx documentation allows to do it easily. Example: In QuerySession.cc, initialize a static logger: {code:c++} namespace lsst { namespace qserv { namespace qproc { LOG_LOGGER QuerySession::_logger = LOG_GET(""lsst.qserv.qproc.QuerySession""); {code} then use it in Query session member functions: {code:c++} if (LOG_CHECK_LVL(_logger, LOG_LVL_DEBUG)) { std::ostringstream stream; _showFinal(stream); LOGF(_logger, LOG_LVL_DEBUG, ""Query Plugins applied:\n %1%"" % stream.str()); } {code} And use log4cxx.property to easily configure, AT RUNTIME, logging for each Qserv module class: {code} # logger for all module will inherit this one log4j.logger.lsst.qserv=ERROR # this could be generalized to all Qserv modules: log4j.logger.lsst.qserv.qproc=INFO # can also be done at the class level for advanced debugging #log4j.logger.lsst.qserv.qproc.QuerySession=DEBUG {code} And then in the log: {code} /home/qserv/qserv-run/2015_03/var/log/qserv-czar.log:0319 17:08:48.786 [0x7f208da93740] DEBUG lsst.qserv.qproc.QuerySession (build/qproc/QuerySession.cc:118) - Query Plugins applied: {code} This proposal is a draft and should be improved before implementing it."
"evaluate NCSA proposal to investigate CEPH in the context of NCSA Integrated systems lab The integrated systems lab (ISL) is the orgianizational vehicle used to investigate pre-production technologies at NCSA.   Since  We still lack the ability to procure goods,  I evaluated and commented on an ISL proposal to investigate the CEPH file system for its properties as an alternative to the LSST baseline file system GPFS. ",1,DM-2375,datamanagement,evaluate ncsa proposal investigate ceph context ncsa integrated system lab integrate system lab isl orgianizational vehicle investigate pre production technology ncsa lack ability procure good evaluate comment isl proposal investigate ceph file system property alternative lsst baseline file system gpfs,"evaluate NCSA proposal to investigate CEPH in the context of NCSA Integrated systems lab The integrated systems lab (ISL) is the orgianizational vehicle used to investigate pre-production technologies at NCSA. Since We still lack the ability to procure goods, I evaluated and commented on an ISL proposal to investigate the CEPH file system for its properties as an alternative to the LSST baseline file system GPFS."
"revise and circulate data center requirements note Reconvert the ~10 TBD's  in the priori version of the note  — I’ve kept the stipulation that end of service-lifee stuff will leave these spaces  but added an appendix that “this is what the central space should provide”  My understanding is  there are now discussions on whether that central space will exist of not.   The requirements  can be promoted to center requirements no central space is evient. — The maximal average weight for a rack was computed from LDM-144 and is given.  — There are more cu ft estimates for  the need to dispose of dunnage and packing material. — TBD’s w.r.t overhead cabling are specificed. — The non- LSST requrements are in there, and have been as far as I am able to ascertain them from   champaign urbana.   Ron has been stating requirements as “rows”   I have never fixed row length  thinking  that is for the designer to do.   We’ve stated that rows are shareable, but racks are not. so what’s in the docs is definitive unless/until non LSST requirements can be stated in the same terms.     ""Shall support 16 racks for the NOAO tenant"". is what I had.  power requirements as per  the common space, becasue we want a maximally flexible space. ",2,DM-2376,datamanagement,revise circulate datum center requirement note reconvert tbd priori version note ve keep stipulation end service lifee stuff leave space add appendix central space provide understanding discussion central space exist requirement promote center requirement central space evient maximal average weight rack compute ldm-144 give cu ft estimate need dispose dunnage packing material tbd w.r.t overhead cable specifice non- lsst requrement far able ascertain champaign urbana ron state requirement row fix row length thinking designer ve state row shareable rack doc definitive non lsst requirement state term shall support 16 rack noao tenant power requirement common space becasue want maximally flexible space,"revise and circulate data center requirements note Reconvert the ~10 TBD's in the priori version of the note I ve kept the stipulation that end of service-lifee stuff will leave these spaces but added an appendix that this is what the central space should provide My understanding is there are now discussions on whether that central space will exist of not. The requirements can be promoted to center requirements no central space is evient. The maximal average weight for a rack was computed from LDM-144 and is given. There are more cu ft estimates for the need to dispose of dunnage and packing material. TBD s w.r.t overhead cabling are specificed. The non- LSST requrements are in there, and have been as far as I am able to ascertain them from champaign urbana. Ron has been stating requirements as rows I have never fixed row length thinking that is for the designer to do. We ve stated that rows are shareable, but racks are not. so what s in the docs is definitive unless/until non LSST requirements can be stated in the same terms. ""Shall support 16 racks for the NOAO tenant"". is what I had. power requirements as per the common space, becasue we want a maximally flexible space."
"Management Meetings -- Monday CAM meeting, Friday standup and infrastructure.  Internal NCSA group meeting,  Internal NCSA ""comp pol"" technical coordination meeting.  Screen existing candidate pool for likely people to fill opening,  Interviewed one person checked references + Misc.",3,DM-2377,datamanagement,management meetings monday cam meeting friday standup infrastructure internal ncsa group meeting internal ncsa comp pol technical coordination meeting screen exist candidate pool likely people fill opening interview person check reference misc,"Management Meetings -- Monday CAM meeting, Friday standup and infrastructure. Internal NCSA group meeting, Internal NCSA ""comp pol"" technical coordination meeting. Screen existing candidate pool for likely people to fill opening, Interviewed one person checked references + Misc."
"Retrieve HSC engineering data HSC data becomes public 18 months after it was taken, so data taken during commissioning are now available.  We would like to use this data for testing the LSST pipeline.  It needs to be downloaded from Japan.",2,DM-2380,datamanagement,retrieve hsc engineering datum hsc datum public 18 month take datum take commission available like use datum test lsst pipeline need download japan,"Retrieve HSC engineering data HSC data becomes public 18 months after it was taken, so data taken during commissioning are now available. We would like to use this data for testing the LSST pipeline. It needs to be downloaded from Japan."
Make sure the command-line parser warns loudly enough if no data found A user recently got confused when calling parseAndRun didn't call the task's run method. It turns out there was no data matching the specified data ID. Make sure this generates a loud and clear warning.,1,DM-2382,datamanagement,sure command line parser warn loudly datum find user recently get confuse call parseandrun task run method turn datum match specify data id sure generate loud clear warning,Make sure the command-line parser warns loudly enough if no data found A user recently got confused when calling parseAndRun didn't call the task's run method. It turns out there was no data matching the specified data ID. Make sure this generates a loud and clear warning.
migrate package deps from sandbox-stackbuild to a proper puppet module There is a growing list of known package dependencies in the sandbox-stackbuild repo and a need to use this information for independent environments (such as CI).  This list of packages should be lifted out into an independent puppet module that can be reused.,2,DM-2383,datamanagement,migrate package dep sandbox stackbuild proper puppet module grow list know package dependency sandbox stackbuild repo need use information independent environment ci list package lift independent puppet module reuse,migrate package deps from sandbox-stackbuild to a proper puppet module There is a growing list of known package dependencies in the sandbox-stackbuild repo and a need to use this information for independent environments (such as CI). This list of packages should be lifted out into an independent puppet module that can be reused.
Implement data loading in worker manager service This is a separate ticket for implementation of the data loading part of the worker management service (started in DM-2176). Some ideas and thoughts are outlined in that ticket.,6,DM-2385,datamanagement,implement datum loading worker manager service separate ticket implementation datum load worker management service start dm-2176 idea thought outline ticket,Implement data loading in worker manager service This is a separate ticket for implementation of the data loading part of the worker management service (started in DM-2176). Some ideas and thoughts are outlined in that ticket.
Build testQDisp.cc on ubuntu testQDisp.cc needs flags -lpthread -lboost_regex to build on ubuntu.,1,DM-2387,datamanagement,build testqdisp.cc ubuntu testqdisp.cc need flag -lpthread -lboost_regex build ubuntu,Build testQDisp.cc on ubuntu testQDisp.cc needs flags -lpthread -lboost_regex to build on ubuntu.
Errors need to be checked in UserQueryFactory from QuerySession objects UserQueryFactory doesn't check its QuerySession object for errors after setQuery. Thus it continues setting things up after the QuerySession knows the state is invalid.,1,DM-2390,datamanagement,error need check userqueryfactory querysession object userqueryfactory check querysession object error setquery continue set thing querysession know state invalid,Errors need to be checked in UserQueryFactory from QuerySession objects UserQueryFactory doesn't check its QuerySession object for errors after setQuery. Thus it continues setting things up after the QuerySession knows the state is invalid.
"Allow qserv-admin.py to delete a node Registered workers in CSS with qserv-admin.py are currently not able to be removed (no DELETE NODE type command). Also, changing node status from ACTIVE to INACTIVE needs to be fixed.",1,DM-2411,datamanagement,allow qserv-admin.py delete node register worker css qserv-admin.py currently able remove delete node type command change node status active inactive need fix,"Allow qserv-admin.py to delete a node Registered workers in CSS with qserv-admin.py are currently not able to be removed (no DELETE NODE type command). Also, changing node status from ACTIVE to INACTIVE needs to be fixed."
Change integration test user from root to qsmaster Currently integration tests use root account as default user - this should be changed to qsmaster for the future.,2,DM-2412,datamanagement,change integration test user root qsmaster currently integration test use root account default user change qsmaster future,Change integration test user from root to qsmaster Currently integration tests use root account as default user - this should be changed to qsmaster for the future.
investigate configuration management for jenkins The most popular Puppet module for managing Jenkin's {code}jenkinsci/puppet-jenkins{code} is able to create a master and build slaves but is missing the functionality to manage several master configuration options that otherwise require manual setup.  We need to investigate the difficulty of managing a Jenkin's master configuration values in an idempotent manner.,4,DM-2414,datamanagement,investigate configuration management jenkin popular puppet module manage jenkin code}jenkinsci puppet jenkins{code able create master build slave miss functionality manage master configuration option require manual setup need investigate difficulty manage jenkin master configuration value idempotent manner,investigate configuration management for jenkins The most popular Puppet module for managing Jenkin's {code}jenkinsci/puppet-jenkins{code} is able to create a master and build slaves but is missing the functionality to manage several master configuration options that otherwise require manual setup. We need to investigate the difficulty of managing a Jenkin's master configuration values in an idempotent manner.
"convert Statistics to use ndarray natively The Statistics class predates ndarray, and hence uses some hackish Image-class emulators/wrappers to deal with 1-d arrays.  It'd clean things up considerably to have it use ndarray under the hood, and have the Image-based interfaces interact via their ndarray views.",3,DM-2415,datamanagement,convert statistics use ndarray natively statistics class predate ndarray use hackish image class emulator wrapper deal array clean thing considerably use ndarray hood image base interface interact ndarray view,"convert Statistics to use ndarray natively The Statistics class predates ndarray, and hence uses some hackish Image-class emulators/wrappers to deal with 1-d arrays. It'd clean things up considerably to have it use ndarray under the hood, and have the Image-based interfaces interact via their ndarray views."
"Data loader script crashes trying to create chunk table Vaikunth discovered a bug in data loader when trying to load a data into Object table: {noformat} [CRITICAL] root: Exception occured: Table 'Object_7480' already exists Traceback (most recent call last):   File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 318, in <module>     sys.exit(loader.run())   File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 254, in run     self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 171, in load     return self._run(database, table, schema, data)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 209, in _run     self._loadData(database, table, files)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 586, in _loadData     self._loadChunkedData(database, table)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 653, in _loadChunkedData     self._makeChunkAndOverlapTable(conn, database, table, chunkId)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 727, in _makeChunkAndOverlapTable     cursor.execute(q)   File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 176, in execute     if not self._defer_warnings: self._warning_check()   File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 92, in _warning_check     warn(w[-1], self.Warning, 3) Warning: Table 'Object_7480' already exists {noformat} It looks like I did not do enough testing after my recent improvement in creating chunk tables. It tries to create the chunk table with ""CREATE TABLE IF NOT EXISTS ..."" but that actually generates ""warning exception"" on mysql side when table is already there. Need to catch this exception and ignore it.",1,DM-2417,datamanagement,datum loader script crash try create chunk table vaikunth discover bug datum loader try load datum object table noformat critical root exception occur table object_7480 exist traceback recent file /usr local home vaikunth src qserv bin qserv data loader.py line 318 sys.exit(loader.run file /usr local home vaikunth src qserv bin qserv data loader.py line 254 run self.loader.load(self.args.database self.args.table self.args.schema self.args.data file /usr local home vaikunth src qserv lib python lsst qserv admin dataloader.py line 171 load return self._run(database table schema datum file /usr local home vaikunth src qserv lib python lsst qserv admin dataloader.py line 209 run self._loaddata(database table file file /usr local home vaikunth src qserv lib python lsst qserv admin dataloader.py line 586 loaddata self._loadchunkeddata(database table file /usr local home vaikunth src qserv lib python lsst qserv admin dataloader.py line 653 loadchunkeddata self._makechunkandoverlaptable(conn database table chunkid file /usr local home vaikunth src qserv lib python lsst qserv admin dataloader.py line 727 makechunkandoverlaptable cursor.execute(q file build bdist.linux x86_64 egg mysqldb cursors.py line 176 execute self._defer_warning self._warning_check file build bdist.linux x86_64 egg mysqldb cursors.py line 92 warning_check warn(w[-1 self warning warning table object_7480 exist noformat look like testing recent improvement create chunk table try create chunk table create table exists actually generate warn exception mysql table need catch exception ignore,"Data loader script crashes trying to create chunk table Vaikunth discovered a bug in data loader when trying to load a data into Object table: {noformat} [CRITICAL] root: Exception occured: Table 'Object_7480' already exists Traceback (most recent call last): File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 318, in  sys.exit(loader.run()) File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 254, in run self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data) File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 171, in load return self._run(database, table, schema, data) File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 209, in _run self._loadData(database, table, files) File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 586, in _loadData self._loadChunkedData(database, table) File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 653, in _loadChunkedData self._makeChunkAndOverlapTable(conn, database, table, chunkId) File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 727, in _makeChunkAndOverlapTable cursor.execute(q) File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 176, in execute if not self._defer_warnings: self._warning_check() File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 92, in _warning_check warn(w[-1], self.Warning, 3) Warning: Table 'Object_7480' already exists {noformat} It looks like I did not do enough testing after my recent improvement in creating chunk tables. It tries to create the chunk table with ""CREATE TABLE IF NOT EXISTS ..."" but that actually generates ""warning exception"" on mysql side when table is already there. Need to catch this exception and ignore it."
Implement authentication mechanism for worker management service We need some reasonable security for access to new worker management service. It should be lightweight and not depend on complex things that require infrastructure. Something based on a shared secret should be adequate for our immediate needs and likely for the long term.,4,DM-2419,datamanagement,implement authentication mechanism worker management service need reasonable security access new worker management service lightweight depend complex thing require infrastructure base share secret adequate immediate need likely long term,Implement authentication mechanism for worker management service We need some reasonable security for access to new worker management service. It should be lightweight and not depend on complex things that require infrastructure. Something based on a shared secret should be adequate for our immediate needs and likely for the long term.
"Document API for worker management service New worker management service exposes its API as an interface to RESTful web service. Many or all ""methods"" will be wrapped into some sort of Python API, but it would still be useful to document every web service ""methods"" independently. There is basic documentation in the design document (https://dev.lsstcorp.org/trac/wiki/db/Qserv/WMGRDesign), this needs to be extended with detailed description of what those methods do and what kind of data they accept and return.  This story involves selecting the right tool.",4,DM-2420,datamanagement,document api worker management service new worker management service expose api interface restful web service method wrap sort python api useful document web service method independently basic documentation design document https://dev.lsstcorp.org/trac/wiki/db/qserv/wmgrdesign need extend detailed description method kind datum accept return story involve select right tool,"Document API for worker management service New worker management service exposes its API as an interface to RESTful web service. Many or all ""methods"" will be wrapped into some sort of Python API, but it would still be useful to document every web service ""methods"" independently. There is basic documentation in the design document (https://dev.lsstcorp.org/trac/wiki/db/Qserv/WMGRDesign), this needs to be extended with detailed description of what those methods do and what kind of data they accept and return. This story involves selecting the right tool."
"Improve support for Python modules in Scons it seems we have two tools to manage python modules:  - site_scons/pytarget.py and - site_scons/site_tools/pymod.py (grep for InstallPythonModule) used by admin tools. We could unify this, isn't it?",1,DM-2421,datamanagement,improve support python module scon tool manage python module site_scon pytarget.py site_scon site_tool pymod.py grep installpythonmodule admin tool unify,"Improve support for Python modules in Scons it seems we have two tools to manage python modules: - site_scons/pytarget.py and - site_scons/site_tools/pymod.py (grep for InstallPythonModule) used by admin tools. We could unify this, isn't it?"
"Weighting in photometric calibration is incorrect Dominique points out that the zero point calibration uses errors not inverse errors to calculate the zero point.  git annotate reveals: bq. 24c9149f python/lsst/meas/photocal/PhotoCal.py (Robert Lupton the Good 2010-12-13 05:03:12 +0000 353)     return np.average(dmag, weights=dmagErr), np.std(dmag, ddof=1), len(dmag)  Please fix this.  At the same time, we should add a config parameter to soften the errors. ",1,DM-2423,datamanagement,weighting photometric calibration incorrect dominique point zero point calibration use error inverse error calculate zero point git annotate reveal bq 24c9149f python lsst meas photocal photocal.py robert lupton good 2010 12 13 05:03:12 +0000 353 return np.average(dmag weight dmagerr np.std(dmag ddof=1 len(dmag fix time add config parameter soften error,"Weighting in photometric calibration is incorrect Dominique points out that the zero point calibration uses errors not inverse errors to calculate the zero point. git annotate reveals: bq. 24c9149f python/lsst/meas/photocal/PhotoCal.py (Robert Lupton the Good 2010-12-13 05:03:12 +0000 353) return np.average(dmag, weights=dmagErr), np.std(dmag, ddof=1), len(dmag) Please fix this. At the same time, we should add a config parameter to soften the errors."
"Create transitional duplicate of Span One challenge in switching from {{PTR(Span)}} to {{Span}} in {{Footprint}} is that Swig won't generate wrappers for {{std::vector<Span>}} (or any other container) if {{%shared_ptr(Span)}} is used anywhere else in the codebase.  So, to allow both the old {{Footprint}} class and the new {{SpanRegion}} to coexist (temporily), we need to have two {{Span}} classes, one wrapped with {{%shared_ptr}} and one wrapped without it.  Since we don't want to disrupt the old {{Footprint}} class yet, we should call the new Span something else, and make it the one that's wrapped without {{%shared_ptr}}.  This ticket can be considered complete once we have a unit test demostrating a usable Swig-wrapped {{std::vector<NewSpan>}} while all old {{Footprint}} tests continue to pass.",1,DM-2424,datamanagement,create transitional duplicate span challenge switch ptr(span span footprint swig will generate wrapper std::vector container shared_ptr(span codebase allow old footprint class new spanregion coexist temporily need span class wrap shared_ptr wrap want disrupt old footprint class new span wrap shared_ptr ticket consider complete unit test demostrate usable swig wrap std::vector old footprint test continue pass,"Create transitional duplicate of Span One challenge in switching from {{PTR(Span)}} to {{Span}} in {{Footprint}} is that Swig won't generate wrappers for {{std::vector}} (or any other container) if {{%shared_ptr(Span)}} is used anywhere else in the codebase. So, to allow both the old {{Footprint}} class and the new {{SpanRegion}} to coexist (temporily), we need to have two {{Span}} classes, one wrapped with {{%shared_ptr}} and one wrapped without it. Since we don't want to disrupt the old {{Footprint}} class yet, we should call the new Span something else, and make it the one that's wrapped without {{%shared_ptr}}. This ticket can be considered complete once we have a unit test demostrating a usable Swig-wrapped {{std::vector}} while all old {{Footprint}} tests continue to pass."
"Implement SpanRegion core functionality Implement the core of the SpanRegion class, as prototyped in RFC-37.  This includes the following:  - The private implementation object and copy-on-write utilities (see Schema for an example of copy-on-write, but note that SpanRegion's implementation object can be private, while Schema's is not).  - All STL container methods and typedefs, and their Pythonic counterparts.  - All constructors and assignment operators, except for SpanRegionBuilder.  This includes the ability to detect and fix overlapping Spans.  - All simple accessors.  - {{isContiguous()}}  - The shift and clip methods.",6,DM-2425,datamanagement,implement spanregion core functionality implement core spanregion class prototype rfc-37 include following private implementation object copy write utility schema example copy write note spanregion implementation object private schema stl container method typedef pythonic counterpart constructor assignment operator spanregionbuilder include ability detect fix overlap spans simple accessor iscontiguous shift clip method,"Implement SpanRegion core functionality Implement the core of the SpanRegion class, as prototyped in RFC-37. This includes the following: - The private implementation object and copy-on-write utilities (see Schema for an example of copy-on-write, but note that SpanRegion's implementation object can be private, while Schema's is not). - All STL container methods and typedefs, and their Pythonic counterparts. - All constructors and assignment operators, except for SpanRegionBuilder. This includes the ability to detect and fix overlapping Spans. - All simple accessors. - {{isContiguous()}} - The shift and clip methods."
"Implement SpanRegion+ellipse operations Implement the following SpanRegion operations:  - Construct from an ellipse - note geom::ellipses::PixelRegion; this should do most of the work.  - Compute centroid - see old Footprint implementation  - Compute shape (quadrupole moments) - see old Footprint implementation  One complication here is that this will introduce a circular dependency between afw::geom and afw::geom::ellipses.  That's easy to address at the C++ level, but it's tricky in Python (which package imports the other?)  I'll be emailing dm-devel shortly to start a discussion on how to address this problem.",2,DM-2426,datamanagement,implement spanregion+ellipse operation implement follow spanregion operation construct ellipse note geom::ellipses::pixelregion work compute centroid old footprint implementation compute shape quadrupole moment old footprint implementation complication introduce circular dependency afw::geom afw::geom::ellipse easy address c++ level tricky python package import email dm devel shortly start discussion address problem,"Implement SpanRegion+ellipse operations Implement the following SpanRegion operations: - Construct from an ellipse - note geom::ellipses::PixelRegion; this should do most of the work. - Compute centroid - see old Footprint implementation - Compute shape (quadrupole moments) - see old Footprint implementation One complication here is that this will introduce a circular dependency between afw::geom and afw::geom::ellipses. That's easy to address at the C++ level, but it's tricky in Python (which package imports the other?) I'll be emailing dm-devel shortly to start a discussion on how to address this problem."
"Implement SpanRegion applyFunctor methods Implement methods that apply arbitrary functors to pixels within a SpanRegion, as described on RFC-37.  The only tricky part of this implementation will be the ""traits"" classes that allow different target objects to interpreted differently.  I'd be happy to consult on this; I have a rough idea in my head, but it needs to be fleshed out.",3,DM-2427,datamanagement,implement spanregion applyfunctor method implement method apply arbitrary functor pixels spanregion describe rfc-37 tricky implementation trait class allow different target object interpret differently happy consult rough idea head need flesh,"Implement SpanRegion applyFunctor methods Implement methods that apply arbitrary functors to pixels within a SpanRegion, as described on RFC-37. The only tricky part of this implementation will be the ""traits"" classes that allow different target objects to interpreted differently. I'd be happy to consult on this; I have a rough idea in my head, but it needs to be fleshed out."
"Add aperture corrections to meas_extensions_photometryKron When transitioning {{meas_extensions_photometryKron}} to the new measurement framework, aperture correction was omitted pending the completion of DM-85. It needs to be re-enabled when that epic is complete.",1,DM-2429,datamanagement,add aperture correction meas_extensions_photometrykron transition meas_extensions_photometrykron new measurement framework aperture correction omit pende completion dm-85 need enable epic complete,"Add aperture corrections to meas_extensions_photometryKron When transitioning {{meas_extensions_photometryKron}} to the new measurement framework, aperture correction was omitted pending the completion of DM-85. It needs to be re-enabled when that epic is complete."
"Make qserv server-side log messages more standard Qserv server-side Python logging appears to mostly use a common format: ""{{%(asctime)s %(name)s %(levelname)s: %(message)s}}"".  It also mostly uses a common date format: ""{{%m/%d/%Y %I:%M:%S}}"".  But I see instances of: * ""{{%(asctime)s %(levelname)s %(message)s}}"" * ""{{%(asctime)s - %(name)s - %(levelname)s - %(message)s}}"" *  ""{{%(asctime)s \{%(pathname)s:%(lineno)d\} %(levelname)s %(message)s}}"" * and now, after DM-2176, ""{{%(asctime)s \[PID:%(process)d\] \[%(levelname)s\] (%(funcName)s() at %(filename)s:%(lineno)d) %(name)s: %(message)s}}""  Unless these are used in very different contexts, it will aid automated log processing for them to be more standardized.  In addition, the date format is unacceptable as it does not use RFC 3339 (ISO8601) format and does not include a timezone indicator (which means the default {{datefmt}} is insufficient).  This must be fixed.  See also DM-1203.",1,DM-2430,datamanagement,qserv server log message standard qserv server python logging appear use common format asctime)s name)s levelname)s message)s use common date format m/%d/%y i:%m:%s instance asctime)s levelname)s message)s asctime)s name)s levelname)s message)s asctime)s \{%(pathname)s:%(lineno)d\ levelname)s message)s dm-2176 asctime)s \[pid:%(process)d\ \[%(levelname)s\ funcname)s filename)s:%(lineno)d name)s message)s different context aid automate log processing standardized addition date format unacceptable use rfc 3339 iso8601 format include timezone indicator mean default datefmt insufficient fix dm-1203,"Make qserv server-side log messages more standard Qserv server-side Python logging appears to mostly use a common format: ""{{%(asctime)s %(name)s %(levelname)s: %(message)s}}"". It also mostly uses a common date format: ""{{%m/%d/%Y %I:%M:%S}}"". But I see instances of: * ""{{%(asctime)s %(levelname)s %(message)s}}"" * ""{{%(asctime)s - %(name)s - %(levelname)s - %(message)s}}"" * ""{{%(asctime)s \{%(pathname)s:%(lineno)d\} %(levelname)s %(message)s}}"" * and now, after DM-2176, ""{{%(asctime)s \[PID:%(process)d\] \[%(levelname)s\] (%(funcName)s() at %(filename)s:%(lineno)d) %(name)s: %(message)s}}"" Unless these are used in very different contexts, it will aid automated log processing for them to be more standardized. In addition, the date format is unacceptable as it does not use RFC 3339 (ISO8601) format and does not include a timezone indicator (which means the default {{datefmt}} is insufficient). This must be fixed. See also DM-1203."
"Fork GREAT3 sim code and integrate with LSST stack Get the GREAT3 simulation code running with LSST-provided third-party packages of Python, GalSim, etc, and figure out where we're going to put our modified scripts on GitHub:  - Do we just put things in a fork of the great3 repo, or do we have other repos layered on top of a fork of the great3 repo?  (I think probably the latter, but we should determine how many repos, and for what purposes.)  - Where in GitHub space do we put them (lsst?  lsst-dm? user spaces?)  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).",4,DM-2431,datamanagement,fork great3 sim code integrate lsst stack great3 simulation code run lsst provide party package python galsim etc figure go modify script github thing fork great3 repo repos layered fork great3 repo think probably determine repos purpose github space lsst lsst dm user space issue replace dm-1132 planning stand detailed issue,"Fork GREAT3 sim code and integrate with LSST stack Get the GREAT3 simulation code running with LSST-provided third-party packages of Python, GalSim, etc, and figure out where we're going to put our modified scripts on GitHub: - Do we just put things in a fork of the great3 repo, or do we have other repos layered on top of a fork of the great3 repo? (I think probably the latter, but we should determine how many repos, and for what purposes.) - Where in GitHub space do we put them (lsst? lsst-dm? user spaces?) This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues)."
Increase postage stamp size in simulation scripts The GREAT3 simulations have a fixed postage stamp size (though this may differ between branches).  A first step at modifying the simulation scripts to meet our needs would be to try to change the postage stamp.  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).,2,DM-2432,datamanagement,increase postage stamp size simulation script great3 simulation fix postage stamp size differ branch step modify simulation script meet need try change postage stamp issue replace dm-1132 planning stand detailed issue,Increase postage stamp size in simulation scripts The GREAT3 simulations have a fixed postage stamp size (though this may differ between branches). A first step at modifying the simulation scripts to meet our needs would be to try to change the postage stamp. This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).
"Create simulation script with different constant PSF per galaxy. Modify the GREAT3 simulation scripts to create a branch in which each galaxy gets a different constant PSF, rather than one constant PSF per subfield or a spatially-varying PSF that spans multiple subfields.  This could be done by modifying the control/ground/constant branch or the variable-psf/ground/constant branch, or creating an entirely new branch, or anything else (since we don't actually need multiple branches in our simulations).  At this point, the source of the PSFs doesn't really matter - as long as we have a class that can provide a different one to every image.  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).",6,DM-2433,datamanagement,create simulation script different constant psf galaxy modify great3 simulation script create branch galaxy get different constant psf constant psf subfield spatially vary psf span multiple subfield modify control ground constant branch variable psf ground constant branch create entirely new branch actually need multiple branch simulation point source psf matter long class provide different image issue replace dm-1132 planning stand detailed issue,"Create simulation script with different constant PSF per galaxy. Modify the GREAT3 simulation scripts to create a branch in which each galaxy gets a different constant PSF, rather than one constant PSF per subfield or a spatially-varying PSF that spans multiple subfields. This could be done by modifying the control/ground/constant branch or the variable-psf/ground/constant branch, or creating an entirely new branch, or anything else (since we don't actually need multiple branches in our simulations). At this point, the source of the PSFs doesn't really matter - as long as we have a class that can provide a different one to every image. This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues)."
Draw simulated PSFs from a library of on-disk files Modify the simulation code to draw PSFs at random from a library of on-disk files (whose format and on-disk layout should be specified here).  The PSFs chosen should be deterministic via a random number generator seed specified via config.  This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).,4,DM-2434,datamanagement,draw simulate psf library disk file modify simulation code draw psf random library disk file format disk layout specify psf choose deterministic random number generator seed specify config issue replace dm-1132 planning stand detailed issue,Draw simulated PSFs from a library of on-disk files Modify the simulation code to draw PSFs at random from a library of on-disk files (whose format and on-disk layout should be specified here). The PSFs chosen should be deterministic via a random number generator seed specified via config. This is one of several issues that together will replace DM-1132 (which was just a planning stand-in for these more detailed issues).
"Reading an Exposure from disk aborts if the Psf is of an unknown type Attempting to read an Exposure (in this case via the butler) fails if the PSF class isn't available.  An exception would be reasonable, but an assertion failure is not.  Running the attached script on tiger-sumire with bq. setup python anaconda; setup -T v10_1_rc2 lsst_apps; setup -j distEst -t HSC; setup -j -r ~/LSST/obs/subaru  {code}  WARNING: Could not read PSF; setting to null: PersistableFactory with name 'PsfexPsf' not found, and import of module 'lsst.meas.extensions.psfex' failed (possibly because Python calls were not available from C++). {0}; loading object with id=4, name='PsfexPsf' {1}; loading object with id=28, name='CoaddPsf' {2} python: src/table/io/InputArchive.cc:109: boost::shared_ptr<lsst::afw::table::io::Persistable> lsst::afw::table::io::InputArchive::Impl::get(int, const lsst::afw::table::io::InputArchive&): Assertion `r.first->second' failed. Aborted {code}",1,DM-2435,datamanagement,read exposure disk abort psf unknown type attempt read exposure case butler fail psf class available exception reasonable assertion failure run attached script tiger sumire bq setup python anaconda setup -t v10_1_rc2 lsst_apps setup distest hsc setup ~/lsst obs subaru code warning read psf set null persistablefactory psfexpsf find import module lsst.meas.extensions.psfex fail possibly python call available c++ load object id=4 name='psfexpsf load object id=28 name='coaddpsf python src table io inputarchive.cc:109 boost::shared_ptr lsst::afw::table::io::inputarchive::impl::get(int const lsst::afw::table::io::inputarchive assertion r.first->second fail aborted code,"Reading an Exposure from disk aborts if the Psf is of an unknown type Attempting to read an Exposure (in this case via the butler) fails if the PSF class isn't available. An exception would be reasonable, but an assertion failure is not. Running the attached script on tiger-sumire with bq. setup python anaconda; setup -T v10_1_rc2 lsst_apps; setup -j distEst -t HSC; setup -j -r ~/LSST/obs/subaru {code} WARNING: Could not read PSF; setting to null: PersistableFactory with name 'PsfexPsf' not found, and import of module 'lsst.meas.extensions.psfex' failed (possibly because Python calls were not available from C++). {0}; loading object with id=4, name='PsfexPsf' {1}; loading object with id=28, name='CoaddPsf' {2} python: src/table/io/InputArchive.cc:109: boost::shared_ptr lsst::afw::table::io::InputArchive::Impl::get(int, const lsst::afw::table::io::InputArchive&): Assertion `r.first->second' failed. Aborted {code}"
"Cherry-pick ""fix makeRGB so it can replace saturated pixels and produce an image"" from HSC HSC-1196 includes fixes and test cases for {{afw}}. After review on HSC, they should be checked/merged to LSST.",1,DM-2436,datamanagement,cherry pick fix makergb replace saturated pixel produce image hsc hsc-1196 include fix test case afw review hsc check merge lsst,"Cherry-pick ""fix makeRGB so it can replace saturated pixels and produce an image"" from HSC HSC-1196 includes fixes and test cases for {{afw}}. After review on HSC, they should be checked/merged to LSST."
"Port HSC-side functionality to allow showCamera to display real data via the butler One of the things that exists on the HSC side of things but not LSST is the ability to use showCamera to create full-focal-plane mosaics.  Please convert the code to run with the new cameraGeom    Not only is this generically useful, but it's part of the effort required to make the DM-side visualisation work for the Camera group  ",4,DM-2437,datamanagement,port hsc functionality allow showcamera display real datum butler thing exist hsc thing lsst ability use showcamera create focal plane mosaic convert code run new camerageom generically useful effort require dm visualisation work camera group,"Port HSC-side functionality to allow showCamera to display real data via the butler One of the things that exists on the HSC side of things but not LSST is the ability to use showCamera to create full-focal-plane mosaics. Please convert the code to run with the new cameraGeom Not only is this generically useful, but it's part of the effort required to make the DM-side visualisation work for the Camera group"
"iRODS test: Replicate data between servers A fundamental feature of using iRODS would be to prevent file loss/corruption incidents by replicating data to different physical servers, possibly in geographically disparate locations. We verify that we can replicate data within out test zone/grid.",2,DM-2439,datamanagement,irods test replicate datum server fundamental feature irods prevent file loss corruption incident replicate datum different physical server possibly geographically disparate location verify replicate datum test zone grid,"iRODS test: Replicate data between servers A fundamental feature of using iRODS would be to prevent file loss/corruption incidents by replicating data to different physical servers, possibly in geographically disparate locations. We verify that we can replicate data within out test zone/grid."
"iRODS test:  Virtual collection  iROD manages data as a 'virtual collection', that is, one can have a single logical/virtual view of a collection of files (the appearance of a single file system/tree) while the data with the collection is stored on separate physical servers. We demonstrate this by creating a collection with data targeted/uploaded to different physical resources.",2,DM-2440,datamanagement,irods test virtual collection irod manage datum virtual collection single logical virtual view collection file appearance single file system tree datum collection store separate physical server demonstrate create collection datum target upload different physical resource,"iRODS test: Virtual collection iROD manages data as a 'virtual collection', that is, one can have a single logical/virtual view of a collection of files (the appearance of a single file system/tree) while the data with the collection is stored on separate physical servers. We demonstrate this by creating a collection with data targeted/uploaded to different physical resources."
"iRODS test: Register data in place In our first tests of iRODS, we have used ""iput"" to load data into iRODS cache spaces (the iRODS Vault).  For large collections already in a well known location on a server, one may want to leave the data in place but still manage it with iRODS. To do this one can use ""ireg"" to register the data with IRODS without the upload process.",2,DM-2441,datamanagement,irods test register datum place test irods iput load datum irods cache space irods vault large collection know location server want leave datum place manage irods use ireg register datum irods upload process,"iRODS test: Register data in place In our first tests of iRODS, we have used ""iput"" to load data into iRODS cache spaces (the iRODS Vault). For large collections already in a well known location on a server, one may want to leave the data in place but still manage it with iRODS. To do this one can use ""ireg"" to register the data with IRODS without the upload process."
"iRODS usage, devel survey Read up on current IRODS usage and development track. ",3,DM-2442,datamanagement,irods usage devel survey read current irods usage development track,"iRODS usage, devel survey Read up on current IRODS usage and development track."
"Fix and test CheckAggregation {code:C++} class CheckAggregation { public:  CheckAggregation(bool& hasAgg_) : hasAgg(hasAgg_) {}  inline void operator()(query::ValueExpr::FactorOp const& fo) { if(!fo.factor.get()); {code}  - return is missing here. .get() is not needed, shared_ptr is like regular pointer which is convertible to bool, so whole thing should probably be: if (! fo.factor) return;  - We should have a unit test to show us we have problem here ",4,DM-2444,datamanagement,fix test checkaggregation code c++ class checkaggregation public checkaggregation(bool hasagg hasagg(hasagg inline void operator()(query::valueexpr::factorop const fo if(!fo.factor.get code return miss need shared_ptr like regular pointer convertible bool thing probably fo.factor return unit test problem,"Fix and test CheckAggregation {code:C++} class CheckAggregation { public: CheckAggregation(bool& hasAgg_) : hasAgg(hasAgg_) {} inline void operator()(query::ValueExpr::FactorOp const& fo) { if(!fo.factor.get()); {code} - return is missing here. .get() is not needed, shared_ptr is like regular pointer which is convertible to bool, so whole thing should probably be: if (! fo.factor) return; - We should have a unit test to show us we have problem here"
"Fix query ""SELECT * FROM Object o, Source s WHERE  o.objectId = s.objectId AND o.objectId = 390034570102582 AND    o.latestObsTime = s.taiMidPoint"" Next query fails:  {code:bash}  mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case01_qserv -e   ""SELECT *   FROM Object o, Source s   WHERE  o.objectId = s.objectId   AND    o.objectId = 390034570102582   AND    o.latestObsTime = s.taiMidPoint""  {code}    It seems there's several problems here:    * objectId field is duplicated, zookeeper could be used to know all the fields involved by * in a query, but then it has to know each columns.  * subChunkId and chunkId are also duplicated, this isn't the case in the plain-mysql query.    This duplicated columns prevent the creation of the result table on the czar.  ",6,DM-2445,datamanagement,fix query select object source o.objectid s.objectid o.objectid 390034570102582 o.latestobstime s.taimidpoint query fail code bash mysql --port=4040 qsmaster qservtest_case01_qserv select object source o.objectid s.objectid o.objectid 390034570102582 o.latestobstime s.taimidpoint code problem objectid field duplicate zookeeper know field involve query know column subchunkid chunkid duplicate case plain mysql query duplicate column prevent creation result table czar,"Fix query ""SELECT * FROM Object o, Source s WHERE o.objectId = s.objectId AND o.objectId = 390034570102582 AND o.latestObsTime = s.taiMidPoint"" Next query fails: {code:bash} mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case01_qserv -e ""SELECT * FROM Object o, Source s WHERE o.objectId = s.objectId AND o.objectId = 390034570102582 AND o.latestObsTime = s.taiMidPoint"" {code} It seems there's several problems here: * objectId field is duplicated, zookeeper could be used to know all the fields involved by * in a query, but then it has to know each columns. * subChunkId and chunkId are also duplicated, this isn't the case in the plain-mysql query. This duplicated columns prevent the creation of the result table on the czar."
"v10_1_rc2 build test Test v10_1_rc2 + tickets/DM-2303 on el6, el7, fedora 21, ubuntu 12.04, & ubuntu 14.04.  Results to be reported in http://ls.st/faq .",1,DM-2447,datamanagement,v10_1_rc2 build test test v10_1_rc2 ticket dm-2303 el6 el7 fedora 21 ubuntu 12.04 ubuntu 14.04 result report http://ls.st/faq,"v10_1_rc2 build test Test v10_1_rc2 + tickets/DM-2303 on el6, el7, fedora 21, ubuntu 12.04, & ubuntu 14.04. Results to be reported in http://ls.st/faq ."
"Write additional test for duplicate fields check Alongside:  {code:C++} BOOST_AUTO_TEST_CASE(getDuplicateAndPosition) {code}  Add test for: - no duplicate strings - triplicate - more than one string duplicated  and alongside:  {code:C++} BOOST_AUTO_TEST_CASE(SameNameDifferentTable) {code}  test more than one duplicated column, and a column duplicated more than twice.",3,DM-2448,datamanagement,write additional test duplicate field check alongside code c++ boost_auto_test_case(getduplicateandposition code add test duplicate string triplicate string duplicate alongside code c++ boost_auto_test_case(samenamedifferenttable code test duplicate column column duplicate twice,"Write additional test for duplicate fields check Alongside: {code:C++} BOOST_AUTO_TEST_CASE(getDuplicateAndPosition) {code} Add test for: - no duplicate strings - triplicate - more than one string duplicated and alongside: {code:C++} BOOST_AUTO_TEST_CASE(SameNameDifferentTable) {code} test more than one duplicated column, and a column duplicated more than twice."
"Fix cmsd-server logger configuration cmsd-server logger configuration is incorrect:  see cmsd.log on Qserv worker:  {code:bash} Plugin loaded unreleased QservOssGeneric unknown from osslib libxrdoss.so log4cxx: Could not instantiate class [org.apache.log4j.XrootdAppender]. log4cxx: Class not found: org.apache.log4j.XrootdAppender log4cxx: Could not instantiate appender named ""XrdLog"". log4cxx: No appender could be found for logger (QservOss). log4cxx: Please initialize the log4cxx system properly. QservOss (Qserv Oss for server cmsd) ""worker"" {code}",4,DM-2450,datamanagement,fix cmsd server logg configuration cmsd server logg configuration incorrect cmsd.log qserv worker code bash plugin load unreleased qservossgeneric unknown osslib libxrdoss.so log4cxx instantiate class org.apache.log4j xrootdappender log4cxx class find org.apache.log4j xrootdappender log4cxx instantiate appender name xrdlog log4cxx appender find logg qservoss log4cxx initialize log4cxx system properly qservoss qserv oss server cmsd worker code,"Fix cmsd-server logger configuration cmsd-server logger configuration is incorrect: see cmsd.log on Qserv worker: {code:bash} Plugin loaded unreleased QservOssGeneric unknown from osslib libxrdoss.so log4cxx: Could not instantiate class [org.apache.log4j.XrootdAppender]. log4cxx: Class not found: org.apache.log4j.XrootdAppender log4cxx: Could not instantiate appender named ""XrdLog"". log4cxx: No appender could be found for logger (QservOss). log4cxx: Please initialize the log4cxx system properly. QservOss (Qserv Oss for server cmsd) ""worker"" {code}"
"Fix interface between QservOss and new cmsd version QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet:  {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code} ",1,DM-2451,datamanagement,fix interface qservoss new cmsd version qservoss give error attempt run query worker czar error log snippet code qservoss qserv oss server cmsd worker 150331 16:06:17 9904 meter unable calculate file system space operation support 150331 16:06:17 9904 meter write access staging prohibit worker@lsst-dbdev3.ncsa.illinois.edu phase server initialization complete cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization complete code,"Fix interface between QservOss and new cmsd version QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet: {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code}"
"Replace toString() function See [~salnikov] comment:    Fabrice, anything is possible in C++, if you can define toString() for vectors it should also be possible to define some other construct to format vector into a stream :)  My objection to toString() is based on couple of of observations:      most of the time in our code converting complex objects to string is done to push them to streams or to logging system (logging is also usually based of streams)     methods like toString() are usually implemented using temporary streams.  So if you write code like cout << toString(vector) or LOGF_DEBUG(""vector: %1%"" % toString(vector)) it is very inefficient because it creates temporary stream and temporary string(s).  To make it more efficient you have to define operator<<() which is implemented without using toString(). Then you could implement toString() based on operator<<() but I'd argue that you should avoid it. In case you really need to convert to string there semi-standard tools which already do the same for types that have operator<< defined (like boost::lexical_cast), but again most of the time you only need operator<< as you don't want to mess with strings.  If you want to know how to implement operator<< for vector (or any container) here is the sketch of what I would do (there might be simpler ways):  {code:c++} namespace detail {     template <typename Cont> struct _ContInserterHelper {         const Cont& cont;     };     template <typename Cont> std::ostream& operator<<(std::ostream& out, const _ContInserterHelper<Cont>& cins) {         out << ""["";         const Cont& cont = cins.cont;   // this is container itself         // print container elements with separators         return out << ""]"";     } } template <typename Cont> detail::_ContInserterHelper<Cont> ContInserter(const Cont& cont) {     return detail::_ContInserterHelper<Cont>{cont}; } {code}  And after that you can do:  {code:c++} std::vector<int> v; std::cout << ContInserter(v); {code}  And this has no overhead or any temporary objects created. ",3,DM-2452,datamanagement,"replace tostring function ~salnikov comment fabrice possible c++ define tostring vector possible define construct format vector stream objection tostring base couple observation time code convert complex object string push stream log system log usually base stream method like tostring usually implement temporary stream write code like cout tostring(vector logf_debug(""vector tostring(vector inefficient create temporary stream temporary string(s efficient define operator implement tostring implement tostring base operator argue avoid case need convert string semi standard tool type operator define like boost::lexical_cast time need operator want mess string want know implement operator vector container sketch simple way code c++ namespace detail template struct continserterhelper const cont cont template std::ostream operator<<(std::ostream const continserterhelper cin const cont cont cins.cont container print container element separator return template detail::_continserterhelper continserter(const cont cont return detail::_continserterhelper{cont code code c++ std::vector std::cout continserter(v code overhead temporary object create","Replace toString() function See [~salnikov] comment: Fabrice, anything is possible in C++, if you can define toString() for vectors it should also be possible to define some other construct to format vector into a stream :) My objection to toString() is based on couple of of observations: most of the time in our code converting complex objects to string is done to push them to streams or to logging system (logging is also usually based of streams) methods like toString() are usually implemented using temporary streams. So if you write code like cout << toString(vector) or LOGF_DEBUG(""vector: %1%"" % toString(vector)) it is very inefficient because it creates temporary stream and temporary string(s). To make it more efficient you have to define operator<<() which is implemented without using toString(). Then you could implement toString() based on operator<<() but I'd argue that you should avoid it. In case you really need to convert to string there semi-standard tools which already do the same for types that have operator<< defined (like boost::lexical_cast), but again most of the time you only need operator<< as you don't want to mess with strings. If you want to know how to implement operator<< for vector (or any container) here is the sketch of what I would do (there might be simpler ways): {code:c++} namespace detail { template  struct _ContInserterHelper { const Cont& cont; }; template  std::ostream& operator<<(std::ostream& out, const _ContInserterHelper& cins) { out << ""[""; const Cont& cont = cins.cont; // this is container itself // print container elements with separators return out << ""]""; } } template  detail::_ContInserterHelper ContInserter(const Cont& cont) { return detail::_ContInserterHelper{cont}; } {code} And after that you can do: {code:c++} std::vector v; std::cout << ContInserter(v); {code} And this has no overhead or any temporary objects created."
"investigate github oauth integration for jenkins  We need a means of authenticating and authorizing users to interact with the CI system.  The current seem of using an htpasswd file with buildbot is a hassel both for end user and administratively.  Jenkin's has support for ldap and there is a plugin available for github oauth.  Administratively, and it terms of reliability, it may make more sense to be coupled with github than a a new DM or the exist LSST LDAP instance.",7,DM-2454,datamanagement,investigate github oauth integration jenkin need means authenticating authorize user interact ci system current htpasswd file buildbot hassel end user administratively jenkin support ldap plugin available github oauth administratively term reliability sense couple github new dm exist lsst ldap instance,"investigate github oauth integration for jenkins We need a means of authenticating and authorizing users to interact with the CI system. The current seem of using an htpasswd file with buildbot is a hassel both for end user and administratively. Jenkin's has support for ldap and there is a plugin available for github oauth. Administratively, and it terms of reliability, it may make more sense to be coupled with github than a a new DM or the exist LSST LDAP instance."
"uncaught exceptions in GaussianFlux {{SdssShapeAlgorithm::computeFixedMomentsFlux}}, which is used to implement {{GaussianFlux}}, now throws an exception when the moments it is given are singular.  That shouldn't have affected the behavior of {{GaussianFlux}}, as it contains an earlier check that should have detected all such bad input shapes.  But that doesn't seem to be the case: we now see that exception being thrown and propagating up until it is caught and logged by the measurement framework, resulting in noisy logs.  We need to investigate what's going wrong with these objects, and fix them, which may be in {{SdssShape}} or in the {{SafeShapeExtractor}} {{GaussianFlux}} uses to sanitize its inputs.",1,DM-2455,datamanagement,uncaught exception gaussianflux sdssshapealgorithm::computefixedmomentsflux implement gaussianflux throw exception moment give singular affect behavior gaussianflux contain early check detect bad input shape case exception throw propagate catch log measurement framework result noisy log need investigate go wrong object fix sdssshape safeshapeextractor gaussianflux use sanitize input,"uncaught exceptions in GaussianFlux {{SdssShapeAlgorithm::computeFixedMomentsFlux}}, which is used to implement {{GaussianFlux}}, now throws an exception when the moments it is given are singular. That shouldn't have affected the behavior of {{GaussianFlux}}, as it contains an earlier check that should have detected all such bad input shapes. But that doesn't seem to be the case: we now see that exception being thrown and propagating up until it is caught and logged by the measurement framework, resulting in noisy logs. We need to investigate what's going wrong with these objects, and fix them, which may be in {{SdssShape}} or in the {{SafeShapeExtractor}} {{GaussianFlux}} uses to sanitize its inputs."
Participate in April design process Most work here was with designing firefly tools API related details.,8,DM-2456,datamanagement,participate april design process work design firefly tool api relate detail,Participate in April design process Most work here was with designing firefly tools API related details.
Prepare v10_1 release candidate Candidate is v10_1_rc2 based on EUPS tag b949,6,DM-2463,datamanagement,prepare v10_1 release candidate candidate v10_1_rc2 base eups tag b949,Prepare v10_1 release candidate Candidate is v10_1_rc2 based on EUPS tag b949
"lsstsw ./bin/deploy needs LSSTSW set to install products in the right place I  cloned lsstsw into ~/Desktop/templsstsw and cd'd into it and typed ./bin/deploy and was shocked to find it installed everything into ~/lsstsw, leaving an unsable mess: some files were in templsstsw and some in ~/lsstsw.  The short-term workaround is to manually set LSSTSW before running ./bin/deploy, but this should not be necessary; bin/deploy should either set LSSTSW or not rely on it. I don't recall this problem with earlier versions of lsstsw; I think this is a regression.  For now I updated the instructions at https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool but I look forward to being able to revert that change.",1,DM-2466,datamanagement,lsstsw ./bin deploy need lsstsw set install product right place clone lsstsw ~/desktop templsstsw cd'd type deploy shocked find instal ~/lsstsw leave unsable mess file templsstsw ~/lsstsw short term workaround manually set lsstsw run deploy necessary bin deploy set lsstsw rely recall problem early version lsstsw think regression update instruction https://confluence.lsstcorp.org/display/ldmdg/the+lsst+software+build+tool look forward able revert change,"lsstsw ./bin/deploy needs LSSTSW set to install products in the right place I cloned lsstsw into ~/Desktop/templsstsw and cd'd into it and typed ./bin/deploy and was shocked to find it installed everything into ~/lsstsw, leaving an unsable mess: some files were in templsstsw and some in ~/lsstsw. The short-term workaround is to manually set LSSTSW before running ./bin/deploy, but this should not be necessary; bin/deploy should either set LSSTSW or not rely on it. I don't recall this problem with earlier versions of lsstsw; I think this is a regression. For now I updated the instructions at https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool but I look forward to being able to revert that change."
"Implement stitching multiple patches across tract boundaries in a coadd v2 * Find region that returns multiple tractPatchLists for testing.  * Request region via central point (RA, Dec) with width and height definable in arcseconds and pixels.  * May be extend web interface to other data sets, and/or good seeing SkyMaps. ",8,DM-2467,datamanagement,implement stitch multiple patch tract boundary coadd v2 find region return multiple tractpatchlist testing request region central point ra dec width height definable arcsecond pixel extend web interface data set and/or good see skymaps,"Implement stitching multiple patches across tract boundaries in a coadd v2 * Find region that returns multiple tractPatchLists for testing. * Request region via central point (RA, Dec) with width and height definable in arcseconds and pixels. * May be extend web interface to other data sets, and/or good seeing SkyMaps."
Build 2015_04 Qserv release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,1,DM-2475,datamanagement,build 2015_04 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build 2015_04 Qserv release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
"Design API and RFC design Use the HSC implementation of the base class as a point of reference for designing an integrated Approximate and Interpolate class.  The design take into account Chebyshev, spline, and Gaussian process mechanisms.  Want to take into consideration client code.  I.e. it shouldn't make current consumers more complicated (background and aperture correction to name two).  RFC the designed API.",8,DM-2477,datamanagement,design api rfc design use hsc implementation base class point reference design integrate approximate interpolate class design account chebyshev spline gaussian process mechanism want consideration client code i.e. current consumer complicated background aperture correction rfc design api,"Design API and RFC design Use the HSC implementation of the base class as a point of reference for designing an integrated Approximate and Interpolate class. The design take into account Chebyshev, spline, and Gaussian process mechanisms. Want to take into consideration client code. I.e. it shouldn't make current consumers more complicated (background and aperture correction to name two). RFC the designed API."
Fix-up any code that uses approximate/interpolate The background matcher is one area where the approximate/interpolate class will be used.  This story will find all places (including examples and unit tests) where the old approximate/interpolate mechanisms are used and update them to use the new interface.,4,DM-2479,datamanagement,fix code use approximate interpolate background matcher area approximate interpolate class story find place include example unit test old approximate interpolate mechanism update use new interface,Fix-up any code that uses approximate/interpolate The background matcher is one area where the approximate/interpolate class will be used. This story will find all places (including examples and unit tests) where the old approximate/interpolate mechanisms are used and update them to use the new interface.
"Delete old approximate/interpolate classes Once all updates are done to code and unit tests pass with the new approximate/interpolate interface, the old ones should be completely removed.",2,DM-2480,datamanagement,delete old approximate interpolate class update code unit test pass new approximate interpolate interface old one completely remove,"Delete old approximate/interpolate classes Once all updates are done to code and unit tests pass with the new approximate/interpolate interface, the old ones should be completely removed."
"Justify level of staff at La Serena, to the level of justifying for office space help with the specifications for the buildings in La Serena. The request si enough prove to justify office space for the DM administrators. and for  other staffing needed for DM.  This work will span two weeks and is due this Thursday  April 9.    This story is for the orienting work - -kickoff phone call.",1,DM-2484,datamanagement,justify level staff la serena level justify office space help specification building la serena request si prove justify office space dm administrator staffing need dm work span week thursday april story orienting work -kickoff phone,"Justify level of staff at La Serena, to the level of justifying for office space help with the specifications for the buildings in La Serena. The request si enough prove to justify office space for the DM administrators. and for other staffing needed for DM. This work will span two weeks and is due this Thursday April 9. This story is for the orienting work - -kickoff phone call."
recieve and begin to process document from SET about scalability of CEPH IN the context of ISL investigations into stogie systems the SET group has produced a document  that goes into the scaling of the meta data services.  The concern is that there is a central meta data service ins CEPH.   Began to process this analysis and to think about feasibility of testing program.,1,DM-2485,datamanagement,recieve begin process document set scalability ceph context isl investigation stogie system set group produce document go scaling meta datum service concern central meta datum service in ceph begin process analysis think feasibility testing program,recieve and begin to process document from SET about scalability of CEPH IN the context of ISL investigations into stogie systems the SET group has produced a document that goes into the scaling of the meta data services. The concern is that there is a central meta data service ins CEPH. Began to process this analysis and to think about feasibility of testing program.
"management for week March 30. Investigated invoicing fro storage condo -- appears to be annual fee, OK by Jeff. Investigated attaching  effort breakdown to invoke -- this seems hard as U of I invoicing occurs at quite a distance (procedural) distance from the NCSA business office.  Decided to look at improvements in recording effort in Jira so as to be able to generate report. -- Capture all actuals.  Business office transition  support is transitioning from Matt S. to new person.  Review AMCL sides,  kept tradition generating exponentially more comments, but reduced the exponent.  Process to bill out effort applied to project, but not in standing assignments in the staffing plan.  Internal strategy meeting about agenda items w.r.t VAO given Rap Plante is leaving NCSA. Prep for DM leadership meeting --  synergies at NCSA.  ",4,DM-2486,datamanagement,management week march 30 investigate invoice fro storage condo appear annual fee ok jeff investigate attach effort breakdown invoke hard invoicing occur distance procedural distance ncsa business office decide look improvement recording effort jira able generate report capture actual business office transition support transition matt s. new person review amcl side keep tradition generate exponentially comment reduce exponent process bill effort apply project stand assignment staffing plan internal strategy meeting agenda item w.r.t vao give rap plante leave ncsa prep dm leadership meeting synergy ncsa,"management for week March 30. Investigated invoicing fro storage condo -- appears to be annual fee, OK by Jeff. Investigated attaching effort breakdown to invoke -- this seems hard as U of I invoicing occurs at quite a distance (procedural) distance from the NCSA business office. Decided to look at improvements in recording effort in Jira so as to be able to generate report. -- Capture all actuals. Business office transition support is transitioning from Matt S. to new person. Review AMCL sides, kept tradition generating exponentially more comments, but reduced the exponent. Process to bill out effort applied to project, but not in standing assignments in the staffing plan. Internal strategy meeting about agenda items w.r.t VAO given Rap Plante is leaving NCSA. Prep for DM leadership meeting -- synergies at NCSA."
"security weekly meeting  met with the ISO, looking for ways to more actively engage.  Idea was to focus on the SCADA enclave, and the need was to engage with  German Etc",1,DM-2487,datamanagement,security weekly meeting meet iso look way actively engage idea focus scada enclave need engage german etc,"security weekly meeting met with the ISO, looking for ways to more actively engage. Idea was to focus on the SCADA enclave, and the need was to engage with German Etc"
"Initial survey of Datacat for LSST  Jacek, Brian Van Klaveren have sent along some initial overview/description of their work on Datacat;      https://confluence.slac.stanford.edu/display/~bvan/LSST+Datacat+Overview  We start examining this in the context of our studies of managing data collections at NCSA.",1,DM-2491,datamanagement,initial survey datacat lsst jacek brian van klaveren send initial overview description work datacat https://confluence.slac.stanford.edu/display/~bvan/lsst+datacat+overview start examine context study manage datum collection ncsa,"Initial survey of Datacat for LSST Jacek, Brian Van Klaveren have sent along some initial overview/description of their work on Datacat; https://confluence.slac.stanford.edu/display/~bvan/LSST+Datacat+Overview We start examining this in the context of our studies of managing data collections at NCSA."
"shapelet unit tests attempts to access display on failure When tests/profiles.py tests fail, they attempt to create live plots without checking for any variables that indicate that the display should be used.  These plots should be disabled, as they obscure the real error when the display is not available.",1,DM-2492,datamanagement,shapelet unit test attempt access display failure test profiles.py test fail attempt create live plot check variable indicate display plot disable obscure real error display available,"shapelet unit tests attempts to access display on failure When tests/profiles.py tests fail, they attempt to create live plots without checking for any variables that indicate that the display should be used. These plots should be disabled, as they obscure the real error when the display is not available."
"Fix g++ 4.9 return value implicit conversion incompato g++ 4.9 enforces the ""explicit"" keyword on type conversion operators in return value context.  This mean bool checkers along the lines of  bool isValidFoo() { return _smartPtrFoo; }  require an explicit cast to compile under g++ 4.9 with -std=c++0x.  There were a handful of these in our code; found and fixed.",1,DM-2497,datamanagement,fix g++ 4.9 return value implicit conversion incompato g++ 4.9 enforce explicit keyword type conversion operator return value context mean bool checker line bool isvalidfoo return smartptrfoo require explicit cast compile g++ 4.9 -std c++0x handful code find fix,"Fix g++ 4.9 return value implicit conversion incompato g++ 4.9 enforces the ""explicit"" keyword on type conversion operators in return value context. This mean bool checkers along the lines of bool isValidFoo() { return _smartPtrFoo; } require an explicit cast to compile under g++ 4.9 with -std=c++0x. There were a handful of these in our code; found and fixed."
"Improve db.createTable DM-2417 revealed that the current implementation of createTable in db module behaves differently that mysql: mysql will issue a warning if table exists, and db module will fail with an error. We should make the db behave similarly to how mysql behaves. ",2,DM-2502,datamanagement,improve db.createtable dm-2417 reveal current implementation createtable db module behave differently mysql mysql issue warning table exist db module fail error db behave similarly mysql behave,"Improve db.createTable DM-2417 revealed that the current implementation of createTable in db module behaves differently that mysql: mysql will issue a warning if table exists, and db module will fail with an error. We should make the db behave similarly to how mysql behaves."
Doxygenize db The db module needs to be doxygenized.,1,DM-2503,datamanagement,doxygenize db db module need doxygenize,Doxygenize db The db module needs to be doxygenized.
Optimize support for many identical database schemas - design It is likely we will have 100s or 1000s of identical databases (identical in terms of schema). It'd be good to not repeat the schema information in metaserv. This ticket include coming up with a plan how to implement it.,1,DM-2504,datamanagement,optimize support identical database schemas design likely 100 1000 identical database identical term schema good repeat schema information metaserv ticket include come plan implement,Optimize support for many identical database schemas - design It is likely we will have 100s or 1000s of identical databases (identical in terms of schema). It'd be good to not repeat the schema information in metaserv. This ticket include coming up with a plan how to implement it.
"Optimize support for many identical database schemas - impl It is likely we will have 100s or 1000s of identical databases (identical in terms of schema). It'd be good to not repeat the schema information in metaserv. This ticket include implementing a clean solution, proposed through DM-2504",2,DM-2505,datamanagement,optimize support identical database schema impl likely 100 1000 identical database identical term schema good repeat schema information metaserv ticket include implement clean solution propose dm-2504,"Optimize support for many identical database schemas - impl It is likely we will have 100s or 1000s of identical databases (identical in terms of schema). It'd be good to not repeat the schema information in metaserv. This ticket include implementing a clean solution, proposed through DM-2504"
Document structure of our custom ddl ascii schema Need to better document what is supported / accepted by schemaToMeta.py. We are currently relying on cat/sql/baselineSchema.sql as the guide.,2,DM-2506,datamanagement,document structure custom ddl ascii schema need well document support accept schematometa.py currently rely cat sql baselineschema.sql guide,Document structure of our custom ddl ascii schema Need to better document what is supported / accepted by schemaToMeta.py. We are currently relying on cat/sql/baselineSchema.sql as the guide.
"Information exchange between processes - research We need to identify a reliable and fast way to exchange information between processes (for example, cmsd and xrootd).   This story involves understanding key requirements (structures, scale), and researching what mechanism would be best).   Deliverable: short narrative describing key requirements, and proposed mechanism, including a sketch of the design.",4,DM-2507,datamanagement,information exchange process research need identify reliable fast way exchange information process example cmsd xrootd story involve understand key requirement structure scale research mechanism good deliverable short narrative describe key requirement propose mechanism include sketch design,"Information exchange between processes - research We need to identify a reliable and fast way to exchange information between processes (for example, cmsd and xrootd). This story involves understanding key requirements (structures, scale), and researching what mechanism would be best). Deliverable: short narrative describing key requirements, and proposed mechanism, including a sketch of the design."
"Information exchange between processes - implementation Implement system for information exchange between cmsd and xrootd, per instructions in DM-2507",8,DM-2508,datamanagement,information exchange process implementation implement system information exchange cmsd xrootd instruction dm-2507,"Information exchange between processes - implementation Implement system for information exchange between cmsd and xrootd, per instructions in DM-2507"
"The distance field of match lists should be set The meas_astrom AstrometryTask returns a match list that has distance = 0 for all elements. Neither the matcher nor the WCS fitter are setting this field, and both ought to.",2,DM-2511,datamanagement,distance field match list set meas_astrom astrometrytask return match list distance element matcher wcs fitter set field ought,"The distance field of match lists should be set The meas_astrom AstrometryTask returns a match list that has distance = 0 for all elements. Neither the matcher nor the WCS fitter are setting this field, and both ought to."
Migrate to new WBS for 02C.06 Migrate to the new WBS structure for 02C.06. Work include: * revisiting wbs assignment for all epics * updating [S15 planning 4 DB team|https://confluence.lsstcorp.org/display/DM/S15+planning+4+DB+team] * updating ldm-240 spreadsheet * updating associated budget accounts * tweaking [build-ldm240.py|https://github.com/jbecla/experimental/blob/master/build-LDM-240.py],1,DM-2514,datamanagement,migrate new wbs 02c.06 migrate new wbs structure 02c.06 work include revisit wbs assignment epic update s15 plan db team|https://confluence.lsstcorp.org display dm s15+planning+4+db+team update ldm-240 spreadsheet update associated budget account tweak build ldm240.py|https://github.com jbecla experimental blob master build ldm-240.py,Migrate to new WBS for 02C.06 Migrate to the new WBS structure for 02C.06. Work include: * revisiting wbs assignment for all epics * updating [S15 planning 4 DB team|https://confluence.lsstcorp.org/display/DM/S15+planning+4+DB+team] * updating ldm-240 spreadsheet * updating associated budget accounts * tweaking [build-ldm240.py|https://github.com/jbecla/experimental/blob/master/build-LDM-240.py]
"Catch ""address in use"" I noticed when running integration tests, it failed with the error pasted below. It'd be good to catch it and print something useful. I am not entire sure what port number is in use, and what to kill...   {code}   File ""/usr/local/home/becla/stack_201502/repo/qserv/bin/qservWmgr.py"", line 89, in <module>     sys.exit(main())   File ""/usr/local/home/becla/stack_201502/repo/qserv/bin/qservWmgr.py"", line 85, in main     app.run(host)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/flask/app.py"", line 772, in run     run_simple(host, port, self, **options)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 710, in ru n_simple     inner()   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 692, in in ner     passthrough_errors, ssl_context).serve_forever()   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 486, in ma ke_server     passthrough_errors, ssl_context)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 410, in __ init__     HTTPServer.__init__(self, (host, int(port)), handler)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/SocketServer.py"", line 419, in __init__     self.server_bind()   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/BaseHTTPServer.py"", line 108, in server_bind     SocketServer.TCPServer.server_bind(self)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/SocketServer.py"", line 430, in server_bind     self.socket.bind(self.server_address)   File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/socket.py"", line 224, in meth     return getattr(self._sock,name)(*args) socket.error: [Errno 98] Address already in use {code}",1,DM-2515,datamanagement,catch address use notice run integration test fail error paste good catch print useful entire sure port number use kill code file /usr local home becla stack_201502 repo qserv bin qservwmgr.py line 89 sys.exit(main file /usr local home becla stack_201502 repo qserv bin qservwmgr.py line 85 main app.run(host file /usr local home becla stack_201502 linux64 anaconda/2.1.0 lib python2.7 site package flask app.py line 772 run run_simple(host port self option file /usr local home becla stack_201502 linux64 anaconda/2.1.0 lib python2.7 site package werkzeug serving.py line 710 ru n_simple inner file /usr local home becla stack_201502 linux64 anaconda/2.1.0 lib python2.7 site package werkzeug serving.py line 692 ner passthrough_errors ssl_context).serve_forever file /usr local home becla stack_201502 linux64 anaconda/2.1.0 lib python2.7 site package werkzeug serving.py line 486 ma ke_server passthrough_errors ssl_context file /usr local home becla stack_201502 linux64 anaconda/2.1.0 lib python2.7 site package werkzeug serving.py line 410 init httpserver.__init__(self host int(port handler file /usr local home becla stack_201502 linux64 anaconda/2.1.0 lib python2.7 socketserver.py line 419 init self.server_bind file /usr local home becla stack_201502 linux64 anaconda/2.1.0 lib python2.7 basehttpserver.py line 108 socketserver tcpserver.server_bind(self file /usr local home becla stack_201502 linux64 anaconda/2.1.0 lib python2.7 socketserver.py line 430 server_bind self.socket.bind(self.server_address file /usr local home becla stack_201502 linux64 anaconda/2.1.0 lib python2.7 socket.py line 224 meth return getattr(self._sock name)(*args socket.error errno 98 address use code,"Catch ""address in use"" I noticed when running integration tests, it failed with the error pasted below. It'd be good to catch it and print something useful. I am not entire sure what port number is in use, and what to kill... {code} File ""/usr/local/home/becla/stack_201502/repo/qserv/bin/qservWmgr.py"", line 89, in  sys.exit(main()) File ""/usr/local/home/becla/stack_201502/repo/qserv/bin/qservWmgr.py"", line 85, in main app.run(host) File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/flask/app.py"", line 772, in run run_simple(host, port, self, **options) File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 710, in ru n_simple inner() File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 692, in in ner passthrough_errors, ssl_context).serve_forever() File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 486, in ma ke_server passthrough_errors, ssl_context) File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/site-packages/werkzeug/serving.py"", line 410, in __ init__ HTTPServer.__init__(self, (host, int(port)), handler) File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/SocketServer.py"", line 419, in __init__ self.server_bind() File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/BaseHTTPServer.py"", line 108, in server_bind SocketServer.TCPServer.server_bind(self) File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/SocketServer.py"", line 430, in server_bind self.socket.bind(self.server_address) File ""/usr/local/home/becla/stack_201502/Linux64/anaconda/2.1.0/lib/python2.7/socket.py"", line 224, in meth return getattr(self._sock,name)(*args) socket.error: [Errno 98] Address already in use {code}"
"Add a CFHT-based post-build integration test to the sandbox build From [~boutigny]    I have installed some simple stack validation tools working on CFHT data in {{/lsst8/boutigny/valid_cfht}}    Here is the content of the README file :    ------------------------------------------------------------------------------------------------------------------------  This directory contains a set of utilities to validate a stack release with CFHT data    At the moment, only validation plots for the astrometry are produced    Directories :  -------------  rawDownload     : contain raw CFHT images (flat, dark, bias, fringe,... corrected)  reference_plots : contain reference plots corresponding to the best results obtain so far.    Files :  -------  setup.cfht       : stack environment setup  valid_cfht.sh    : run processCcd taks on the cfht images     valid_cfht.sh init : create the input/ouput directories, ingest raw images and run processCcd     valid_cfht.sh      : without the ""init"" argument, runs processCcd assuming that the directory structure exists and that the raw images have been ingested.  valid_cfht.py    : run some analysis on the output data produced by valid_cfht.sh  processConfig.py : configuration parameters for processCcd  run.list         : list of vistits / ccd to be processed by processCcd    Requirements :  --------------  obs_cfht : tickets/DM-1593  astrometry_net_data : SDSS_DR9 reference catalog corresponding for CFHT Deep Field #3  ------------------------------------------------------------------------------------------------------------------------    Basically it produces a set of plots stored in a png image that can be compared to a reference plot corresponding to the best results obtained so far with stack_v10_0    I hope that this is useful. Just be careful that I wrote these scripts with my own ""fat hand full of fingers"" and that it is just basic code from a non expert. If it is useful, I can certainly add more plots to validate the psf determination, photometry, etc.    Comments, suggestions and criticisms are very welcome.",1,DM-2518,datamanagement,add cfht base post build integration test sandbox build ~boutigny instal simple stack validation tool work cfht datum /lsst8 boutigny valid_cfht content readme file directory contain set utility validate stack release cfht datum moment validation plot astrometry produce directory rawdownload contain raw cfht image flat dark bias fringe correct reference_plot contain reference plot correspond good result obtain far file setup.cfht stack environment setup valid_cfht.sh run processccd tak cfht image valid_cfht.sh init create input ouput directory ingest raw image run processccd valid_cfht.sh init argument run processccd assume directory structure exist raw image ingest valid_cfht.py run analysis output datum produce valid_cfht.sh processconfig.py configuration parameter processccd run.list list vistit ccd process processccd requirement obs_cfht ticket dm-1593 astrometry_net_data sdss_dr9 reference catalog correspond cfht deep field ------------------------------------------------------------------------------------------------------------------------ basically produce set plot store png image compare reference plot correspond good result obtain far stack_v10_0 hope useful careful write script fat hand finger basic code non expert useful certainly add plot validate psf determination photometry etc comment suggestion criticism welcome,"Add a CFHT-based post-build integration test to the sandbox build From [~boutigny] I have installed some simple stack validation tools working on CFHT data in {{/lsst8/boutigny/valid_cfht}} Here is the content of the README file : ------------------------------------------------------------------------------------------------------------------------ This directory contains a set of utilities to validate a stack release with CFHT data At the moment, only validation plots for the astrometry are produced Directories : ------------- rawDownload : contain raw CFHT images (flat, dark, bias, fringe,... corrected) reference_plots : contain reference plots corresponding to the best results obtain so far. Files : ------- setup.cfht : stack environment setup valid_cfht.sh : run processCcd taks on the cfht images valid_cfht.sh init : create the input/ouput directories, ingest raw images and run processCcd valid_cfht.sh : without the ""init"" argument, runs processCcd assuming that the directory structure exists and that the raw images have been ingested. valid_cfht.py : run some analysis on the output data produced by valid_cfht.sh processConfig.py : configuration parameters for processCcd run.list : list of vistits / ccd to be processed by processCcd Requirements : -------------- obs_cfht : tickets/DM-1593 astrometry_net_data : SDSS_DR9 reference catalog corresponding for CFHT Deep Field #3 ------------------------------------------------------------------------------------------------------------------------ Basically it produces a set of plots stored in a png image that can be compared to a reference plot corresponding to the best results obtained so far with stack_v10_0 I hope that this is useful. Just be careful that I wrote these scripts with my own ""fat hand full of fingers"" and that it is just basic code from a non expert. If it is useful, I can certainly add more plots to validate the psf determination, photometry, etc. Comments, suggestions and criticisms are very welcome."
Check for Qserv processes at configuration tool startup Configuration tool has to check for Qserv processes before removing configuration directory (which may contains init.d scripts for these running processes),4,DM-2519,datamanagement,check qserv process configuration tool startup configuration tool check qserv process remove configuration directory contain init.d script running process,Check for Qserv processes at configuration tool startup Configuration tool has to check for Qserv processes before removing configuration directory (which may contains init.d scripts for these running processes)
Proof of concept Python APIs to access Firefly components The pipeline needs to visualize the images using Firefly. We want to provide a few Python APIs for proof of concept that we could do this in Python and IPython notebook. ,6,DM-2520,datamanagement,proof concept python api access firefly component pipeline need visualize image firefly want provide python api proof concept python ipython notebook,Proof of concept Python APIs to access Firefly components The pipeline needs to visualize the images using Firefly. We want to provide a few Python APIs for proof of concept that we could do this in Python and IPython notebook.
Update repo.yaml for first set of Sims Stash repo moves The repos.yaml file needs to be updated with correct repository locations once SIM-1074 is completed.,1,DM-2521,datamanagement,update repo.yaml set sims stash repo move repos.yaml file need update correct repository location sim-1074 complete,Update repo.yaml for first set of Sims Stash repo moves The repos.yaml file needs to be updated with correct repository locations once SIM-1074 is completed.
"Implement distributed database deletion Implement database deletion based on the process defined in DM-1396. Need to deal with situations like worker is offline - might need some infrastructure e.g., running something in background to act when affected workers come back online.  Deliverable: a demonstration of system that deletes a distributed database: user issues ""drop database x"" and all copies of that database on all workers, all replicas of all chunks are deleted. It should be possible to ""create database x"" at any time later.",6,DM-2522,datamanagement,implement distribute database deletion implement database deletion base process define dm-1396 need deal situation like worker offline need infrastructure e.g. run background act affect worker come online deliverable demonstration system delete distribute database user issue drop database copy database worker replica chunk delete possible create database time later,"Implement distributed database deletion Implement database deletion based on the process defined in DM-1396. Need to deal with situations like worker is offline - might need some infrastructure e.g., running something in background to act when affected workers come back online. Deliverable: a demonstration of system that deletes a distributed database: user issues ""drop database x"" and all copies of that database on all workers, all replicas of all chunks are deleted. It should be possible to ""create database x"" at any time later."
Ensure we can delete/create table with the same name Test / ensure that we can create a table with the same name as the table we just deleted.,4,DM-2523,datamanagement,ensure delete create table test ensure create table table delete,Ensure we can delete/create table with the same name Test / ensure that we can create a table with the same name as the table we just deleted.
"Tom Durbin on board as a consultant for the Base site data center. Prep/attend/follow through for meeting with Tom Durbin, the facility manager of the National Petascale Computing Facility, to discuss participating as  consultant to the project as it finds a design contractor, and as the design matures.    ",1,DM-2526,datamanagement,tom durbin board consultant base site datum center prep attend follow meeting tom durbin facility manager national petascale computing facility discuss participate consultant project find design contractor design mature,"Tom Durbin on board as a consultant for the Base site data center. Prep/attend/follow through for meeting with Tom Durbin, the facility manager of the National Petascale Computing Facility, to discuss participating as consultant to the project as it finds a design contractor, and as the design matures."
"Mgt activity summary for week of April 6 - Made inquires about the status of materials contracting - ball in AURA's court. - Prepare for visit to Lyon.  Consult with atoll, researched collaborative structures,  articulated and vetted hardware process, made some slides for the visit. - Spent time thinking about VO protocols and such in prep for the DM F2F discussion. - Edited Job descriptions for the ADS department ,who will recruit for our systems engineers to include LSST, and LSST concerns. - Management / leading by walking around  N.B. Margaret in CAM training (or associated travel)  Tu-F. N.B Don was off 1 1/2 days  ",3,DM-2528,datamanagement,mgt activity summary week april inquire status material contracting ball aura court prepare visit lyon consult atoll research collaborative structure articulate vet hardware process slide visit spend time think vo protocol prep dm f2f discussion edit job description ads department recruit system engineer include lsst lsst concern management lead walk n.b. margaret cam training associate travel tu f. n.b don 1/2 day,"Mgt activity summary for week of April 6 - Made inquires about the status of materials contracting - ball in AURA's court. - Prepare for visit to Lyon. Consult with atoll, researched collaborative structures, articulated and vetted hardware process, made some slides for the visit. - Spent time thinking about VO protocols and such in prep for the DM F2F discussion. - Edited Job descriptions for the ADS department ,who will recruit for our systems engineers to include LSST, and LSST concerns. - Management / leading by walking around N.B. Margaret in CAM training (or associated travel) Tu-F. N.B Don was off 1 1/2 days"
"Resolve outgoing port issues on Blue Waters/Cray systems  pro data system scaling tests on cray system were limited by the number of outgoing ports on a cray node. The limitation had been  ~20 ports, participated in Tests of new system software,limit relaxed to at least ~2000 in tests. Likely greater.",3,DM-2530,datamanagement,resolve outgoing port issue blue waters cray system pro datum system scale test cray system limit number outgoing port cray node limitation port participate test new system software limit relax test likely great,"Resolve outgoing port issues on Blue Waters/Cray systems pro data system scaling tests on cray system were limited by the number of outgoing ports on a cray node. The limitation had been ~20 ports, participated in Tests of new system software,limit relaxed to at least ~2000 in tests. Likely greater."
"discussed the request from the SUI group for authentication guidance  responded to ticket from Jacek, on behalf, I think  of the SUI group asking for guidance on authentication at NCSA.   So far, consulted with Alex Withers,  contemplating the extent of policies so far (not much) an authentication mechanism worth investigtaing and likely policies.  Drew figure for discussion, wrote up in hip chat.",1,DM-2532,datamanagement,discuss request sui group authentication guidance respond ticket jacek behalf think sui group ask guidance authentication ncsa far consult alex withers contemplate extent policy far authentication mechanism worth investigtae likely policy drew figure discussion write hip chat,"discussed the request from the SUI group for authentication guidance responded to ticket from Jacek, on behalf, I think of the SUI group asking for guidance on authentication at NCSA. So far, consulted with Alex Withers, contemplating the extent of policies so far (not much) an authentication mechanism worth investigtaing and likely policies. Drew figure for discussion, wrote up in hip chat."
"Remove version attribute from Schema Remove the Schema attribute and its getters and setters.  This change won't be something we can merge to master on its own, as it doesn't provide backwards-compatible FITS reading that will added in future tasks.",1,DM-2533,datamanagement,remove version attribute schema remove schema attribute getter setter change will merge master provide backwards compatible fit reading add future task,"Remove version attribute from Schema Remove the Schema attribute and its getters and setters. This change won't be something we can merge to master on its own, as it doesn't provide backwards-compatible FITS reading that will added in future tasks."
"Rewrite afw::table FITS reading to be more flexible In order to support backwards-compatible FITS table reading, we need to break the current assumption that everything we need to know about how to read a Record from a FITS file is contained in the Record's Schema.  This issue involves that refactoring, without actually adding the backwards compatibility support.",4,DM-2534,datamanagement,rewrite afw::table fit reading flexible order support backwards compatible fit table reading need break current assumption need know read record fits file contain record schema issue involve refactoring actually add backwards compatibility support,"Rewrite afw::table FITS reading to be more flexible In order to support backwards-compatible FITS table reading, we need to break the current assumption that everything we need to know about how to read a Record from a FITS file is contained in the Record's Schema. This issue involves that refactoring, without actually adding the backwards compatibility support."
"Backwards compatibility for reading compound fields from FITS Read old-style afw::table compound fields in as scalar fields, using the new FunctorKey conventions.",2,DM-2535,datamanagement,backwards compatibility read compound field fits read old style afw::table compound field scalar field new functorkey convention,"Backwards compatibility for reading compound fields from FITS Read old-style afw::table compound fields in as scalar fields, using the new FunctorKey conventions."
Backwards compatibility for reading slots and measurements from FITS Rename fields to match the new slot and measurement naming conventions.,2,DM-2536,datamanagement,backwards compatibility read slot measurement fits rename field match new slot measurement naming convention,Backwards compatibility for reading slots and measurements from FITS Rename fields to match the new slot and measurement naming conventions.
"Contextual error handling There are cases when an empty result might have different errors than the top error, and it would be good to unwrap the context in which the error occured. Example: GET /meta/v0/db/L3/joe_myDb/tables/Object, the result might be empty because the database does not exist, or the v0 is not a supported version, etc.",4,DM-2537,datamanagement,contextual error handling case result different error error good unwrap context error occur example /meta v0 db l3 joe_mydb table object result database exist v0 support version etc,"Contextual error handling There are cases when an empty result might have different errors than the top error, and it would be good to unwrap the context in which the error occured. Example: GET /meta/v0/db/L3/joe_myDb/tables/Object, the result might be empty because the database does not exist, or the v0 is not a supported version, etc."
RESTful python client Develop basic abstractions for restful apis in a python client,3,DM-2538,datamanagement,restful python client develop basic abstraction restful apis python client,RESTful python client Develop basic abstractions for restful apis in a python client
"Research Ceph file system Research Ceph as possible networked filesystem for LSST usage to replace NFS. Estimate spending 10-20 hours of work with result being a wiki page of suggestions, limitations, etc.  (Implementation will be a different task, presuming we want to implement.)",6,DM-2541,datamanagement,research ceph file system research ceph possible networked filesystem lsst usage replace nfs estimate spend 10 20 hour work result wiki page suggestion limitation etc implementation different task presuming want implement,"Research Ceph file system Research Ceph as possible networked filesystem for LSST usage to replace NFS. Estimate spending 10-20 hours of work with result being a wiki page of suggestions, limitations, etc. (Implementation will be a different task, presuming we want to implement.)"
Python APIs for Firefly  We need Python APIs to interface with Firefly visualization components.  This is the first set of many functions.  ,8,DM-2543,datamanagement,python api firefly need python api interface firefly visualization component set function,Python APIs for Firefly We need Python APIs to interface with Firefly visualization components. This is the first set of many functions.
ctrl_events build issue Had a problem where ctrl_events was having build issues.,1,DM-2544,datamanagement,ctrl_event build issue problem ctrl_event have build issue,ctrl_events build issue Had a problem where ctrl_events was having build issues.
"LaTeX support in Doxygen broken LaTeX markup in Doxygen documentation ought to be rendered properly for display in HTML. It isn't: it's just dumped to the page as raw text. See, for example, [the documentation for {{AffineTransform}}|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_04_15_07.01.28/classlsst_1_1afw_1_1geom_1_1_affine_transform.html#details].",1,DM-2545,datamanagement,latex support doxygen break latex markup doxygen documentation ought render properly display html dump page raw text example documentation affinetransform}}|https://lsst web.ncsa.illinois.edu doxygen xlink_master_2015_04_15_07.01.28 classlsst_1_1afw_1_1geom_1_1_affine_transform.html#details,"LaTeX support in Doxygen broken LaTeX markup in Doxygen documentation ought to be rendered properly for display in HTML. It isn't: it's just dumped to the page as raw text. See, for example, [the documentation for {{AffineTransform}}|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_04_15_07.01.28/classlsst_1_1afw_1_1geom_1_1_affine_transform.html#details]."
Host.cc doesn't find gethostname and HOST_NAME_MAX under el7 el7 gives an error that it can't find HOST_NAME_MAX.,1,DM-2546,datamanagement,host.cc find gethostname host_name_max el7 el7 give error find host_name_max,Host.cc doesn't find gethostname and HOST_NAME_MAX under el7 el7 gives an error that it can't find HOST_NAME_MAX.
"Fix again interface between QservOss and new cmsd version QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet:  {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code} ",8,DM-2547,datamanagement,fix interface qservoss new cmsd version qservoss give error attempt run query worker czar error log snippet code qservoss qserv oss server cmsd worker 150331 16:06:17 9904 meter unable calculate file system space operation support 150331 16:06:17 9904 meter write access staging prohibit worker@lsst-dbdev3.ncsa.illinois.edu phase server initialization complete cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization complete code,"Fix again interface between QservOss and new cmsd version QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet: {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code}"
"The string repr of Coord should show the coordsys and angles in degrees The default string representation of Coord (e.g. std::cout << coord in C++ and str(coord) in Python) is to show class name and a pair of angles in radians.  It would be much more useful if the default display showed the angles in degrees, as that is what people are used to. Also, it would be very helpful if the display included the name of the coordinate system. This is especially needed for the base class, as it is quite common to get shared_ptr to Coord and have no idea what coordinate system it is.  At present there is a lot of code that unpacks the angles and explicitly displays them as degrees to get around this problem. But it seems silly to have to do that.",2,DM-2549,datamanagement,string repr coord coordsy angle degree default string representation coord e.g. std::cout coord c++ str(coord python class pair angle radian useful default display show angle degree people helpful display include coordinate system especially need base class common shared_ptr coord idea coordinate system present lot code unpack angle explicitly display degree problem silly,"The string repr of Coord should show the coordsys and angles in degrees The default string representation of Coord (e.g. std::cout << coord in C++ and str(coord) in Python) is to show class name and a pair of angles in radians. It would be much more useful if the default display showed the angles in degrees, as that is what people are used to. Also, it would be very helpful if the display included the name of the coordinate system. This is especially needed for the base class, as it is quite common to get shared_ptr to Coord and have no idea what coordinate system it is. At present there is a lot of code that unpacks the angles and explicitly displays them as degrees to get around this problem. But it seems silly to have to do that."
"ANetAstrometryTask's debug doesn't fully work {{ANetAstrometryTask}}'s debug code calls (deprecated) method {{Task.display}}, which raises an AttributeError on this coce:  {code}  try:      sources[0][0]  except IndexError:              # empty list      pass  except (TypeError, NotImplementedError): # not a list of sets of sources  {code}  ",1,DM-2551,datamanagement,anetastrometrytask debug fully work anetastrometrytask debug code call deprecate method task.display raise attributeerror coce code try sources[0][0 indexerror list pass typeerror notimplementederror list set source code,"ANetAstrometryTask's debug doesn't fully work {{ANetAstrometryTask}}'s debug code calls (deprecated) method {{Task.display}}, which raises an AttributeError on this coce: {code} try: sources[0][0] except IndexError: # empty list pass except (TypeError, NotImplementedError): # not a list of sets of sources {code}"
"xrootd can't be started via ssh {code:bash} qserv@clrinfopc04:~/src/qserv$ ssh localhost -vvv ""~qserv/qserv-run/2015_02/etc/init.d/xrootd start"" ... debug3: Ignored env _ debug1: Sending command: ~qserv/qserv-run/2015_02/etc/init.d/xrootd start debug2: channel 0: request exec confirm 1 debug2: callback done debug2: channel 0: open confirm rwindow 0 rmax 32768 debug2: channel 0: rcvd adjust 2097152 debug2: channel_input_status_confirm: type 99 id 0 debug2: exec request accepted on channel 0 Starting xrootd.. debug1: client_input_channel_req: channel 0 rtype exit-status reply 0 debug1: client_input_channel_req: channel 0 rtype eow@openssh.com reply 0 debug2: channel 0: rcvd eow debug2: channel 0: close_read debug2: channel 0: input open -> closed {code}  Here ssh command freeze, it is possible to lauch xrootd with this (example) script: {code:bash} set -e set -x  . /qserv/run/etc/sysconfig/qserv export QSW_XRDQUERYPATH=""/q"" export QSW_DBSOCK=""${MYSQLD_SOCK}"" export QSW_MYSQLDUMP=`which mysqldump` QSW_SCRATCHPATH=""${QSERV_RUN_DIR}/tmp"" QSW_SCRATCHDB=""qservScratch"" export QSW_RESULTPATH=""${XROOTD_RUN_DIR}/result"" export LSST_LOG_CONFIG=""${QSERV_RUN_DIR}/etc/log4xrootd.properties""  eval '/qserv/stack/Linux64/xrootd/xssi-1.0.0/bin/xrootd -c /qserv/run/etc/lsp.cf -l /qserv/run/var/log/xrootd.log -n worker -I v4 &'  echo ""SCRIPT STARTED"" {code} and the same problem occurs. So the problem seems to be with xrootd, and not the startup scripts.   ",5,DM-2552,datamanagement,"xrootd start ssh code bash qserv@clrinfopc04:~/src qserv$ ssh localhost ~qserv qserv run/2015_02 etc init.d xrootd start debug3 ignored env debug1 send command ~qserv qserv run/2015_02 etc init.d xrootd start debug2 channel request exec confirm debug2 callback debug2 channel open confirm rwindow rmax 32768 debug2 channel rcvd adjust 2097152 debug2 channel_input_status_confirm type 99 debug2 exec request accept channel start xrootd debug1 client_input_channel_req channel rtype exit status reply debug1 client_input_channel_req channel rtype eow@openssh.com reply debug2 channel rcvd eow debug2 channel close_read debug2 channel input open closed code ssh command freeze possible lauch xrootd example script code bash set set run etc sysconfig qserv export qsw_xrdquerypath=""/q export qsw_dbsock=""${mysqld_sock export qsw_mysqldump=`which mysqldump qsw_scratchpath=""${qserv_run_dir}/tmp qsw_scratchdb=""qservscratch export qsw_resultpath=""${xrootd_run_dir}/result export lsst_log_config=""${qserv_run_dir}/etc log4xrootd.properties eval /qserv stack linux64 xrootd xssi-1.0.0 bin xrootd run etc lsp.cf run var log xrootd.log worker -i v4 echo script start code problem occur problem xrootd startup script","xrootd can't be started via ssh {code:bash} qserv@clrinfopc04:~/src/qserv$ ssh localhost -vvv ""~qserv/qserv-run/2015_02/etc/init.d/xrootd start"" ... debug3: Ignored env _ debug1: Sending command: ~qserv/qserv-run/2015_02/etc/init.d/xrootd start debug2: channel 0: request exec confirm 1 debug2: callback done debug2: channel 0: open confirm rwindow 0 rmax 32768 debug2: channel 0: rcvd adjust 2097152 debug2: channel_input_status_confirm: type 99 id 0 debug2: exec request accepted on channel 0 Starting xrootd.. debug1: client_input_channel_req: channel 0 rtype exit-status reply 0 debug1: client_input_channel_req: channel 0 rtype eow@openssh.com reply 0 debug2: channel 0: rcvd eow debug2: channel 0: close_read debug2: channel 0: input open -> closed {code} Here ssh command freeze, it is possible to lauch xrootd with this (example) script: {code:bash} set -e set -x . /qserv/run/etc/sysconfig/qserv export QSW_XRDQUERYPATH=""/q"" export QSW_DBSOCK=""${MYSQLD_SOCK}"" export QSW_MYSQLDUMP=`which mysqldump` QSW_SCRATCHPATH=""${QSERV_RUN_DIR}/tmp"" QSW_SCRATCHDB=""qservScratch"" export QSW_RESULTPATH=""${XROOTD_RUN_DIR}/result"" export LSST_LOG_CONFIG=""${QSERV_RUN_DIR}/etc/log4xrootd.properties"" eval '/qserv/stack/Linux64/xrootd/xssi-1.0.0/bin/xrootd -c /qserv/run/etc/lsp.cf -l /qserv/run/var/log/xrootd.log -n worker -I v4 &' echo ""SCRIPT STARTED"" {code} and the same problem occurs. So the problem seems to be with xrootd, and not the startup scripts."
"Remove most compound fields from afw::table Remove all Point, Moment, Coord, and Covariance compound fields.  Array fields should be retained for now; it's not clear if we want to remove it or not, or how to handle variable-length arrays if we do.",2,DM-2554,datamanagement,remove compound field afw::table remove point moment coord covariance compound field array field retain clear want remove handle variable length array,"Remove most compound fields from afw::table Remove all Point, Moment, Coord, and Covariance compound fields. Array fields should be retained for now; it's not clear if we want to remove it or not, or how to handle variable-length arrays if we do."
Create and advertise Firefly mailing list Create an IPAC mailing list for all users of Firefly.  Advertise it to the interested communities (including the LSST Camera group) and through the Github site.  The mailing list firefly@ipac.caltech.edu has been created and all the interested partied have been subscribed to the list.,1,DM-2555,datamanagement,create advertise firefly mailing list create ipac mailing list user firefly advertise interested community include lsst camera group github site mailing list firefly@ipac.caltech.edu create interested partied subscribe list,Create and advertise Firefly mailing list Create an IPAC mailing list for all users of Firefly. Advertise it to the interested communities (including the LSST Camera group) and through the Github site. The mailing list firefly@ipac.caltech.edu has been created and all the interested partied have been subscribed to the list.
Vectorize methods for locating objects on detectors vectorize _transformSingleSys and _findDetectors in afw.cameraGeom so that the sims_coordUtils method findChipName (which finds the chips that an object lands on) runs faster.,2,DM-2557,datamanagement,vectorize method locate object detector vectorize transformsinglesys finddetector afw.camerageom sims_coordutil method findchipname find chip object land run fast,Vectorize methods for locating objects on detectors vectorize _transformSingleSys and _findDetectors in afw.cameraGeom so that the sims_coordUtils method findChipName (which finds the chips that an object lands on) runs faster.
"read and understood proposal to consider CAS/crowd system  the FERMI telescope has an authentication system based on CAS/Crowd. The  benefit of the system is that it can be use as an authentication system for both web and command line.      Download materials, and acquire the  understanding from a review of documentation . Discuss with the ISO,  propose discussion for vTony's visit to NCSA (may 21).",1,DM-2573,datamanagement,read understand proposal consider cas crowd system fermi telescope authentication system base cas crowd benefit system use authentication system web command line download material acquire understanding review documentation discuss iso propose discussion vtony visit ncsa 21,"read and understood proposal to consider CAS/crowd system the FERMI telescope has an authentication system based on CAS/Crowd. The benefit of the system is that it can be use as an authentication system for both web and command line. Download materials, and acquire the understanding from a review of documentation . Discuss with the ISO, propose discussion for vTony's visit to NCSA (may 21)."
"management activities for week of April 13 Read proposed  ""Hardware"" contract amendment, sent marked up comments to Julie Robinson, U of I contract negotiator.  Major points are that Hardware is not descriptive of all purchases  needed to fulfill SOW.  The procurement approval process needs spelling out. Detailed guidance in comments inserted into contract.  Along with M. Gelman met with the NCSA business people to fully understand the U  of I invoicing process, and the information in the existing business processes. prior to inventing processes for the  supplementing the U of I invoice with the more detailed annotations (hours by WBS) agreed to in the LSST contract.  Obtained help from the NSCS IT group. Documented in tow page note.   Met concerning seemingly large amount of effort to respond to hip chat take about slowness in the NCSA development system.     Miscellaneous and meetings.  ",5,DM-2574,datamanagement,management activity week april 13 read propose hardware contract amendment send mark comment julie robinson contract negotiator major point hardware descriptive purchase need fulfill sow procurement approval process need spell detailed guidance comment insert contract m. gelman meet ncsa business people fully understand invoicing process information exist business process prior invent process supplement invoice detailed annotation hour wbs agree lsst contract obtain help nscs group document tow page note met concern seemingly large effort respond hip chat slowness ncsa development system miscellaneous meeting,"management activities for week of April 13 Read proposed ""Hardware"" contract amendment, sent marked up comments to Julie Robinson, U of I contract negotiator. Major points are that Hardware is not descriptive of all purchases needed to fulfill SOW. The procurement approval process needs spelling out. Detailed guidance in comments inserted into contract. Along with M. Gelman met with the NCSA business people to fully understand the U of I invoicing process, and the information in the existing business processes. prior to inventing processes for the supplementing the U of I invoice with the more detailed annotations (hours by WBS) agreed to in the LSST contract. Obtained help from the NSCS IT group. Documented in tow page note. Met concerning seemingly large amount of effort to respond to hip chat take about slowness in the NCSA development system. Miscellaneous and meetings."
"Research BeeGFS file system Research BeeGFS as possible networked filesystem for LSST usage to replace parts of NFS. Estimate spending 10 hours of work with result being a wiki page of suggestions, limitations, etc. (Any implementation will be a different task, presuming we want to implement.)  http://www.beegfs.com/content/  BeeGFS (formerly FhGFS) is a parallel cluster file system, developed with a strong focus on performance and designed for very easy installation and management. If I/O intensive workloads are your problem, BeeGFS is the solution.  Likely not good replacement for formal/managed data, but perhaps great option for shared scratch file systems.",5,DM-2578,datamanagement,research beegfs file system research beegfs possible networked filesystem lsst usage replace part nfs estimate spend 10 hour work result wiki page suggestion limitation etc implementation different task presuming want implement beegfs fhgfs parallel cluster file system develop strong focus performance design easy installation management intensive workload problem beegfs solution likely good replacement formal manage datum great option share scratch file system,"Research BeeGFS file system Research BeeGFS as possible networked filesystem for LSST usage to replace parts of NFS. Estimate spending 10 hours of work with result being a wiki page of suggestions, limitations, etc. (Any implementation will be a different task, presuming we want to implement.) http://www.beegfs.com/content/ BeeGFS (formerly FhGFS) is a parallel cluster file system, developed with a strong focus on performance and designed for very easy installation and management. If I/O intensive workloads are your problem, BeeGFS is the solution. Likely not good replacement for formal/managed data, but perhaps great option for shared scratch file systems."
"Calling AliasMap::get("""") can return incorrect results It looks like empty string arguments can cause AliasMap to produce some incorrect results, probably due to the partial-match logic being overzealous.",1,DM-2579,datamanagement,call aliasmap::get return incorrect result look like string argument cause aliasmap produce incorrect result probably partial match logic overzealous,"Calling AliasMap::get("""") can return incorrect results It looks like empty string arguments can cause AliasMap to produce some incorrect results, probably due to the partial-match logic being overzealous."
"Implement user-friendly template customization Qserv configuration tool has to be improved to allow developers/sysadmin to easily use their custom configuration files (with custom log level, ...) for each Qserv services.    An optional custom/ config file directory will be added, and configuration files templates which will be here will override the ones in the install directory.    This should be thinked alongside configuration management inside Docker container.",5,DM-2580,datamanagement,implement user friendly template customization qserv configuration tool improve allow developer sysadmin easily use custom configuration file custom log level qserv service optional custom/ config file directory add configuration file template override one install directory think alongside configuration management inside docker container,"Implement user-friendly template customization Qserv configuration tool has to be improved to allow developers/sysadmin to easily use their custom configuration files (with custom log level, ...) for each Qserv services. An optional custom/ config file directory will be added, and configuration files templates which will be here will override the ones in the install directory. This should be thinked alongside configuration management inside Docker container."
log4cxx build failure on OS X [~frossie] writes:  {quote} I have a log4cxx failure on a Macp while building lsst_distrib. Attaching file in case someone has any bright ideas for me in the morning {quote},1,DM-2581,datamanagement,log4cxx build failure os ~frossie write quote log4cxx failure macp build lsst_distrib attach file case bright idea morning quote,log4cxx build failure on OS X [~frossie] writes: {quote} I have a log4cxx failure on a Macp while building lsst_distrib. Attaching file in case someone has any bright ideas for me in the morning {quote}
"Research MaxScale as a mysql-proxy replacement We have been told by Monty that MaxScale is the replacement of the mysql-proxy. Based on DM-2057 the sentiment is that it won't work for our needs. We should very briefly document what our needs are, how we use the proxy now, and if we think MaxScale is not good-enough, say it why, and discuss with Monty and his team.",5,DM-2582,datamanagement,research maxscale mysql proxy replacement tell monty maxscale replacement mysql proxy base dm-2057 sentiment will work need briefly document need use proxy think maxscale good discuss monty team,"Research MaxScale as a mysql-proxy replacement We have been told by Monty that MaxScale is the replacement of the mysql-proxy. Based on DM-2057 the sentiment is that it won't work for our needs. We should very briefly document what our needs are, how we use the proxy now, and if we think MaxScale is not good-enough, say it why, and discuss with Monty and his team."
Remove obsolete hinting code in proxy Remove now dead code related to sending hints from proxy to czar,1,DM-2592,datamanagement,remove obsolete hinting code proxy remove dead code relate send hint proxy czar,Remove obsolete hinting code in proxy Remove now dead code related to sending hints from proxy to czar
"Client API for new worker management service We have new worker management service which has HTTP interface, now we need to provide simple way to access it from Python basically wrapping all HTTP details into simple Python API. ",8,DM-2593,datamanagement,client api new worker management service new worker management service http interface need provide simple way access python basically wrap http detail simple python api,"Client API for new worker management service We have new worker management service which has HTTP interface, now we need to provide simple way to access it from Python basically wrapping all HTTP details into simple Python API."
Change repos.yaml for next set of Simulations Stash repos The next set of Simulations Stash repository migrations is laid out in SIM-1121.,1,DM-2594,datamanagement,change repos.yaml set simulations stash repos set simulations stash repository migration lay sim-1121,Change repos.yaml for next set of Simulations Stash repos The next set of Simulations Stash repository migrations is laid out in SIM-1121.
"Symlink data directory at configuration We decided to introduce symlinks in order to protect data. This is in particular useful when we need to reinstall qserv, but we have valuable, large data set that we want to preserve. This story introduces symlinks to data: when Qserv is reinstalled, only the symlink is destroyed, and the data stay untouched.",5,DM-2595,datamanagement,symlink data directory configuration decide introduce symlink order protect datum particular useful need reinstall qserv valuable large datum set want preserve story introduce symlink datum qserv reinstall symlink destroy datum stay untouched,"Symlink data directory at configuration We decided to introduce symlinks in order to protect data. This is in particular useful when we need to reinstall qserv, but we have valuable, large data set that we want to preserve. This story introduces symlinks to data: when Qserv is reinstalled, only the symlink is destroyed, and the data stay untouched."
afw.Image.ExposureF('file.fits.fz[i]') returns the image in 'file.fits.fz[1]'  It seems that afwImage.ExposureF ignores the extension number when this is passed on as part of the filename and uses the image in extension number 1. This is not the case with afwImage.MaskedImageF which correctly uses the input extension number passed in the same way.  The problem has been checked on OSX Yosemite 10.10.3 with  the is illustrated in  the following code https://gist.github.com/anonymous/d10c4a79d94c1393a493  which also requires the following image in the working directory: http://www.astro.washington.edu/users/krughoff/data/c4d_130830_040651_ooi_g_d1.fits.fz ,3,DM-2599,datamanagement,afw image exposuref('file.fits.fz[i return image file.fits.fz[1 afwimage exposuref ignore extension number pass filename use image extension number case afwimage maskedimagef correctly use input extension number pass way problem check osx yosemite 10.10.3 illustrate follow code https://gist.github.com/anonymous/d10c4a79d94c1393a493 require follow image work directory http://www.astro.washington.edu/users/krughoff/data/c4d_130830_040651_ooi_g_d1.fits.fz,afw.Image.ExposureF('file.fits.fz[i]') returns the image in 'file.fits.fz[1]' It seems that afwImage.ExposureF ignores the extension number when this is passed on as part of the filename and uses the image in extension number 1. This is not the case with afwImage.MaskedImageF which correctly uses the input extension number passed in the same way. The problem has been checked on OSX Yosemite 10.10.3 with the is illustrated in the following code https://gist.github.com/anonymous/d10c4a79d94c1393a493 which also requires the following image in the working directory: http://www.astro.washington.edu/users/krughoff/data/c4d_130830_040651_ooi_g_d1.fits.fz
"'eups distrib install flask -t qserv' fails on Ubuntu 14.04 Qserv now depends on Flask, so this blocks all Qserv install which rely on eups.  Comman below works with system-python but not with anaconda:  {code} qserv@clrinfoport09:~/stack/EupsBuildDir/Linux64/flask-0.10.1/flask-0.10.1⟫ python setup.py install --home /home/qserv/stack/Linux64/flask/0.10.1                                                                   running install Traceback (most recent call last):   File ""setup.py"", line 110, in <module>     test_suite='flask.testsuite.suite'   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/core.py"", line 151, in setup     dist.run_commands()   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/dist.py"", line 953, in run_commands     self.run_command(cmd)   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/dist.py"", line 972, in run_command     cmd_obj.run()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/install.py"", line 73, in run     self.do_egg_install()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/install.py"", line 82, in do_egg_install     cmd.ensure_finalized()  # finalize before bdist_egg munges install cmd   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensure_finalized     self.finalize_options()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 274, in finalize_options     ('install_dir','install_dir')   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 298, in set_undefined_options     src_cmd_obj.ensure_finalized()   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensure_finalized     self.finalize_options()   File ""/usr/lib/python2.7/dist-packages/setuptools/command/install_lib.py"", line 13, in finalize_options     self.set_undefined_options('install',('install_layout','install_layout'))   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 302, in set_undefined_options     getattr(src_cmd_obj, src_option))   File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 105, in __getattr__     raise AttributeError, attr AttributeError: install_layout {code}",5,DM-2601,datamanagement,"eup distrib install flask qserv fail ubuntu 14.04 qserv depend flask block qserv install rely eup comman work system python anaconda code qserv@clrinfoport09:~/stack eupsbuilddir linux64 flask-0.10.1 flask-0.10.1 python setup.py install --home /home qserv stack linux64 flask/0.10.1 run install traceback recent file setup.py line 110 test_suite='flask.testsuite.suite file /home qserv stack linux64 anaconda master g68783b1848 lib python2.7 distutils core.py line 151 setup dist.run_command file /home qserv stack linux64 anaconda master g68783b1848 lib python2.7 distutil dist.py line 953 run_command self.run_command(cmd file /home qserv stack linux64 anaconda master g68783b1848 lib python2.7 distutil dist.py line 972 run_command cmd_obj.run file /usr lib python2.7 dist package setuptool command install.py line 73 run self.do_egg_install file /usr lib python2.7 dist package setuptool command install.py line 82 do_egg_install cmd.ensure_finalized finalize bdist_egg munge install cmd file /home qserv stack linux64 anaconda master g68783b1848 lib python2.7 distutils cmd.py line 109 ensure_finalize self.finalize_option file /usr lib python2.7 dist package setuptool command easy_install.py line 274 finalize_option install_dir','install_dir file /home qserv stack linux64 anaconda master g68783b1848 lib python2.7 distutils cmd.py line 298 set_undefined_option src_cmd_obj.ensure_finalize file /home qserv stack linux64 anaconda master g68783b1848 lib python2.7 distutils cmd.py line 109 ensure_finalize self.finalize_option file /usr lib python2.7 dist package setuptool command install_lib.py line 13 finalize_option self.set_undefined_options('install',('install_layout','install_layout file /home qserv stack linux64 anaconda master g68783b1848 lib python2.7 distutils cmd.py line 302 set_undefined_option getattr(src_cmd_obj src_option file /home qserv stack linux64 anaconda master g68783b1848 lib python2.7 distutils cmd.py line 105 raise attributeerror attr attributeerror install_layout code","'eups distrib install flask -t qserv' fails on Ubuntu 14.04 Qserv now depends on Flask, so this blocks all Qserv install which rely on eups. Comman below works with system-python but not with anaconda: {code} qserv@clrinfoport09:~/stack/EupsBuildDir/Linux64/flask-0.10.1/flask-0.10.1 python setup.py install --home /home/qserv/stack/Linux64/flask/0.10.1 running install Traceback (most recent call last): File ""setup.py"", line 110, in  test_suite='flask.testsuite.suite' File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/core.py"", line 151, in setup dist.run_commands() File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/dist.py"", line 953, in run_commands self.run_command(cmd) File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/dist.py"", line 972, in run_command cmd_obj.run() File ""/usr/lib/python2.7/dist-packages/setuptools/command/install.py"", line 73, in run self.do_egg_install() File ""/usr/lib/python2.7/dist-packages/setuptools/command/install.py"", line 82, in do_egg_install cmd.ensure_finalized() # finalize before bdist_egg munges install cmd File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensure_finalized self.finalize_options() File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 274, in finalize_options ('install_dir','install_dir') File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 298, in set_undefined_options src_cmd_obj.ensure_finalized() File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 109, in ensure_finalized self.finalize_options() File ""/usr/lib/python2.7/dist-packages/setuptools/command/install_lib.py"", line 13, in finalize_options self.set_undefined_options('install',('install_layout','install_layout')) File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 302, in set_undefined_options getattr(src_cmd_obj, src_option)) File ""/home/qserv/stack/Linux64/anaconda/master-g68783b1848/lib/python2.7/distutils/cmd.py"", line 105, in __getattr__ raise AttributeError, attr AttributeError: install_layout {code}"
"Package flask dependencies We packaged flask (see dm-1797) and we are using it via eups, but we have not packaged flask dependencies, and we are still relying on anaconda to get them. This story involve packaging the dependencies.",3,DM-2605,datamanagement,package flask dependency package flask dm-1797 eup package flask dependency rely anaconda story involve package dependency,"Package flask dependencies We packaged flask (see dm-1797) and we are using it via eups, but we have not packaged flask dependencies, and we are still relying on anaconda to get them. This story involve packaging the dependencies."
HSC backport: recent Footprint fixes This is a backport issue to capture subsequent HSC-side work on features already backported to afw.  It includes (so far) the following HSC issues:   - [HSC-1135|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1135]   - [HSC-1129|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1129]   - [HSC-1215|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1215],2,DM-2606,datamanagement,hsc backport recent footprint fix backport issue capture subsequent hsc work feature backporte afw include far follow hsc issue hsc-1135|https://hsc jira.astro.princeton.edu jira browse hsc-1135 hsc-1129|https://hsc jira.astro.princeton.edu jira browse hsc-1129 hsc-1215|https://hsc jira.astro.princeton.edu jira browse hsc-1215,HSC backport: recent Footprint fixes This is a backport issue to capture subsequent HSC-side work on features already backported to afw. It includes (so far) the following HSC issues: - [HSC-1135|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1135] - [HSC-1129|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1129] - [HSC-1215|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1215]
"weekly liaison with ISO Discussed ""should we piggyback signing of the LSST AUP with a capability offered by OSG""? with Alex.  Additional understanding of Authorization/Authentication.",1,DM-2613,datamanagement,weekly liaison iso discussed piggyback signing lsst aup capability offer osg alex additional understanding authorization authentication,"weekly liaison with ISO Discussed ""should we piggyback signing of the LSST AUP with a capability offered by OSG""? with Alex. Additional understanding of Authorization/Authentication."
"Management activities for week of april 21 Met with Julie Robinson, the Illinois contract negotiator, w.r.t. aura ""hardware"" contract amendment.  re-drafted long paragraph i AURA section, breaking it down into separate items for each party, and addressed what I see a grave flaws so that a discussion could be held.  Did work w Margaret do arrange for business proscesst discussion relating to monthly reporting to LSST appended to invoices, basic   Interviewed Martin Paegert (one day visit).  Further work on other other matters relating to open requisitions of people. Other work on personal matters  Responded to comments about the NCSA WBS and overall project WBS not being aligned.  on LLDM-240 -- provided example of working one case -- scheduled for next wee'k LT.  Misc.",5,DM-2614,datamanagement,management activity week april 21 met julie robinson illinois contract negotiator w.r.t aura hardware contract amendment draft long paragraph aura section break separate item party address grave flaw discussion hold work margaret arrange business proscesst discussion relate monthly reporting lsst append invoice basic interviewed martin paegert day visit work matter relate open requisition people work personal matter respond comment ncsa wbs overall project wbs align lldm-240 provide example work case schedule wee'k lt misc,"Management activities for week of april 21 Met with Julie Robinson, the Illinois contract negotiator, w.r.t. aura ""hardware"" contract amendment. re-drafted long paragraph i AURA section, breaking it down into separate items for each party, and addressed what I see a grave flaws so that a discussion could be held. Did work w Margaret do arrange for business proscesst discussion relating to monthly reporting to LSST appended to invoices, basic Interviewed Martin Paegert (one day visit). Further work on other other matters relating to open requisitions of people. Other work on personal matters Responded to comments about the NCSA WBS and overall project WBS not being aligned. on LLDM-240 -- provided example of working one case -- scheduled for next wee'k LT. Misc."
"Reimplement Data Loader Using Worker Mgmt Service Current loader depends on ssh, need to switch to the new service, http based.",8,DM-2619,datamanagement,reimplement data loader worker mgmt service current loader depend ssh need switch new service http base,"Reimplement Data Loader Using Worker Mgmt Service Current loader depends on ssh, need to switch to the new service, http based."
"Add version stamping in czar and ssi service DM-2547 will introduce compile-time version generation of a header file that has macros defining version strings. Ideally, each running process using qserv code (e.g., czar, cmsd, xrootd, and mysql-proxy), and perhaps one-shot binaries (loader?) would print version information when logging to improve debuggability.  DM-2547 focused on the osslib plugin (libxrdoss) for the cmsd. The next important processes are the czar and xrootd (libxrdsvc). This story covers inclusion of version identifiers (w/ commit hash) in the czar and xrootd logs. Hopefully this will end any confusion about versions when reading log files sent from colleagues.",3,DM-2621,datamanagement,add version stamping czar ssi service dm-2547 introduce compile time version generation header file macro define version string ideally running process qserv code e.g. czar cmsd xrootd mysql proxy shot binary loader print version information log improve debuggability dm-2547 focus osslib plugin libxrdoss cmsd important process czar xrootd libxrdsvc story cover inclusion version identifier w/ commit hash czar xrootd log hopefully end confusion version read log file send colleague,"Add version stamping in czar and ssi service DM-2547 will introduce compile-time version generation of a header file that has macros defining version strings. Ideally, each running process using qserv code (e.g., czar, cmsd, xrootd, and mysql-proxy), and perhaps one-shot binaries (loader?) would print version information when logging to improve debuggability. DM-2547 focused on the osslib plugin (libxrdoss) for the cmsd. The next important processes are the czar and xrootd (libxrdsvc). This story covers inclusion of version identifiers (w/ commit hash) in the czar and xrootd logs. Hopefully this will end any confusion about versions when reading log files sent from colleagues."
"Modify czar to support table deletion Czar needs to handle table deletion. In practice that means mysql proxy should let DROP TABLE queries through, and czar should modify appropriate table-related metadata structures in CSS. This is part of work proposed in  DM-1896.  ",6,DM-2622,datamanagement,modify czar support table deletion czar need handle table deletion practice mean mysql proxy let drop table query czar modify appropriate table relate metadata structure css work propose dm-1896,"Modify czar to support table deletion Czar needs to handle table deletion. In practice that means mysql proxy should let DROP TABLE queries through, and czar should modify appropriate table-related metadata structures in CSS. This is part of work proposed in DM-1896."
"Design Basic Watcher Design watcher, including its interactions with other components (mysql, css, etc). In the near term, the watcher will handle deleting tables and databases.",2,DM-2623,datamanagement,design basic watcher design watcher include interaction component mysql css etc near term watcher handle delete table database,"Design Basic Watcher Design watcher, including its interactions with other components (mysql, css, etc). In the near term, the watcher will handle deleting tables and databases."
Implement DROP table in watcher Implement DROP table using the watcher designed in DM-2623.,1,DM-2624,datamanagement,implement drop table watcher implement drop table watcher design dm-2623,Implement DROP table in watcher Implement DROP table using the watcher designed in DM-2623.
Create service for managing watcher We need to be able to start/stop the watcher implemented through DM-2624. This story involves extending our scripts for starting various qserv services to manage watcher.,1,DM-2625,datamanagement,create service manage watcher need able start stop watcher implement dm-2624 story involve extend script start qserv service manage watcher,Create service for managing watcher We need to be able to start/stop the watcher implemented through DM-2624. This story involves extending our scripts for starting various qserv services to manage watcher.
"Add support for configuring multi-node integration tests The multi-node integration test software produced through DM-2175 has hardcoded node names. This story will allow user to configure it. Current plan is to pre-set integration test for several different configurations, e.g., single-node, 2-node, 8-node (and maybe eg 24-node), and user would supply node names through a configuration file.",5,DM-2627,datamanagement,add support configure multi node integration test multi node integration test software produce dm-2175 hardcode node name story allow user configure current plan pre set integration test different configuration e.g. single node node node maybe eg 24 node user supply node name configuration file,"Add support for configuring multi-node integration tests The multi-node integration test software produced through DM-2175 has hardcoded node names. This story will allow user to configure it. Current plan is to pre-set integration test for several different configurations, e.g., single-node, 2-node, 8-node (and maybe eg 24-node), and user would supply node names through a configuration file."
"Integration test succeeds when individual tests fail Integration test behaves strangely, it always succeeds even though there may be tests that fail. Here is what I ge when I run individual case: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ qserv-check-integration.py -i 01 -l ............... 2015-04-28 11:26:12,137 - lsst.qserv.tests.benchmark - ERROR - MySQL/Qserv differs for 4 queries: 2015-04-28 11:26:12,138 - lsst.qserv.tests.benchmark - ERROR - Broken queries list in /usr/local/home/salnikov/qserv-run/2015_04/tmp/qservTest_case01/outputs/qserv: ['0001_fetchObjectById.txt', '0003_selectMetadataForOneGalaxy_withUSING.txt', '0003_selectMetadataForOneGalaxy_classicJOIN.txt', '0003_selectMetadataForOneGalaxy.txt'] 2015-04-28 11:26:12,138 - root - CRITICAL - Test case #01 failed {noformat}  But if I run integration test it says everything is OK: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ qserv-test-integration.py ................... ok  ---------------------------------------------------------------------- Ran 5 tests in 160.058s  OK {noformat}  There are actually messages about failed test in the output but you have to look very closely not to miss them. ",1,DM-2628,datamanagement,"integration test succeed individual test fail integration test behave strangely succeed test fail ge run individual case noformat salnikov@lsst dbdev4 dm-2617]$ qserv-check-integration.py -i 01 2015 04 28 11:26:12,137 lsst.qserv.tests.benchmark error mysql qserv differ query 2015 04 28 11:26:12,138 lsst.qserv.tests.benchmark error break query list /usr local home salnikov qserv run/2015_04 tmp qservtest_case01 output qserv 0001_fetchobjectbyid.txt 0003_selectmetadataforonegalaxy_withusing.txt 0003_selectmetadataforonegalaxy_classicjoin.txt 0003_selectmetadataforonegalaxy.txt 2015 04 28 11:26:12,138 root critical test case 01 fail noformat run integration test say ok noformat salnikov@lsst dbdev4 dm-2617]$ qserv-test-integration.py ok ran test 160.058s ok noformat actually message fail test output look closely miss","Integration test succeeds when individual tests fail Integration test behaves strangely, it always succeeds even though there may be tests that fail. Here is what I ge when I run individual case: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ qserv-check-integration.py -i 01 -l ............... 2015-04-28 11:26:12,137 - lsst.qserv.tests.benchmark - ERROR - MySQL/Qserv differs for 4 queries: 2015-04-28 11:26:12,138 - lsst.qserv.tests.benchmark - ERROR - Broken queries list in /usr/local/home/salnikov/qserv-run/2015_04/tmp/qservTest_case01/outputs/qserv: ['0001_fetchObjectById.txt', '0003_selectMetadataForOneGalaxy_withUSING.txt', '0003_selectMetadataForOneGalaxy_classicJOIN.txt', '0003_selectMetadataForOneGalaxy.txt'] 2015-04-28 11:26:12,138 - root - CRITICAL - Test case #01 failed {noformat} But if I run integration test it says everything is OK: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ qserv-test-integration.py ................... ok ---------------------------------------------------------------------- Ran 5 tests in 160.058s OK {noformat} There are actually messages about failed test in the output but you have to look very closely not to miss them."
Fix build for gcc 4.7.2 and gcc 4.8.2 #include <condition_variable> is missing in threadSafe.h,1,DM-2629,datamanagement,fix build gcc 4.7.2 gcc 4.8.2 include miss,Fix build for gcc 4.7.2 and gcc 4.8.2 #include  is missing in threadSafe.h
"Document configuration tool main use cases - Document main use case for qserv-configure.py: install Qserv master/worker node with externalized data directory  - Hide complex configuration options?  {code} Configuration steps:   General configuration steps    -d, --directory-tree  Create directory tree in QSERV_RUN_DIR, eventually                         create symbolic link from QSERV_RUN_DIR/var/lib to                         QSERV_DATA_DIR.   -e, --etc             Create Qserv configuration files in QSERV_RUN_DIR                         using values issued from meta-config file                         QSERV_RUN_DIR/qserv-meta.conf   -c, --client          Create client configuration file (used by integration                         tests for example)  Components configuration:   Configuration of external components    -X, --xrootd          Create xrootd query and result directories   -C, --css-watcher     Configure CSS-watcher (i.e. MySQL credentials)  Database components configuration:   Configuration of external components impacting data,   launched if and only if QSERV_DATA_DIR is empty    -M, --mysql           Remove MySQL previous data, install db and set                         password   -Q, --qserv-czar      Initialize Qserv master database   -W, --qserv-worker    Initialize Qserv worker database   -S, --scisql          Install and configure SciSQL {code}  ",3,DM-2630,datamanagement,document configuration tool main use case document main use case qserv-configure.py install qserv master worker node externalize data directory hide complex configuration option code configuration step general configuration step --directory tree create directory tree qserv_run_dir eventually create symbolic link qserv_run_dir var lib qserv_data_dir create qserv configuration file qserv_run_dir value issue meta config file qserv_run_dir qserv meta.conf -c create client configuration file integration test example component configuration configuration external component -x create xrootd query result directory -c --css watcher configure css watcher i.e. mysql credential database component configuration configuration external component impact datum launch qserv_data_dir -m remove mysql previous datum install db set password -q czar initialize qserv master database -w worker initialize qserv worker database -s install configure scisql code,"Document configuration tool main use cases - Document main use case for qserv-configure.py: install Qserv master/worker node with externalized data directory - Hide complex configuration options? {code} Configuration steps: General configuration steps -d, --directory-tree Create directory tree in QSERV_RUN_DIR, eventually create symbolic link from QSERV_RUN_DIR/var/lib to QSERV_DATA_DIR. -e, --etc Create Qserv configuration files in QSERV_RUN_DIR using values issued from meta-config file QSERV_RUN_DIR/qserv-meta.conf -c, --client Create client configuration file (used by integration tests for example) Components configuration: Configuration of external components -X, --xrootd Create xrootd query and result directories -C, --css-watcher Configure CSS-watcher (i.e. MySQL credentials) Database components configuration: Configuration of external components impacting data, launched if and only if QSERV_DATA_DIR is empty -M, --mysql Remove MySQL previous data, install db and set password -Q, --qserv-czar Initialize Qserv master database -W, --qserv-worker Initialize Qserv worker database -S, --scisql Install and configure SciSQL {code}"
"add new image stretch algorithm to Firefly visualization  There is a need to include two new stretch algorithms, which are asinh and power law gamma.  The algorithm is as follow: * asinh ## input        zp: zero point of data        mp: maximum point of data        dr:  dynamic range scaling factor of data.  It ranges from 1-100,000        bp: black point for image display        wp: white point for image display ## calculate rescaled data value        rd = dr *(xPix - zp)/mp ## calculate normalized stretch data value         nsd = asinh(rd)/asinh(mp-zp) ## calculate display pixel value        dPix = 255 * (nsd-bp)/wp       Note: The bp, wp values specify how far outside of the scale data one wants the image to display.  By default, setting bp=0 and wp=dr.    * power law gamma ## input \br        zp: zero point of data        mp: maximum point of data        gamma: gamma value for exponent ## calculate rescaled data value        rd = xPix - zp ## calculate normalized stretch data value         nsd =  rd^(1/gamma) / (mp0zp)^(1/gamma) ##  calculate display pixel data value         dPix = 255 * nsd       ",8,DM-2634,datamanagement,"add new image stretch algorithm firefly visualization need include new stretch algorithm asinh power law gamma algorithm follow asinh input zp zero point datum mp maximum point datum dr dynamic range scaling factor datum range 100,000 bp black point image display wp white point image display calculate rescale data value rd dr xpix zp)/mp calculate normalize stretch datum value nsd asinh(rd)/asinh(mp zp calculate display pixel value dpix 255 nsd bp)/wp note bp wp value specify far outside scale datum want image display default set bp=0 wp dr power law gamma input \br zp zero point datum mp maximum point datum gamma gamma value exponent calculate rescale data value rd xpix zp calculate normalize stretch datum value nsd rd^(1 gamma mp0zp)^(1 gamma calculate display pixel data value dpix 255 nsd","add new image stretch algorithm to Firefly visualization There is a need to include two new stretch algorithms, which are asinh and power law gamma. The algorithm is as follow: * asinh ## input zp: zero point of data mp: maximum point of data dr: dynamic range scaling factor of data. It ranges from 1-100,000 bp: black point for image display wp: white point for image display ## calculate rescaled data value rd = dr *(xPix - zp)/mp ## calculate normalized stretch data value nsd = asinh(rd)/asinh(mp-zp) ## calculate display pixel value dPix = 255 * (nsd-bp)/wp Note: The bp, wp values specify how far outside of the scale data one wants the image to display. By default, setting bp=0 and wp=dr. * power law gamma ## input \br zp: zero point of data mp: maximum point of data gamma: gamma value for exponent ## calculate rescaled data value rd = xPix - zp ## calculate normalized stretch data value nsd = rd^(1/gamma) / (mp0zp)^(1/gamma) ## calculate display pixel data value dPix = 255 * nsd"
"Provide a function to return the path to a package, given its name As per RFC-44 we want a simple function in utils that returns the path to a package given a package name. This has the same API as eups.getProductDir, but hides our dependence on eups, as per the RFC.",2,DM-2635,datamanagement,provide function return path package give rfc-44 want simple function util return path package give package api eups.getproductdir hide dependence eup rfc,"Provide a function to return the path to a package, given its name As per RFC-44 we want a simple function in utils that returns the path to a package given a package name. This has the same API as eups.getProductDir, but hides our dependence on eups, as per the RFC."
Update code to use the function provided in DM-2635 As per RFC-44: update existing code that finds packages using eups.getProductDir or by using environment variables to use the function added in DM-2635,3,DM-2636,datamanagement,update code use function provide dm-2635 rfc-44 update exist code find package eups.getproductdir environment variable use function add dm-2635,Update code to use the function provided in DM-2635 As per RFC-44: update existing code that finds packages using eups.getProductDir or by using environment variables to use the function added in DM-2635
Run large scale tests Coordinate running large scale tests.,6,DM-2638,datamanagement,run large scale test coordinate run large scale test,Run large scale tests Coordinate running large scale tests.
"missing dependencies in scons builds ndarray and afw have some headers generated via m4, and while those are built when the package is installed, if someone just tries to build other targets, they aren't - leading to build failures.  We also need to add a dependency from the ""lib"" target to the ""python"" target, because we can't link the Python libraries against the C++ library until it's built.  That needs to be changed in sconsUtils. ",1,DM-2642,datamanagement,miss dependency scon build ndarray afw header generate m4 build package instal try build target lead build failure need add dependency lib target python target link python librarie c++ library build need change sconsutil,"missing dependencies in scons builds ndarray and afw have some headers generated via m4, and while those are built when the package is installed, if someone just tries to build other targets, they aren't - leading to build failures. We also need to add a dependency from the ""lib"" target to the ""python"" target, because we can't link the Python libraries against the C++ library until it's built. That needs to be changed in sconsUtils."
Migrate Qserv to ssi v2 ssi v2 including comple objectification of the interface. Need to migrate qserv to the new interface.,6,DM-2643,datamanagement,migrate qserv ssi v2 ssi v2 include comple objectification interface need migrate qserv new interface,Migrate Qserv to ssi v2 ssi v2 including comple objectification of the interface. Need to migrate qserv to the new interface.
Switch to using shpgeom and remove duplicate code Qserv is currently relying on a copy of the spherical geometry code (in core/modules/sg) instead of relying on the sphgeom module. This needs to be cleaned once we sort out the build issues with sphgeom (DM-2262).,1,DM-2646,datamanagement,switch shpgeom remove duplicate code qserv currently rely copy spherical geometry code core module sg instead rely sphgeom module need clean sort build issue sphgeom dm-2262,Switch to using shpgeom and remove duplicate code Qserv is currently relying on a copy of the spherical geometry code (in core/modules/sg) instead of relying on the sphgeom module. This needs to be cleaned once we sort out the build issues with sphgeom (DM-2262).
"MGT for balance of April  recruiting for open positions Work on accounting infrastrucutre. ""hardware"" contact - -   outline to Jeff over the phone what is coming  work through  inventory infrastructure for materials for La Sereba  review budget and effort projections.  hear file system invesitigations meeting. ",3,DM-2648,datamanagement,mgt balance april recruit open position work accounting infrastrucutre hardware contact outline jeff phone come work inventory infrastructure material la sereba review budget effort projection hear file system invesitigation meeting,"MGT for balance of April recruiting for open positions Work on accounting infrastrucutre. ""hardware"" contact - - outline to Jeff over the phone what is coming work through inventory infrastructure for materials for La Sereba review budget and effort projections. hear file system invesitigations meeting."
Fix thread leak in Qserv Qserv is currently leaking a thread per query. Executing a simple query list select count(*) from Object in a loop results in everything hanging after qserv is up to 67 threads.,6,DM-2653,datamanagement,fix thread leak qserv qserv currently leak thread query execute simple query list select count object loop result hang qserv 67 thread,Fix thread leak in Qserv Qserv is currently leaking a thread per query. Executing a simple query list select count(*) from Object in a loop results in everything hanging after qserv is up to 67 threads.
"Configuration mechanism for GalSim galaxy generation This is an additional script for great3sims to allow simple configuration of the great3sims.run().  Most of the parameters which need to be set are in great3sims.constants.py, though some additional command line parameters may be needed for the run method.",2,DM-2657,datamanagement,configuration mechanism galsim galaxy generation additional script great3sim allow simple configuration great3sims.run parameter need set great3sims.constants.py additional command line parameter need run method,"Configuration mechanism for GalSim galaxy generation This is an additional script for great3sims to allow simple configuration of the great3sims.run(). Most of the parameters which need to be set are in great3sims.constants.py, though some additional command line parameters may be needed for the run method."
"Build Psf Libraries from PhoSim Images Takes output provided by Debbie from PhoSim runs and use them to create libraries of Psfs.  Warp to remove camera distortion if necessary.  This issue does not include figuring out what different categories of Psfs are required, but all of the process issues should be covered in this issue.",4,DM-2658,datamanagement,build psf libraries phosim images take output provide debbie phosim run use create library psfs warp remove camera distortion necessary issue include figure different category psfs require process issue cover issue,"Build Psf Libraries from PhoSim Images Takes output provided by Debbie from PhoSim runs and use them to create libraries of Psfs. Warp to remove camera distortion if necessary. This issue does not include figuring out what different categories of Psfs are required, but all of the process issues should be covered in this issue."
"Categorize Psfs and Distributions Required from PhoSim Request a full focal plane of Psf images. Write code to allow them to be stored in a way which allows us to sample randomly from a full focal plane.  There will be multiple such focal planes, so we also need to be able to pass the information to the measurement algorithm which will allow us to categorize measurements by visit.  This will be done in the Psf Library building code, and will then be passes to the measurement algorithm through the great3sims code which constructs the data for the measurement algorithm.",2,DM-2659,datamanagement,categorize psfs distribution require phosim request focal plane psf image write code allow store way allow sample randomly focal plane multiple focal plane need able pass information measurement algorithm allow categorize measurement visit psf library build code pass measurement algorithm great3sim code construct datum measurement algorithm,"Categorize Psfs and Distributions Required from PhoSim Request a full focal plane of Psf images. Write code to allow them to be stored in a way which allows us to sample randomly from a full focal plane. There will be multiple such focal planes, so we also need to be able to pass the information to the measurement algorithm which will allow us to categorize measurements by visit. This will be done in the Psf Library building code, and will then be passes to the measurement algorithm through the great3sims code which constructs the data for the measurement algorithm."
"Produce HSC Psf sample for use in algorithm testing Produce a set of well distributed Psfs from HSC data.  As long as the Wcs info is also provided, the code to warp them should have been done in a separate issue.",4,DM-2660,datamanagement,produce hsc psf sample use algorithm testing produce set distribute psfs hsc datum long wcs info provide code warp separate issue,"Produce HSC Psf sample for use in algorithm testing Produce a set of well distributed Psfs from HSC data. As long as the Wcs info is also provided, the code to warp them should have been done in a separate issue."
"Alternative parameterized Psfs from PhoSim Michael Schneider has suggested that he can do a better job of creating realistic Psfs from Psf models which he is working on, and which he intends to integrate into GalSim.  These are intriguing, but depend on work which hasn't been done yet.  When these models are fully available in GalSim and supported through the yaml configuration interface, we should work with them.  But this is currently an ""as time permits"" issue.",6,DM-2661,datamanagement,alternative parameterized psfs phosim michael schneider suggest well job create realistic psfs psf model work intend integrate galsim intriguing depend work model fully available galsim support yaml configuration interface work currently time permit issue,"Alternative parameterized Psfs from PhoSim Michael Schneider has suggested that he can do a better job of creating realistic Psfs from Psf models which he is working on, and which he intends to integrate into GalSim. These are intriguing, but depend on work which hasn't been done yet. When these models are fully available in GalSim and supported through the yaml configuration interface, we should work with them. But this is currently an ""as time permits"" issue."
"Prototype test harness for testing measurement algorithms This is a relatively simple task, which will take the Galaxy images from the great3sims modifications and run measurement algorithms on the individual postage stamps.  The result will be a catalog of the measurement outputs, cross-references against the galaxy and psf parmeters used for a given postage stamp.  To do this, we need to combine information from the galaxy catalog and psf catalog into an input catalog for the algorithm.  A source needs to be created for each galaxy which will contain at least the galaxy centroid and footprint relative to the postage stamp.  The postage stamp with Psf appended and the above source much be fed to the measurement algorithm",4,DM-2662,datamanagement,prototype test harness testing measurement algorithm relatively simple task galaxy image great3sim modification run measurement algorithm individual postage stamp result catalog measurement output cros reference galaxy psf parmeter give postage stamp need combine information galaxy catalog psf catalog input catalog algorithm source need create galaxy contain galaxy centroid footprint relative postage stamp postage stamp psf append source feed measurement algorithm,"Prototype test harness for testing measurement algorithms This is a relatively simple task, which will take the Galaxy images from the great3sims modifications and run measurement algorithms on the individual postage stamps. The result will be a catalog of the measurement outputs, cross-references against the galaxy and psf parmeters used for a given postage stamp. To do this, we need to combine information from the galaxy catalog and psf catalog into an input catalog for the algorithm. A source needs to be created for each galaxy which will contain at least the galaxy centroid and footprint relative to the postage stamp. The postage stamp with Psf appended and the above source much be fed to the measurement algorithm"
"Do time tests running measurement algorithms against sample galaxies Jim has suggest that we use cmodel to run these tests, since he is not committing to completing a complete shape measurement algorithm during the next sprint.  So we will do our timing test using cmodel and shapelet approximation, and switch to the new algorithm from Jim when it is available.",5,DM-2663,datamanagement,time test run measurement algorithm sample galaxy jim suggest use cmodel run test commit complete complete shape measurement algorithm sprint timing test cmodel shapelet approximation switch new algorithm jim available,"Do time tests running measurement algorithms against sample galaxies Jim has suggest that we use cmodel to run these tests, since he is not committing to completing a complete shape measurement algorithm during the next sprint. So we will do our timing test using cmodel and shapelet approximation, and switch to the new algorithm from Jim when it is available."
"Find an adequate process platform for shape measurement tests This issue requires an estimate of how many measurements  we will need to run during S 15.  And it also needs an estimate of how long it will take to measure a single galaxy.  We should be able to guess how many galaxies are required to do an accurate assessment of a single parameterization of the shape measurement algorithm.  We probably cannot accurately estimate how much of the parameter space of the shape measurement algorithm we will have to explore.   The total amount of processing required should tell us whether this can be done with simple multi-core systems, or if a more sophisticated parallel process environment is required.  There is a large additional task if ordinary multi-core processing isn't adequate, so this task may spawn a rather large additional issue.",4,DM-2664,datamanagement,find adequate process platform shape measurement test issue require estimate measurement need run 15 need estimate long measure single galaxy able guess galaxy require accurate assessment single parameterization shape measurement algorithm probably accurately estimate parameter space shape measurement algorithm explore total processing require tell simple multi core system sophisticated parallel process environment require large additional task ordinary multi core processing adequate task spawn large additional issue,"Find an adequate process platform for shape measurement tests This issue requires an estimate of how many measurements we will need to run during S 15. And it also needs an estimate of how long it will take to measure a single galaxy. We should be able to guess how many galaxies are required to do an accurate assessment of a single parameterization of the shape measurement algorithm. We probably cannot accurately estimate how much of the parameter space of the shape measurement algorithm we will have to explore. The total amount of processing required should tell us whether this can be done with simple multi-core systems, or if a more sophisticated parallel process environment is required. There is a large additional task if ordinary multi-core processing isn't adequate, so this task may spawn a rather large additional issue."
"Create Analysis code for Constant Shear Tests For any test of shear measurement vs. input shear (where input shear is constant), plot the measured shear vs. input shear and fit the multiplicative bias m and additive bias c.",8,DM-2666,datamanagement,create analysis code constant shear test test shear measurement vs. input shear input shear constant plot measure shear vs. input shear fit multiplicative bias additive bias c.,"Create Analysis code for Constant Shear Tests For any test of shear measurement vs. input shear (where input shear is constant), plot the measured shear vs. input shear and fit the multiplicative bias m and additive bias c."
Build 2015_05 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,1,DM-2671,datamanagement,build 2015_05 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build 2015_05 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
Build 2015_06 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,1,DM-2672,datamanagement,build 2015_06 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build 2015_06 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
Research Serf and Consul Serf: https://serfdom.io  Consul: https://www.consul.io  ,4,DM-2675,datamanagement,research serf consul serf https://serfdom.io consul https://www.consul.io,Research Serf and Consul Serf: https://serfdom.io Consul: https://www.consul.io
"Investigate loading of binary data The binary entries in qserv_testdata are stored as binary values in text files and there is no reason to believe that they are being read into the database correctly, see qserv_testdata/datasets/case01/data/Science_Ccd_Exposure.tsv.gz.     Binary data from text files needs to be in hex format along with whatever other changes need to be made to reliably load the data into the database. Note that this story involves just investigating, it is not yet clear how much work will be needed to properly implement it. This story will help up understand the effort needed.",5,DM-2678,datamanagement,investigate loading binary datum binary entry qserv_testdata store binary value text file reason believe read database correctly qserv_testdata dataset case01 data science_ccd_exposure.tsv.gz binary datum text file need hex format change need reliably load datum database note story involve investigate clear work need properly implement story help understand effort need,"Investigate loading of binary data The binary entries in qserv_testdata are stored as binary values in text files and there is no reason to believe that they are being read into the database correctly, see qserv_testdata/datasets/case01/data/Science_Ccd_Exposure.tsv.gz. Binary data from text files needs to be in hex format along with whatever other changes need to be made to reliably load the data into the database. Note that this story involves just investigating, it is not yet clear how much work will be needed to properly implement it. This story will help up understand the effort needed."
"Fix default LOAD DATA options Integration tests in multi-node produced the following error during data loading: {code} 2015-05-01 17:03:03,030 - lsst.qserv.admin.dataLoader - CRITICAL - Failed to load data into non-partitioned table: Data truncated for column 'poly' at row 60 2015-05-01 17:03:03,031 - root - CRITICAL - Exception occured: Data truncated for column 'poly' at row 60 {code}  The default options for MySQL LOAD DATA need to be fixed for this.",1,DM-2679,datamanagement,"fix default load data option integration test multi node produce follow error datum loading code 2015 05 01 17:03:03,030 lsst.qserv.admin.dataloader critical fail load datum non partitioned table datum truncate column poly row 60 2015 05 01 17:03:03,031 root critical exception occur datum truncate column poly row 60 code default option mysql load datum need fix","Fix default LOAD DATA options Integration tests in multi-node produced the following error during data loading: {code} 2015-05-01 17:03:03,030 - lsst.qserv.admin.dataLoader - CRITICAL - Failed to load data into non-partitioned table: Data truncated for column 'poly' at row 60 2015-05-01 17:03:03,031 - root - CRITICAL - Exception occured: Data truncated for column 'poly' at row 60 {code} The default options for MySQL LOAD DATA need to be fixed for this."
"Fix race condition in userQueryProxy In UserQuery_kill, depending on timing, the call ""uqManager.get(session)->kill()"" can fail if kill is called more than once by user, because the session might get deleted by the earlier kill. To simulate this, I modified the code to delay the second kill as follows:  {code} void UserQuery_kill(int session) {     static int killNo = 0;     killNo ++;     LOGF_INFO(""EXECUTING UserQuery_kill(%1%), %2%"" % session % killNo);     if (killNo > 1) {         sleep(10);     }     uqManager.get(session)->kill(); } {code}  We need to revisit if other functions in this class might suffer in similar way.",4,DM-2681,datamanagement,"fix race condition userqueryproxy userquery_kill depend timing uqmanager.get(session)->kill fail kill call user session delete early kill simulate modify code delay second kill follow code void userquery_kill(int session static int killno killno logf_info(""executing userquery_kill(%1 session killno killno sleep(10 uqmanager.get(session)->kill code need revisit function class suffer similar way","Fix race condition in userQueryProxy In UserQuery_kill, depending on timing, the call ""uqManager.get(session)->kill()"" can fail if kill is called more than once by user, because the session might get deleted by the earlier kill. To simulate this, I modified the code to delay the second kill as follows: {code} void UserQuery_kill(int session) { static int killNo = 0; killNo ++; LOGF_INFO(""EXECUTING UserQuery_kill(%1%), %2%"" % session % killNo); if (killNo > 1) { sleep(10); } uqManager.get(session)->kill(); } {code} We need to revisit if other functions in this class might suffer in similar way."
Add missing empty-chunk-path on Ubuntu 14.04 QSERV_DATA_DIR/var/lib/qserv wasn't created on Ubuntu 14.04 and this was breaking loader script. It was working on SL7 for unknown reason. Creation of the directory has been added to qserv-czar config script.,1,DM-2682,datamanagement,add miss chunk path ubuntu 14.04 qserv_data_dir var lib qserv create ubuntu 14.04 break loader script work sl7 unknown reason creation directory add qserv czar config script,Add missing empty-chunk-path on Ubuntu 14.04 QSERV_DATA_DIR/var/lib/qserv wasn't created on Ubuntu 14.04 and this was breaking loader script. It was working on SL7 for unknown reason. Creation of the directory has been added to qserv-czar config script.
Fix case05 3009_countObjectInRegionWithZFlux freeze This prevents 2014_05 release to pass integration tests.,1,DM-2683,datamanagement,fix case05 3009_countobjectinregionwithzflux freeze prevent 2014_05 release pass integration test,Fix case05 3009_countObjectInRegionWithZFlux freeze This prevents 2014_05 release to pass integration tests.
"Clean up FITS binary table writing FITS binary table is being refactored by necessity on DM-2534, and while there's no similarly urgent need to clean up the writing code, we should do it at some point, as the refactoring of the read code broke some symmetries and made it even harder to follow the writing code that it already was.",4,DM-2687,datamanagement,clean fit binary table write fits binary table refactore necessity dm-2534 similarly urgent need clean writing code point refactoring read code break symmetry hard follow writing code,"Clean up FITS binary table writing FITS binary table is being refactored by necessity on DM-2534, and while there's no similarly urgent need to clean up the writing code, we should do it at some point, as the refactoring of the read code broke some symmetries and made it even harder to follow the writing code that it already was."
Review with Telefonica revised path Tololo - Pachon A meeting in Santiago  with Reuna and Telefonica to discuss the difference in price for the new path from Tololo to Pachon,2,DM-2689,datamanagement,review telefonica revise path tololo pachon meeting santiago reuna telefonica discuss difference price new path tololo pachon,Review with Telefonica revised path Tololo - Pachon A meeting in Santiago with Reuna and Telefonica to discuss the difference in price for the new path from Tololo to Pachon
May 1 management Met with A. Withers first cut read of scada plan. Contact session AURA -- explain gross contract changes accepted i principle),1,DM-2691,datamanagement,management met a. withers cut read scada plan contact session aura explain gross contract change accept principle,May 1 management Met with A. Withers first cut read of scada plan. Contact session AURA -- explain gross contract changes accepted i principle)
"Rule for automatic replication in iRODS Maintaining extra copies/replicas on separate resources is an important tenet in iRODS, with this practice considered key for prevention of data loss. The automatic replication of files upon ingest can be encoded via a system rule, so that data is preserved as a inherent part of storing in iRODS.",2,DM-2692,datamanagement,rule automatic replication irods maintain extra copy replicas separate resource important tenet irods practice consider key prevention data loss automatic replication file ingest encode system rule datum preserve inherent store irods,"Rule for automatic replication in iRODS Maintaining extra copies/replicas on separate resources is an important tenet in iRODS, with this practice considered key for prevention of data loss. The automatic replication of files upon ingest can be encoded via a system rule, so that data is preserved as a inherent part of storing in iRODS."
"Revisit mysql connections usage in integration tests Recent changes in integration tests require too many connections. We need to understand what changes that is now requiring so many connections, and fix it.",4,DM-2693,datamanagement,revisit mysql connection usage integration test recent change integration test require connection need understand change require connection fix,"Revisit mysql connections usage in integration tests Recent changes in integration tests require too many connections. We need to understand what changes that is now requiring so many connections, and fix it."
"Revisit mysql connections from worker Revisit the code that handles mysql connections in qserv. At the moment Qserv will maintain a connection per chunk-query, up to a hardcoded limit (GroupScheduler: 4, ScanScheduler:32).  Also, we have to gracefully handle connection issues (such as dropped connection, or if we hit the max_connections limit).",8,DM-2694,datamanagement,revisit mysql connection worker revisit code handle mysql connection qserv moment qserv maintain connection chunk query hardcode limit groupscheduler scanscheduler:32 gracefully handle connection issue drop connection hit max_connection limit,"Revisit mysql connections from worker Revisit the code that handles mysql connections in qserv. At the moment Qserv will maintain a connection per chunk-query, up to a hardcoded limit (GroupScheduler: 4, ScanScheduler:32). Also, we have to gracefully handle connection issues (such as dropped connection, or if we hit the max_connections limit)."
Research GPFS Server for Performant Access to Condo Storage Work with NCSA SET to figure out requirements for LSST GPFS Server access to our Condo storage. Implementation will be a different story. Expect this to take 3-6 story points over the next couple of weeks.,1,DM-2696,datamanagement,research gpfs server performant access condo storage work ncsa set figure requirement lsst gpfs server access condo storage implementation different story expect story point couple week,Research GPFS Server for Performant Access to Condo Storage Work with NCSA SET to figure out requirements for LSST GPFS Server access to our Condo storage. Implementation will be a different story. Expect this to take 3-6 story points over the next couple of weeks.
"Fix connection leak Fix connection leak: 1 connection is leaking per chunk-query, in practice ~30+ connections for a query that touches many chunks.  It is a real blocker, and we need to fix it asap.",6,DM-2698,datamanagement,fix connection leak fix connection leak connection leak chunk query practice connection query touch chunk real blocker need fix asap,"Fix connection leak Fix connection leak: 1 connection is leaking per chunk-query, in practice ~30+ connections for a query that touches many chunks. It is a real blocker, and we need to fix it asap."
"Final cleanup of Query cancellation code The query cancellation code that went in through DM-1716 works fine, however we feel it'd be good to do another pass and double check we are applying the cancellation consistently. Some potential places to clean: 1. in ccontrol/UserQuery.cc we changed the semantics of discard() 2. QueryRequest needs some cleanup: it'd be better to call Finished() from one place  More regarding the former (from DM-1716 PR): ""if a query is cancelled, none of the cleanup below happens in discard() anymore -- presumably we are now waiting for object deletion to do the cleanup.  If object deletion is sufficient to do this cleanup, do we need discard() at all anymore? It would be best if cleanup always occured in the same place rather than having two different control paths for it?""  Regarding the latter - see comment in https://jira.lsstcorp.org/browse/DM-1716",4,DM-2699,datamanagement,final cleanup query cancellation code query cancellation code go dm-1716 work fine feel good pass double check apply cancellation consistently potential place clean ccontrol userquery.cc change semantic discard queryrequest need cleanup well finished place dm-1716 pr query cancel cleanup happen discard anymore presumably wait object deletion cleanup object deletion sufficient cleanup need discard anymore good cleanup occur place have different control path comment https://jira.lsstcorp.org/browse/dm-1716,"Final cleanup of Query cancellation code The query cancellation code that went in through DM-1716 works fine, however we feel it'd be good to do another pass and double check we are applying the cancellation consistently. Some potential places to clean: 1. in ccontrol/UserQuery.cc we changed the semantics of discard() 2. QueryRequest needs some cleanup: it'd be better to call Finished() from one place More regarding the former (from DM-1716 PR): ""if a query is cancelled, none of the cleanup below happens in discard() anymore -- presumably we are now waiting for object deletion to do the cleanup. If object deletion is sufficient to do this cleanup, do we need discard() at all anymore? It would be best if cleanup always occured in the same place rather than having two different control paths for it?"" Regarding the latter - see comment in https://jira.lsstcorp.org/browse/DM-1716"
"Fix memory leak in Executive There is a memory leak, most likely in Executive, related to _requesters. It looks like the ~MergingRequester() is never during normal operations (it is called when there are abnormal conditions and different parts of the code are triggered).   As a result _infileMerger kept inside MergineRequester is not called either, which results in 2 connection leaks per query.",6,DM-2703,datamanagement,fix memory leak executive memory leak likely executive relate requester look like ~mergingrequester normal operation call abnormal condition different part code trigger result infilemerger keep inside merginerequester call result connection leak query,"Fix memory leak in Executive There is a memory leak, most likely in Executive, related to _requesters. It looks like the ~MergingRequester() is never during normal operations (it is called when there are abnormal conditions and different parts of the code are triggered). As a result _infileMerger kept inside MergineRequester is not called either, which results in 2 connection leaks per query."
"Understand race condition in Executive::_dispatchQuery Inserting a log (presumably just a delay) in Executive::_dispatchQuery after the new QueryResource but before the Provision call causes queries to fail.  The particular test query was ""select count(*) from Object"" on test case 01.",2,DM-2708,datamanagement,understand race condition executive::_dispatchquery insert log presumably delay executive::_dispatchquery new queryresource provision cause query fail particular test query select count object test case 01,"Understand race condition in Executive::_dispatchQuery Inserting a log (presumably just a delay) in Executive::_dispatchQuery after the new QueryResource but before the Provision call causes queries to fail. The particular test query was ""select count(*) from Object"" on test case 01."
"Convert the ds9 interface to follow RFC-42 RFC-42 (provide a backend-agnostic interface to displays) being accepted, please implement it.    For now, provide compatibility code so that the old way (import lsst.afw.display.ds9) still works.  ",6,DM-2709,datamanagement,convert ds9 interface follow rfc-42 rfc-42 provide backend agnostic interface display accept implement provide compatibility code old way import lsst.afw.display.ds9 work,"Convert the ds9 interface to follow RFC-42 RFC-42 (provide a backend-agnostic interface to displays) being accepted, please implement it. For now, provide compatibility code so that the old way (import lsst.afw.display.ds9) still works."
"Mutex use before creation qana/QueryPlugin.cc contains a static boost::mutex, that is used by static class member functions to register plugin implementations. Its constructor is not guaranteed to be called before the static registerXXXPlugin (see e.g. qana/AggregatePlugin.cc) instances use it to register plugin classes.",1,DM-2710,datamanagement,mutex use creation qana queryplugin.cc contain static boost::mutex static class member function register plugin implementation constructor guarantee call static registerxxxplugin e.g. qana aggregateplugin.cc instance use register plugin class,"Mutex use before creation qana/QueryPlugin.cc contains a static boost::mutex, that is used by static class member functions to register plugin implementations. Its constructor is not guaranteed to be called before the static registerXXXPlugin (see e.g. qana/AggregatePlugin.cc) instances use it to register plugin classes."
Migrate boost:thread to std::thread We are mixing boost and std threading libraries. This should be cleaned up - use std:thread consistently everywhere.,5,DM-2711,datamanagement,migrate boost thread std::thread mix boost std threading library clean use std thread consistently,Migrate boost:thread to std::thread We are mixing boost and std threading libraries. This should be cleaned up - use std:thread consistently everywhere.
"Migrate boost::shared_ptr to std::shared_ptr We are mixing boost and std shared_ptrs. This should be cleaned up - use std:shared_ptr consistently everywhere. In a few places we have other types of pointers, (e.g weak_ptr). Migrate these too.",2,DM-2712,datamanagement,migrate boost::shared_ptr std::shared_ptr mix boost std shared_ptrs clean use std shared_ptr consistently place type pointer e.g weak_ptr migrate,"Migrate boost::shared_ptr to std::shared_ptr We are mixing boost and std shared_ptrs. This should be cleaned up - use std:shared_ptr consistently everywhere. In a few places we have other types of pointers, (e.g weak_ptr). Migrate these too."
Fix connection leak (2nd iteration) Fix connection leak (and memory leak and thread leak) -- we are leaking 2 per query.,2,DM-2716,datamanagement,fix connection leak 2nd iteration fix connection leak memory leak thread leak leak query,Fix connection leak (2nd iteration) Fix connection leak (and memory leak and thread leak) -- we are leaking 2 per query.
"Add test involving many chunks It might be useful to add a test to the integration test suite that involves a large number of chunks per node. I think I'd try something like 200-300 chunks. I'd 1. add case06 2. get one table, say Object from case05 and configure partitioning to ensure we have 200-300 chunks. 3. Run several queries that touch all chunks.",5,DM-2717,datamanagement,add test involve chunk useful add test integration test suite involve large number chunk node think try like 200 300 chunk add case06 table object case05 configure partition ensure 200 300 chunk run query touch chunk,"Add test involving many chunks It might be useful to add a test to the integration test suite that involves a large number of chunks per node. I think I'd try something like 200-300 chunks. I'd 1. add case06 2. get one table, say Object from case05 and configure partitioning to ensure we have 200-300 chunks. 3. Run several queries that touch all chunks."
"Upgrade EUPS used by lsstsw As discussed, bump it up when you get a chance please. ",1,DM-2718,datamanagement,upgrade eups lsstsw discuss bump chance,"Upgrade EUPS used by lsstsw As discussed, bump it up when you get a chance please."
"Migrate boost::scoped_ptr to std We have a few places where we are using boost::scoped_ptr. Given we migrated shared_ptrs, we might want to move scoped_ptrs too (most likely to std::unique_ptr).",1,DM-2720,datamanagement,migrate boost::scoped_ptr std place boost::scoped_ptr give migrate shared_ptrs want scoped_ptrs likely std::unique_ptr,"Migrate boost::scoped_ptr to std We have a few places where we are using boost::scoped_ptr. Given we migrated shared_ptrs, we might want to move scoped_ptrs too (most likely to std::unique_ptr)."
"Revisit design of query poisoner As we discovered through DM-2698, poisoner tends to hold onto query resources even after the query completes. We should revisit whether than can be redesigned and improved, so that when query finishes, all resources related to that query are immediately automatically released. This story involves just the planning part, implementation will be done through separate stories.",1,DM-2722,datamanagement,revisit design query poisoner discover dm-2698 poisoner tend hold query resource query complete revisit redesign improve query finish resource relate query immediately automatically release story involve planning implementation separate story,"Revisit design of query poisoner As we discovered through DM-2698, poisoner tends to hold onto query resources even after the query completes. We should revisit whether than can be redesigned and improved, so that when query finishes, all resources related to that query are immediately automatically released. This story involves just the planning part, implementation will be done through separate stories."
Package Python requests package To complete DM-2593 we need to package and install `requests` as a separate package instead using one from anaconda.,1,DM-2727,datamanagement,package python request package complete dm-2593 need package install request separate package instead anaconda,Package Python requests package To complete DM-2593 we need to package and install `requests` as a separate package instead using one from anaconda.
"Build should fail if node.js is not present Problem: I built Firefly by mistake w/o having node on my path. The build didn't signal any errors, but generated an unusable webapp that wouldn't load.  Expected behavior: the build should have failed and warned the user that node.js is missing.",2,DM-2728,datamanagement,build fail node.js present problem build firefly mistake w/o have node path build signal error generate unusable webapp load expect behavior build fail warn user node.js miss,"Build should fail if node.js is not present Problem: I built Firefly by mistake w/o having node on my path. The build didn't signal any errors, but generated an unusable webapp that wouldn't load. Expected behavior: the build should have failed and warned the user that node.js is missing."
"Fix a few more g++ 4.9.2 compatos Some of the recent boost -> std changes don't compile/link under gcc 4.9.2, because of some poor #include hygiene (including <thread> when we should include <condition_variable>, not explicitly including <unistd.h>, etc.)  Also, -pthread linker option is required when using std::thread under gcc 4.9.2. ",1,DM-2729,datamanagement,fix g++ 4.9.2 compatos recent boost std change compile link gcc 4.9.2 poor include hygiene include include explicitly include etc -pthread linker option require std::thread gcc 4.9.2,"Fix a few more g++ 4.9.2 compatos Some of the recent boost -> std changes don't compile/link under gcc 4.9.2, because of some poor #include hygiene (including  when we should include , not explicitly including , etc.) Also, -pthread linker option is required when using std::thread under gcc 4.9.2."
"Add config file for test dataset 04 tables Following the changes to default LOAD DATA settings in DM-2679, two tables in test case 04 need to have a config file to include their in.csv format.",1,DM-2734,datamanagement,add config file test dataset 04 table follow change default load data setting dm-2679 table test case 04 need config file include in.csv format,"Add config file for test dataset 04 tables Following the changes to default LOAD DATA settings in DM-2679, two tables in test case 04 need to have a config file to include their in.csv format."
"optimistic matcher may match the same reference object to more than one source The optimistic pattern matcher in meas_astrom, adapted from hscAstrom, does not check if reference objects have been used before when finding the reference object nearest to each source. As a result the same reference object may be matched to more than one source. This should not happen.",4,DM-2735,datamanagement,optimistic matcher match reference object source optimistic pattern matcher meas_astrom adapt hscastrom check reference object find reference object near source result reference object match source happen,"optimistic matcher may match the same reference object to more than one source The optimistic pattern matcher in meas_astrom, adapted from hscAstrom, does not check if reference objects have been used before when finding the reference object nearest to each source. As a result the same reference object may be matched to more than one source. This should not happen."
"Log xrootd client debug messages in Qserv czar xrootd client print it's debug messages to stdout. This ticket aims at redirecting them to Qserv logger, if possible.",4,DM-2736,datamanagement,log xrootd client debug message qserv czar xrootd client print debug message stdout ticket aim redirect qserv logger possible,"Log xrootd client debug messages in Qserv czar xrootd client print it's debug messages to stdout. This ticket aims at redirecting them to Qserv logger, if possible."
"Build a DiscreteSkyMap that covers a collection of input exposures This is essentially a rehash of the old trac Ticket #[2702| https://dev.lsstcorp.org/trac/ticket/2702], originally reported by [~jbosch], which reads:  ""I'd like to add a Task and bin script to create a DiscreteSkyMap that bounds a set of calexps specified by their data IDs. This makeDiscreteSkyMap.py could be used instead of makeSkyMap.py when the user would rather compute the pointing and size of the skymap from the input data than decide it manually.""  The work was done by [~jbosch] & [~price] and exists on branch {{u/price/2702}} in {{pipe_tasks}}, but it was never merged to master.  I plan to simply rebase the commits in that branch onto master.",1,DM-2737,datamanagement,build discreteskymap cover collection input exposure essentially rehash old trac ticket 2702| https://dev.lsstcorp.org/trac/ticket/2702 originally report ~jbosch read like add task bin script create discreteskymap bound set calexps specify datum id makediscreteskymap.py instead makeskymap.py user compute pointing size skymap input datum decide manually work ~jbosch ~price exist branch price/2702 pipe_tasks merge master plan simply rebase commit branch master,"Build a DiscreteSkyMap that covers a collection of input exposures This is essentially a rehash of the old trac Ticket #[2702| https://dev.lsstcorp.org/trac/ticket/2702], originally reported by [~jbosch], which reads: ""I'd like to add a Task and bin script to create a DiscreteSkyMap that bounds a set of calexps specified by their data IDs. This makeDiscreteSkyMap.py could be used instead of makeSkyMap.py when the user would rather compute the pointing and size of the skymap from the input data than decide it manually."" The work was done by [~jbosch] & [~price] and exists on branch {{u/price/2702}} in {{pipe_tasks}}, but it was never merged to master. I plan to simply rebase the commits in that branch onto master."
"Remove #include ""XrdOuc/XrdOucTrace.hh"" from Qserv code See next emails:  Hi Fabrice,  Absolutely!  Andy  On Wed, 13 May 2015, Fabrice Jammes wrote:  > Hi Andy, > > Thanks, > > In my understanding, you're ok if I remove the existing > #include ""XrdOuc/XrdOucTrace.hh"" > from Qserv source code. I'll do it soon. > > Have a nice day, > > Fabrice > > Le 12/05/2015 23:41, Andrew Hanushevsky a écrit : >> Hi Fabrice, >> >> Well, no. We have a long-standing approach that qserv should not depend on anything outside of XrdSsi public interfaces. This is the only way to easily protect sqserv code from infrastructure changes. So, I would not. If you want to copy something like that for >> >> qserv please do, it's simple enough. But in the end qserv needs to be self-contained in that it does not depend on xrootd code just the public ssi interfaces. >> >> Andy >> >> -----Original Message----- From: Fabrice Jammes >> Sent: Tuesday, May 12, 2015 9:06 AM >> To: Andrew Hanushevsky >> Subject: About xrdssi client logging >> >> Hi Andy, >> >> Hope you're doing well. >> Could you please tell me if its usefull to include >> #include ""XrdOuc/XrdOucTrace.hh"" >> in our xrdssi client code? >> >> Indeed client seems to only print DBG macro output, that's why I was >> wondering if XrdOucTrace was only use on the server side. >> If yes, I will remove it from our client. >> >> Thanks, and have a nice day, >> >> Fabrice ",1,DM-2738,datamanagement,remove include xrdouc xrdouctrace.hh qserv code email hi fabrice absolutely andy 13 2015 fabrice jammes write hi andy thank understanding ok remove exist include xrdouc xrdouctrace.hh qserv source code soon nice day fabrice le 12/05/2015 23:41 andrew hanushevsky crit hi fabrice long stand approach qserv depend outside xrdssi public interface way easily protect sqserv code infrastructure change want copy like qserv simple end qserv need self contain depend xrootd code public ssi interface andy -----original message----- fabrice jammes send tuesday 12 2015 9:06 andrew hanushevsky subject xrdssi client log hi andy hope tell usefull include include xrdouc xrdouctrace.hh xrdssi client code client print dbg macro output wonder xrdouctrace use server yes remove client thank nice day fabrice,"Remove #include ""XrdOuc/XrdOucTrace.hh"" from Qserv code See next emails: Hi Fabrice, Absolutely! Andy On Wed, 13 May 2015, Fabrice Jammes wrote: > Hi Andy, > > Thanks, > > In my understanding, you're ok if I remove the existing > #include ""XrdOuc/XrdOucTrace.hh"" > from Qserv source code. I'll do it soon. > > Have a nice day, > > Fabrice > > Le 12/05/2015 23:41, Andrew Hanushevsky a crit : >> Hi Fabrice, >> >> Well, no. We have a long-standing approach that qserv should not depend on anything outside of XrdSsi public interfaces. This is the only way to easily protect sqserv code from infrastructure changes. So, I would not. If you want to copy something like that for >> >> qserv please do, it's simple enough. But in the end qserv needs to be self-contained in that it does not depend on xrootd code just the public ssi interfaces. >> >> Andy >> >> -----Original Message----- From: Fabrice Jammes >> Sent: Tuesday, May 12, 2015 9:06 AM >> To: Andrew Hanushevsky >> Subject: About xrdssi client logging >> >> Hi Andy, >> >> Hope you're doing well. >> Could you please tell me if its usefull to include >> #include ""XrdOuc/XrdOucTrace.hh"" >> in our xrdssi client code? >> >> Indeed client seems to only print DBG macro output, that's why I was >> wondering if XrdOucTrace was only use on the server side. >> If yes, I will remove it from our client. >> >> Thanks, and have a nice day, >> >> Fabrice"
"Make ANetAstrometryTask more configurable The current ANetAstrometryTask has a solver that is not easy to retarget. This makes testing with hscAstrom needlessly difficult. My suggestion is to make the solver a true Task instead of a task-like object, and make it retargetable using a ConfigurableField instead of a ConfigField. This is very easy to do because the solver is already a task in all but name. ",2,DM-2740,datamanagement,anetastrometrytask configurable current anetastrometrytask solver easy retarget make testing hscastrom needlessly difficult suggestion solver true task instead task like object retargetable configurablefield instead configfield easy solver task,"Make ANetAstrometryTask more configurable The current ANetAstrometryTask has a solver that is not easy to retarget. This makes testing with hscAstrom needlessly difficult. My suggestion is to make the solver a true Task instead of a task-like object, and make it retargetable using a ConfigurableField instead of a ConfigField. This is very easy to do because the solver is already a task in all but name."
"sandbox selection of newinstall.sh source url Frossie would like the ability to control the source URL for the newinstall.sh script in sandbox-stackbuild.  The newinstall.sh installation logic needs to be migration to the puppet-lsststack module, converted into a defined type, and have unit+ acceptance tests written for it.",1,DM-2743,datamanagement,sandbox selection newinstall.sh source url frossie like ability control source url newinstall.sh script sandbox stackbuild newinstall.sh installation logic need migration puppet lsststack module convert define type unit+ acceptance test write,"sandbox selection of newinstall.sh source url Frossie would like the ability to control the source URL for the newinstall.sh script in sandbox-stackbuild. The newinstall.sh installation logic needs to be migration to the puppet-lsststack module, converted into a defined type, and have unit+ acceptance tests written for it."
Second Review with Chris Smith AURA head Went over the process relating to AURA and NSF,4,DM-2744,datamanagement,second review chris smith aura head go process relate aura nsf,Second Review with Chris Smith AURA head Went over the process relating to AURA and NSF
"Add clear message when integration test fails Integration test fails without printing a clear message at the end, and for now a query is broken: 0011_selectDeepCoadd.txt but it isn't printed at the end of tet output.",2,DM-2748,datamanagement,add clear message integration test fail integration test fail print clear message end query break 0011_selectdeepcoadd.txt print end tet output,"Add clear message when integration test fails Integration test fails without printing a clear message at the end, and for now a query is broken: 0011_selectDeepCoadd.txt but it isn't printed at the end of tet output."
"Fix case04/0011_selectDeepCoadd.txt It seems --config=/path/to/table.cfg param can be duplicated (see dbLoader l.77 and QservDbLoader l. 87)    Futthermore there is an enclosing pb and it can be solved for this query by passing correct cfg table (which in.csv.enclose correct parameter), but then next query fails: it seems some cfg parameters of table.cfg aren't managed correctly by the loader in plain MySQL mode.     This need further investigations.",6,DM-2750,datamanagement,fix case04/0011_selectdeepcoadd.txt table.cfg param duplicate dbloader l.77 qservdbloader l. 87 futthermore enclose pb solve query pass correct cfg table in.csv.enclose correct parameter query fail cfg parameter table.cfg manage correctly loader plain mysql mode need investigation,"Fix case04/0011_selectDeepCoadd.txt It seems --config=/path/to/table.cfg param can be duplicated (see dbLoader l.77 and QservDbLoader l. 87) Futthermore there is an enclosing pb and it can be solved for this query by passing correct cfg table (which in.csv.enclose correct parameter), but then next query fails: it seems some cfg parameters of table.cfg aren't managed correctly by the loader in plain MySQL mode. This need further investigations."
Allow lsst/log library to log PID on the C++ side lsst/log should be able to log application PID,3,DM-2751,datamanagement,allow lsst log library log pid c++ lsst log able log application pid,Allow lsst/log library to log PID on the C++ side lsst/log should be able to log application PID
"db 10.1+4 tests randomly fail with python egg installation error The unit tests for DB seem to fail at random and always pass on a second build attempt.  My hunch is that multiple tests are running in parallel all attempting to install the mysql module but I haven't investigated.  {code}                   db: 10.1+4 ERROR (0 sec). *** error building product db. *** exit code = 2 *** log is in /home/build0/lsstsw/build/db/_build.log *** last few lines: :::::  [2015-05-15T19:12:35.557258Z] scons: done reading SConscript files. :::::  [2015-05-15T19:12:35.558276Z] scons: Building targets ... :::::  [2015-05-15T19:12:35.558409Z] scons: Nothing to be done for `python'. :::::  [2015-05-15T19:12:35.570007Z] makeVersionModule([""python/lsst/db/version.py""], []) :::::  [2015-05-15T19:12:35.686733Z] running tests/testDbLocal.py... running tests/testDbRemote.py... running tests/testDbPool.py... failed :::::  [2015-05-15T19:12:35.695011Z] passed :::::  [2015-05-15T19:12:35.698811Z] passed :::::  [2015-05-15T19:12:35.706360Z] 1 tests failed :::::  [2015-05-15T19:12:35.706703Z] scons: *** [checkTestStatus] Error 1 :::::  [2015-05-15T19:12:35.708443Z] scons: building terminated because of errors. {code}  {code} [root@ip-192-168-123-151 .tests]# cat * tests/testDbLocal.py  Traceback (most recent call last):   File ""tests/testDbLocal.py"", line 53, in <module>     from lsst.db.db import Db, DbException   File ""/home/build0/lsstsw/build/db/python/lsst/db/db.py"", line 49, in <module>     import MySQLdb   File ""build/bdist.linux-x86_64/egg/MySQLdb/__init__.py"", line 19, in <module>        File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 7, in <module>   File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 4, in __bootstrap__   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 937, in resource_filename   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1632, in get_resource_filename   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1662, in _extract_resource   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1003, in get_cache_path   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 983, in extraction_error pkg_resources.ExtractionError: Can't extract file(s) to egg cache  The following error occurred while trying to extract file(s) to the Python egg cache:    [Errno 17] File exists: '/home/build0/.python-eggs'  The Python egg cache directory is currently set to:    /home/build0/.python-eggs  Perhaps your account does not have write access to this directory?  You can change the cache directory by setting the PYTHON_EGG_CACHE environment variable to point to an accessible directory.  tests/testDbPool.py  /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-test.txt' not found. tests/testDbRemote.py  /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-testRemote.txt' not found. {code}",1,DM-2752,datamanagement,"db 10.1 test randomly fail python egg installation error unit test db fail random pass second build attempt hunch multiple test run parallel attempt install mysql module investigate code db 10.1 error sec error building product db exit code log build0 lsstsw build db/_build.log line 2015 05 15t19:12:35.557258z scon read sconscript file 2015 05 15t19:12:35.558276z scon build target 2015 05 15t19:12:35.558409z scon python 2015 05 15t19:12:35.570007z makeversionmodule([""python lsst db version.py 2015 05 15t19:12:35.686733z run test testdblocal.py run test testdbremote.py run test testdbpool.py fail 2015 05 15t19:12:35.695011z pass 2015 05 15t19:12:35.698811z pass 2015 05 15t19:12:35.706360z test fail 2015 05 15t19:12:35.706703z scon checkteststatus error 2015 05 15t19:12:35.708443z scon building terminate error code code root@ip-192 168 123 151 .test cat test testdblocal.py traceback recent file test testdblocal.py line 53 lsst.db.db import db dbexception file /home build0 lsstsw build db python lsst db db.py line 49 import mysqldb file build bdist.linux x86_64 egg mysqldb/__init__.py line 19 file build bdist.linux x86_64 egg/_mysql.py line file build bdist.linux x86_64 egg/_mysql.py line bootstrap file /home build0 lsstsw anaconda lib python2.7 site package setuptools-5.8 py2.7.egg pkg_resources.py line 937 resource_filename file /home build0 lsstsw anaconda lib python2.7 site package setuptools-5.8 py2.7.egg pkg_resources.py line 1632 get_resource_filename file /home build0 lsstsw anaconda lib python2.7 site package setuptools-5.8 py2.7.egg pkg_resources.py line 1662 extract_resource file /home build0 lsstsw anaconda lib python2.7 site package setuptools-5.8 py2.7.egg pkg_resources.py line 1003 get_cache_path file /home build0 lsstsw anaconda lib python2.7 site package setuptools-5.8 py2.7.egg pkg_resources.py line 983 extraction_error pkg_resource extractionerror extract file(s egg cache follow error occur try extract file(s python egg cache errno 17 file exist /home build0/.python egg python egg cache directory currently set /home build0/.python egg account write access directory change cache directory set python_egg_cache environment variable point accessible directory test testdbpool.py /home build0 lsstsw anaconda lib python2.7 site package setuptools-5.8 py2.7.egg pkg_resources.py:1032 userwarning /home build0/.python egg writable group vulnerable attack get_resource_filename consider secure location set python_egg_cache environment variable 05/15/2015 07:12:35 root warning require file credential /home build0/.lsst dbauth test.txt find test testdbremote.py build0 lsstsw anaconda lib python2.7 site package setuptools-5.8 py2.7.egg pkg_resources.py:1032 userwarning /home build0/.python egg writable group vulnerable attack get_resource_filename consider secure location set python_egg_cache environment variable 05/15/2015 07:12:35 root warning require file credential /home build0/.lsst dbauth testremote.txt find code","db 10.1+4 tests randomly fail with python egg installation error The unit tests for DB seem to fail at random and always pass on a second build attempt. My hunch is that multiple tests are running in parallel all attempting to install the mysql module but I haven't investigated. {code} db: 10.1+4 ERROR (0 sec). *** error building product db. *** exit code = 2 *** log is in /home/build0/lsstsw/build/db/_build.log *** last few lines: ::::: [2015-05-15T19:12:35.557258Z] scons: done reading SConscript files. ::::: [2015-05-15T19:12:35.558276Z] scons: Building targets ... ::::: [2015-05-15T19:12:35.558409Z] scons: Nothing to be done for `python'. ::::: [2015-05-15T19:12:35.570007Z] makeVersionModule([""python/lsst/db/version.py""], []) ::::: [2015-05-15T19:12:35.686733Z] running tests/testDbLocal.py... running tests/testDbRemote.py... running tests/testDbPool.py... failed ::::: [2015-05-15T19:12:35.695011Z] passed ::::: [2015-05-15T19:12:35.698811Z] passed ::::: [2015-05-15T19:12:35.706360Z] 1 tests failed ::::: [2015-05-15T19:12:35.706703Z] scons: *** [checkTestStatus] Error 1 ::::: [2015-05-15T19:12:35.708443Z] scons: building terminated because of errors. {code} {code} [root@ip-192-168-123-151 .tests]# cat * tests/testDbLocal.py Traceback (most recent call last): File ""tests/testDbLocal.py"", line 53, in  from lsst.db.db import Db, DbException File ""/home/build0/lsstsw/build/db/python/lsst/db/db.py"", line 49, in  import MySQLdb File ""build/bdist.linux-x86_64/egg/MySQLdb/__init__.py"", line 19, in  File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 7, in  File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 4, in __bootstrap__ File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 937, in resource_filename File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1632, in get_resource_filename File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1662, in _extract_resource File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1003, in get_cache_path File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 983, in extraction_error pkg_resources.ExtractionError: Can't extract file(s) to egg cache The following error occurred while trying to extract file(s) to the Python egg cache: [Errno 17] File exists: '/home/build0/.python-eggs' The Python egg cache directory is currently set to: /home/build0/.python-eggs Perhaps your account does not have write access to this directory? You can change the cache directory by setting the PYTHON_EGG_CACHE environment variable to point to an accessible directory. tests/testDbPool.py /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-test.txt' not found. tests/testDbRemote.py /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-testRemote.txt' not found. {code}"
"Write example-based documentation for multiband processing The multi-band coadd processing tasks we're porting over from the HSC side don't have the high-quality example-based documentation we typically provide for Tasks on the LSST side, so we need to write it from scratch.",6,DM-2754,datamanagement,write example base documentation multiband process multi band coadd processing task port hsc high quality example base documentation typically provide tasks lsst need write scratch,"Write example-based documentation for multiband processing The multi-band coadd processing tasks we're porting over from the HSC side don't have the high-quality example-based documentation we typically provide for Tasks on the LSST side, so we need to write it from scratch."
"Improve selection criteria for sources Dominique Boutigny has demonstrated that one reason the new astrometry task is working so poorly is that it is not selective enough about which sources it uses. This ticket is to be used to improve that situation.  Another problem Dominique discovered is that the TAN-SIP WCS fitter needs to be iterated to work properly, and that work may also be done on this ticket. ",4,DM-2755,datamanagement,improve selection criterion source dominique boutigny demonstrate reason new astrometry task work poorly selective source use ticket improve situation problem dominique discover tan sip wcs fitter need iterate work properly work ticket,"Improve selection criteria for sources Dominique Boutigny has demonstrated that one reason the new astrometry task is working so poorly is that it is not selective enough about which sources it uses. This ticket is to be used to improve that situation. Another problem Dominique discovered is that the TAN-SIP WCS fitter needs to be iterated to work properly, and that work may also be done on this ticket."
Administrative - 6-2015 Meetings and reporting and such,2,DM-2757,datamanagement,administrative 2015 meetings reporting,Administrative - 6-2015 Meetings and reporting and such
Onboarding Humberto Efforts in helping new employee Humberto come up to speed in his role as lead on perfsonar deployments,1,DM-2759,datamanagement,onboarding humberto efforts help new employee humberto come speed role lead perfsonar deployment,Onboarding Humberto Efforts in helping new employee Humberto come up to speed in his role as lead on perfsonar deployments
"Avoid leaking memory allocated by mysql_thread_init mysql/MySqlConnection.cc contains the following comment: {code}     // Dangerous to use mysql_thread_end(), because caller may belong to a     // different thread other than the one that called mysql_init(). Suggest     // using thread-local-storage to track users of mysql_init(), and to call     // mysql_thread_end() appropriately. Not an easy thing to do right now, and     // shouldn't be a big deal because we thread-pool anyway. {code}  The comment is not really correct with regards to thread pooling. Instead, each rproc::InfileMerger has an rproc::InfileMerger::Mgr which contains a util::WorkQueue that spawns a thread, and so we are failing to call mysql_thread_end at least once per user query. This has been verified using the memcheck valgrind tool. ",3,DM-2762,datamanagement,avoid leak memory allocate mysql_thread_init mysql mysqlconnection.cc contain following comment code dangerous use mysql_thread_end caller belong // different thread call mysql_init suggest thread local storage track user mysql_init // mysql_thread_end appropriately easy thing right big deal thread pool code comment correct regard thread pooling instead rproc::infilemerger rproc::infilemerger::mgr contain util::workqueue spawn thread fail mysql_thread_end user query verify memcheck valgrind tool,"Avoid leaking memory allocated by mysql_thread_init mysql/MySqlConnection.cc contains the following comment: {code} // Dangerous to use mysql_thread_end(), because caller may belong to a // different thread other than the one that called mysql_init(). Suggest // using thread-local-storage to track users of mysql_init(), and to call // mysql_thread_end() appropriately. Not an easy thing to do right now, and // shouldn't be a big deal because we thread-pool anyway. {code} The comment is not really correct with regards to thread pooling. Instead, each rproc::InfileMerger has an rproc::InfileMerger::Mgr which contains a util::WorkQueue that spawns a thread, and so we are failing to call mysql_thread_end at least once per user query. This has been verified using the memcheck valgrind tool."
"Fix ORDER BY in integration test query case03 0019.1.0  ORDER BY fails sometimes for unknown reason, see  datasets/case03/queries/0019.1.0_selectRunDeepSourceDeepcoaddDeepsrcmatchRefobject.sql.FIXME",4,DM-2766,datamanagement,fix order integration test query case03 0019.1.0 order fail unknown reason dataset case03 queries/0019.1.0_selectrundeepsourcedeepcoadddeepsrcmatchrefobject.sql fixme,"Fix ORDER BY in integration test query case03 0019.1.0 ORDER BY fails sometimes for unknown reason, see datasets/case03/queries/0019.1.0_selectRunDeepSourceDeepcoaddDeepsrcmatchRefobject.sql.FIXME"
"sconsUtil has a hard dependency on EUPS for both tests and installation After some discussion on Data Management, its clear that sconsUtils is a hard requirement on EUPS for both tests and installation.  It was decided by RFC-44 that tests should not depend on EUPS.  However, I'd argue that sconsUtils should also not depend on EUPS as any package that uses sconsUtils (the virtual entirety of the stack) can not build or run tests without the presence of EUPS.  The current situation is that the complete stack has a hard dependency on EUPS.    Attempting to build sconUtils without the presence of EUPS.  The tests fail attempting to import the eups module.  {code}  $ SCONSUTILS_DIR=. scons -Q  Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Unable to import eups; guessing flavor  Doxygen is not setup; skipping documentation build.  ImportError: No module named eups:    File ""/home/vagrant/sconsUtils/SConstruct"", line 9:      scripts.BasicSConstruct.initialize(packageName=""sconsUtils"")    File ""python/lsst/sconsUtils/scripts.py"", line 106:      SCons.Script.SConscript(os.path.join(root, ""SConscript""))    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 609:      return method(*args, **kw)    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 546:      return _SConscript(self.fs, *files, **subst_kw)    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 260:      exec _file_ in call_stack[-1].globals    File ""/home/vagrant/sconsUtils/tests/SConscript"", line 5:      import eups  {code}    Attempting to bypass the test failures:  {code}  [vagrant@jenkins-el7-1 sconsUtils]$ rm -rf tests  [vagrant@jenkins-el7-1 sconsUtils]$ SCONSUTILS_DIR=. scons -Q install  Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Error with git version: uncommitted changes  Found problem with version number; update or specify force=True to proceed  {code}",6,DM-2769,datamanagement,"sconsutil hard dependency eups test installation discussion data management clear sconsutil hard requirement eups test installation decide rfc-44 test depend eups argue sconsutil depend eups package use sconsutil virtual entirety stack build run test presence eups current situation complete stack hard dependency eups attempt build sconutil presence eups test fail attempt import eup module code sconsutils_dir=. scon -q unable import eup guess flavor cc gcc version 4.8.3 checking c++11 support c++11 support -std c++11 unable import eup guess flavor doxygen setup skip documentation build importerror module name eup file /home vagrant sconsutils sconstruct line script basicsconstruct.initialize(packagename=""sconsutil file python lsst sconsutils scripts.py line 106 scon script sconscript(os.path.join(root sconscript file /usr lib scon scons script sconscript.py line 609 return method(*args kw file /usr lib scon scons script sconscript.py line 546 return sconscript(self.fs file subst_kw file /usr lib scon scons script sconscript.py line 260 exec file call_stack[-1].global file /home vagrant sconsutils test sconscript line import eup code attempt bypass test failure code vagrant@jenkins el7 sconsutils]$ rm -rf test vagrant@jenkins el7 sconsutils]$ sconsutils_dir=. scon -q install unable import eup guess flavor cc gcc version 4.8.3 checking c++11 support c++11 support -std c++11 error git version uncommitted change find problem version number update specify force true proceed code","sconsUtil has a hard dependency on EUPS for both tests and installation After some discussion on Data Management, its clear that sconsUtils is a hard requirement on EUPS for both tests and installation. It was decided by RFC-44 that tests should not depend on EUPS. However, I'd argue that sconsUtils should also not depend on EUPS as any package that uses sconsUtils (the virtual entirety of the stack) can not build or run tests without the presence of EUPS. The current situation is that the complete stack has a hard dependency on EUPS. Attempting to build sconUtils without the presence of EUPS. The tests fail attempting to import the eups module. {code} $ SCONSUTILS_DIR=. scons -Q Unable to import eups; guessing flavor CC is gcc version 4.8.3 Checking for C++11 support C++11 supported with '-std=c++11' Unable to import eups; guessing flavor Doxygen is not setup; skipping documentation build. ImportError: No module named eups: File ""/home/vagrant/sconsUtils/SConstruct"", line 9: scripts.BasicSConstruct.initialize(packageName=""sconsUtils"") File ""python/lsst/sconsUtils/scripts.py"", line 106: SCons.Script.SConscript(os.path.join(root, ""SConscript"")) File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 609: return method(*args, **kw) File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 546: return _SConscript(self.fs, *files, **subst_kw) File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 260: exec _file_ in call_stack[-1].globals File ""/home/vagrant/sconsUtils/tests/SConscript"", line 5: import eups {code} Attempting to bypass the test failures: {code} [vagrant@jenkins-el7-1 sconsUtils]$ rm -rf tests [vagrant@jenkins-el7-1 sconsUtils]$ SCONSUTILS_DIR=. scons -Q install Unable to import eups; guessing flavor CC is gcc version 4.8.3 Checking for C++11 support C++11 supported with '-std=c++11' Error with git version: uncommitted changes Found problem with version number; update or specify force=True to proceed {code}"
"sconsUtil install target does not respond to either force=True or --force I've been unable to figure out how to bypass the install 'force' check, but have confirmed that this is the correct expression by commenting it out:    https://github.com/lsst/sconsUtils/blob/54c983ffe9714a33657c4388de3506fe7a40518d/python/lsst/sconsUtils/installation.py#L92    {code}  $ SCONSUTILS_DIR=. scons -Q force=True install   Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Error with git version: uncommitted changes  Found problem with version number; update or specify force=True to proceed  {code}  ",1,DM-2770,datamanagement,sconsutil install target respond force true unable figure bypass install force check confirm correct expression comment https://github.com/lsst/sconsutils/blob/54c983ffe9714a33657c4388de3506fe7a40518d/python/lsst/sconsutils/installation.py#l92 code sconsutils_dir=. scon -q force true install unable import eup guess flavor cc gcc version 4.8.3 checking c++11 support c++11 support -std c++11 error git version uncommitted change find problem version number update specify force true proceed code,"sconsUtil install target does not respond to either force=True or --force I've been unable to figure out how to bypass the install 'force' check, but have confirmed that this is the correct expression by commenting it out: https://github.com/lsst/sconsUtils/blob/54c983ffe9714a33657c4388de3506fe7a40518d/python/lsst/sconsUtils/installation.py#L92 {code} $ SCONSUTILS_DIR=. scons -Q force=True install Unable to import eups; guessing flavor CC is gcc version 4.8.3 Checking for C++11 support C++11 supported with '-std=c++11' Error with git version: uncommitted changes Found problem with version number; update or specify force=True to proceed {code}"
"Improve SIP fitting Dominique Boutigny says ""the tan-sip fitter is very sensitive to bad matches. This is a weakness of the fitter and I think that it could (should) be rewritten in such a way to reject the outliers internally.""  This has resulted in iteration in the matching (DM-2755), which should be unnecessary (or at least minimised).    Additionally, it seems the SIP fitter fits for x and y in subsequent iterations, which can confuse users.    We should:  1. Make the SIP fitter fit for x and y concurrently.  2. Add rejection iterations in the SIP fitter.  3. Remove or minimise the iterations in the matching.",6,DM-2775,datamanagement,improve sip fit dominique boutigny say tan sip fitter sensitive bad match weakness fitter think rewrite way reject outlier internally result iteration matching dm-2755 unnecessary minimise additionally sip fitter fit subsequent iteration confuse user sip fitter fit concurrently add rejection iteration sip fitter remove minimise iteration matching,"Improve SIP fitting Dominique Boutigny says ""the tan-sip fitter is very sensitive to bad matches. This is a weakness of the fitter and I think that it could (should) be rewritten in such a way to reject the outliers internally."" This has resulted in iteration in the matching (DM-2755), which should be unnecessary (or at least minimised). Additionally, it seems the SIP fitter fits for x and y in subsequent iterations, which can confuse users. We should: 1. Make the SIP fitter fit for x and y concurrently. 2. Add rejection iterations in the SIP fitter. 3. Remove or minimise the iterations in the matching."
"Fix races in BlendScheduler _integrityHelper() from wsched/BlendScheduler inspects a map of tasks and is sometimes called without holding the corresponding mutex. My theory is that it is observing the map in an inconsistent state, leading to assert failure and hence worker death, and finally to hangs/timeouts on the czar.",2,DM-2777,datamanagement,fix race blendscheduler integrityhelper wsched blendscheduler inspect map task call hold corresponding mutex theory observe map inconsistent state lead assert failure worker death finally hang timeout czar,"Fix races in BlendScheduler _integrityHelper() from wsched/BlendScheduler inspects a map of tasks and is sometimes called without holding the corresponding mutex. My theory is that it is observing the map in an inconsistent state, leading to assert failure and hence worker death, and finally to hangs/timeouts on the czar."
HSC backport: allow for use of Approximate model in background estimation This issue involves transferring changesets from the following HSC issues:    - [HSC-145|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-145]  Investigate approximating rather than interpolating backgrounds  - [HSC-1047|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1047] Background object cannot be loaded with butler  - [HSC-1213|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1213] Set background 'approximate' control settings when background control is created.  - [HSC-1221|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1221] tests failing in ip_diffim  - [HSC-1217|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1217] Verify backgroundList IO works properly when Approximate is enabled in background control - HSC JIRA    The Approximate (Chebyshev) approach greatly improves the background subtraction around bright objects compared with the interpolation scheme currently in use (which over-subtracts near bright objects).,6,DM-2778,datamanagement,hsc backport allow use approximate model background estimation issue involve transfer changeset following hsc issue hsc-145|https://hsc jira.astro.princeton.edu jira browse hsc-145 investigate approximate interpolate background hsc-1047|https://hsc jira.astro.princeton.edu jira browse hsc-1047 background object load butler hsc-1213|https://hsc jira.astro.princeton.edu jira browse hsc-1213 set background approximate control setting background control create hsc-1221|https://hsc jira.astro.princeton.edu jira browse hsc-1221 test fail ip_diffim hsc-1217|https://hsc jira.astro.princeton.edu jira browse hsc-1217 verify backgroundlist io work properly approximate enable background control hsc jira approximate chebyshev approach greatly improve background subtraction bright object compare interpolation scheme currently use subtract near bright object,HSC backport: allow for use of Approximate model in background estimation This issue involves transferring changesets from the following HSC issues: - [HSC-145|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-145] Investigate approximating rather than interpolating backgrounds - [HSC-1047|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1047] Background object cannot be loaded with butler - [HSC-1213|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1213] Set background 'approximate' control settings when background control is created. - [HSC-1221|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1221] tests failing in ip_diffim - [HSC-1217|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1217] Verify backgroundList IO works properly when Approximate is enabled in background control - HSC JIRA The Approximate (Chebyshev) approach greatly improves the background subtraction around bright objects compared with the interpolation scheme currently in use (which over-subtracts near bright objects).
Fix race in Foreman The Foreman implementation passes a TaskQueue pointer corresponding to running tasks down to the task scheduler without holding a lock. This means that the scheduler can inspect the running task list (usually to determine its size) while it is being mutated.,2,DM-2779,datamanagement,fix race foreman foreman implementation pass taskqueue pointer correspond run task task scheduler hold lock mean scheduler inspect run task list usually determine size mutate,Fix race in Foreman The Foreman implementation passes a TaskQueue pointer corresponding to running tasks down to the task scheduler without holding a lock. This means that the scheduler can inspect the running task list (usually to determine its size) while it is being mutated.
"Firefly Tools API: Add advance region support Firefly Tools API: Add advance region support  Improve firefly's region functionality to support a ""dynamic region"".  Data can be added or removed from this region by API calls.  Allow any amount of region lines to be added or removed.  Make sure performance is good.  Also, document the current Firefly region support.",2,DM-2782,datamanagement,firefly tools api add advance region support firefly tools api add advance region support improve firefly region functionality support dynamic region datum add remove region api call allow region line add remove sure performance good document current firefly region support,"Firefly Tools API: Add advance region support Firefly Tools API: Add advance region support Improve firefly's region functionality to support a ""dynamic region"". Data can be added or removed from this region by API calls. Allow any amount of region lines to be added or removed. Make sure performance is good. Also, document the current Firefly region support."
"Control firefly viewer tri-view mode When table data is add to Firefly Viewer, control whether it goes into tri-view or just overlay data on FITS, or just does an XYPlot, etc",6,DM-2783,datamanagement,control firefly view tri view mode table datum add firefly viewer control go tri view overlay datum fits xyplot etc,"Control firefly viewer tri-view mode When table data is add to Firefly Viewer, control whether it goes into tri-view or just overlay data on FITS, or just does an XYPlot, etc"
"FFTools python wrapper: make launch Browser smarter.  FireflyClient.launchBrowser() needs to send an event to the server who will attempt to guess if there is an existing connection.  It will not be launch in that case.  This way it can be called every time without creating tons of tabs, who are all talking to the same channel.    Also, launchBrowser really should not return until the tab is ready to receive events from the websocket channel.  Both of these feature are going to take some thought on how to do.  This is a multi-threaded problem on both the client and the server.    ",4,DM-2785,datamanagement,fftool python wrapper launch browser smart fireflyclient.launchbrowser need send event server attempt guess exist connection launch case way call time create ton tab talk channel launchbrowser return tab ready receive event websocket channel feature go thought multi thread problem client server,"FFTools python wrapper: make launch Browser smarter. FireflyClient.launchBrowser() needs to send an event to the server who will attempt to guess if there is an existing connection. It will not be launch in that case. This way it can be called every time without creating tons of tabs, who are all talking to the same channel. Also, launchBrowser really should not return until the tab is ready to receive events from the websocket channel. Both of these feature are going to take some thought on how to do. This is a multi-threaded problem on both the client and the server."
"Footprint dilation performance regression In DM-1128 we implemented span-based dilation for footprints. A brief test on synthetic data indicated that this was a performance win over the previous version of the code.    In May 2015, this code was merged to HSC and applied to significant quantities of real data for the first time. A major performance regression was identified:    {quote}  [May-9 00:26] Paul Price: processCcd is now crazy slow.  [May-9 00:29] Paul Price: Profiling...  [May-9 00:40] Paul Price: I'm thinking it's the Footprint grow code...  [May-9 00:44] Paul Price: And the winner is…. Footprint construction:  [May-9 00:44] Paul Price: 2    0.000    0.000  702.280  351.140 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:191(makeSourceCatalog)         2    0.005    0.002  702.274  351.137 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:228(detectFootprints)       15    0.001    0.000  698.597  46.573 /home/pprice/hsc/afw/python/lsst/afw/detection/detectionLib.py:3448(__init__)       15  698.596  46.573  698.596  46.573 {_detectionLib.new_FootprintSet}  [May-9 00:53] Paul Price: If I revert HSC-1243 (""Port better Footprint-grow code from LSST""), then the performance regression goes away.  @jbosch @jds may be interested...  {quote}    The source of the regression must be identified and resolved for both HSC and LSST.",5,DM-2787,datamanagement,footprint dilation performance regression dm-1128 implement span base dilation footprint brief test synthetic datum indicate performance win previous version code 2015 code merge hsc apply significant quantity real datum time major performance regression identify quote may-9 00:26 paul price processccd crazy slow may-9 00:29 paul price profiling may-9 00:40 paul price think footprint grow code may-9 00:44 paul price winner footprint construction may-9 00:44 paul price 0.000 0.000 702.280 351.140 /home astro hsc product linux64 meas_algorithms hsc-3.8.0 python lsst meas algorithm detection.py:191(makesourcecatalog 0.005 0.002 702.274 351.137 /home astro hsc product linux64 meas_algorithms hsc-3.8.0 python lsst meas algorithm detection.py:228(detectfootprint 15 0.001 0.000 698.597 46.573 /home pprice hsc afw python lsst afw detection detectionlib.py:3448(__init 15 698.596 46.573 698.596 46.573 detectionlib.new_footprintset may-9 00:53 paul price revert hsc-1243 port well footprint grow code lsst performance regression go away @jbosch @jds interested quote source regression identify resolve hsc lsst,"Footprint dilation performance regression In DM-1128 we implemented span-based dilation for footprints. A brief test on synthetic data indicated that this was a performance win over the previous version of the code. In May 2015, this code was merged to HSC and applied to significant quantities of real data for the first time. A major performance regression was identified: {quote} [May-9 00:26] Paul Price: processCcd is now crazy slow. [May-9 00:29] Paul Price: Profiling... [May-9 00:40] Paul Price: I'm thinking it's the Footprint grow code... [May-9 00:44] Paul Price: And the winner is . Footprint construction: [May-9 00:44] Paul Price: 2 0.000 0.000 702.280 351.140 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:191(makeSourceCatalog) 2 0.005 0.002 702.274 351.137 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:228(detectFootprints) 15 0.001 0.000 698.597 46.573 /home/pprice/hsc/afw/python/lsst/afw/detection/detectionLib.py:3448(__init__) 15 698.596 46.573 698.596 46.573 {_detectionLib.new_FootprintSet} [May-9 00:53] Paul Price: If I revert HSC-1243 (""Port better Footprint-grow code from LSST""), then the performance regression goes away. @jbosch @jds may be interested... {quote} The source of the regression must be identified and resolved for both HSC and LSST."
"rename CameraMapper.getEupsProductName() to getPackageName() and convert to abstract method Per discussion on this PR related to DM-2636:  https://github.com/lsst/daf_butlerUtils/pull/1#issuecomment-104785055    The CameraMapper.getEupsProductName() should be renamed to getPackageName() and converted to an abstract method.  This will eliminates a runtime, and thus ""test time"", dependency on EUPS.  As part of the rename/conversion, all subclasses that are not already overriding getEupsProductName() will concurrently need to have getPackageName() implemented.",3,DM-2789,datamanagement,rename cameramapper.geteupsproductname getpackagename convert abstract method discussion pr relate dm-2636 https://github.com/lsst/daf_butlerutils/pull/1#issuecomment-104785055 cameramapper.geteupsproductname rename getpackagename convert abstract method eliminate runtime test time dependency eups rename conversion subclass override geteupsproductname concurrently need getpackagename implement,"rename CameraMapper.getEupsProductName() to getPackageName() and convert to abstract method Per discussion on this PR related to DM-2636: https://github.com/lsst/daf_butlerUtils/pull/1#issuecomment-104785055 The CameraMapper.getEupsProductName() should be renamed to getPackageName() and converted to an abstract method. This will eliminates a runtime, and thus ""test time"", dependency on EUPS. As part of the rename/conversion, all subclasses that are not already overriding getEupsProductName() will concurrently need to have getPackageName() implemented."
"meas_modelfit not in full-stack doxygen build I'm fairly certain meas_modelfit is included in lsst_apps and hence in CI, but id doesn't seem to be included in the LSST Doxygen build:    http://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/search.php?query=modelfit",1,DM-2790,datamanagement,meas_modelfit stack doxygen build fairly certain meas_modelfit include lsst_apps ci include lsst doxygen build http://lsst-web.ncsa.illinois.edu/doxygen/x_masterdoxydoc/search.php?query=modelfit,"meas_modelfit not in full-stack doxygen build I'm fairly certain meas_modelfit is included in lsst_apps and hence in CI, but id doesn't seem to be included in the LSST Doxygen build: http://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/search.php?query=modelfit"
"Make the new astrometry task the default task The new astrometry task should be the default astrometry task, but we need to make sure it is good enough first.",1,DM-2792,datamanagement,new astrometry task default task new astrometry task default astrometry task need sure good,"Make the new astrometry task the default task The new astrometry task should be the default astrometry task, but we need to make sure it is good enough first."
"Improve behavior of new matcher on highly distorted fields The optimistic matcher used by the new astrometry task probably does not handle highly distorted fields well. The issue is that it tries to match in X-Y space, and if that has significant curvature then the match is not optimal.    I suggest matching in RA/Dec space, as per Tabur's original algorithm (on which this code is based). This is simple and easy to understand    An alternative is to use the old technique of ""undistorting"" source and reference object positions before matching. This works, but is complicated, difficult to understand and adds an unnecessary step.  ",8,DM-2793,datamanagement,improve behavior new matcher highly distorted field optimistic matcher new astrometry task probably handle highly distorted field issue try match space significant curvature match optimal suggest match ra dec space tabur original algorithm code base simple easy understand alternative use old technique undistorte source reference object position match work complicated difficult understand add unnecessary step,"Improve behavior of new matcher on highly distorted fields The optimistic matcher used by the new astrometry task probably does not handle highly distorted fields well. The issue is that it tries to match in X-Y space, and if that has significant curvature then the match is not optimal. I suggest matching in RA/Dec space, as per Tabur's original algorithm (on which this code is based). This is simple and easy to understand An alternative is to use the old technique of ""undistorting"" source and reference object positions before matching. This works, but is complicated, difficult to understand and adds an unnecessary step."
Fiber from Tololo to Pachon Preparation for fiber cable from Cerro Tololo to Cerro Pachon,5,DM-2794,datamanagement,fiber tololo pachon preparation fiber cable cerro tololo cerro pachon,Fiber from Tololo to Pachon Preparation for fiber cable from Cerro Tololo to Cerro Pachon
Various fixes for broken code within display=True clauses and/or using --debug Running with the display and/or debug options turned on is revealing many instances of code that is now suffering from bit rot.  This ticket will be used to track those encountered while trying to debug issues arising while porting HSC code and running processing tasks on real data.,4,DM-2795,datamanagement,fix broken code display true clause and/or run display and/or debug option turn reveal instance code suffer bit rot ticket track encounter try debug issue arise port hsc code run processing task real datum,Various fixes for broken code within display=True clauses and/or using --debug Running with the display and/or debug options turned on is revealing many instances of code that is now suffering from bit rot. This ticket will be used to track those encountered while trying to debug issues arising while porting HSC code and running processing tasks on real data.
"Implement HSC improvements to Colorterm Paul Price recommended some HSC changes for the Colorterm class. To quote Paul: changed Colorterms so it's not a global, and it can now be configured using Config. See https://github.com/HyperSuprime-Cam/meas_astrom/blob/master/python/lsst/meas/photocal/colorterms.py    This sounds useful. Note that the HSC colorterms.py is in meas_astrom but as of DM-1578 the LSST version is in pipe_tasks.",6,DM-2797,datamanagement,implement hsc improvement colorterm paul price recommend hsc change colorterm class quote paul change colorterm global configure config https://github.com/hypersuprime-cam/meas_astrom/blob/master/python/lsst/meas/photocal/colorterms.py sound useful note hsc colorterms.py meas_astrom dm-1578 lsst version pipe_task,"Implement HSC improvements to Colorterm Paul Price recommended some HSC changes for the Colorterm class. To quote Paul: changed Colorterms so it's not a global, and it can now be configured using Config. See https://github.com/HyperSuprime-Cam/meas_astrom/blob/master/python/lsst/meas/photocal/colorterms.py This sounds useful. Note that the HSC colorterms.py is in meas_astrom but as of DM-1578 the LSST version is in pipe_tasks."
Tests for daf_butlerUtils should not depend on obs_lsstSim Currently two of the tests in {{daf_butlerUtils}} depend on {{obs_lsstSim}}. They will never run in a normal build because {{obs_}} packages can not be a dependency on {{daf_butlerUtils}}.    After discussing the options with [~ktl] the feeling is that {{ticket1640}} should be rewritten to remove the dependency and {{ticket1580}} can probably be removed.,2,DM-2799,datamanagement,test daf_butlerutil depend obs_lsstsim currently test daf_butlerutils depend obs_lsstsim run normal build obs package dependency daf_butlerutils discuss option ~ktl feeling ticket1640 rewrite remove dependency ticket1580 probably remove,Tests for daf_butlerUtils should not depend on obs_lsstSim Currently two of the tests in {{daf_butlerUtils}} depend on {{obs_lsstSim}}. They will never run in a normal build because {{obs_}} packages can not be a dependency on {{daf_butlerUtils}}. After discussing the options with [~ktl] the feeling is that {{ticket1640}} should be rewritten to remove the dependency and {{ticket1580}} can probably be removed.
Review with Contractors preparing fiber path Hold conversations with the two major fiber laying contractors to prepare the path from Tololo to Pachon with a trench,2,DM-2800,datamanagement,review contractor prepare fiber path hold conversation major fiber lay contractor prepare path tololo pachon trench,Review with Contractors preparing fiber path Hold conversations with the two major fiber laying contractors to prepare the path from Tololo to Pachon with a trench
Document NCSA Wide Area Network status now and in the near future Write up a document explaining how the wide area network is evolving at NCSA.,2,DM-2801,datamanagement,document ncsa wide area network status near future write document explain wide area network evolve ncsa,Document NCSA Wide Area Network status now and in the near future Write up a document explaining how the wide area network is evolving at NCSA.
"Adapt multi-node tests to latest version of qserv / loader The multi-node integration tests have to be updated to work with the latest changes to qserv, in particular the loader, which broke already working tests lately.",8,DM-2803,datamanagement,adapt multi node test late version qserv loader multi node integration test update work late change qserv particular loader break work test lately,"Adapt multi-node tests to latest version of qserv / loader The multi-node integration tests have to be updated to work with the latest changes to qserv, in particular the loader, which broke already working tests lately."
Implement query metadata skeleton Skeleton implementation of the Query Metadata - including the APIs and core functionality (accepting long running query and saving the info about it),8,DM-2804,datamanagement,implement query metadata skeleton skeleton implementation query metadata include api core functionality accept long run query save info,Implement query metadata skeleton Skeleton implementation of the Query Metadata - including the APIs and core functionality (accepting long running query and saving the info about it)
Creation of chunked views in wmgr Current implementation of the creating chunks for views in wmgr is likely not doing right thing. Need to find an example of the partitioned views and implement correct procedure.,3,DM-2817,datamanagement,creation chunk view wmgr current implementation create chunk view wmgr likely right thing need find example partition view implement correct procedure,Creation of chunked views in wmgr Current implementation of the creating chunks for views in wmgr is likely not doing right thing. Need to find an example of the partitioned views and implement correct procedure.
Document architecture of the data loader Fabrice requested documentation for the overall architecture of the data loader.,2,DM-2818,datamanagement,document architecture datum loader fabrice request documentation overall architecture datum loader,Document architecture of the data loader Fabrice requested documentation for the overall architecture of the data loader.
"Implement RESTful interfaces for Database (POST) Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""POST"" type requests only, ""GET"" will be handled separately.",8,DM-2827,datamanagement,implement restful interface database post implement restful interface database https://confluence.lsstcorp.org/display/dm/api base prototype develop dm-1695 work include add support return appropriately format result support common format cover post type request handle separately,"Implement RESTful interfaces for Database (POST) Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""POST"" type requests only, ""GET"" will be handled separately."
"Add unit tests for the new colorterms code The new colorterms code that we adopted from HSC may not have complete unit tests. The existing colorterms test is pretty good, but may have holes. I'm more concerned about the unit test for PhotoCalTask, which does not apply colorterms at all (likely an existing issue).    Also, be sure to test that the obs_cfht config override loads correctly (presumably with a unit test in obs_cfht) and similarly for obs_subaru.",4,DM-2837,datamanagement,add unit test new colorterm code new colorterm code adopt hsc complete unit test exist colorterm test pretty good hole concerned unit test photocaltask apply colorterm likely exist issue sure test obs_cfht config override load correctly presumably unit test obs_cfht similarly obs_subaru,"Add unit tests for the new colorterms code The new colorterms code that we adopted from HSC may not have complete unit tests. The existing colorterms test is pretty good, but may have holes. I'm more concerned about the unit test for PhotoCalTask, which does not apply colorterms at all (likely an existing issue). Also, be sure to test that the obs_cfht config override loads correctly (presumably with a unit test in obs_cfht) and similarly for obs_subaru."
sconsUtils should notice when SWIG python file has been modified Currently {{scons}} will not rerun tests if a {{.i}} file has been modified if the only outcome of that modification was a change to the {{.py}} file. {{sconsUtils}} should be modified to look for changes in both SWIG output files.,2,DM-2839,datamanagement,sconsutils notice swig python file modify currently scon rerun test .i file modify outcome modification change .py file sconsutils modify look change swig output file,sconsUtils should notice when SWIG python file has been modified Currently {{scons}} will not rerun tests if a {{.i}} file has been modified if the only outcome of that modification was a change to the {{.py}} file. {{sconsUtils}} should be modified to look for changes in both SWIG output files.
"Add support for listing async queries Modify mysql proxy and implement ""show processlist"" command, which should display list of currently running queries.",4,DM-2840,datamanagement,add support list async query modify mysql proxy implement processlist command display list currently run query,"Add support for listing async queries Modify mysql proxy and implement ""show processlist"" command, which should display list of currently running queries."
"Write Qserv User Guide It'd be useful to write a document about Qserv geared towards users, describing what queries Qserv supports now, what will be supported in the future, what restrictions we are imposing and such. ",8,DM-2841,datamanagement,write qserv user guide useful write document qserv gear user describe query qserv support support future restriction impose,"Write Qserv User Guide It'd be useful to write a document about Qserv geared towards users, describing what queries Qserv supports now, what will be supported in the future, what restrictions we are imposing and such."
"Tweaks to OO display interface When I wrote the initial version of display_firefly I found a few minor issues in the way I'd designed the Display class; at the same time, [~lauren] found some missing functions in the backward-compatibility support for ds9.    Please fix these;  note that this implies changes to afw, display_ds9, and display_firefly.  ",2,DM-2849,datamanagement,tweak oo display interface write initial version display_firefly find minor issue way design display class time ~lauren find miss function backward compatibility support ds9 fix note imply change afw display_ds9 display_firefly,"Tweaks to OO display interface When I wrote the initial version of display_firefly I found a few minor issues in the way I'd designed the Display class; at the same time, [~lauren] found some missing functions in the backward-compatibility support for ds9. Please fix these; note that this implies changes to afw, display_ds9, and display_firefly."
"getSchemaCatalogs() breaks Task encapsulation The {{getSchemaCatalogs()}} method was added to {{Task}} to allow {{CmdLineTasks}} to introspect their subtasks for schemas they produce, but it requires the subtasks to report the schemas by butler dataset.  This limits subtask reusability by locking them into producing a particular Butler dataset (or, as in DM-2191, requiring additional arguments from their parent task that they wouldn't need with a better design).    Instead, we should have per-subtask-slot interfaces (i.e. an interface for all subtasks that could fill a particular role in a CmdLineTask) for how the parent tasks should retrieve their schemas.  This will require `CmdLineTask` subclasses to implement the `writeSchemas` method themselves, instead of inheriting an implementation from `CmdLineTask` itself.",2,DM-2850,datamanagement,getschemacatalogs break task encapsulation getschemacatalogs method add task allow cmdlinetasks introspect subtask schema produce require subtask report schema butler dataset limit subtask reusability lock produce particular butler dataset dm-2191 require additional argument parent task need well design instead subtask slot interface i.e. interface subtask fill particular role cmdlinetask parent task retrieve schema require cmdlinetask subclass implement writeschemas method instead inherit implementation cmdlinetask,"getSchemaCatalogs() breaks Task encapsulation The {{getSchemaCatalogs()}} method was added to {{Task}} to allow {{CmdLineTasks}} to introspect their subtasks for schemas they produce, but it requires the subtasks to report the schemas by butler dataset. This limits subtask reusability by locking them into producing a particular Butler dataset (or, as in DM-2191, requiring additional arguments from their parent task that they wouldn't need with a better design). Instead, we should have per-subtask-slot interfaces (i.e. an interface for all subtasks that could fill a particular role in a CmdLineTask) for how the parent tasks should retrieve their schemas. This will require `CmdLineTask` subclasses to implement the `writeSchemas` method themselves, instead of inheriting an implementation from `CmdLineTask` itself."
"Build the recent 10.1 release  & Gather strace logs for file system testing We build the recent Version 10.1 stack release in a Centos 6.6 docker container. As we do so, we also gather strace logs for candidate packages  (for example, afw) for analysis within an effort to create load simulators for file system testing/profiling.   As another product of the effort,  I will make a docker image of the latest release installed on Centos 6.6 and push to  docker hub.",4,DM-2851,datamanagement,build recent 10.1 release gather strace log file system testing build recent version 10.1 stack release centos 6.6 docker container gather strace log candidate package example afw analysis effort create load simulator file system testing profiling product effort docker image late release instal centos 6.6 push docker hub,"Build the recent 10.1 release & Gather strace logs for file system testing We build the recent Version 10.1 stack release in a Centos 6.6 docker container. As we do so, we also gather strace logs for candidate packages (for example, afw) for analysis within an effort to create load simulators for file system testing/profiling. As another product of the effort, I will make a docker image of the latest release installed on Centos 6.6 and push to docker hub."
"Put together a few slides for NCSA-IN2P3 meeting  I put together a few slides for the NCSA-IN2P3 meeting describing previous scaling, middleware, and processing efforts of LSST DM. ",1,DM-2853,datamanagement,slide ncsa in2p3 meeting slide ncsa in2p3 meeting describe previous scaling middleware processing effort lsst dm,"Put together a few slides for NCSA-IN2P3 meeting I put together a few slides for the NCSA-IN2P3 meeting describing previous scaling, middleware, and processing efforts of LSST DM."
"Fix Qserv SsiSession worker race The worker SsiSession implementation calls ReleaseRequestBuffer after handing the bound request to the foreman for processing. It therefore becomes possible for request processing to finish before ReleaseRequestBuffer is called by the submitting thread, resulting in a memory leak.",2,DM-2854,datamanagement,fix qserv ssisession worker race worker ssisession implementation call releaserequestbuffer hand bind request foreman processing possible request processing finish releaserequestbuffer call submitting thread result memory leak,"Fix Qserv SsiSession worker race The worker SsiSession implementation calls ReleaseRequestBuffer after handing the bound request to the foreman for processing. It therefore becomes possible for request processing to finish before ReleaseRequestBuffer is called by the submitting thread, resulting in a memory leak."
"Multi-processing capability for shear test measurements A suitable multi-cpu capability must be created for measurement tests.  We are hoping to just use a pipe_task, but to do so, the butler must be customized to allow it to read our cutouts and psfs from galaxies and psfs generated from GalSim and PhoSim.    This will be a relatively simple story if pipe_tasks running on 2 or 3 machines at UC Davis proves to be an adequate solution for running our shear experiments.",6,DM-2856,datamanagement,multi processing capability shear test measurement suitable multi cpu capability create measurement test hope use pipe_task butler customize allow read cutout psfs galaxy psfs generate galsim phosim relatively simple story pipe_task run machine uc davis prove adequate solution run shear experiment,"Multi-processing capability for shear test measurements A suitable multi-cpu capability must be created for measurement tests. We are hoping to just use a pipe_task, but to do so, the butler must be customized to allow it to read our cutouts and psfs from galaxies and psfs generated from GalSim and PhoSim. This will be a relatively simple story if pipe_tasks running on 2 or 3 machines at UC Davis proves to be an adequate solution for running our shear experiments."
"Add support for ""ORDER BY f1, f2"" for has-chunks query {code}   QuerySession description:    original: SELECT objectId, taiMidPoint FROM   Source ORDER BY objectId, taiMidPoint ASC;  has chunks: 1    needs merge: 1    1st parallel statement: SELECT objectId,taiMidPoint FROM LSST.Source_%CC% AS QST_1_    merge statement: SELECT objectId,taiMidPoint ORDER BY objectId,,taiMidPoint ASC    ScanTable: LSST.Source  {code}    Merge statement has syntax error",4,DM-2858,datamanagement,"add support order f1 f2 chunk query code querysession description original select objectid taimidpoint source order objectid taimidpoint asc chunk need merge 1st parallel statement select objectid taimidpoint lsst.source_%cc% qst_1 merge statement select objectid taimidpoint order objectid,,taimidpoint asc scantable lsst.source code merge statement syntax error","Add support for ""ORDER BY f1, f2"" for has-chunks query {code} QuerySession description: original: SELECT objectId, taiMidPoint FROM Source ORDER BY objectId, taiMidPoint ASC; has chunks: 1 needs merge: 1 1st parallel statement: SELECT objectId,taiMidPoint FROM LSST.Source_%CC% AS QST_1_ merge statement: SELECT objectId,taiMidPoint ORDER BY objectId,,taiMidPoint ASC ScanTable: LSST.Source {code} Merge statement has syntax error"
"Return error for ""SELECT a FROM T ORDER BY b"" for has-chunks query ORDER BY field has to be in result table => it has to be in select list.  Return clear error message to user if not.",4,DM-2859,datamanagement,return error select order chunk query order field result table select list return clear error message user,"Return error for ""SELECT a FROM T ORDER BY b"" for has-chunks query ORDER BY field has to be in result table => it has to be in select list. Return clear error message to user if not."
Enquiry into MiniSub FO cable Obtaining a quote from a company in Canada for a special clad cable for the Tololo-Pachon link,2,DM-2861,datamanagement,enquiry minisub fo cable obtain quote company canada special clothe cable tololo pachon link,Enquiry into MiniSub FO cable Obtaining a quote from a company in Canada for a special clad cable for the Tololo-Pachon link
RFI with vendors in Vina Open day with all interested vendors  to layout the projects for equipment on Mountain-Base and La Serena-Santiago links.,8,DM-2862,datamanagement,rfi vendor vina open day interested vendor layout project equipment mountain base la serena santiago link,RFI with vendors in Vina Open day with all interested vendors to layout the projects for equipment on Mountain-Base and La Serena-Santiago links.
"Validate wmgr client / server versions  If the client and server are on different versions, unexpected things can happen. (example: we run old version of the server, and use latest client). We need to check the version on both sides. ",4,DM-2863,datamanagement,validate wmgr client server version client server different version unexpected thing happen example run old version server use late client need check version side,"Validate wmgr client / server versions If the client and server are on different versions, unexpected things can happen. (example: we run old version of the server, and use latest client). We need to check the version on both sides."
"Fix bug related to selecting rows by objectId from non-director table The following example illustrates the problem:    Let's select one raw from qservTest_case01_qserv    {code}  select sourceId, objectId FROM Source LIMIT 1;  +-------------------+-----------------+  | sourceId          | objectId        |  +-------------------+-----------------+  | 29763859300222250 | 386942193651348 |  +-------------------+-----------------+  {code}    Then select it, but use ""sourceId"" in the query, all good here:  {code}  select sourceId, objectId FROM Source WHERE sourceId=29763859300222250;  +-------------------+-----------------+  | sourceId          | objectId        |  +-------------------+-----------------+  | 29763859300222250 | 386942193651348 |  +-------------------+-----------------+  {code}    But if we add ""objectId"", the row is not found:    {code}  select sourceId, objectId FROM Source WHERE sourceId=29763859300222250 and objectId=386942193651348;  Empty set (0.09 sec)  {code}    Similarly, even without sourceId constraint, the query fails:  {code}  select sourceId, objectId FROM Source WHERE objectId=386942193651348;  Empty set (0.09 sec)  {code}    ",8,DM-2864,datamanagement,fix bug relate select row objectid non director table follow example illustrate problem let select raw qservtest_case01_qserv code select sourceid objectid source limit -------------------+-----------------+ sourceid objectid -------------------+-----------------+ 29763859300222250 386942193651348 -------------------+-----------------+ code select use sourceid query good code select sourceid objectid source sourceid=29763859300222250 -------------------+-----------------+ sourceid objectid -------------------+-----------------+ 29763859300222250 386942193651348 -------------------+-----------------+ code add objectid row find code select sourceid objectid source sourceid=29763859300222250 objectid=386942193651348 set 0.09 sec code similarly sourceid constraint query fail code select sourceid objectid source objectid=386942193651348 set 0.09 sec code,"Fix bug related to selecting rows by objectId from non-director table The following example illustrates the problem: Let's select one raw from qservTest_case01_qserv {code} select sourceId, objectId FROM Source LIMIT 1; +-------------------+-----------------+ | sourceId | objectId | +-------------------+-----------------+ | 29763859300222250 | 386942193651348 | +-------------------+-----------------+ {code} Then select it, but use ""sourceId"" in the query, all good here: {code} select sourceId, objectId FROM Source WHERE sourceId=29763859300222250; +-------------------+-----------------+ | sourceId | objectId | +-------------------+-----------------+ | 29763859300222250 | 386942193651348 | +-------------------+-----------------+ {code} But if we add ""objectId"", the row is not found: {code} select sourceId, objectId FROM Source WHERE sourceId=29763859300222250 and objectId=386942193651348; Empty set (0.09 sec) {code} Similarly, even without sourceId constraint, the query fails: {code} select sourceId, objectId FROM Source WHERE objectId=386942193651348; Empty set (0.09 sec) {code}"
"Merge BoundedField from HSC as is To make headway on aperture corrections, we are bringing the HSC implementation of BoundedField over.",2,DM-2865,datamanagement,merge boundedfield hsc headway aperture correction bring hsc implementation boundedfield,"Merge BoundedField from HSC as is To make headway on aperture corrections, we are bringing the HSC implementation of BoundedField over."
Learn about Butler Transferring knowledge from K-T to the DB team.,2,DM-2866,datamanagement,learn butler transfer knowledge db team,Learn about Butler Transferring knowledge from K-T to the DB team.
Setting with CoordKey doesn't support non-IcrsCoord arguments Something in the {{FunctorKey}} template resolution doesn't allow {{Coord}} arguments to be used when setting record values with a {{CoordKey}} (only {{IcrsCoord}} arguments work.,1,DM-2872,datamanagement,set coordkey support non icrscoord argument functorkey template resolution allow coord argument set record value coordkey icrscoord argument work,Setting with CoordKey doesn't support non-IcrsCoord arguments Something in the {{FunctorKey}} template resolution doesn't allow {{Coord}} arguments to be used when setting record values with a {{CoordKey}} (only {{IcrsCoord}} arguments work.
"Handle ""where objectId between"" Query in a form:    {code}  select objectId   from Object   where objectId between 386942193651347 and 386942193651349  {code}    currently fails with  {code}  ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150602-20:41:43, Complete (success), 0,   Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150602-20:41:43, State error (unrecognized), 0,   Ref=3 Resource(/chk/qservTest_case01_qserv/6800): 20150602-20 (-1)  {code}    We already documented that such queries are not advised, but nethertheless qserv should handle it better, e.g., return a message ""not supported"" ",6,DM-2873,datamanagement,handle objectid query form code select objectid object objectid 386942193651347 386942193651349 code currently fail code error 4120 proxy error execution ref=1 resource(/chk qservtest_case01_qserv/6630 20150602 20:41:43 complete success ref=2 resource(/chk qservtest_case01_qserv/6631 20150602 20:41:43 state error unrecognized ref=3 resource(/chk qservtest_case01_qserv/6800 20150602 20 -1 code document query advise nethertheless qserv handle well e.g. return message support,"Handle ""where objectId between"" Query in a form: {code} select objectId from Object where objectId between 386942193651347 and 386942193651349 {code} currently fails with {code} ERROR 4120 (Proxy): Error during execution: -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150602-20:41:43, Complete (success), 0, Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150602-20:41:43, State error (unrecognized), 0, Ref=3 Resource(/chk/qservTest_case01_qserv/6800): 20150602-20 (-1) {code} We already documented that such queries are not advised, but nethertheless qserv should handle it better, e.g., return a message ""not supported"""
"demo package should contain the same comparison script used by CI The lsst_dm_stack_demo package currently used in the CI system contains a {{bin/compare}} script that doesn't do all of the checks done by the numdiff script that buildbot runs.  These need to be unified, so users can anticipate buildbot results and reproduce buildbot failures locally, especially when making changes to the expected-results file.    While the {{numdiff}} script currently checks more columns than the {{compare}} script, I believe the compare script follows a much better approach and should be extended to be used in both places ({{numdiff}} converts everything to ascii then compares text files; {{compare}} works directly from the binary results and uses NumPy to do the comparisons).",2,DM-2877,datamanagement,demo package contain comparison script ci lsst_dm_stack_demo package currently ci system contain bin compare script check numdiff script buildbot run need unify user anticipate buildbot result reproduce buildbot failure locally especially make change expect result file numdiff script currently check column compare script believe compare script follow well approach extend place numdiff convert ascii compare text file compare work directly binary result use numpy comparison,"demo package should contain the same comparison script used by CI The lsst_dm_stack_demo package currently used in the CI system contains a {{bin/compare}} script that doesn't do all of the checks done by the numdiff script that buildbot runs. These need to be unified, so users can anticipate buildbot results and reproduce buildbot failures locally, especially when making changes to the expected-results file. While the {{numdiff}} script currently checks more columns than the {{compare}} script, I believe the compare script follows a much better approach and should be extended to be used in both places ({{numdiff}} converts everything to ascii then compares text files; {{compare}} works directly from the binary results and uses NumPy to do the comparisons)."
Add transformation tasks for new Butler dataset types The new Butler dataset types created as part of the HSC deblender merge will need transformation tasks so they can be ingested to the database.    See also DM-2191.,2,DM-2879,datamanagement,add transformation task new butler dataset type new butler dataset type create hsc deblender merge need transformation task ingest database dm-2191,Add transformation tasks for new Butler dataset types The new Butler dataset types created as part of the HSC deblender merge will need transformation tasks so they can be ingested to the database. See also DM-2191.
"wcslib is unable to read PTF headers with PV1_{1..16} cards SCAMP writes distortion headers in form of PVi_nn (i=1..x, nn=5..16) cards, but this is rejected (correctly) by wcslib 4.14;  there is a discussion at https://github.com/astropy/astropy/issues/299    The simplest ""solution"" is to strip the values PV1_nn (nn=5..16) in makeWcs()  for CTYPEs of TAN or TAN-SIP and this certainly works.    I propose that we adopt this solution for now.  ",1,DM-2883,datamanagement,wcslib unable read ptf header pv1_{1 16 card scamp write distortion header form pvi_nn i=1 nn=5 16 card reject correctly wcslib 4.14 discussion https://github.com/astropy/astropy/issues/299 simple solution strip value pv1_nn nn=5 16 makewcs ctypes tan tan sip certainly work propose adopt solution,"wcslib is unable to read PTF headers with PV1_{1..16} cards SCAMP writes distortion headers in form of PVi_nn (i=1..x, nn=5..16) cards, but this is rejected (correctly) by wcslib 4.14; there is a discussion at https://github.com/astropy/astropy/issues/299 The simplest ""solution"" is to strip the values PV1_nn (nn=5..16) in makeWcs() for CTYPEs of TAN or TAN-SIP and this certainly works. I propose that we adopt this solution for now."
LOG() macro fails if message is a simple std::string lsst:log LOG() macro crash with fatal error if message is a simple string.,2,DM-2884,datamanagement,log macro fail message simple std::string lsst log log macro crash fatal error message simple string,LOG() macro fails if message is a simple std::string lsst:log LOG() macro crash with fatal error if message is a simple string.
"Improve confusing error message Selecting a column that does not exist results in confusing error. Example:    {code}  SELECT badColumnName  FROM qservTest_case01_qserv.Object   WHERE objectId=386942193651348;  {code}    ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:42, Error in result data., 1, (-1)    Similarly,     {code}  select whatever   FROM qservTest_case01_qserv.Object;  {code}    prints  ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:52, Error in result data., 1,   Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150605-16:23:52, Error merging result, 1990, Cancellation requested  Ref=3 Resource(/chk/qservTest_case01_qs (-1)    (note, sourceId does not exist in Object table)      ",5,DM-2885,datamanagement,improve confusing error message selecting column exist result confuse error example code select badcolumnname qservtest_case01_qserv object objectid=386942193651348 code error 4120 proxy error execution ref=1 resource(/chk qservtest_case01_qserv/6630 20150605 16:23:42 error result datum -1 similarly code select qservtest_case01_qserv object code print error 4120 proxy error execution ref=1 resource(/chk qservtest_case01_qserv/6630 20150605 16:23:52 error result datum ref=2 resource(/chk qservtest_case01_qserv/6631 20150605 16:23:52 error merging result 1990 cancellation request ref=3 resource(/chk qservtest_case01_qs -1 note sourceid exist object table,"Improve confusing error message Selecting a column that does not exist results in confusing error. Example: {code} SELECT badColumnName FROM qservTest_case01_qserv.Object WHERE objectId=386942193651348; {code} ERROR 4120 (Proxy): Error during execution: -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:42, Error in result data., 1, (-1) Similarly, {code} select whatever FROM qservTest_case01_qserv.Object; {code} prints ERROR 4120 (Proxy): Error during execution: -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:52, Error in result data., 1, Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150605-16:23:52, Error merging result, 1990, Cancellation requested Ref=3 Resource(/chk/qservTest_case01_qs (-1) (note, sourceId does not exist in Object table)"
"Fix broken IN - it now takes first element only IN is broken - it only uses the first element from the list. Here is the proof:    {code}  select COUNT(*) AS N FROM qservTest_case01_qserv.Source   WHERE objectId=386950783579546;  +------+  | N    |  +------+  |   56 |  +------+  1 row in set (0.10 sec)    mysql> select count(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId=386942193651348;  +------+  | N    |  +------+  |   39 |  +------+  1 row in set (0.09 sec)    mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId IN(386942193651348, 386950783579546);  +------+  | N    |  +------+  |   39 |  +------+  1 row in set (0.09 sec)    mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId IN(386950783579546, 386942193651348);  +------+  | N    |  +------+  |   56 |  +------+  1 row in set (0.11 sec)  {code}",8,DM-2887,datamanagement,fix break take element break use element list proof code select count qservtest_case01_qserv source objectid=386950783579546 ------+ ------+ 56 ------+ row set 0.10 sec mysql select count qservtest_case01_qserv source objectid=386942193651348 ------+ ------+ 39 ------+ row set 0.09 sec mysql select count qservtest_case01_qserv source objectid in(386942193651348 386950783579546 ------+ ------+ 39 ------+ row set 0.09 sec mysql select count qservtest_case01_qserv source objectid in(386950783579546 386942193651348 ------+ ------+ 56 ------+ row set 0.11 sec code,"Fix broken IN - it now takes first element only IN is broken - it only uses the first element from the list. Here is the proof: {code} select COUNT(*) AS N FROM qservTest_case01_qserv.Source WHERE objectId=386950783579546; +------+ | N | +------+ | 56 | +------+ 1 row in set (0.10 sec) mysql> select count(*) AS N FROM qservTest_case01_qserv.Source WHERE objectId=386942193651348; +------+ | N | +------+ | 39 | +------+ 1 row in set (0.09 sec) mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source WHERE objectId IN(386942193651348, 386950783579546); +------+ | N | +------+ | 39 | +------+ 1 row in set (0.09 sec) mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source WHERE objectId IN(386950783579546, 386942193651348); +------+ | N | +------+ | 56 | +------+ 1 row in set (0.11 sec) {code}"
isrTask assumes that the Exposure has a Detector While trying to use the isrTask to interpolate over bad columns in PTF data I discovered that the code assumes that the Exposure has a Detector attached.    Please remove this restriction.  ,1,DM-2890,datamanagement,assume exposure detector try use isrtask interpolate bad column ptf datum discover code assume exposure detector attach remove restriction,isrTask assumes that the Exposure has a Detector While trying to use the isrTask to interpolate over bad columns in PTF data I discovered that the code assumes that the Exposure has a Detector attached. Please remove this restriction.
"Keep track of database of the director table An L3 child table might very well have an LSST data release Object table as its director, while almost certainly not living in the DR database. To support it, we should keep track of the database name holding director's table. Note, this is related to DM-2864 - the code touched in that ticket should be checking the director's db name.    Don't forget to add a unit test that will exercise it!",1,DM-2892,datamanagement,track database director table l3 child table lsst data release object table director certainly live dr database support track database hold director table note relate dm-2864 code touch ticket check director db forget add unit test exercise,"Keep track of database of the director table An L3 child table might very well have an LSST data release Object table as its director, while almost certainly not living in the DR database. To support it, we should keep track of the database name holding director's table. Note, this is related to DM-2864 - the code touched in that ticket should be checking the director's db name. Don't forget to add a unit test that will exercise it!"
"Improve qproc unit testing framework qproc unit testing framework allow to test the whole query analysis pipeline, it has grow and should be re-organized to be easilly understandable, maintainable.",5,DM-2893,datamanagement,improve qproc unit testing framework qproc unit testing framework allow test query analysis pipeline grow organized easilly understandable maintainable,"Improve qproc unit testing framework qproc unit testing framework allow to test the whole query analysis pipeline, it has grow and should be re-organized to be easilly understandable, maintainable."
"treat lsst_apps, lsst_libs and lsst_thirdparty as top level products not required by lsst_distrib Per discussion on RFC-55, it was determined that  lsst_apps and lsst_libs and lsst_thirdparty maybe be treated as separate top level products that lsst_distrib need not depend on them nor do they need to be included as part of CI builds.",1,DM-2895,datamanagement,treat lsst_apps lsst_libs lsst_thirdparty level product require lsst_distrib discussion rfc-55 determine lsst_apps lsst_libs lsst_thirdparty maybe treat separate level product lsst_distrib need depend need include ci build,"treat lsst_apps, lsst_libs and lsst_thirdparty as top level products not required by lsst_distrib Per discussion on RFC-55, it was determined that lsst_apps and lsst_libs and lsst_thirdparty maybe be treated as separate top level products that lsst_distrib need not depend on them nor do they need to be included as part of CI builds."
Host/Attend DM LT meeting  host/attend DM LT meeting ,7,DM-2899,datamanagement,host attend dm lt meeting host attend dm lt meeting,Host/Attend DM LT meeting host/attend DM LT meeting
Add queries that exercise non-box spatial constraints Qserv has code to support:   * qserv_areaspec_box   * qserv_areaspec_circle   * qserv_areaspec_ellipse   * qserv_areaspec_poly    but only the first one (box) is exercised in our integration tests. This story involves adding queries to test the other 3.,2,DM-2900,datamanagement,add query exercise non box spatial constraint qserv code support qserv_areaspec_box qserv_areaspec_circle qserv_areaspec_poly box exercise integration test story involve add query test,Add queries that exercise non-box spatial constraints Qserv has code to support: * qserv_areaspec_box * qserv_areaspec_circle * qserv_areaspec_ellipse * qserv_areaspec_poly but only the first one (box) is exercised in our integration tests. This story involves adding queries to test the other 3.
"Clean up gitolite We need to clean up gitolite and cgit:  * Repositories that have moved to GitHub should be removed (or, possibly, mirrored back from GitHub).  * Empty repositories (like contrib/eups.git) and obsolete repositories (like LSST/DMS/afw_extensions_rgb.git) should be removed altogether.  * contrib/data_products.git (the Data Products Definition Document source) should be moved to GitHub in the lsst org.  * contrib/processFile.git should be moved to GitHub in the lsst-dm org unless the author adds some test cases and it can be integrated into the CI system as a top-level product (in which case it can go into the lsst org).  * The primary authors of other contrib repositories should be contacted to see if they should be moved to GitHub in the lsst-dm or another org (possibly a new lsst-contrib org).  In particular, contrib/plotz/* (Paul Lotz of the Telescope and Site subsystem) and contrib/pyreb (IN2P3 work for the Camera subsystem) contain current work that should be moved.",4,DM-2903,datamanagement,clean gitolite need clean gitolite cgit repository move github remove possibly mirror github repository like contrib eups.git obsolete repository like lsst dms afw_extensions_rgb.git remove altogether contrib data_products.git data products definition document source move github lsst org contrib processfile.git move github lsst dm org author add test case integrate ci system level product case lsst org primary author contrib repository contact move github lsst dm org possibly new lsst contrib org particular contrib plotz/ paul lotz telescope site subsystem contrib pyreb in2p3 work camera subsystem contain current work move,"Clean up gitolite We need to clean up gitolite and cgit: * Repositories that have moved to GitHub should be removed (or, possibly, mirrored back from GitHub). * Empty repositories (like contrib/eups.git) and obsolete repositories (like LSST/DMS/afw_extensions_rgb.git) should be removed altogether. * contrib/data_products.git (the Data Products Definition Document source) should be moved to GitHub in the lsst org. * contrib/processFile.git should be moved to GitHub in the lsst-dm org unless the author adds some test cases and it can be integrated into the CI system as a top-level product (in which case it can go into the lsst org). * The primary authors of other contrib repositories should be contacted to see if they should be moved to GitHub in the lsst-dm or another org (possibly a new lsst-contrib org). In particular, contrib/plotz/* (Paul Lotz of the Telescope and Site subsystem) and contrib/pyreb (IN2P3 work for the Camera subsystem) contain current work that should be moved."
Update Scons to v2.3.4 Scons has not been updated in over a year. RFC-61 agreed that we should upgrade it now before tackling some other {{scons}} issues.,1,DM-2905,datamanagement,update scon v2.3.4 scons update year rfc-61 agree upgrade tackle scon issue,Update Scons to v2.3.4 Scons has not been updated in over a year. RFC-61 agreed that we should upgrade it now before tackling some other {{scons}} issues.
"Add Gaussian PSF example to measurement task documentation I see some documentation on how to add a placeholder Gaussian Psf to an image (to work around the fact that some algorithms require a Psf) was recently added to the release notes.  I don't think that's actually appropriate, as the same algorithms also required Psfs in the framework - the failure mode was just different (previously, it'd just result in all objects being flagged and the peak position used for the centroid, so it may have been easy to miss - hence the change to a fatal error).    I propose moving the example to the documentation for SingleFrameMeasurementTask, and taking it out of the release notes.  I'll also make sure there's a link from the Measurement Framework Overhaul Release Notes page to the Doxygen for SingleFrameMeasurementTask - I'm not sure if that's sufficient to make up the visibility gap between the Doxygen docs and the release notes in Confluence, but I don't have any other short-term ideas.",1,DM-2908,datamanagement,add gaussian psf example measurement task documentation documentation add placeholder gaussian psf image work fact algorithm require psf recently add release note think actually appropriate algorithm require psfs framework failure mode different previously result object flag peak position centroid easy miss change fatal error propose move example documentation singleframemeasurementtask take release note sure link measurement framework overhaul release notes page doxygen singleframemeasurementtask sure sufficient visibility gap doxygen doc release note confluence short term idea,"Add Gaussian PSF example to measurement task documentation I see some documentation on how to add a placeholder Gaussian Psf to an image (to work around the fact that some algorithms require a Psf) was recently added to the release notes. I don't think that's actually appropriate, as the same algorithms also required Psfs in the framework - the failure mode was just different (previously, it'd just result in all objects being flagged and the peak position used for the centroid, so it may have been easy to miss - hence the change to a fatal error). I propose moving the example to the documentation for SingleFrameMeasurementTask, and taking it out of the release notes. I'll also make sure there's a link from the Measurement Framework Overhaul Release Notes page to the Doxygen for SingleFrameMeasurementTask - I'm not sure if that's sufficient to make up the visibility gap between the Doxygen docs and the release notes in Confluence, but I don't have any other short-term ideas."
Remove unused code from sconsUtils The code in {{deprecated.py}} in {{sconsUtils}} is not used by anything anywhere. [~jbosch] has indicated that the file can simply be removed.,1,DM-2909,datamanagement,remove unused code sconsutil code deprecated.py sconsutils ~jbosch indicate file simply remove,Remove unused code from sconsUtils The code in {{deprecated.py}} in {{sconsUtils}} is not used by anything anywhere. [~jbosch] has indicated that the file can simply be removed.
"obs_cfht is broken with the current stack obs_cfht's camera mapper is missing the new packageName class variable, so it is not compatible with the current stack.    I suggest fixing obs_sdss and obs_subaru as well, if they need it.",1,DM-2910,datamanagement,obs_cfht break current stack obs_cfht camera mapper miss new packagename class variable compatible current stack suggest fix obs_sdss obs_subaru need,"obs_cfht is broken with the current stack obs_cfht's camera mapper is missing the new packageName class variable, so it is not compatible with the current stack. I suggest fixing obs_sdss and obs_subaru as well, if they need it."
Build 2015_07 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,1,DM-2911,datamanagement,build 2015_07 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build 2015_07 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
"Port HSC Curve-of-Growth code Port content from HSC-1144, HSC-1236, HSC-1223, HSC-1219, HSC-1203, HSC-1153.",6,DM-2913,datamanagement,port hsc curve growth code port content hsc-1144 hsc-1236 hsc-1223 hsc-1219 hsc-1203 hsc-1153,"Port HSC Curve-of-Growth code Port content from HSC-1144, HSC-1236, HSC-1223, HSC-1219, HSC-1203, HSC-1153."
"obs_cfht unit tests are broken obs_cfht has one unit test ""testButler"" that uses git://git.lsstcorp.org/contrib/price/testdata_cfht. 4 of the tests fail, as shown below.    In addition, testdata_cfht is huge, and the tests barely use any of it. It's worth considering making a new test repo that is smaller, or if the amount of data is small enough, move it into afwdata or obs_cfht itself.    {code}  localhost$ tests/testButler.py   CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  .CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  E.  ======================================================================  ERROR: testBias (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 122, in testBias      self.getDetrend(""bias"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testFlat (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 117, in testFlat      self.getDetrend(""flat"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testFringe (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 127, in testFringe      self.getDetrend(""fringe"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testRaw (__main__.GetRawTestCase)  Test retrieval of raw image  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 101, in testRaw      raw = self.butler.get(""raw"", self.dataId, ccd=ccd, immediate=True)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 244, in get      return callback()    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 242, in <lambda>      innerCallback(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 238, in <lambda>      callback = lambda: self._read(pythonType, location)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 426, in _read      location.getCppType(), storageList, additionalData)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/persistenceLib.py"", line 1430, in unsafeRetrieve      return _persistenceLib.Persistence_unsafeRetrieve(self, *args)  FitsError:     File ""src/fits.cc"", line 1064, in lsst::afw::fits::Fits::Fits(const std::string &, const std::string &, int)      cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r' {0}  lsst::afw::fits::FitsError: 'cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r''      ----------------------------------------------------------------------  Ran 6 tests in 3.544s    FAILED (errors=4)  {code}",1,DM-2917,datamanagement,"obs_cfht unit test break obs_cfht unit test testbutler use git://git.lsstcorp.org/contrib/price/testdata_cfht test fail show addition testdata_cfht huge test barely use worth consider make new test repo small datum small afwdata obs_cfht code localhost$ test testbutler.py cameramapper loading registry registry /users rowen lsst code testdata testdata_cfht data registry.sqlite3 cameramapper loading calibregistry registry /users rowen lsst code testdata testdata_cfht calib calibregistry.sqlite3 ecameramapper loading registry registry /users rowen lsst code testdata testdata_cfht data registry.sqlite3 cameramapper loading calibregistry registry /users rowen lsst code testdata testdata_cfht calib calibregistry.sqlite3 ecameramapper loading registry registry /users rowen lsst code testdata testdata_cfht data registry.sqlite3 cameramapper loading calibregistry registry /users rowen lsst code testdata testdata_cfht calib calibregistry.sqlite3 ecameramapper loading registry registry /users rowen lsst code testdata testdata_cfht data registry.sqlite3 cameramapper loading calibregistry registry /users rowen lsst code testdata testdata_cfht calib calibregistry.sqlite3 loading registry registry /users rowen lsst code testdata testdata_cfht data registry.sqlite3 cameramapper loading calibregistry registry /users rowen lsst code testdata testdata_cfht calib calibregistry.sqlite3 e. error testbias main__.getrawtestcase traceback recent file test testbutler.py line 122 testbias self.getdetrend(""bias file test testbutler.py line 110 getdetrend flat self.butler.get(detrend self.dataid ccd ccd file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence butler.py line 218 location self.mapper.map(datasettype dataid file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence mapper.py line 116 map return func(self.validate(dataid write file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil cameramapper.py line 287 mapclosure return mapping.map(mapper dataid write file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 118 map actualid self.need(self.keydict.iterkey dataid file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 199 need lookup self.lookup(newprop newid file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 345 lookup return mapping.lookup(self property newid file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 168 lookup self.range value file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil registries.py line 120 executequery self.conn.execute(cmd value operationalerror column extension error testflat main__.getrawtestcase traceback recent file test testbutler.py line 117 testflat self.getdetrend(""flat file test testbutler.py line 110 getdetrend flat self.butler.get(detrend self.dataid ccd ccd file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence butler.py line 218 location self.mapper.map(datasettype dataid file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence mapper.py line 116 map return func(self.validate(dataid write file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil cameramapper.py line 287 mapclosure return mapping.map(mapper dataid write file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 118 map actualid self.need(self.keydict.iterkey dataid file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 199 need lookup self.lookup(newprop newid file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 345 lookup return mapping.lookup(self property newid file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 168 lookup self.range value file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil registries.py line 120 executequery self.conn.execute(cmd value operationalerror column extension error testfringe main__.getrawtestcase traceback recent file test testbutler.py line 127 testfringe self.getdetrend(""fringe file test testbutler.py line 110 getdetrend flat self.butler.get(detrend self.dataid ccd ccd file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence butler.py line 218 location self.mapper.map(datasettype dataid file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence mapper.py line 116 map return func(self.validate(dataid write file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil cameramapper.py line 287 mapclosure return mapping.map(mapper dataid write file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 118 map actualid self.need(self.keydict.iterkey dataid file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 199 need lookup self.lookup(newprop newid file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 345 lookup return mapping.lookup(self property newid file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil mapping.py line 168 lookup self.range value file /users rowen lsst lsstsw stack darwinx86 daf_butlerutils/10.1 g302a9ed python lsst daf butlerutil registries.py line 120 executequery self.conn.execute(cmd value operationalerror column extension error testraw main__.getrawtestcase test retrieval raw image traceback recent file test testbutler.py line 101 testraw raw self.butler.get(""raw self.dataid ccd ccd immediate true file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence butler.py line 244 return callback file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence butler.py line 242 innercallback dataid file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence butler.py line 238 callback lambda self._read(pythontype location file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence butler.py line 426 read location.getcpptype storagelist additionaldata file /users rowen lsst lsstsw stack darwinx86 daf_persistence/10.1 g6edbc00 python lsst daf persistence persistencelib.py line 1430 unsaferetrieve return persistence_unsaferetrieve(self args fitserror file src fits.cc line 1064 lsst::afw::fits::fits::fits(const std::string const std::string int cfitsio error open name file 104 opening file /users rowen lsst code testdata testdata_cfht data raw/08bl05 w2.+2 2/2008 11 01 i2/1038843o.fits.fz[1 mode lsst::afw::fits::fitserror cfitsio error open name file 104 opening file /users rowen lsst code testdata testdata_cfht data raw/08bl05 w2.+2 2/2008 11 01 i2/1038843o.fits.fz[1 mode ran test 3.544s failed errors=4 code","obs_cfht unit tests are broken obs_cfht has one unit test ""testButler"" that uses git://git.lsstcorp.org/contrib/price/testdata_cfht. 4 of the tests fail, as shown below. In addition, testdata_cfht is huge, and the tests barely use any of it. It's worth considering making a new test repo that is smaller, or if the amount of data is small enough, move it into afwdata or obs_cfht itself. {code} localhost$ tests/testButler.py CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3 CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3 ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3 CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3 ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3 CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3 ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3 CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3 .CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3 CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3 E. ====================================================================== ERROR: testBias (__main__.GetRawTestCase) ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/testButler.py"", line 122, in testBias self.getDetrend(""bias"") File ""tests/testButler.py"", line 110, in getDetrend flat = self.butler.get(detrend, self.dataId, ccd=ccd) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get location = self.mapper.map(datasetType, dataId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map return func(self.validate(dataId), write) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure return mapping.map(mapper, dataId, write) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map actualId = self.need(self.keyDict.iterkeys(), dataId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need lookups = self.lookup(newProps, newId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup return Mapping.lookup(self, properties, newId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup where, self.range, values) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery c = self.conn.execute(cmd, values) OperationalError: no such column: extension ====================================================================== ERROR: testFlat (__main__.GetRawTestCase) ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/testButler.py"", line 117, in testFlat self.getDetrend(""flat"") File ""tests/testButler.py"", line 110, in getDetrend flat = self.butler.get(detrend, self.dataId, ccd=ccd) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get location = self.mapper.map(datasetType, dataId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map return func(self.validate(dataId), write) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure return mapping.map(mapper, dataId, write) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map actualId = self.need(self.keyDict.iterkeys(), dataId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need lookups = self.lookup(newProps, newId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup return Mapping.lookup(self, properties, newId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup where, self.range, values) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery c = self.conn.execute(cmd, values) OperationalError: no such column: extension ====================================================================== ERROR: testFringe (__main__.GetRawTestCase) ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/testButler.py"", line 127, in testFringe self.getDetrend(""fringe"") File ""tests/testButler.py"", line 110, in getDetrend flat = self.butler.get(detrend, self.dataId, ccd=ccd) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get location = self.mapper.map(datasetType, dataId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map return func(self.validate(dataId), write) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure return mapping.map(mapper, dataId, write) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map actualId = self.need(self.keyDict.iterkeys(), dataId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need lookups = self.lookup(newProps, newId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup return Mapping.lookup(self, properties, newId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup where, self.range, values) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery c = self.conn.execute(cmd, values) OperationalError: no such column: extension ====================================================================== ERROR: testRaw (__main__.GetRawTestCase) Test retrieval of raw image ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/testButler.py"", line 101, in testRaw raw = self.butler.get(""raw"", self.dataId, ccd=ccd, immediate=True) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 244, in get return callback() File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 242, in  innerCallback(), dataId) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 238, in  callback = lambda: self._read(pythonType, location) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 426, in _read location.getCppType(), storageList, additionalData) File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/persistenceLib.py"", line 1430, in unsafeRetrieve return _persistenceLib.Persistence_unsafeRetrieve(self, *args) FitsError: File ""src/fits.cc"", line 1064, in lsst::afw::fits::Fits::Fits(const std::string &, const std::string &, int) cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r' {0} lsst::afw::fits::FitsError: 'cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r'' ---------------------------------------------------------------------- Ran 6 tests in 3.544s FAILED (errors=4) {code}"
"PhotoCalTask mis-calling Colorterm methods When I implemented DM-2797 I made a few errors in pipe_tasks:  - PhotoCalTask mis-calls two methods of Colorterm by providing filterName, which is not needed  - ColortermLibrary.getColorterm mis-handles glob expressions (the two arguments to fnmatch.fnmatch are swapped).    We also need a unit test for applying colorterms, but that will require enough work that I have made a separate ticket for it: DM-2918. Meanwhile I have tested my changes by running Dominique's CFHT demo. This proves that the colorterm code runs, but does not prove that the terms are correctly applied.",1,DM-2919,datamanagement,photocaltask mis call colorterm method implement dm-2797 error pipe_task photocaltask mis call method colorterm provide filtername need colortermlibrary.getcolorterm mis handle glob expression argument fnmatch.fnmatch swap need unit test apply colorterm require work separate ticket dm-2918 test change run dominique cfht demo prove colorterm code run prove term correctly apply,"PhotoCalTask mis-calling Colorterm methods When I implemented DM-2797 I made a few errors in pipe_tasks: - PhotoCalTask mis-calls two methods of Colorterm by providing filterName, which is not needed - ColortermLibrary.getColorterm mis-handles glob expressions (the two arguments to fnmatch.fnmatch are swapped). We also need a unit test for applying colorterms, but that will require enough work that I have made a separate ticket for it: DM-2918. Meanwhile I have tested my changes by running Dominique's CFHT demo. This proves that the colorterm code runs, but does not prove that the terms are correctly applied."
"Clean up code in afw for Approximate background estimation The intention is to eventually set {{useApprox=True}} (i.e. Chebychev Approximation)  as the default for background estimation.  However, in looking into the relevant code in afw/math while working on DM-2778, there is some clean-up and restructuring that needs to be done before resetting the defaults (which may also require adjusting some defaults in the calibrate stage to be more appropriate for the approximation, as opposed to inperpolation, scheme).  This issue is to clean up the code and make sure it all operates coherently.  A seperate ticket will be to actually reset the defaults and make any other config default changes required.    In particular, the config setting of approxOrderX/binSize are not being assessed properly, nor is the behavior of the given undersampleStyle being executed.  The under-sampling checks are currently only being done against the interpoation settings, which is not appropriate when useApprox=True.  A temporary check was added in {{meas.algoritms.detection.getBackground()}} in DM-2778 so that it is currently ""safe"" to run with useApprox=True and any other user overridden config setting (binSize, approxOrder, undersampleStyle) (and there currently exists similar checks in {{pipe.tasks.matchBackground}}), but these should be removed once this issue has been implemented. ",3,DM-2920,datamanagement,clean code afw approximate background estimation intention eventually set useapprox true i.e. chebychev approximation default background estimation look relevant code afw math work dm-2778 clean restructuring need reset default require adjust default calibrate stage appropriate approximation oppose inperpolation scheme issue clean code sure operate coherently seperate ticket actually reset default config default change require particular config setting approxorderx assess properly behavior give undersamplestyle execute sample check currently interpoation setting appropriate useapprox true temporary check add meas.algoritms.detection.getbackground dm-2778 currently safe run useapprox true user overridden config set binsize approxord undersamplestyle currently exist similar check pipe.tasks.matchbackground remove issue implement,"Clean up code in afw for Approximate background estimation The intention is to eventually set {{useApprox=True}} (i.e. Chebychev Approximation) as the default for background estimation. However, in looking into the relevant code in afw/math while working on DM-2778, there is some clean-up and restructuring that needs to be done before resetting the defaults (which may also require adjusting some defaults in the calibrate stage to be more appropriate for the approximation, as opposed to inperpolation, scheme). This issue is to clean up the code and make sure it all operates coherently. A seperate ticket will be to actually reset the defaults and make any other config default changes required. In particular, the config setting of approxOrderX/binSize are not being assessed properly, nor is the behavior of the given undersampleStyle being executed. The under-sampling checks are currently only being done against the interpoation settings, which is not appropriate when useApprox=True. A temporary check was added in {{meas.algoritms.detection.getBackground()}} in DM-2778 so that it is currently ""safe"" to run with useApprox=True and any other user overridden config setting (binSize, approxOrder, undersampleStyle) (and there currently exists similar checks in {{pipe.tasks.matchBackground}}), but these should be removed once this issue has been implemented."
"Set Approximation as default for background subtraction Once the Approximate code in {{afw.math}} has been cleaned up (see DM-2920), set the default for background subtraction to be the Chebychev Approximation (i.e. useApprox=True).  Ensure any other relevant config defaults (e.g. binSize, approxOrderX) are adjusted appropriately.  This will change the outputs of the {{lsst_dm_stack_demo}}, so the ""expected"" files will need to be replaced along with this change in default settings (see DM-2778 for some comparisons of the demo outputs using the interpolation vs. approximation background estimation schemes).",1,DM-2921,datamanagement,set approximation default background subtraction approximate code afw.math clean dm-2920 set default background subtraction chebychev approximation i.e. useapprox true ensure relevant config default e.g. binsize approxorderx adjust appropriately change output lsst_dm_stack_demo expect file need replace change default setting dm-2778 comparison demo output interpolation vs. approximation background estimation scheme,"Set Approximation as default for background subtraction Once the Approximate code in {{afw.math}} has been cleaned up (see DM-2920), set the default for background subtraction to be the Chebychev Approximation (i.e. useApprox=True). Ensure any other relevant config defaults (e.g. binSize, approxOrderX) are adjusted appropriately. This will change the outputs of the {{lsst_dm_stack_demo}}, so the ""expected"" files will need to be replaced along with this change in default settings (see DM-2778 for some comparisons of the demo outputs using the interpolation vs. approximation background estimation schemes)."
Initial DC Base dseign Prepare a Document for the Initial Design of the Base and Summit Networks,6,DM-2922,datamanagement,initial dc base dseign prepare document initial design base summit networks,Initial DC Base dseign Prepare a Document for the Initial Design of the Base and Summit Networks
Port HSC-1199 to LSST (UNMASKEDNAN mask propagates to all amplifiers) Port issue HSC-1199 to LSST stack to address UNMASKEDNAN mask propagates to all amplifiers,4,DM-2923,datamanagement,port hsc-1199 lsst unmaskednan mask propagate amplifier port issue hsc-1199 lsst stack address unmaskednan mask propagate amplifier,Port HSC-1199 to LSST (UNMASKEDNAN mask propagates to all amplifiers) Port issue HSC-1199 to LSST stack to address UNMASKEDNAN mask propagates to all amplifiers
"Add RFD issue type to RFC project To support the RFD process adopted in [RFC-53], an RFD issue type in the RFC project is required.  While we could add RFD-specific fields to it, I think it's simplest if it's just generic with details provided in the Description.",1,DM-2934,datamanagement,add rfd issue type rfc project support rfd process adopt rfc-53 rfd issue type rfc project require add rfd specific field think simplest generic detail provide description,"Add RFD issue type to RFC project To support the RFD process adopted in [RFC-53], an RFD issue type in the RFC project is required. While we could add RFD-specific fields to it, I think it's simplest if it's just generic with details provided in the Description."
"Modernize sconsUtils code to python 2.7 standard As part of the work investigating DM-2839 I modernized the sconsUtils code to meet current coding standards (using {{in}} rather than {{has_key}}, using {{items()}} rather than {{iteritems}} etc). Since I'm highly doubtful that DM-2839 is going to be closed any time soon I will separate out the modernization patches into this ticket.",1,DM-2927,datamanagement,modernize sconsutils code python 2.7 standard work investigate dm-2839 modernize sconsutil code meet current code standard has_key item iteritem etc highly doubtful dm-2839 go close time soon separate modernization patch ticket,"Modernize sconsUtils code to python 2.7 standard As part of the work investigating DM-2839 I modernized the sconsUtils code to meet current coding standards (using {{in}} rather than {{has_key}}, using {{items()}} rather than {{iteritems}} etc). Since I'm highly doubtful that DM-2839 is going to be closed any time soon I will separate out the modernization patches into this ticket."
"move old ingest scripts into and retire old packages This ticket implements RFC-57, by:   - renaming datarel to daf_ingest (there is already a daf_ingest package, but it's *completely* empty, so I'll just force-push it all away)   - removing everything from the renamed package that doesn't relate to ingest (including pruning dependencies)   - removing ap and testing_endToEnd from the CI system  ",1,DM-2928,datamanagement,old ingest script retire old package ticket implement rfc-57 rename datarel daf_ingest daf_ingest package completely force push away remove rename package relate ingest include pruning dependency remove ap testing_endtoend ci system,"move old ingest scripts into and retire old packages This ticket implements RFC-57, by: - renaming datarel to daf_ingest (there is already a daf_ingest package, but it's *completely* empty, so I'll just force-push it all away) - removing everything from the renamed package that doesn't relate to ingest (including pruning dependencies) - removing ap and testing_endToEnd from the CI system"
"Some AFW tests are not enabled with no explanation Running {{coverage.py}} on the AFW test suite indicated that two test classes in {{tests/wcs1.py}} are disabled. {{WCSTestCaseCFHT}} was added by [~rhl] in 2007 but disabled during a merge a long time ago by [~jbosch] in 2010 but with no indication as to why. {{WCSRotateFlip}} appeared in 2012 (added by [~krughoff]) but doesn't appear in the {{suite}} list at the end and so does not execute.    Similarly {{testSchema.py}} has two tests that are not run: {{xtestSchema}} and {{testJoin}}. I assume {{xtestSchema}} is deliberately disabled but could there at least be a comment in the test explaining why?    My feeling is that we should either run the tests or they should be removed. Having them their gives the impression they are doing something useful.    Less importantly, {{warpExposure.py}} has some support code for comparing masked images that was written in 2009 by [~rowen] but which is not used anywhere in the test.",2,DM-2929,datamanagement,afw test enable explanation run coverage.py afw test suite indicate test class test wcs1.py disable wcstestcasecfht add ~rhl 2007 disabled merge long time ago ~jbosch 2010 indication wcsrotateflip appear 2012 add ~krughoff appear suite list end execute similarly testschema.py test run xtestschema testjoin assume xtestschema deliberately disabled comment test explain feeling run test remove have give impression useful importantly warpexposure.py support code compare mask image write 2009 ~rowen test,"Some AFW tests are not enabled with no explanation Running {{coverage.py}} on the AFW test suite indicated that two test classes in {{tests/wcs1.py}} are disabled. {{WCSTestCaseCFHT}} was added by [~rhl] in 2007 but disabled during a merge a long time ago by [~jbosch] in 2010 but with no indication as to why. {{WCSRotateFlip}} appeared in 2012 (added by [~krughoff]) but doesn't appear in the {{suite}} list at the end and so does not execute. Similarly {{testSchema.py}} has two tests that are not run: {{xtestSchema}} and {{testJoin}}. I assume {{xtestSchema}} is deliberately disabled but could there at least be a comment in the test explaining why? My feeling is that we should either run the tests or they should be removed. Having them their gives the impression they are doing something useful. Less importantly, {{warpExposure.py}} has some support code for comparing masked images that was written in 2009 by [~rowen] but which is not used anywhere in the test."
"Fix problem with Qserv related to restarting mysql I noticed some strange (reproducible!) behavior: if I run:    {code}qserv-check-integration.py --case=01{code}    then restart mysqld    {code}<runDir>/etc/init.d/mysqld restart{code}    then the query:  {code}mysql --host=127.0.0.1 --port=4040 --user=qsmaster   qservTest_case01_qserv -e   ""SELECT COUNT(*) as OBJ_COUNT   FROM Object   WHERE qserv_areaspec_box(0.1, -6, 4, 6)""{code}    consistently fails every single time.    To fix it, it is enough to restart xrootd.",5,DM-2930,datamanagement,"fix problem qserv relate restart mysql notice strange reproducible behavior run code}qserv check integration.py --case=01{code restart mysqld code}/etc init.d mysqld restart{code query code}mysql --port=4040 qsmaster qservtest_case01_qserv select count obj_count object qserv_areaspec_box(0.1 -6 6)""{code consistently fail single time fix restart xrootd","Fix problem with Qserv related to restarting mysql I noticed some strange (reproducible!) behavior: if I run: {code}qserv-check-integration.py --case=01{code} then restart mysqld {code}/etc/init.d/mysqld restart{code} then the query: {code}mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case01_qserv -e ""SELECT COUNT(*) as OBJ_COUNT FROM Object WHERE qserv_areaspec_box(0.1, -6, 4, 6)""{code} consistently fails every single time. To fix it, it is enough to restart xrootd."
"We write truncated Wcs data to  extended HDU tables in Exposures When we write Wcs to extra HDUs in Exposures they are truncated if other than TAN/TAN-SIP.  Please don't write them.    A better long term solution is needed.  In particular, we shouldn't be duplicating this information unnecessarily, and we need to be able to persist e.g. TPV to the tables so as to support CoaddPsf.  These issues are not included here.",1,DM-2931,datamanagement,write truncate wcs datum extend hdu table exposure write wcs extra hdu exposures truncate tan tan sip write well long term solution need particular duplicate information unnecessarily need able persist e.g. tpv table support coaddpsf issue include,"We write truncated Wcs data to extended HDU tables in Exposures When we write Wcs to extra HDUs in Exposures they are truncated if other than TAN/TAN-SIP. Please don't write them. A better long term solution is needed. In particular, we shouldn't be duplicating this information unnecessarily, and we need to be able to persist e.g. TPV to the tables so as to support CoaddPsf. These issues are not included here."
qserv-admin CREATE NODE fails {noformat}  qserv > CREATE NODE worker1 type=worker host=worker-1 port=5012 runDir=1;  06/15/2015 05:59:52 QADM ERROR: Missing parameter. (mysqlConn)  ERROR:  Missing parameter. (mysqlConn)  {noformat}  ,1,DM-2935,datamanagement,qserv admin create node fail noformat qserv create node worker1 type worker host worker-1 port=5012 rundir=1 06/15/2015 05:59:52 qadm error miss parameter mysqlconn error miss parameter mysqlconn noformat,qserv-admin CREATE NODE fails {noformat} qserv > CREATE NODE worker1 type=worker host=worker-1 port=5012 runDir=1; 06/15/2015 05:59:52 QADM ERROR: Missing parameter. (mysqlConn) ERROR: Missing parameter. (mysqlConn) {noformat}
"Refactor Histogram in edu.caltech.ipac.visualize.plot package. The Histogram has 6 constructors to handle 6 bitpixel data types which are byte, short integer,  integer, long integer, float and double.  Since FitsRead has now only works on float, there the  Histogram should be refactored accordingly.",3,DM-2936,datamanagement,refactor histogram edu.caltech.ipac.visualize.plot package histogram constructor handle bitpixel datum type byte short integer integer long integer float double fitsread work float histogram refactore accordingly,"Refactor Histogram in edu.caltech.ipac.visualize.plot package. The Histogram has 6 constructors to handle 6 bitpixel data types which are byte, short integer, integer, long integer, float and double. Since FitsRead has now only works on float, there the Histogram should be refactored accordingly."
"CalibrateTask has an unwanted ""raise"" in it On 2014-06-30 commit 696b641 a developer added a bare ""raise"" as a debugging aid to the CalibrateTask in pipe_tasks. That change was accidentally merged to master. I confirmed it was an accident and am filing this ticket as a way to remove the raise and run buildbot before merging to master.",1,DM-2938,datamanagement,calibratetask unwanted raise 2014 06 30 commit 696b641 developer add bare raise debug aid calibratetask pipe_task change accidentally merge master confirm accident file ticket way remove raise run buildbot merge master,"CalibrateTask has an unwanted ""raise"" in it On 2014-06-30 commit 696b641 a developer added a bare ""raise"" as a debugging aid to the CalibrateTask in pipe_tasks. That change was accidentally merged to master. I confirmed it was an accident and am filing this ticket as a way to remove the raise and run buildbot before merging to master."
"fix usage of obsolete astrometry interfaces in ProcessImageTask As discussed recently on HipChat (Science Pipelines Standup), there's code in {{ProocessImageTask}} that assumes an ""astrometer"" attribute on a {{CalibrateTask}} instance.  Since this is just needed to match a new set of sources against the reference catalog here, we should probably be using one of the new matcher objects, either by getting one from {{CalibrateTask}} via a documented interface, or by constructing a new one.",4,DM-2939,datamanagement,fix usage obsolete astrometry interface processimagetask discuss recently hipchat science pipelines standup code proocessimagetask assume astrometer attribute calibratetask instance need match new set source reference catalog probably new matcher object get calibratetask document interface construct new,"fix usage of obsolete astrometry interfaces in ProcessImageTask As discussed recently on HipChat (Science Pipelines Standup), there's code in {{ProocessImageTask}} that assumes an ""astrometer"" attribute on a {{CalibrateTask}} instance. Since this is just needed to match a new set of sources against the reference catalog here, we should probably be using one of the new matcher objects, either by getting one from {{CalibrateTask}} via a documented interface, or by constructing a new one."
DS9 tests fail if DS9 not running in some configurations There are a few issues with the robustness of the {{testDs9.py}} tests in AFW.    * The tests are skipped if the {{display_ds9}} package can not be loaded but they should also skip if {{ds9}} is missing or if {{ds9}} can not be loaded. The latter is especially important during builds that unset {{$DISPLAY}}.  * The launching code in {{initDS9}} can not notice the simple case of {{ds9}} immediately failing to load. It simply assumes that there are delays in launch. The reason for this is that {{os.system}} does not return bad status if the command has been started in the background. Another scheme for starting {{ds9}} should be considered. Maybe a different exception could be raised specifically for failing to start it.  * At the moment each test independently has a go at starting {{ds9}}. This makes the tests take a very long time (made worse by {{_mtv}} also trying multiple times) despite it being clear pretty quickly that {{ds9}} is never going to work.  * Currently the {{mtv}} tests must run early as they are the only tests that attempt to start {{ds9}} if it is not running. If the two tests that call {{mtv}} are disabled two other tests fail. Ideally the {{initDS9}} code should be called in all cases.,1,DM-2940,datamanagement,ds9 test fail ds9 run configuration issue robustness testds9.py test afw test skip display_ds9 package load skip ds9 miss ds9 load especially important build unset display launch code initds9 notice simple case ds9 immediately fail load simply assume delay launch reason os.system return bad status command start background scheme start ds9 consider maybe different exception raise specifically fail start moment test independently start ds9 make test long time bad mtv try multiple time despite clear pretty quickly ds9 go work currently mtv test run early test attempt start ds9 run test mtv disable test fail ideally initds9 code call case,DS9 tests fail if DS9 not running in some configurations There are a few issues with the robustness of the {{testDs9.py}} tests in AFW. * The tests are skipped if the {{display_ds9}} package can not be loaded but they should also skip if {{ds9}} is missing or if {{ds9}} can not be loaded. The latter is especially important during builds that unset {{$DISPLAY}}. * The launching code in {{initDS9}} can not notice the simple case of {{ds9}} immediately failing to load. It simply assumes that there are delays in launch. The reason for this is that {{os.system}} does not return bad status if the command has been started in the background. Another scheme for starting {{ds9}} should be considered. Maybe a different exception could be raised specifically for failing to start it. * At the moment each test independently has a go at starting {{ds9}}. This makes the tests take a very long time (made worse by {{_mtv}} also trying multiple times) despite it being clear pretty quickly that {{ds9}} is never going to work. * Currently the {{mtv}} tests must run early as they are the only tests that attempt to start {{ds9}} if it is not running. If the two tests that call {{mtv}} are disabled two other tests fail. Ideally the {{initDS9}} code should be called in all cases.
Wmgr refuses to serve queries from remote interface Vaikunth discovered that wmgr returns 404 for all operations. It looks like wmgr can serve requests coming from 127.0.0.1 interface but returns 404 for queries from non-local interface.,1,DM-2945,datamanagement,wmgr refuse serve query remote interface vaikunth discover wmgr return 404 operation look like wmgr serve request come 127.0.0.1 interface return 404 query non local interface,Wmgr refuses to serve queries from remote interface Vaikunth discovered that wmgr returns 404 for all operations. It looks like wmgr can serve requests coming from 127.0.0.1 interface but returns 404 for queries from non-local interface.
"Remove explicit buildbot dependency on datarel The buildbot scripts have an explicit dependency on the {{datarel}} package, which we'd like to remove from the stack.  It uses {{datarel}} as the top-level product when building the cross-linked HTML documentation; {{lsstDoxygen}}'s {{makeDocs}} script takes a single package, and generates the list of packages to include in the Doxygen build by finding all dependencies of that package.    So, to remove the explicit dependency on {{datarel}}, we need to either:   - find a new top-level product with a Doxygen build to pass to {{makeDocs}} (e.g. by adding a trivial Doxygen build to {{lsst_distrib}})   - modify the argument parsing in {{lsstDoxygen}} to take a list of multiple products (it *looks* like the limitation to one package is only in the argument parsing), and pass it a list of top-level products in the buildbot scripts.    This is currently a blocker for DM-2928, which itself a blocker for DM-1766, which has now been lingering for a few weeks now.  I'm going to look for other ways to remove the block on the latter, but I don't have a solution yet.",3,DM-2948,datamanagement,remove explicit buildbot dependency datarel buildbot script explicit dependency datarel package like remove stack use datarel level product build cross link html documentation lsstdoxygen makedocs script take single package generate list package include doxygen build find dependency package remove explicit dependency datarel need find new level product doxygen build pass makedocs e.g. add trivial doxygen build lsst_distrib modify argument parse lsstdoxygen list multiple product look like limitation package argument parse pass list level product buildbot script currently blocker dm-2928 blocker dm-1766 linger week go look way remove block solution,"Remove explicit buildbot dependency on datarel The buildbot scripts have an explicit dependency on the {{datarel}} package, which we'd like to remove from the stack. It uses {{datarel}} as the top-level product when building the cross-linked HTML documentation; {{lsstDoxygen}}'s {{makeDocs}} script takes a single package, and generates the list of packages to include in the Doxygen build by finding all dependencies of that package. So, to remove the explicit dependency on {{datarel}}, we need to either: - find a new top-level product with a Doxygen build to pass to {{makeDocs}} (e.g. by adding a trivial Doxygen build to {{lsst_distrib}}) - modify the argument parsing in {{lsstDoxygen}} to take a list of multiple products (it *looks* like the limitation to one package is only in the argument parsing), and pass it a list of top-level products in the buildbot scripts. This is currently a blocker for DM-2928, which itself a blocker for DM-1766, which has now been lingering for a few weeks now. I'm going to look for other ways to remove the block on the latter, but I don't have a solution yet."
"remove dead code and dependencies from datarel Removing the {{datarel}} package entirely has proved to be difficult (DM-2928, DM-2948), so instead I'm simply going to remove non-ingest code (and dead ingest code) from the package, along with its dependencies on {{ap}} and {{testing_endToEnd}}.  Other dependencies will be retained even if they aren't necessary for the code that will remain in {{datarel}}, to support {{lsstDoxygen}}'s use of {{datarel}} as a top-level package for documentation generation.",1,DM-2949,datamanagement,remove dead code dependency datarel remove datarel package entirely prove difficult dm-2928 dm-2948 instead simply go remove non ingest code dead ingest code package dependency ap testing_endtoend dependency retain necessary code remain datarel support lsstdoxygen use datarel level package documentation generation,"remove dead code and dependencies from datarel Removing the {{datarel}} package entirely has proved to be difficult (DM-2928, DM-2948), so instead I'm simply going to remove non-ingest code (and dead ingest code) from the package, along with its dependencies on {{ap}} and {{testing_endToEnd}}. Other dependencies will be retained even if they aren't necessary for the code that will remain in {{datarel}}, to support {{lsstDoxygen}}'s use of {{datarel}} as a top-level package for documentation generation."
Refactoring the class CropAndCenter This class contains the codes which are not used.  It needs to be simplified and refactored. ,4,DM-2951,datamanagement,refactore class cropandcenter class contain code need simplify refactore,Refactoring the class CropAndCenter This class contains the codes which are not used. It needs to be simplified and refactored.
Crop needs to be refactored This class needs to be refactored to be in consist with FitsRead class which treats all data type as float.  Thus the bitpix in this class does not have to be treated based on its value.,3,DM-2952,datamanagement,crop need refactore class need refactore consist fitsread class treat datum type float bitpix class treat base value,Crop needs to be refactored This class needs to be refactored to be in consist with FitsRead class which treats all data type as float. Thus the bitpix in this class does not have to be treated based on its value.
"Qserv code cleanup and auto_ptr --> unique_ptr migration Code cleanup, including migrating some parts to c++11 (in particular, auto_ptr --> unique_ptr)",4,DM-2953,datamanagement,qserv code cleanup auto_ptr unique_ptr migration code cleanup include migrate part c++11 particular auto_ptr unique_ptr,"Qserv code cleanup and auto_ptr --> unique_ptr migration Code cleanup, including migrating some parts to c++11 (in particular, auto_ptr --> unique_ptr)"
Add a unit test for aperture corrections in measurement task DM-436 adds code to meas_base that allows one to run a subset of measurement algorithms based on execution order. This addition should have a unit test.    DM-436 also tasks to measure and apply aperture correction. Those tasks should have unit tests.,6,DM-2954,datamanagement,add unit test aperture correction measurement task dm-436 add code meas_base allow run subset measurement algorithm base execution order addition unit test dm-436 task measure apply aperture correction task unit test,Add a unit test for aperture corrections in measurement task DM-436 adds code to meas_base that allows one to run a subset of measurement algorithms based on execution order. This addition should have a unit test. DM-436 also tasks to measure and apply aperture correction. Those tasks should have unit tests.
"Setting up and running PhoSim for Psf Library Debbie Bard leaving created some new work creating the Psf Libraries we need.  While this od not a major task except for computer time, there is some setup required.  I will get an account at SLAC and learn to run the PhoSim utilities she and Michael have developed.    In the short term, Simon is going to do some runs for me.  Meanwhile, I will get into Debbie's account and run her configuration at SLAC.    The outputs then need to be checked to be sure that the Psfs are reasonable.",4,DM-2955,datamanagement,set run phosim psf library debbie bard leave create new work create psf libraries need od major task computer time setup require account slac learn run phosim utility michael develop short term simon go run debbie account run configuration slac output need check sure psfs reasonable,"Setting up and running PhoSim for Psf Library Debbie Bard leaving created some new work creating the Psf Libraries we need. While this od not a major task except for computer time, there is some setup required. I will get an account at SLAC and learn to run the PhoSim utilities she and Michael have developed. In the short term, Simon is going to do some runs for me. Meanwhile, I will get into Debbie's account and run her configuration at SLAC. The outputs then need to be checked to be sure that the Psfs are reasonable."
"Review ITIL V3.0 as prep for input to IT use case ITIL is a standard breakdown of processes used in an IT system.  While full ITIL may very well be too heavy LSST operations, it provides a useful checklist for he use cases being developed in the TOWG.   I created an ITIL type spreadsheet to check against the dump of the workflows in the EA  tool  ",3,DM-2958,datamanagement,review itil v3.0 prep input use case itil standard breakdown process system itil heavy lsst operation provide useful checklist use case develop towg create itil type spreadsheet check dump workflow ea tool,"Review ITIL V3.0 as prep for input to IT use case ITIL is a standard breakdown of processes used in an IT system. While full ITIL may very well be too heavy LSST operations, it provides a useful checklist for he use cases being developed in the TOWG. I created an ITIL type spreadsheet to check against the dump of the workflows in the EA tool"
"add metric to  application specific IO benchmarking tool iosim is an application-specific benchmarking tool that is developed to be responsive to the request from LSST to select and investigate file systems ahead of actual benchmarks, and in advance of having workflow and other infrastructures needed to investigate file systems under realistic load.  The week the software was    -- Optimized to allow for faster test cycles.   -- Threads were supported by squashing all thread IO into a single simulation process.  -- Information from the original ""model"" program is propagated to the simulation program, allowing for comparison to the model.  -- initial matplotlib plots allowing a degree of visualization of the performance of a simulated run was added    The goals to deliver this capability in a few week when common systems at NCSA are available for testing.",4,DM-2959,datamanagement,add metric application specific io benchmarking tool iosim application specific benchmarking tool develop responsive request lsst select investigate file system ahead actual benchmark advance have workflow infrastructure need investigate file system realistic load week software optimize allow fast test cycle thread support squash thread io single simulation process information original model program propagate simulation program allow comparison model initial matplotlib plot allow degree visualization performance simulated run add goal deliver capability week common system ncsa available testing,"add metric to application specific IO benchmarking tool iosim is an application-specific benchmarking tool that is developed to be responsive to the request from LSST to select and investigate file systems ahead of actual benchmarks, and in advance of having workflow and other infrastructures needed to investigate file systems under realistic load. The week the software was -- Optimized to allow for faster test cycles. -- Threads were supported by squashing all thread IO into a single simulation process. -- Information from the original ""model"" program is propagated to the simulation program, allowing for comparison to the model. -- initial matplotlib plots allowing a degree of visualization of the performance of a simulated run was added The goals to deliver this capability in a few week when common systems at NCSA are available for testing."
Management for Don for week of June 15 On boarded Mattais Carrasco-Kind to work in the process execution in the context of Level 3 processing.   Misc.,4,DM-2960,datamanagement,management don week june 15 board mattais carrasco kind work process execution context level processing misc,Management for Don for week of June 15 On boarded Mattais Carrasco-Kind to work in the process execution in the context of Level 3 processing. Misc.
"Prototype iRODS tiered resource with NERSC HPSS iRODS can support access to a tape archive with the use of a ""tiered resource"" where one resource has the role of the cache, and a second has the role of archive.    Use of such a tiered resource construct could be valuable to data management.   Because a iRODS plugin for HPSS is readily available, we examine the set up and use of the tiered resource testing against NERSC HPSS.",4,DM-2962,datamanagement,prototype irods tiered resource nersc hpss irods support access tape archive use tiered resource resource role cache second role archive use tiered resource construct valuable data management irods plugin hpss readily available examine set use tiered resource testing nersc hpss,"Prototype iRODS tiered resource with NERSC HPSS iRODS can support access to a tape archive with the use of a ""tiered resource"" where one resource has the role of the cache, and a second has the role of archive. Use of such a tiered resource construct could be valuable to data management. Because a iRODS plugin for HPSS is readily available, we examine the set up and use of the tiered resource testing against NERSC HPSS."
"Design CSS that supports updates Design how to redesign CSS, we currently take a snapshot when char starts. It is too static. ",2,DM-2966,datamanagement,design css support update design redesign css currently snapshot char start static,"Design CSS that supports updates Design how to redesign CSS, we currently take a snapshot when char starts. It is too static."
"Fix to DM-2883 isn't quite right The fix to DM-2883 (remove illegal PVi_j cards) isn't quite right, and the error was masked by a piece of code elsewhere that duplicated the functionality.    The issues is that while PV1_[1-4] cards are indeed valid, the ones that SCAMP writes are not.  So we should remove them too, if there are any other SCAMP TPV coefficients.    The masking code was a unilateral removal of PVi_j cards dating back years.  ",1,DM-2967,datamanagement,fix dm-2883 right fix dm-2883 remove illegal pvi_j card right error mask piece code duplicate functionality issue pv1_[1 card valid one scamp write remove scamp tpv coefficient mask code unilateral removal pvi_j card date year,"Fix to DM-2883 isn't quite right The fix to DM-2883 (remove illegal PVi_j cards) isn't quite right, and the error was masked by a piece of code elsewhere that duplicated the functionality. The issues is that while PV1_[1-4] cards are indeed valid, the ones that SCAMP writes are not. So we should remove them too, if there are any other SCAMP TPV coefficients. The masking code was a unilateral removal of PVi_j cards dating back years."
Discourse evaluation (Part 1) Work in support of evaluation Discourse as a DM platform for internal and external interactions.,3,DM-2972,datamanagement,discourse evaluation work support evaluation discourse dm platform internal external interaction,Discourse evaluation (Part 1) Work in support of evaluation Discourse as a DM platform for internal and external interactions.
Quantify how much objects are blended It would be useful to have a parameter that indicates how much any given galaxy is blended. This will be useful for testing how photometry or shears are affected by blending effects.    Ports code from HSC-1260.,2,DM-2975,datamanagement,quantify object blend useful parameter indicate give galaxy blend useful test photometry shear affect blend effect port code hsc-1260,Quantify how much objects are blended It would be useful to have a parameter that indicates how much any given galaxy is blended. This will be useful for testing how photometry or shears are affected by blending effects. Ports code from HSC-1260.
SourceCatalog.getChildren requires preconditions but does not check them This is a code transfer from HSC-1247.,2,DM-2976,datamanagement,sourcecatalog.getchildren require precondition check code transfer hsc-1247,SourceCatalog.getChildren requires preconditions but does not check them This is a code transfer from HSC-1247.
"Miscellaneous CModel improvements from HSC This improves handling of several edge case failure modes, tweaks the configuration to improve performance, and adds some introspection useful for Jose Garmilla's tests.    Includes HSC-1288, HSC-1284, HSC-1228, HSC-1250, HSC-1264, HSC-1273, HSC-1240, HSC-1249, HSC-1238, HSC-990, HSC-1155, HSC-1191",2,DM-2977,datamanagement,miscellaneous cmodel improvement hsc improve handling edge case failure mode tweak configuration improve performance add introspection useful jose garmilla test include hsc-1288 hsc-1284 hsc-1228 hsc-1250 hsc-1264 hsc-1273 hsc-1240 hsc-1249 hsc-1238 hsc-990 hsc-1155 hsc-1191,"Miscellaneous CModel improvements from HSC This improves handling of several edge case failure modes, tweaks the configuration to improve performance, and adds some introspection useful for Jose Garmilla's tests. Includes HSC-1288, HSC-1284, HSC-1228, HSC-1250, HSC-1264, HSC-1273, HSC-1240, HSC-1249, HSC-1238, HSC-990, HSC-1155, HSC-1191"
"FootprintMerge: fix bug when identifying existing peaks in a merge. If two separate footprints from the same catalog happen to be merged because an existing merged object overlaps both of them, the flags of which peaks are being detected in which bands is not being propagated. This is causing the apparent dropout of some sources in a merged catalog which were detected in single frame processing.    Taken from ticket HSC-1270",1,DM-2978,datamanagement,footprintmerge fix bug identify exist peak merge separate footprint catalog happen merge exist merged object overlap flag peak detect band propagate cause apparent dropout source merge catalog detect single frame processing take ticket hsc-1270,"FootprintMerge: fix bug when identifying existing peaks in a merge. If two separate footprints from the same catalog happen to be merged because an existing merged object overlaps both of them, the flags of which peaks are being detected in which bands is not being propagated. This is causing the apparent dropout of some sources in a merged catalog which were detected in single frame processing. Taken from ticket HSC-1270"
"refactor coaddition code The HSC fork has coaddition code in two places: pipe_tasks and hscPipe.  The code in hscPipe is what we use (though that depends on the code in pipe_tasks in places), while the code in pipe_tasks is more similar to what's currently on the LSST side.    We want to bring the refactored version in hscPipe back to LSST, but we want to put it directly in pipe_tasks to remove the code duplication that currently exists on the HSC side.    Work on this issue should begin with an RFC that details the proposed changes.    Note that this should not bring over the ""safe coadd clipping"" code, which is DM-2915.",5,DM-2980,datamanagement,refactor coaddition code hsc fork coaddition code place pipe_task hscpipe code hscpipe use depend code pipe_task place code pipe_task similar currently lsst want bring refactore version hscpipe lsst want directly pipe_task remove code duplication currently exist hsc work issue begin rfc detail propose change note bring safe coadd clip code dm-2915,"refactor coaddition code The HSC fork has coaddition code in two places: pipe_tasks and hscPipe. The code in hscPipe is what we use (though that depends on the code in pipe_tasks in places), while the code in pipe_tasks is more similar to what's currently on the LSST side. We want to bring the refactored version in hscPipe back to LSST, but we want to put it directly in pipe_tasks to remove the code duplication that currently exists on the HSC side. Work on this issue should begin with an RFC that details the proposed changes. Note that this should not bring over the ""safe coadd clipping"" code, which is DM-2915."
"polygon masking in CoaddPsf We need to create polygon-based masks of the usable area of the focal plane, persist them with exposure, and include them in coaddition of PSFs and aperture corrections.    This includes HSC issues HSC-972, HSC-973, HSC-974, HSC-975, HSC-976.    At least some of this will be blocked by DM-833, which is the port issue for coaddition of aperture corrections.",8,DM-2981,datamanagement,polygon mask coaddpsf need create polygon base mask usable area focal plane persist exposure include coaddition psf aperture correction include hsc issue hsc-972 hsc-973 hsc-974 hsc-975 hsc-976 block dm-833 port issue coaddition aperture correction,"polygon masking in CoaddPsf We need to create polygon-based masks of the usable area of the focal plane, persist them with exposure, and include them in coaddition of PSFs and aperture corrections. This includes HSC issues HSC-972, HSC-973, HSC-974, HSC-975, HSC-976. At least some of this will be blocked by DM-833, which is the port issue for coaddition of aperture corrections."
"Updating node status in qserv-admin to INACTIVE fails In qserv-admin.py when attempting to update a node status from ACTIVE to INACTIVE the following error is produced:    {code}  > update node worker2 state=INACTIVE;  Traceback (most recent call last):  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 650, in <module>  main()  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 645, in main  parser.receiveCommands()  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 163, in receiveCommands  self.parse(cmd[:pos])  File ""/usr/l  ocal/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 180, in parse      self._funcMap[t](tokens[1:])    File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 380, in _parseUpdate      self._parseUpdateNode(tokens[1:])    File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 405, in _parseUpdateNode      self._impl.setNodeState(**options)    File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/qservAdmin.py"", line 660, in setNodeState      self._kvI.set(nodeKey, state)    File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/css/kvInterface.py"", line 415, in set      self._zk.set(k, v)    File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1170, in set      return self.set_async(path, value, version).get()    File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1182, in set_async      raise TypeError(""value must be a byte string"")  {code}",1,DM-2982,datamanagement,"update node status qserv admin inactive fail qserv-admin.py attempt update node status active inactive follow error produce code update node worker2 state inactive traceback recent file /usr local home vaikunth src qserv bin qserv admin.py line 650 main file /usr local home vaikunth src qserv bin qserv admin.py line 645 main parser.receivecommand file /usr local home vaikunth src qserv bin qserv admin.py line 163 receivecommand self.parse(cmd[:po file /usr ocal home vaikunth src qserv bin qserv admin.py line 180 parse self._funcmap[t](tokens[1 file /usr local home vaikunth src qserv bin qserv admin.py line 380 parseupdate self._parseupdatenode(tokens[1 file /usr local home vaikunth src qserv bin qserv admin.py line 405 parseupdatenode self._impl.setnodestate(**option file /usr local home vaikunth src qserv lib python lsst qserv admin qservadmin.py line 660 setnodestate self._kvi.set(nodekey state file /usr local home vaikunth src qserv lib python lsst qserv css kvinterface.py line 415 set self._zk.set(k file /usr local home vaikunth qserv linux64 kazoo/2.0b1 lib python kazoo-2.0b1 py2.7.egg kazoo client.py line 1170 set return self.set_async(path value version).get file /usr local home vaikunth qserv linux64 kazoo/2.0b1 lib python kazoo-2.0b1 py2.7.egg kazoo client.py line 1182 set_async raise typeerror(""value byte string code","Updating node status in qserv-admin to INACTIVE fails In qserv-admin.py when attempting to update a node status from ACTIVE to INACTIVE the following error is produced: {code} > update node worker2 state=INACTIVE; Traceback (most recent call last): File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 650, in  main() File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 645, in main parser.receiveCommands() File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 163, in receiveCommands self.parse(cmd[:pos]) File ""/usr/l ocal/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 180, in parse self._funcMap[t](tokens[1:]) File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 380, in _parseUpdate self._parseUpdateNode(tokens[1:]) File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 405, in _parseUpdateNode self._impl.setNodeState(**options) File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/qservAdmin.py"", line 660, in setNodeState self._kvI.set(nodeKey, state) File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/css/kvInterface.py"", line 415, in set self._zk.set(k, v) File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1170, in set return self.set_async(path, value, version).get() File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1182, in set_async raise TypeError(""value must be a byte string"") {code}"
"Backport HSC parallelization code Assuming RFC-68 is approved, transfer the HSC code to LSST as described there.",4,DM-2983,datamanagement,backport hsc parallelization code assume rfc-68 approve transfer hsc code lsst describe,"Backport HSC parallelization code Assuming RFC-68 is approved, transfer the HSC code to LSST as described there."
Integrate javascript build with gradle Integrate javascript build tools webpack with gradle.,2,DM-2985,datamanagement,integrate javascript build gradle integrate javascript build tool webpack gradle,Integrate javascript build with gradle Integrate javascript build tools webpack with gradle.
Conversion of FITS binary table extension to IPAC table format.  FITS binary table contains data types and structures that cannot map directly to IPAC table.  We need to define ways to handle these cases.,6,DM-2986,datamanagement,conversion fits binary table extension ipac table format fit binary table contain data type structure map directly ipac table need define way handle case,Conversion of FITS binary table extension to IPAC table format. FITS binary table contains data types and structures that cannot map directly to IPAC table. We need to define ways to handle these cases.
Modify IpacTableParser to support extra wide table. IpacTableParser fail to load IPAC table with extra wide headers and columns.  Replace the logic for reading headers and columns information so that it will support any file/size.,2,DM-2987,datamanagement,modify ipactableparser support extra wide table ipactableparser fail load ipac table extra wide header column replace logic read header column information support file size,Modify IpacTableParser to support extra wide table. IpacTableParser fail to load IPAC table with extra wide headers and columns. Replace the logic for reading headers and columns information so that it will support any file/size.
"XY plot need to be able to handle multiple tables with the same name XY plot was relying on a table request object to cache previously loaded tables.  This was done for performance reason.  However, table request is not reliable since the same request may be submitted multiple times.",2,DM-2989,datamanagement,xy plot need able handle multiple table xy plot rely table request object cache previously load table performance reason table request reliable request submit multiple time,"XY plot need to be able to handle multiple tables with the same name XY plot was relying on a table request object to cache previously loaded tables. This was done for performance reason. However, table request is not reliable since the same request may be submitted multiple times."
Add XYPlot to Python interface Make it possible to add plots (not connected to a displayed table) to StandaloneUI.  Add showXYPlot to python API.,6,DM-2990,datamanagement,add xyplot python interface possible add plot connect display table add showxyplot python api,Add XYPlot to Python interface Make it possible to add plots (not connected to a displayed table) to StandaloneUI. Add showXYPlot to python API.
"Search processors to get image, table, or json from an external task Implement three search processors, which use the External Task Launcher (DM-2991):    - to get a table (possibly in binary FITS format)  - to get an image  - to get JSON",8,DM-2992,datamanagement,search processor image table json external task implement search processor use external task launcher dm-2991 table possibly binary fits format image json,"Search processors to get image, table, or json from an external task Implement three search processors, which use the External Task Launcher (DM-2991): - to get a table (possibly in binary FITS format) - to get an image - to get JSON"
"Products must not depend on anaconda {{setupRequired(anaconda)}} should be removed from webservcommon.table.    We want to keep the stack buildable with any python 2.7, and should not explicitly depend on anaconda.",1,DM-2993,datamanagement,product depend anaconda setuprequired(anaconda remove webservcommon.table want stack buildable python 2.7 explicitly depend anaconda,"Products must not depend on anaconda {{setupRequired(anaconda)}} should be removed from webservcommon.table. We want to keep the stack buildable with any python 2.7, and should not explicitly depend on anaconda."
"Understand and improve error code management It seems there is several constants to store Qserv error code (for example see msgCode.h and util::ErrorCode, or MsgState::RESULT_ERR,JobStatus::RESULT_ERROR). This could certainly be simplified and clarified?    Furthermore in util::Error it seems there's a confusion between code and statuses",8,DM-2996,datamanagement,understand improve error code management constant store qserv error code example msgcode.h util::errorcode msgstate::result_err jobstatus::result_error certainly simplify clarify furthermore util::error confusion code status,"Understand and improve error code management It seems there is several constants to store Qserv error code (for example see msgCode.h and util::ErrorCode, or MsgState::RESULT_ERR,JobStatus::RESULT_ERROR). This could certainly be simplified and clarified? Furthermore in util::Error it seems there's a confusion between code and statuses"
"Begin to write a note for the TOWG using ITIL as a checklist Began to work use cases, and found that they were long, considering the number r of aspects that need to be considered,  Presented to the TOWG,  got guidance to think in terms of processes, but to take that the effort estimates would  have a reasonable basis.  Got guidance to think about the sites's needs but to congress a central approach.    Agree that this would be the struggle for the week.",4,DM-2998,datamanagement,begin write note towg itil checklist begin work use case find long consider number aspect need consider present towg get guidance think term process effort estimate reasonable basis got guidance think site need congress central approach agree struggle week,"Begin to write a note for the TOWG using ITIL as a checklist Began to work use cases, and found that they were long, considering the number r of aspects that need to be considered, Presented to the TOWG, got guidance to think in terms of processes, but to take that the effort estimates would have a reasonable basis. Got guidance to think about the sites's needs but to congress a central approach. Agree that this would be the struggle for the week."
Management work for Don in the week of June 22 Internal and external recruiting.   Input on NCSA re-organizaiton to ensure proper placement of LSST activities in the NCSA organization.  Meetings.,3,DM-2999,datamanagement,management work don week june 22 internal external recruiting input ncsa organizaiton ensure proper placement lsst activity ncsa organization meeting,Management work for Don in the week of June 22 Internal and external recruiting. Input on NCSA re-organizaiton to ensure proper placement of LSST activities in the NCSA organization. Meetings.
Whitepaper submission to NSF Cyber Summit Working on drafting whitepaper and abstract for SCADA security challenges faced by LSST.,1,DM-3001,datamanagement,whitepaper submission nsf cyber summit working draft whitepaper abstract scada security challenge face lsst,Whitepaper submission to NSF Cyber Summit Working on drafting whitepaper and abstract for SCADA security challenges faced by LSST.
"ISO presentation to all-hands meeting Presentation giving overview of ISO work, esp. w.r.t AUP and master security plan.",1,DM-3002,datamanagement,iso presentation hand meeting presentation give overview iso work esp w.r.t aup master security plan,"ISO presentation to all-hands meeting Presentation giving overview of ISO work, esp. w.r.t AUP and master security plan."
"Preliminary Process Execution Framework work Preliminary work to extend the Process Execution Framework to accomodate changes needed by SUI and others.    This story captures work done in June, prior to incorporating the activity into the baseline plan. Work will be logged under DM-3003 starting July 1st.",4,DM-3004,datamanagement,preliminary process execution framework work preliminary work extend process execution framework accomodate change need sui story capture work june prior incorporate activity baseline plan work log dm-3003 start july 1st,"Preliminary Process Execution Framework work Preliminary work to extend the Process Execution Framework to accomodate changes needed by SUI and others. This story captures work done in June, prior to incorporating the activity into the baseline plan. Work will be logged under DM-3003 starting July 1st."
"Meeting with CTSC at CLHS Portland OR Discussed LSST security plan going forward.  Specifically work on SCADA security plan.  Meeting held at conference in Portland OR, June 14th, ACM CLHS.",1,DM-3016,datamanagement,meet ctsc clhs portland discussed lsst security plan go forward specifically work scada security plan meeting hold conference portland june 14th acm clhs,"Meeting with CTSC at CLHS Portland OR Discussed LSST security plan going forward. Specifically work on SCADA security plan. Meeting held at conference in Portland OR, June 14th, ACM CLHS."
Improve region support Some parts of the region support has been more testing because of the python interface.  It is now clear what we should do.,4,DM-3021,datamanagement,improve region support part region support testing python interface clear,Improve region support Some parts of the region support has been more testing because of the python interface. It is now clear what we should do.
Review LSE-78 Review either the current version #26 and/or the newest version when it become available.,2,DM-3023,datamanagement,review lse-78 review current version 26 and/or new version available,Review LSE-78 Review either the current version #26 and/or the newest version when it become available.
"Provide network support for ceph and openstack lsst storage server efforts work done to provide network connectivity, troubleshoot and monitor connections for the above efforts",1,DM-3025,datamanagement,provide network support ceph openstack lsst storage server effort work provide network connectivity troubleshoot monitor connection effort,"Provide network support for ceph and openstack lsst storage server efforts work done to provide network connectivity, troubleshoot and monitor connections for the above efforts"
"DLP/LDM-240 support chages   - JIRA changes to create DLP project    - lsst-sqre/sqre-jirakit to generate LDM-240-like display     - iterate with T/CAMs, Kevin, Jeff",6,DM-3026,datamanagement,dlp ldm-240 support chage jira change create dlp project lsst sqre sqre jirakit generate ldm-240 like display iterate cams kevin jeff,"DLP/LDM-240 support chages - JIRA changes to create DLP project - lsst-sqre/sqre-jirakit to generate LDM-240-like display - iterate with T/CAMs, Kevin, Jeff"
"Early access user onboarding and feedback  Getting comments, testing, hipchat/JIRA changes",2,DM-3027,datamanagement,early access user onboarding feedback get comment testing hipchat jira change,"Early access user onboarding and feedback Getting comments, testing, hipchat/JIRA changes"
"Display stories in JIRA epic table display   Solved with Issue Matrix plugin; unfortunately this removed the ""create issue in this epic"" functionality, so that needs to be a new ticket.",1,DM-3028,datamanagement,display story jira epic table display solve issue matrix plugin unfortunately remove create issue epic functionality need new ticket,"Display stories in JIRA epic table display Solved with Issue Matrix plugin; unfortunately this removed the ""create issue in this epic"" functionality, so that needs to be a new ticket."
Set up Slack for evaluation   Free account procured and tested by various volunteers; next step is to apply for non-profit status which gives us the first paid tier free to 100 users. ,1,DM-3029,datamanagement,set slack evaluation free account procure test volunteer step apply non profit status give pay tier free 100 user,Set up Slack for evaluation Free account procured and tested by various volunteers; next step is to apply for non-profit status which gives us the first paid tier free to 100 users.
Set up Discourse for evaluation.   Server up on DO at community.lsst.org. Email needs fixing before volunteer users can be invited. ,1,DM-3030,datamanagement,set discourse evaluation server community.lsst.org email need fix volunteer user invite,Set up Discourse for evaluation. Server up on DO at community.lsst.org. Email needs fixing before volunteer users can be invited.
Addressing File corruption in iRODS 4.1.x We examine solutions for repairing corrupt files within an iRODS 4.1.x zone.,2,DM-3031,datamanagement,addressing file corruption irods 4.1.x examine solution repair corrupt file irods 4.1.x zone,Addressing File corruption in iRODS 4.1.x We examine solutions for repairing corrupt files within an iRODS 4.1.x zone.
Read revised LSE-209 and LSE-70 Read over the revised LSE-209 and LSE-70 documents,4,DM-3032,datamanagement,read revise lse-209 lse-70 read revise lse-209 lse-70 document,Read revised LSE-209 and LSE-70 Read over the revised LSE-209 and LSE-70 documents
"Add Sdss3Mapper to ingest, convert and map SDSS-III ""frame"" files SDSS-III does not use the fpC file format for science images.  Science images are now released as [""frame"" files. | http://data.sdss3.org/datamodel/files/BOSS_PHOTOOBJ/frames/RERUN/RUN/CAMCOL/frame.html]  The primary science image (hdu0) comes background subtracted and calibrated to units of nanomaggies, with the backgrounds and flat-field conversions included as extensions. The astrometric information is in hdu3 instead of a separate asTrans file.     obs_sdss should be able to ingest frame files and map them to load as dataset ""raw."" It should also optionally replace the backgrounds and de-calibrate to convert the units back from nanomaggies to counts.     This will be implemented as lsst.obs.sdss.sdssMapper.Sdss3Mapper.",5,DM-3033,datamanagement,add sdss3mapper ingest convert map sdss iii frame file sdss iii use fpc file format science image science image release frame file http://data.sdss3.org/datamodel/files/boss_photoobj/frames/rerun/run/camcol/frame.html primary science image hdu0 come background subtract calibrate unit nanomaggie background flat field conversion include extension astrometric information hdu3 instead separate astrans file obs_sdss able ingest frame file map load dataset raw optionally replace background de calibrate convert unit nanomaggie count implement lsst.obs.sdss.sdssmapper sdss3mapper,"Add Sdss3Mapper to ingest, convert and map SDSS-III ""frame"" files SDSS-III does not use the fpC file format for science images. Science images are now released as [""frame"" files. | http://data.sdss3.org/datamodel/files/BOSS_PHOTOOBJ/frames/RERUN/RUN/CAMCOL/frame.html] The primary science image (hdu0) comes background subtracted and calibrated to units of nanomaggies, with the backgrounds and flat-field conversions included as extensions. The astrometric information is in hdu3 instead of a separate asTrans file. obs_sdss should be able to ingest frame files and map them to load as dataset ""raw."" It should also optionally replace the backgrounds and de-calibrate to convert the units back from nanomaggies to counts. This will be implemented as lsst.obs.sdss.sdssMapper.Sdss3Mapper."
"Check czar->proxy messages size These messages are stored in VARCHAR(255) (FYI, MEMORY tables can't contain TEXT). We just need to make sure we have a reasonable fixed size CHAR (and maybe check whether we are hitting the limit, and log it somewhere)",4,DM-3035,datamanagement,check czar->proxy message size message store varchar(255 fyi memory table contain text need sure reasonable fix size char maybe check hit limit log,"Check czar->proxy messages size These messages are stored in VARCHAR(255) (FYI, MEMORY tables can't contain TEXT). We just need to make sure we have a reasonable fixed size CHAR (and maybe check whether we are hitting the limit, and log it somewhere)"
Move Qserv code comment to LSST documentation standards LSST documentation standards: https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards#DocumentationStandards-RequiredDocumentationStyle  is different from the previous standards used by Qserv (i.e. /// text).     We should convert everything to LSST documentation standards.,6,DM-3036,datamanagement,qserv code comment lsst documentation standard lsst documentation standard https://confluence.lsstcorp.org/display/ldmdg/documentation+standards#documentationstandards-requireddocumentationstyle different previous standard qserv i.e. /// text convert lsst documentation standard,Move Qserv code comment to LSST documentation standards LSST documentation standards: https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards#DocumentationStandards-RequiredDocumentationStyle is different from the previous standards used by Qserv (i.e. /// text). We should convert everything to LSST documentation standards.
"remove lsst/log wrapper from Qserv lsst/log API looks stable now, so removing the wrapper would simplify the code.",1,DM-3037,datamanagement,remove lsst log wrapper qserv lsst log api look stable remove wrapper simplify code,"remove lsst/log wrapper from Qserv lsst/log API looks stable now, so removing the wrapper would simplify the code."
Implement test suite for new class SqlTransaction Some test that shows that transactions are properly committed/aborted would be nice to have.,1,DM-3090,datamanagement,implement test suite new class sqltransaction test show transaction properly commit aborted nice,Implement test suite for new class SqlTransaction Some test that shows that transactions are properly committed/aborted would be nice to have.
"Remove unused function populateState()  Qserv doesn't seem to relaunch no more chunk query in case it fails (see DM-2643)    And this function is now unused:  {code:bash}  qserv@clrinfopc04:~/src/qserv (master)$ grep -r populateState core/  core/modules/qdisp/Executive.cc:void populateState(lsst::qserv::qdisp::ExecStatus& es,  {code}  ",1,DM-3091,datamanagement,remove unused function populatestate qserv relaunch chunk query case fail dm-2643 function unused code bash qserv@clrinfopc04:~/src qserv master)$ grep populatestate core/ core module qdisp executive.cc void populatestate(lsst::qserv::qdisp::execstatus es code,"Remove unused function populateState() Qserv doesn't seem to relaunch no more chunk query in case it fails (see DM-2643) And this function is now unused: {code:bash} qserv@clrinfopc04:~/src/qserv (master)$ grep -r populateState core/ core/modules/qdisp/Executive.cc:void populateState(lsst::qserv::qdisp::ExecStatus& es, {code}"
Creation of XML descriptions of messages sent to OCS Create XML descriptions of messages sent to the OCS. Upload these to a new github repository.,4,DM-3101,datamanagement,creation xml description message send ocs create xml description message send ocs upload new github repository,Creation of XML descriptions of messages sent to OCS Create XML descriptions of messages sent to the OCS. Upload these to a new github repository.
"Resolve segmentation fault in LoggingEvent destructor There seems to be a possible race condition in log4cxx::spi::LoggingEvent::~LoggingEvent. I've had multiple segmentation faults in that function. In all cases, another thread was involved in writing. In at least 2 cases, the second thread was in XrdCl::LogOutFile::Write.  ",5,DM-3102,datamanagement,resolve segmentation fault loggingevent destructor possible race condition log4cxx::spi::loggingevent::~loggingevent multiple segmentation fault function case thread involve writing case second thread xrdcl::logoutfile::write,"Resolve segmentation fault in LoggingEvent destructor There seems to be a possible race condition in log4cxx::spi::LoggingEvent::~LoggingEvent. I've had multiple segmentation faults in that function. In all cases, another thread was involved in writing. In at least 2 cases, the second thread was in XrdCl::LogOutFile::Write."
"Add ""ORDER BY"" clause to lua SQL query on result table If user query has ""ORDER BY"", then lua  can't just execute ""SELECT * FROM result"" because the order for such query is not guaranteed. To fix that, we need to add ""ORDER BY"" clause to the ""SELECT * FROM result"" query on the lua side.    Once we have the above, we might want to remove ""ORDER BY"" from the query class which runs a merge step on the czar (this has to be done in query analysis step).",8,DM-3104,datamanagement,add order clause lua sql query result table user query order lua execute select result order query guarantee fix need add order clause select result query lua want remove order query class run merge step czar query analysis step,"Add ""ORDER BY"" clause to lua SQL query on result table If user query has ""ORDER BY"", then lua can't just execute ""SELECT * FROM result"" because the order for such query is not guaranteed. To fix that, we need to add ""ORDER BY"" clause to the ""SELECT * FROM result"" query on the lua side. Once we have the above, we might want to remove ""ORDER BY"" from the query class which runs a merge step on the czar (this has to be done in query analysis step)."
"Add assertXNearlyEqual methods for image-like classes Presently one can compare two image-like objects using free functions imagesDiffer, masksDiffer and maskedImagesDiffer in lsst.afw.image.testUtils. These should be replaced by assertXNearlyEqual methods that afw adds to lsst.utils.tests, as per DM-2193.    If necessary, we could leave the old functions around for awhile. But I would prefer to simply get rid of them if we can.    One subtlety is that the current functions take numpy arrays, not afw image-like class instances. Examine the existing users of the code to determine how best to deal with that.",4,DM-3105,datamanagement,add assertxnearlyequal method image like class presently compare image like object free function imagesdiffer masksdiffer maskedimagesdiffer lsst.afw.image.testutils replace assertxnearlyequal method afw add lsst.utils.test dm-2193 necessary leave old function awhile prefer simply rid subtlety current function numpy array afw image like class instance examine exist user code determine good deal,"Add assertXNearlyEqual methods for image-like classes Presently one can compare two image-like objects using free functions imagesDiffer, masksDiffer and maskedImagesDiffer in lsst.afw.image.testUtils. These should be replaced by assertXNearlyEqual methods that afw adds to lsst.utils.tests, as per DM-2193. If necessary, we could leave the old functions around for awhile. But I would prefer to simply get rid of them if we can. One subtlety is that the current functions take numpy arrays, not afw image-like class instances. Examine the existing users of the code to determine how best to deal with that."
Add slot for calibration flux This is a port of [HSC-1005|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1005].,2,DM-3106,datamanagement,add slot calibration flux port hsc-1005|https://hsc jira.astro.princeton.edu jira browse hsc-1005,Add slot for calibration flux This is a port of [HSC-1005|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1005].
Use aperture flux for photometric calibration This is a port of work performed on HSC but without a ticket. Relevant commits are:    * [05bef6|https://github.com/HyperSuprime-Cam/meas_astrom/commit/05bef629adc37e44ea8482aab88e2eb38a47e3a0]  * [4a6be5|https://github.com/HyperSuprime-Cam/meas_astrom/commit/4a6be51c53f61e70f151de7f29863cb723197a99]  * [69d35a|https://github.com/HyperSuprime-Cam/obs_subaru/commit/69d35a890234e37c1142ddbeff43e62fe36e6c45]  * [9c996d|https://github.com/HyperSuprime-Cam/obs_subaru/commit/9c996d75c423ce03fb54c4300d9c7561b5c1ea99],1,DM-3108,datamanagement,use aperture flux photometric calibration port work perform hsc ticket relevant commit 05bef6|https://github.com hypersuprime cam meas_astrom commit/05bef629adc37e44ea8482aab88e2eb38a47e3a0 4a6be5|https://github.com hypersuprime cam meas_astrom commit/4a6be51c53f61e70f151de7f29863cb723197a99 69d35a|https://github.com hypersuprime cam obs_subaru commit/69d35a890234e37c1142ddbeff43e62fe36e6c45 9c996d|https://github.com hypersuprime cam obs_subaru commit/9c996d75c423ce03fb54c4300d9c7561b5c1ea99,Use aperture flux for photometric calibration This is a port of work performed on HSC but without a ticket. Relevant commits are: * [05bef6|https://github.com/HyperSuprime-Cam/meas_astrom/commit/05bef629adc37e44ea8482aab88e2eb38a47e3a0] * [4a6be5|https://github.com/HyperSuprime-Cam/meas_astrom/commit/4a6be51c53f61e70f151de7f29863cb723197a99] * [69d35a|https://github.com/HyperSuprime-Cam/obs_subaru/commit/69d35a890234e37c1142ddbeff43e62fe36e6c45] * [9c996d|https://github.com/HyperSuprime-Cam/obs_subaru/commit/9c996d75c423ce03fb54c4300d9c7561b5c1ea99]
"Add support for accessing schema from QueryContext When we are analyzing a query, sometimes there are situations where we need to know the schema of tables involved in a query. It will also be useful for checking if user is authorized to run query, and for queries like ""SHOW CREATE TABLE"". This story involves writing code that will provide access to schema.",3,DM-3109,datamanagement,add support access schema querycontext analyze query situation need know schema table involve query useful check user authorize run query query like create table story involve write code provide access schema,"Add support for accessing schema from QueryContext When we are analyzing a query, sometimes there are situations where we need to know the schema of tables involved in a query. It will also be useful for checking if user is authorized to run query, and for queries like ""SHOW CREATE TABLE"". This story involves writing code that will provide access to schema."
"qserv code cleanup I made some random cleanup of the qserv code while playing with css v2. I want to push these changes to master, thus I am creating this story for this. It involves improvements to logging in UserQueryFactory and Facade (both are now per-module), removing unnecessary namespace qualifiers, and whitspace cleanup.",1,DM-3110,datamanagement,qserv code cleanup random cleanup qserv code play css v2 want push change master create story involve improvement log userqueryfactory facade module remove unnecessary namespace qualifier whitspace cleanup,"qserv code cleanup I made some random cleanup of the qserv code while playing with css v2. I want to push these changes to master, thus I am creating this story for this. It involves improvements to logging in UserQueryFactory and Facade (both are now per-module), removing unnecessary namespace qualifiers, and whitspace cleanup."
"Enable aperture correction in the integration test The present integration test does not enable aperture correction. This should be enabled and the results sanity-checked.    This is a separate ticket rather than DM-436 at Jim Bosch's suggestion, to avoid ticket bloat.    It requires two separate changes:  - update obs_sdss's SdssCalibrateTask to measure and apply aperture correction  - update the expected results from the integration test lsst_dm_stack_demo",4,DM-3114,datamanagement,enable aperture correction integration test present integration test enable aperture correction enable result sanity check separate ticket dm-436 jim bosch suggestion avoid ticket bloat require separate change update obs_sdss sdsscalibratetask measure apply aperture correction update expect result integration test lsst_dm_stack_demo,"Enable aperture correction in the integration test The present integration test does not enable aperture correction. This should be enabled and the results sanity-checked. This is a separate ticket rather than DM-436 at Jim Bosch's suggestion, to avoid ticket bloat. It requires two separate changes: - update obs_sdss's SdssCalibrateTask to measure and apply aperture correction - update the expected results from the integration test lsst_dm_stack_demo"
Graphical communication interface Creating a graphical representation of execution framework,2,DM-3121,datamanagement,graphical communication interface create graphical representation execution framework,Graphical communication interface Creating a graphical representation of execution framework
"basic monitoring of jenkins nodes with notification This last weekend, the build slaves el6-2 and el7-2 ran out of disk space and were causing stack-os-matrix build failures.  We should have an active monitoring system that sends notifications via at least one of hipchat/email/pagerduty.  There is disk utilitization information present in jenkins itself, aws cloudwatch, and ganglia (as of v0.2.x of the demo).  However, it may make be more convenient (read: expedient) to use a dedicated monitoring system such as sensu instead of mining existing data sources.    We should also investigate if we can configure jenkins to not schedule jobs on a slaves with low disk space.",6,DM-3125,datamanagement,basic monitoring jenkins nod notification weekend build slave el6 el7 run disk space cause stack os matrix build failure active monitoring system send notification hipchat email pagerduty disk utilitization information present jenkin aw cloudwatch ganglia v0.2.x demo convenient read expedient use dedicated monitoring system sensu instead mine exist datum source investigate configure jenkin schedule job slave low disk space,"basic monitoring of jenkins nodes with notification This last weekend, the build slaves el6-2 and el7-2 ran out of disk space and were causing stack-os-matrix build failures. We should have an active monitoring system that sends notifications via at least one of hipchat/email/pagerduty. There is disk utilitization information present in jenkins itself, aws cloudwatch, and ganglia (as of v0.2.x of the demo). However, it may make be more convenient (read: expedient) to use a dedicated monitoring system such as sensu instead of mining existing data sources. We should also investigate if we can configure jenkins to not schedule jobs on a slaves with low disk space."
"gcc 4.8 package does not create a symlink bin/cc I created a new lsst package named ""gcc"" that contains Mario's gcc 4.8 package. I used it to build lsst_distrib on lsst-dev and it worked just fine. Unfortunately the package does not include bin/cc (which should be a symlink to bin/gcc), and this is wanted because the LSST build system uses cc to build C code.    The desired fix is to modify the installer to make a symlink bin/cc that points to bin/gcc.",2,DM-3126,datamanagement,gcc 4.8 package create symlink bin cc create new lsst package name gcc contain mario gcc 4.8 package build lsst_distrib lsst dev work fine unfortunately package include bin cc symlink bin gcc want lsst build system use cc build code desire fix modify installer symlink bin cc point bin gcc,"gcc 4.8 package does not create a symlink bin/cc I created a new lsst package named ""gcc"" that contains Mario's gcc 4.8 package. I used it to build lsst_distrib on lsst-dev and it worked just fine. Unfortunately the package does not include bin/cc (which should be a symlink to bin/gcc), and this is wanted because the LSST build system uses cc to build C code. The desired fix is to modify the installer to make a symlink bin/cc that points to bin/gcc."
"add ""dax_"" prefix to data access related packages As agreed at [Data Access Mtg 2015/07/13|https://confluence.lsstcorp.org/display/DM/Data+Access+Meeting+2015-07-13], add dax_ prefix towebserv, webservcommon, webserv_client, dbserv, imgserv, metaserv",1,DM-3133,datamanagement,add dax prefix datum access relate package agree data access mtg 2015/07/13|https://confluence.lsstcorp.org display dm data+access+meeting+2015 07 13 add dax prefix towebserv webservcommon webserv_client dbserv imgserv metaserv,"add ""dax_"" prefix to data access related packages As agreed at [Data Access Mtg 2015/07/13|https://confluence.lsstcorp.org/display/DM/Data+Access+Meeting+2015-07-13], add dax_ prefix towebserv, webservcommon, webserv_client, dbserv, imgserv, metaserv"
"Add & use new mask plane for out-of-bounds regions Add a new mask plane for regions with no data - fully vignetted, edge patches in coadd.    This is a port of [HSC-669|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-669].",2,DM-3136,datamanagement,add use new mask plane bound region add new mask plane region data fully vignette edge patch coadd port hsc-669|https://hsc jira.astro.princeton.edu jira browse hsc-669,"Add & use new mask plane for out-of-bounds regions Add a new mask plane for regions with no data - fully vignetted, edge patches in coadd. This is a port of [HSC-669|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-669]."
"Handle bad pixels in image stacker We currently OR together all mask bits, but we need to be cleverer about how we handle pixels that are bad in some but not all inputs.    This is a port of work carried out on [HSC-152|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-152].",1,DM-3137,datamanagement,handle bad pixel image stacker currently mask bit need cleverer handle pixel bad input port work carry hsc-152|https://hsc jira.astro.princeton.edu jira browse hsc-152,"Handle bad pixels in image stacker We currently OR together all mask bits, but we need to be cleverer about how we handle pixels that are bad in some but not all inputs. This is a port of work carried out on [HSC-152|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-152]."
Open day for RFP from Equipment vendors Open day with vendors to describe the needs and requirements of the mountain to base and La Serena to Santiago networks. This is the first formal meeting in the procurement process. Vendors will be invited to propose their solution along with cost.,8,DM-3138,datamanagement,open day rfp equipment vendor open day vendor describe need requirement mountain base la serena santiago network formal meeting procurement process vendor invite propose solution cost,Open day for RFP from Equipment vendors Open day with vendors to describe the needs and requirements of the mountain to base and La Serena to Santiago networks. This is the first formal meeting in the procurement process. Vendors will be invited to propose their solution along with cost.
"HSC backport: extra ""refColumn"" class attributes in multiband This is a transfer for changesets for [HSC-1283|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1283].  ",1,DM-3139,datamanagement,hsc backport extra refcolumn class attribute multiband transfer changeset hsc-1283|https://hsc jira.astro.princeton.edu jira browse hsc-1283,"HSC backport: extra ""refColumn"" class attributes in multiband This is a transfer for changesets for [HSC-1283|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1283]."
add gcc to list of packages in lsstsw Add gcc to the list of packages in etc/repos.yaml in lsstsw,1,DM-3140,datamanagement,add gcc list package lsstsw add gcc list package etc repos.yaml lsstsw,add gcc to list of packages in lsstsw Add gcc to the list of packages in etc/repos.yaml in lsstsw
"Reduce verbosity of astrometry The astrometry.net solver that runs by default in meas_astrom 10.1 is very verbose.  Here's an example running HSC data with an SDSS reference catalog:  {code}  $ processCcd.py /tigress/HSC/HSC --output /tigress/pprice/lsst --id visit=904020 ccd=49 --clobber-config  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/processCcd.py'  WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer  hscAstrom is not setup; using LSST's meas_astrom instead  Cannot import lsst.meas.multifit: disabling CModel measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/hsc/processCcd.py'  : input=/tigress/HSC/HSC  : calib=None  : output=/tigress/pprice/lsst  CameraMapper: Loading registry registry from /tigress/pprice/lsst/_parent/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3  processCcd: Processing {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr: Performing ISR on sensor {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  processCcd.isr: Applying linearity corrections to Ccd 49  processCcd.isr.crosstalk: Applying crosstalk correction  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  : Empty WCS extension, using FITS header  processCcd.isr: Set 0 BAD pixels to 647.04  processCcd.isr WARNING: There were 6192 unmasked NaNs  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  processCcd.isr: Flattened sky level: 647.130493 +/- 12.733898  processCcd.isr: Measuring sky levels in 8x16 grids: 648.106765  processCcd.isr: Sky flatness in 8x16 grids - pp: 0.024087 rms: 0.006057  processCcd.calibrate: installInitialPsf fwhm=5.88235294312 pixels; size=15 pixels  processCcd.calibrate.repair: Identified 80 cosmic rays.  processCcd.calibrate.detection: Detected 303 positive sources to 5 sigma.  processCcd.calibrate.detection: Resubtracting the background after object detection  processCcd.calibrate.initialMeasurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  LoadReferenceObjects: read index files  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667372351 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.307471 vs. 0.320229 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.047945 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.measurePsf: Measuring PSF  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(""Mean of empty slice."", RuntimeWarning)  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:71: RuntimeWarning: invalid value encountered in double_scalars    ret = ret.dtype.type(ret / rcount)  /home/pprice/LSST/meas/algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py:143: RuntimeWarning: invalid value encountered in less    update = dist < minDist  processCcd.calibrate.measurePsf: PSF star selector found 163 candidates  processCcd.calibrate.measurePsf: PSF determination using 114/163 stars.  processCcd.calibrate.repair: Identified 92 cosmic rays.  processCcd.calibrate: Fit and subtracted background  processCcd.calibrate.measurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  Solver:    Arcsec per pix range: 0.153025, 0.18516    Image size: 2054 x 4186    Quad size range: 205.4, 4662.78    Objs: 0, 50    Parity: 0, normal    Use_radec? yes, (320.343, 0.500178), radius 1 deg    Verify_pix: 1    Code tol: 0.01    Dist from quad bonus: yes    Distractor ratio: 0.25    Log tune-up threshold: inf    Log bail threshold: -230.259    Log stoplooking threshold: inf    Maxquads 0    Maxmatches 0    Set CRPIX? no    Tweak? no    Indexes: 3      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_0.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_1.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_2.fits    Field: 258 stars  Quad scale range: [641.674, 2208.56] pixels  object 1 of 50: 0 quads tried, 0 matched.  object 2 of 50: 0 quads tried, 0 matched.  object 3 of 50: 0 quads tried, 0 matched.  object 4 of 50: 0 quads tried, 0 matched.  object 5 of 50: 0 quads tried, 0 matched.  object 6 of 50: 0 quads tried, 0 matched.  Got a new best match: logodds 787.099.    log-odds ratio 787.099 (inf), 178 match, 1 conflict, 75 distractors, 220 index.    RA,Dec = (320.343,0.500213), pixel scale 0.167612 arcsec/pix.    Hit/miss:   Hit/miss: ++-+++++-++++++++++++--++-+--+++++-+-+++++++++-+++++++-+++++-+++++++++++++-++++++-++++++-+++++++-+++  Pixel scale: 0.167612 arcsec/pix.  Parity: pos.  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667328272 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.306732 vs. 0.320115 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.048271 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.photocal: Not applying color terms because config.applyColorTerms is False  processCcd.calibrate.photocal: Magnitude zero point: 30.685281 +/- 0.058711 from 173 stars  processCcd.calibrate: Photometric zero-point: 30.685281  processCcd.detection: Detected 1194 positive sources to 5 sigma.  processCcd.detection: Resubtracting the background after object detection  processCcd.deblend: Deblending 1194 sources  processCcd.deblend: Deblended: of 1194 sources, 143 were deblended, creating 358 children, total 1552 sources  processCcd.measurement: Measuring 1552 sources (1194 parents, 358 children)   processCcd WARNING: Persisting background models  processCcd: Matching icSource and Source catalogs to propagate flags.  processCcd: Matching src to reference catalogue  LoadReferenceObjects: getting reference objects using center (1023.5, 2087.5) pix = Fk5Coord(320.3429016, 0.5001781, 2000.00) sky and radius 0.00195667 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3429016, 0.5001781, 2000.00) with radius 0.112109149864 deg  LoadReferenceObjects: found 499 objects  LoadReferenceObjects: trimmed 261 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 1 because it had less linear scatter than the next iter: 0.300624 vs. 0.300652 pixels  {code}    The verbosity of the astrometry module is out of proportion with the rest of the modules, which makes it difficult to follow the processing.    This is a pull request for fixes I have made.",1,DM-3141,datamanagement,"reduce verbosity astrometry astrometry.net solver run default meas_astrom 10.1 verbose example run hsc datum sdss reference catalog code processccd.py hsc hsc /tigress pprice lsst --id visit=904020 ccd=49 --clobber config loading config overrride file /home pprice lsst obs subaru config processccd.py warning unable use psfex module name extensions.psfex.psfexpsfdetermin hscastrom setup lsst meas_astrom instead import lsst.meas.multifit disable cmodel measurement import lsst.meas.extensions.photometrykron disable kron measurement enable shapehsm meas_extensions_shapehsm_dir disable hsm shape measurement import lsst.meas.extensions.photometrykron disable kron measurement enable shapehsm meas_extensions_shapehsm_dir disable hsm shape measurement loading config overrride file /home pprice lsst obs subaru config hsc processccd.py input=/tigress hsc hsc calib output=/tigress pprice lsst cameramapper loading registry registry /tigress pprice lsst/_parent registry.sqlite3 cameramapper loading calibregistry registry hsc hsc calib calibregistry.sqlite3 processccd processing taiobs 2013 11 02 point 671 visit 904020 dateobs 2013 11 02 filter hsc field stripe82l ccd 49 exptime 30.0 processccd.isr perform isr sensor taiobs 2013 11 02 point 671 visit 904020 dateobs 2013 11 02 filter hsc field stripe82l ccd 49 exptime 30.0 processccd.isr warning write thumbnail image hsc.fitsthumb import maskedimage warning expect extension type find image processccd.isr apply linearity correction ccd 49 processccd.isr.crosstalk apply crosstalk correction afw.image maskedimage warning expect extension type find image wcs extension fits header processccd.isr set bad pixel 647.04 warning 6192 unmasked nans warning write thumbnail image hsc.fitsthumb import processccd.isr flattened sky level 647.130493 /- 12.733898 processccd.isr measure sky level 8x16 grid 648.106765 processccd.isr sky flatness 8x16 grid pp 0.024087 rm 0.006057 processccd.calibrate installinitialpsf fwhm=5.88235294312 pixel pixel processccd.calibrate.repair identify 80 cosmic ray processccd.calibrate.detection detect 303 positive source sigma processccd.calibrate.detection resubtracte background object detection measure 303 source 303 parent child processccd.calibrate.astrometry apply distortion correction processccd.calibrate.astrometry solve astrometry loadreferenceobject read index file processccd.calibrate.astrometry.solver number select source astrometry 258 processccd.calibrate.astrometry.solver got astrometric solution astrometry.net loadreferenceobjects get reference object center 1023.5 2084.5 pix fk5coord(320.3431396 0.5002365 2000.00 sky radius 0.00194896 rad loadreferenceobjects search object fk5coord(320.3431396 0.5002365 2000.00 radius 0.111667372351 deg loadreferenceobjects find 495 object loadreferenceobject trim 257 bbox object leave 238 processccd.calibrate.astrometry.solver fit wcs use iter linear scatter iter 0.307471 vs. 0.320229 pixel processccd.calibrate.astrometry 186 astrometric match processccd.calibrate.astrometry refit wcs processccd.calibrate.astrometry astrometric scatter 0.047945 arcsec non linear term 174 match 12 reject processccd.calibrate.measurepsf measure psf hsc lsst stack10_1 linux64 anaconda/2.1.0 g35ca374 lib python2.7 site package numpy core/_methods.py:59 runtimewarning mean slice warnings.warn(""mean slice runtimewarning hsc lsst stack10_1 linux64 anaconda/2.1.0 g35ca374 lib python2.7 site package numpy core/_methods.py:71 runtimewarning invalid value encounter double_scalar ret ret.dtype.type(ret rcount pprice lsst meas algorithm python lsst meas algorithm objectsizestarselector.py:143 runtimewarning invalid value encounter update dist mindist processccd.calibrate.measurepsf psf star selector find 163 candidate processccd.calibrate.measurepsf psf determination 114/163 star processccd.calibrate.repair identify 92 cosmic ray processccd.calibrate fit subtract background processccd.calibrate.measurement measure 303 source 303 parent child processccd.calibrate.astrometry apply distortion correction processccd.calibrate.astrometry solve astrometry processccd.calibrate.astrometry.solver number select source astrometry 258 solver arcsec pix range 0.153025 0.18516 image size 2054 4186 quad size range 205.4 4662.78 objs 50 parity normal use_radec yes 320.343 0.500178 radius deg verify_pix code tol 0.01 dist quad bonus yes distractor ratio 0.25 log tune threshold inf log bail threshold -230.259 log stoplooking threshold inf maxquad maxmatche set crpix tweak index /tigress hsc astrometry_net_data sdss dr9 fink v5b sdss dr9 fink v5b_and_263_0.fits /tigress hsc astrometry_net_data sdss dr9 fink v5b sdss dr9 fink v5b_and_263_1.fit /tigress hsc astrometry_net_data sdss dr9 fink v5b sdss dr9 fink v5b_and_263_2.fit field 258 star quad scale range 641.674 2208.56 pixel object 50 quad try match object 50 quad try match object 50 quad try match object 50 quad try match object 50 quad try match object 50 quad try match got new good match logodds 787.099 log odd ratio 787.099 inf 178 match conflict 75 distractor 220 index ra dec 320.343,0.500213 pixel scale 0.167612 arcsec pix hit miss hit miss -+++++-++++++++++++--++-+--+++++-+-+++++++++-+++++++-+++++-+++++++++++++-++++++-++++++-+++++++-+++ pixel scale 0.167612 arcsec pix parity pos processccd.calibrate.astrometry.solver get astrometric solution astrometry.net loadreferenceobjects get reference object center 1023.5 2084.5 pix fk5coord(320.3431396 0.5002365 2000.00 sky radius 0.00194896 rad loadreferenceobjects search object fk5coord(320.3431396 0.5002365 2000.00 radius 0.111667328272 deg loadreferenceobjects find 495 object loadreferenceobject trim 257 bbox object leave 238 processccd.calibrate.astrometry.solver fit wcs use iter linear scatter iter 0.306732 vs. 0.320115 pixel processccd.calibrate.astrometry 186 astrometric match processccd.calibrate.astrometry refit wcs processccd.calibrate.astrometry astrometric scatter 0.048271 arcsec non linear term 174 match 12 reject processccd.calibrate.photocal apply color term config.applycolorterm false processccd.calibrate.photocal magnitude zero point 30.685281 /- 0.058711 173 star processccd.calibrate photometric zero point 30.685281 processccd.detection detect 1194 positive source sigma processccd.detection resubtracte background object detection processccd.deblend deblende 1194 source processccd.deblend deblende 1194 source 143 deblende create 358 child total 1552 source processccd.measurement measure 1552 source 1194 parent 358 child processccd warning persist background model processccd matching icsource source catalog propagate flag processccd match src reference catalogue loadreferenceobject get reference object center 1023.5 2087.5 pix fk5coord(320.3429016 0.5001781 2000.00 sky radius 0.00195667 rad loadreferenceobjects search object fk5coord(320.3429016 0.5001781 2000.00 radius 0.112109149864 deg loadreferenceobjects find 499 object loadreferenceobject trim 261 bbox object leave 238 processccd.calibrate.astrometry.solver fit wcs use iter linear scatter iter 0.300624 vs. 0.300652 pixel code verbosity astrometry module proportion rest module make difficult follow processing pull request fix","Reduce verbosity of astrometry The astrometry.net solver that runs by default in meas_astrom 10.1 is very verbose. Here's an example running HSC data with an SDSS reference catalog: {code} $ processCcd.py /tigress/HSC/HSC --output /tigress/pprice/lsst --id visit=904020 ccd=49 --clobber-config : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/processCcd.py' WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer hscAstrom is not setup; using LSST's meas_astrom instead Cannot import lsst.meas.multifit: disabling CModel measurements Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/hsc/processCcd.py' : input=/tigress/HSC/HSC : calib=None : output=/tigress/pprice/lsst CameraMapper: Loading registry registry from /tigress/pprice/lsst/_parent/registry.sqlite3 CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3 processCcd: Processing {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0} processCcd.isr: Performing ISR on sensor {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0} processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported. afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE processCcd.isr: Applying linearity corrections to Ccd 49 processCcd.isr.crosstalk: Applying crosstalk correction afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE : Empty WCS extension, using FITS header processCcd.isr: Set 0 BAD pixels to 647.04 processCcd.isr WARNING: There were 6192 unmasked NaNs processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported. processCcd.isr: Flattened sky level: 647.130493 +/- 12.733898 processCcd.isr: Measuring sky levels in 8x16 grids: 648.106765 processCcd.isr: Sky flatness in 8x16 grids - pp: 0.024087 rms: 0.006057 processCcd.calibrate: installInitialPsf fwhm=5.88235294312 pixels; size=15 pixels processCcd.calibrate.repair: Identified 80 cosmic rays. processCcd.calibrate.detection: Detected 303 positive sources to 5 sigma. processCcd.calibrate.detection: Resubtracting the background after object detection processCcd.calibrate.initialMeasurement: Measuring 303 sources (303 parents, 0 children) processCcd.calibrate.astrometry: Applying distortion correction processCcd.calibrate.astrometry: Solving astrometry LoadReferenceObjects: read index files processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258 processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667372351 deg LoadReferenceObjects: found 495 objects LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238 processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.307471 vs. 0.320229 pixels processCcd.calibrate.astrometry: 186 astrometric matches processCcd.calibrate.astrometry: Refitting WCS processCcd.calibrate.astrometry: Astrometric scatter: 0.047945 arcsec (with non-linear terms, 174 matches, 12 rejected) processCcd.calibrate.measurePsf: Measuring PSF /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice. warnings.warn(""Mean of empty slice."", RuntimeWarning) /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:71: RuntimeWarning: invalid value encountered in double_scalars ret = ret.dtype.type(ret / rcount) /home/pprice/LSST/meas/algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py:143: RuntimeWarning: invalid value encountered in less update = dist < minDist processCcd.calibrate.measurePsf: PSF star selector found 163 candidates processCcd.calibrate.measurePsf: PSF determination using 114/163 stars. processCcd.calibrate.repair: Identified 92 cosmic rays. processCcd.calibrate: Fit and subtracted background processCcd.calibrate.measurement: Measuring 303 sources (303 parents, 0 children) processCcd.calibrate.astrometry: Applying distortion correction processCcd.calibrate.astrometry: Solving astrometry processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258 Solver: Arcsec per pix range: 0.153025, 0.18516 Image size: 2054 x 4186 Quad size range: 205.4, 4662.78 Objs: 0, 50 Parity: 0, normal Use_radec? yes, (320.343, 0.500178), radius 1 deg Verify_pix: 1 Code tol: 0.01 Dist from quad bonus: yes Distractor ratio: 0.25 Log tune-up threshold: inf Log bail threshold: -230.259 Log stoplooking threshold: inf Maxquads 0 Maxmatches 0 Set CRPIX? no Tweak? no Indexes: 3 /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_0.fits /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_1.fits /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_2.fits Field: 258 stars Quad scale range: [641.674, 2208.56] pixels object 1 of 50: 0 quads tried, 0 matched. object 2 of 50: 0 quads tried, 0 matched. object 3 of 50: 0 quads tried, 0 matched. object 4 of 50: 0 quads tried, 0 matched. object 5 of 50: 0 quads tried, 0 matched. object 6 of 50: 0 quads tried, 0 matched. Got a new best match: logodds 787.099. log-odds ratio 787.099 (inf), 178 match, 1 conflict, 75 distractors, 220 index. RA,Dec = (320.343,0.500213), pixel scale 0.167612 arcsec/pix. Hit/miss: Hit/miss: ++-+++++-++++++++++++--++-+--+++++-+-+++++++++-+++++++-+++++-+++++++++++++-++++++-++++++-+++++++-+++ Pixel scale: 0.167612 arcsec/pix. Parity: pos. processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667328272 deg LoadReferenceObjects: found 495 objects LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238 processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.306732 vs. 0.320115 pixels processCcd.calibrate.astrometry: 186 astrometric matches processCcd.calibrate.astrometry: Refitting WCS processCcd.calibrate.astrometry: Astrometric scatter: 0.048271 arcsec (with non-linear terms, 174 matches, 12 rejected) processCcd.calibrate.photocal: Not applying color terms because config.applyColorTerms is False processCcd.calibrate.photocal: Magnitude zero point: 30.685281 +/- 0.058711 from 173 stars processCcd.calibrate: Photometric zero-point: 30.685281 processCcd.detection: Detected 1194 positive sources to 5 sigma. processCcd.detection: Resubtracting the background after object detection processCcd.deblend: Deblending 1194 sources processCcd.deblend: Deblended: of 1194 sources, 143 were deblended, creating 358 children, total 1552 sources processCcd.measurement: Measuring 1552 sources (1194 parents, 358 children) processCcd WARNING: Persisting background models processCcd: Matching icSource and Source catalogs to propagate flags. processCcd: Matching src to reference catalogue LoadReferenceObjects: getting reference objects using center (1023.5, 2087.5) pix = Fk5Coord(320.3429016, 0.5001781, 2000.00) sky and radius 0.00195667 rad LoadReferenceObjects: search for objects at Fk5Coord(320.3429016, 0.5001781, 2000.00) with radius 0.112109149864 deg LoadReferenceObjects: found 499 objects LoadReferenceObjects: trimmed 261 out-of-bbox objects, leaving 238 processCcd.calibrate.astrometry.solver: Fit WCS: use iter 1 because it had less linear scatter than the next iter: 0.300624 vs. 0.300652 pixels {code} The verbosity of the astrometry module is out of proportion with the rest of the modules, which makes it difficult to follow the processing. This is a pull request for fixes I have made."
"Port HSC optimisations for reading astrometry.net catalog Some astrometry.net catalogs used in production can be quite large, and currently all of the catalog must be read in order to determine bounds for each component.  This can make the loading of the catalog quite slow (e.g., 144 sec out of 177 sec to process an HSC image, using an SDSS DR9 catalog).  We have HSC code that caches the required information, making the catalog load much faster.  The code is from the following HSC issues:    * [HSC-1087: Make astrometry faster|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1087]  * [HSC-1143: Floating point exception in astrometry|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1143]  * [HSC-1178: Faster construction of Astrometry.net catalog|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178]  * [HSC-1179: Assertion failure in astrometry.net|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178]    While there have been some changes to the LSST astrometry code that will mean we can't directly cherry-pick the HSC code, yet I think the main structure remains, so the approach can be copied without much effort.",3,DM-3142,datamanagement,port hsc optimisation read astrometry.net catalog astrometry.net catalog production large currently catalog read order determine bound component loading catalog slow e.g. 144 sec 177 sec process hsc image sdss dr9 catalog hsc code cache require information make catalog load fast code following hsc issue hsc-1087 astrometry faster|https://hsc jira.astro.princeton.edu jira browse hsc-1087 hsc-1143 float point exception astrometry|https://hsc jira.astro.princeton.edu jira browse hsc-1143 hsc-1178 fast construction astrometry.net catalog|https://hsc jira.astro.princeton.edu jira browse hsc-1178 hsc-1179 assertion failure astrometry.net|https://hsc jira.astro.princeton.edu jira browse hsc-1178 change lsst astrometry code mean directly cherry pick hsc code think main structure remain approach copy effort,"Port HSC optimisations for reading astrometry.net catalog Some astrometry.net catalogs used in production can be quite large, and currently all of the catalog must be read in order to determine bounds for each component. This can make the loading of the catalog quite slow (e.g., 144 sec out of 177 sec to process an HSC image, using an SDSS DR9 catalog). We have HSC code that caches the required information, making the catalog load much faster. The code is from the following HSC issues: * [HSC-1087: Make astrometry faster|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1087] * [HSC-1143: Floating point exception in astrometry|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1143] * [HSC-1178: Faster construction of Astrometry.net catalog|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178] * [HSC-1179: Assertion failure in astrometry.net|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178] While there have been some changes to the LSST astrometry code that will mean we can't directly cherry-pick the HSC code, yet I think the main structure remains, so the approach can be copied without much effort."
"Audit and improve warm-start configuration options Many of our command-line tasks - particularly the high-level MPI-based drivers we're moving over from the HSC side - typically reuse intermediate data products they find on disk rather than regenerate them by default, and have a suite of configuration options to control this behavior.    While this aids in faster reprocessing of aborted or failed jobs, it can produce results users would consider surprising (""I re-ran with a new version of the pipeline and nothing changed""), and (IMO) should not be the default behavior for any task.    We also need to guarantee that any warm-start reprocessing always produces the exact same results as a single consistent run.  I do not believe this is currently the case for some of the options in ProcessCcdTask.    Finally, we should put these configuration options somewhere other than the main task config, because they control how the processing is done, not what the results are, and hence should not be checked against existing config files in an output data repo before running.  This will probably require a new mechanism in pipe_base; perhaps a second config class associated with each Task, containing only options that affect the ""how"" of processing without affecting the ""what"".    The first step of implementing this issue should be an RFC.",8,DM-3144,datamanagement,audit improve warm start configuration option command line task particularly high level mpi base driver move hsc typically reuse intermediate datum product find disk regenerate default suite configuration option control behavior aid fast reprocessing aborted fail job produce result user consider surprising run new version pipeline change imo default behavior task need guarantee warm start reprocess produce exact result single consistent run believe currently case option processccdtask finally configuration option main task config control processing result check exist config file output datum repo run probably require new mechanism pipe_base second config class associate task contain option affect process affect step implement issue rfc,"Audit and improve warm-start configuration options Many of our command-line tasks - particularly the high-level MPI-based drivers we're moving over from the HSC side - typically reuse intermediate data products they find on disk rather than regenerate them by default, and have a suite of configuration options to control this behavior. While this aids in faster reprocessing of aborted or failed jobs, it can produce results users would consider surprising (""I re-ran with a new version of the pipeline and nothing changed""), and (IMO) should not be the default behavior for any task. We also need to guarantee that any warm-start reprocessing always produces the exact same results as a single consistent run. I do not believe this is currently the case for some of the options in ProcessCcdTask. Finally, we should put these configuration options somewhere other than the main task config, because they control how the processing is done, not what the results are, and hence should not be checked against existing config files in an output data repo before running. This will probably require a new mechanism in pipe_base; perhaps a second config class associated with each Task, containing only options that affect the ""how"" of processing without affecting the ""what"". The first step of implementing this issue should be an RFC."
Install switches and transceivers on Pachon and Tololo Buy switches to deploy MPLS over the fibers. ,8,DM-3147,datamanagement,install switch transceiver pachon tololo buy switch deploy mpls fiber,Install switches and transceivers on Pachon and Tololo Buy switches to deploy MPLS over the fibers.
Configure switches for AURA tenants Switch configuration for use by individual tenants of AURA,8,DM-3148,datamanagement,configure switch aura tenant switch configuration use individual tenant aura,Configure switches for AURA tenants Switch configuration for use by individual tenants of AURA
"CI validation of lsstsw's repos.yaml Having some sort of automatic ""lint check"" of the repos.yaml file is desirable due to the length of time required to do a full up test of lsstsw.  It should be possible to cobble a sanity checker together that can be run from travis-ci.",1,DM-3151,datamanagement,ci validation lsstsw repos.yaml have sort automatic lint check repos.yaml file desirable length time require test lsstsw possible cobble sanity checker run travis ci,"CI validation of lsstsw's repos.yaml Having some sort of automatic ""lint check"" of the repos.yaml file is desirable due to the length of time required to do a full up test of lsstsw. It should be possible to cobble a sanity checker together that can be run from travis-ci."
meas_base still uses eups in tests {{tests/centroid.py}} uses EUPS to determine the location of the data file used by the test. This needs to be fixed to use a location relative to the test file.,1,DM-3153,datamanagement,meas_base use eup test test centroid.py use eups determine location data file test need fix use location relative test file,meas_base still uses eups in tests {{tests/centroid.py}} uses EUPS to determine the location of the data file used by the test. This needs to be fixed to use a location relative to the test file.
"meas_astrom still using eups in tests In DM-2636 we modified the tests to be skipped if EUPS is not available. I've had a closer look and all the ones I have glanced at seem to be easily fixable to run without EUPS. The tests seem to be using EUPS to locate the {{meas_astrom}} (effectively asking EUPS for the location of the test file), then a path to the astrometry.net test data within the {{tests/}} directory is located and then EUPS is asked to setup {{astrometry_net_data}} using that path. Since the table files are all empty this is the equivalent to simply assigning the {{ASTROMETRY_NET_DATA_DIR}} environment variable directly to the path in the tests sub-directory.    Making this change to one of the tests seems to work so I will change the rest.",2,DM-3154,datamanagement,meas_astrom eup test dm-2636 modify test skip eups available close look one glance easily fixable run eups test eups locate meas_astrom effectively ask eups location test file path astrometry.net test datum tests/ directory locate eups ask setup astrometry_net_data path table file equivalent simply assign astrometry_net_data_dir environment variable directly path test sub directory make change test work change rest,"meas_astrom still using eups in tests In DM-2636 we modified the tests to be skipped if EUPS is not available. I've had a closer look and all the ones I have glanced at seem to be easily fixable to run without EUPS. The tests seem to be using EUPS to locate the {{meas_astrom}} (effectively asking EUPS for the location of the test file), then a path to the astrometry.net test data within the {{tests/}} directory is located and then EUPS is asked to setup {{astrometry_net_data}} using that path. Since the table files are all empty this is the equivalent to simply assigning the {{ASTROMETRY_NET_DATA_DIR}} environment variable directly to the path in the tests sub-directory. Making this change to one of the tests seems to work so I will change the rest."
Improve name and default value of MeasureApCorrConfig.refFluxAlg The config name refFluxAlg should be refFluxField (since it is a flux field name prefix) and the default should be  base_CircularApertureFlux_5 instead of base_CircularApertureFlux_0 (thus giving a reasonable radius instead of one that is ridiculously too small).    I should have handled it on DM-436 but it slipped through.,1,DM-3160,datamanagement,improve default value measureapcorrconfig.reffluxalg config reffluxalg reffluxfield flux field prefix default base_circularapertureflux_5 instead base_circularapertureflux_0 give reasonable radius instead ridiculously small handle dm-436 slip,Improve name and default value of MeasureApCorrConfig.refFluxAlg The config name refFluxAlg should be refFluxField (since it is a flux field name prefix) and the default should be base_CircularApertureFlux_5 instead of base_CircularApertureFlux_0 (thus giving a reasonable radius instead of one that is ridiculously too small). I should have handled it on DM-436 but it slipped through.
Extend KVInterface - add support for updates The CSS Facade and KVInterface currently do not support updates. This story covers adding support for basic updates.,4,DM-3162,datamanagement,extend kvinterface add support update css facade kvinterface currently support update story cover add support basic update,Extend KVInterface - add support for updates The CSS Facade and KVInterface currently do not support updates. This story covers adding support for basic updates.
"In CalibrateTask if one disables psf determination then aperture correction will fail In pipe_tasks CalibrateTask, by default aperture correction uses source flag ""calib_psfUsed"" to decide if a source is acceptable to use for measuring aperture correction. If PSF determination is disabled then this flag is never set and aperture correction will fail with a complaint that there are 0 sources.  ",1,DM-3173,datamanagement,calibratetask disable psf determination aperture correction fail pipe_task calibratetask default aperture correction use source flag calib_psfuse decide source acceptable use measure aperture correction psf determination disabled flag set aperture correction fail complaint source,"In CalibrateTask if one disables psf determination then aperture correction will fail In pipe_tasks CalibrateTask, by default aperture correction uses source flag ""calib_psfUsed"" to decide if a source is acceptable to use for measuring aperture correction. If PSF determination is disabled then this flag is never set and aperture correction will fail with a complaint that there are 0 sources."
"CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the wrong schema CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the initial schema ""schema1"" instead of the final schema. Normally this would not matter since most of the fields are shared, but aperture correction wants aperture flux at a larger radius than the narrowest option, and schema1 may only provide the narrowest option.    In any case it is safer to instantiate those three subtasks using the final schema, since they are only ever run on the final schema. (Several other subtasks are run on both the initial and final schema, and should continue to be instantiated using schema1).",1,DM-3174,datamanagement,calibratetask instantiate measureapcorr applyapcorr photocal subtask wrong schema calibratetask instantiate measureapcorr applyapcorr photocal subtask initial schema schema1 instead final schema normally matter field share aperture correction want aperture flux large radius narrow option schema1 provide narrow option case safe instantiate subtask final schema run final schema subtask run initial final schema continue instantiate schema1,"CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the wrong schema CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the initial schema ""schema1"" instead of the final schema. Normally this would not matter since most of the fields are shared, but aperture correction wants aperture flux at a larger radius than the narrowest option, and schema1 may only provide the narrowest option. In any case it is safer to instantiate those three subtasks using the final schema, since they are only ever run on the final schema. (Several other subtasks are run on both the initial and final schema, and should continue to be instantiated using schema1)."
Build 2015_08 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,1,DM-3175,datamanagement,build 2015_08 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build 2015_08 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
Build and Test 2015_09 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,3,DM-3176,datamanagement,build test 2015_09 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build and Test 2015_09 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
Build and Test 2015_10 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,3,DM-3177,datamanagement,build test 2015_10 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build and Test 2015_10 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
Build and Test 2015_11 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,3,DM-3178,datamanagement,build test 2015_11 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build and Test 2015_11 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
Build and Test 2015_12 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,3,DM-3179,datamanagement,build test 2015_12 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build and Test 2015_12 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
Build and Test 2016_01 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,3,DM-3180,datamanagement,build test 2016_01 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build and Test 2016_01 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
Build and Test 2016_02 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.,1,DM-3181,datamanagement,build test 2016_02 qserv release https://confluence.lsstcorp.org/display/dm/qserv+release+procedure recipe,Build and Test 2016_02 Qserv Release See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.
"Aperture correction not applied for some measurements Aperture correction needs to be applied every time a measurement is run after it is first measured in CalibrateTask. As of DM-436 aperture correction is only being applied in CalibrateTask, which for example means the information is overwritten during the final measurement of ProcessImageTask.run.    This is probably best done by adding code to apply aperture correction to BaseMeasurementTask, so it is inherited by SingleFrameMeasurementTask and ForcedMeasurementTask.",5,DM-3182,datamanagement,aperture correction apply measurement aperture correction need apply time measurement run measure calibratetask dm-436 aperture correction apply calibratetask example mean information overwrite final measurement processimagetask.run probably well add code apply aperture correction basemeasurementtask inherit singleframemeasurementtask forcedmeasurementtask,"Aperture correction not applied for some measurements Aperture correction needs to be applied every time a measurement is run after it is first measured in CalibrateTask. As of DM-436 aperture correction is only being applied in CalibrateTask, which for example means the information is overwritten during the final measurement of ProcessImageTask.run. This is probably best done by adding code to apply aperture correction to BaseMeasurementTask, so it is inherited by SingleFrameMeasurementTask and ForcedMeasurementTask."
"CalibrateTask instantiates some subtasks with the wrong schema CalibrateTask instantiates some subtasks with the wrong schema, in particular:  - astrometry is instantiated with the final schema but run on schema1  - measureApCorr, applyApCorr and photocal are instantiated with schema1 but run on the final schema    One way this can cause problems is that schema1 may not have the data needed to measure aperture correction (e.g. it may contain only one tiny radius of aperture flux), as came to light when running the lsst stack demo.",1,DM-3183,datamanagement,calibratetask instantiate subtask wrong schema calibratetask instantiate subtask wrong schema particular astrometry instantiate final schema run schema1 measureapcorr applyapcorr photocal instantiate schema1 run final schema way cause problem schema1 datum need measure aperture correction e.g. contain tiny radius aperture flux come light run lsst stack demo,"CalibrateTask instantiates some subtasks with the wrong schema CalibrateTask instantiates some subtasks with the wrong schema, in particular: - astrometry is instantiated with the final schema but run on schema1 - measureApCorr, applyApCorr and photocal are instantiated with schema1 but run on the final schema One way this can cause problems is that schema1 may not have the data needed to measure aperture correction (e.g. it may contain only one tiny radius of aperture flux), as came to light when running the lsst stack demo."
"Add PT.12 Filter/Science_Ccd_Exposure tables to extend test query coverage Filter table is missing from case02, case05 data, so next query can't be tested:  {code:sql}  -- datasets/case02/queries/3023_joinObjectSourceFilter.sql.FIXME  --- Join on Source and Filter and select specific filter in region  --- https://dev.lsstcorp.org/trac/wiki/db/Qserv/IN2P3/BenchmarkMarch2013  --- https://dev.lsstcorp.org/trac/wiki/db/queries/007  -  SELECT objectId, taiMidPoint, fluxToAbMag(psfMag)  FROM   Source  JOIN   Object USING(objectId)  JOIN   Filter USING(filterId)  -WHERE   ra_PS BETWEEN 1 AND 2 -- noQserv  AND decl_PS BETWEEN 3 AND 4 -- noQserv  --- withQserv  qserv_areaspec_box(1,3,2,4)  AND  filterName = 'u'  AND  variability BETWEEN 0 AND 2    {code}    Same thing for case02:1011_objectsForExposure and case02:1030_timeSeries.sql",4,DM-3186,datamanagement,"add pt.12 filter science_ccd_exposure table extend test query coverage filter table miss case02 case05 datum query test code sql dataset case02 queries/3023_joinobjectsourcefilter.sql fixme join source filter select specific filter region https://dev.lsstcorp.org/trac/wiki/db/qserv/in2p3/benchmarkmarch2013 https://dev.lsstcorp.org/trac/wiki/db/queries/007 select objectid taimidpoint fluxtoabmag(psfmag source join object using(objectid join filter using(filterid ra_ps noqserv decl_ps noqserv withqserv qserv_areaspec_box(1,3,2,4 filtername variability code thing case02:1011_objectsforexposure case02:1030_timeseries.sql","Add PT.12 Filter/Science_Ccd_Exposure tables to extend test query coverage Filter table is missing from case02, case05 data, so next query can't be tested: {code:sql} -- datasets/case02/queries/3023_joinObjectSourceFilter.sql.FIXME --- Join on Source and Filter and select specific filter in region --- https://dev.lsstcorp.org/trac/wiki/db/Qserv/IN2P3/BenchmarkMarch2013 --- https://dev.lsstcorp.org/trac/wiki/db/queries/007 - SELECT objectId, taiMidPoint, fluxToAbMag(psfMag) FROM Source JOIN Object USING(objectId) JOIN Filter USING(filterId) -WHERE ra_PS BETWEEN 1 AND 2 -- noQserv AND decl_PS BETWEEN 3 AND 4 -- noQserv --- withQserv qserv_areaspec_box(1,3,2,4) AND filterName = 'u' AND variability BETWEEN 0 AND 2 {code} Same thing for case02:1011_objectsForExposure and case02:1030_timeSeries.sql"
"Fix UDF for case01 query: 3005_orderByRA.sql query    {code:bash}  mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch qservTest_case01_qserv -e ""SELECT * FROM Object WHERE qserv_areaspec_box(0.,1.,0.,1.)""  {code}  returns nothing whereas   {code}  SELECT *   FROM Object  WHERE ra_PS BETWEEN 0. AND 1.   -- noQserv  AND decl_PS BETWEEN 0. AND 1.  {code} does (but doesn't use geom index)",6,DM-3188,datamanagement,"fix udf case01 query 3005_orderbyra.sql query code bash mysql --port=4040 qsmaster qservtest_case01_qserv select object qserv_areaspec_box(0.,1.,0.,1 code return code select object ra_ps noqserv decl_ps code use geom index","Fix UDF for case01 query: 3005_orderByRA.sql query {code:bash} mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch qservTest_case01_qserv -e ""SELECT * FROM Object WHERE qserv_areaspec_box(0.,1.,0.,1.)"" {code} returns nothing whereas {code} SELECT * FROM Object WHERE ra_PS BETWEEN 0. AND 1. -- noQserv AND decl_PS BETWEEN 0. AND 1. {code} does (but doesn't use geom index)"
"Remove _chunkId, _subChunkId column from case02:Object table This columns are Qserv internal and shouldn't be in input data. For example, this prevents case02:3021_selectObjectSortedByRA to work.    Check also that these columns aren't in other test data set and remove FIXME suffix from related broken query.",4,DM-3189,datamanagement,remove chunkid subchunkid column case02 object table column qserv internal input datum example prevent work check column test datum set remove fixme suffix related broken query,"Remove _chunkId, _subChunkId column from case02:Object table This columns are Qserv internal and shouldn't be in input data. For example, this prevents case02:3021_selectObjectSortedByRA to work. Check also that these columns aren't in other test data set and remove FIXME suffix from related broken query."
"Document deprecation of DecoratedImage According to discussion on Hipchat (20 July 2015)    {quote}  Jim Bosch: [...] DecoratedImage is strongly deprecated, though  {quote}    This was news to me, and certainly isn't reflected [in (at least the obvious place) in Doxygen|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/afw_sec_image.html]. It should be.",1,DM-3190,datamanagement,document deprecation decoratedimage accord discussion hipchat 20 july 2015 quote jim bosch ... decoratedimage strongly deprecate quote news certainly reflect obvious place doxygen|https://lsst web.ncsa.illinois.edu doxygen x_masterdoxydoc afw_sec_image.html,"Document deprecation of DecoratedImage According to discussion on Hipchat (20 July 2015) {quote} Jim Bosch: [...] DecoratedImage is strongly deprecated, though {quote} This was news to me, and certainly isn't reflected [in (at least the obvious place) in Doxygen|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/afw_sec_image.html]. It should be."
"Re-implement watcher based on new CSS implementation Current watcher implementation (in {{admin/bin/watcher.py}}) is based on direct watching of zookeeper updates via kazoo. If we are to re-implement CSS based on mysql then watcher needs to be updated to support it. Mysql does not have watch mechanism, so it has to be done via polling or using some other mechanism if synchronous notifications are needed.",8,DM-3192,datamanagement,implement watcher base new css implementation current watcher implementation admin bin watcher.py base direct watching zookeeper update kazoo implement css base mysql watcher need update support mysql watch mechanism polling mechanism synchronous notification need,"Re-implement watcher based on new CSS implementation Current watcher implementation (in {{admin/bin/watcher.py}}) is based on direct watching of zookeeper updates via kazoo. If we are to re-implement CSS based on mysql then watcher needs to be updated to support it. Mysql does not have watch mechanism, so it has to be done via polling or using some other mechanism if synchronous notifications are needed."
Audit existing test and development system Document in the wiki how the test and development systems are connected and configured.,3,DM-3193,datamanagement,audit exist test development system document wiki test development system connect configure,Audit existing test and development system Document in the wiki how the test and development systems are connected and configured.
Fix cluster install procedure and improve docker support Document how-to update cluster from Qserv release:    See  http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/cluster-deployment.html,1,DM-3194,datamanagement,fix cluster install procedure improve docker support document update cluster qserv release http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/how-to/cluster-deployment.html,Fix cluster install procedure and improve docker support Document how-to update cluster from Qserv release: See http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/cluster-deployment.html
"makeWcs() chokes on decam images in 10.1 In 10.0, processCcdDecam.py could process decam images to completion (whether the WCS was read correctly is a different question). Now it fails on makeWcs() (see traceback below), and I suspect this change in behavior is related to DM-2883 and DM-2967.    Repository with both data and code to reproduce:  http://www.astro.washington.edu/users/yusra/reproduce/reproduceMakeWcsErr.tar.gz  (apologies for the size)    The attachment is a document describing the WCS representation in the images from the community pipeline, courtesy of Francisco Forster.    Please advise. This ticket captures any changes made to afw.     {code}  D-108-179-166-118:decam yusra$ processCcdDecam.py newTestRepo/ --id visit=0232847 ccdnum=10 --config calibrate.doPhotoCal=False calibrate.doAstrometry=False calibrate.measurePsf.starSelector.name=""secondMoment"" doWriteCalibrateMatches=False --clobber-config  : Loading config overrride file '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/processCcdDecam.py'  : Config override file does not exist: '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/decam/processCcdDecam.py'  : input=/Users/yusra/decam/newTestRepo  : calib=None  : output=None  CameraMapper: Loading registry registry from /Users/yusra/decam/newTestRepo/registry.sqlite3  processCcdDecam: Processing {'visit': 232847, 'ccdnum': 10}  makeWcs WARNING: Stripping PVi_j keys from projection RA---TPV/DEC--TPV  processCcdDecam FATAL: Failed on dataId={'visit': 232847, 'ccdnum': 10}:     File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs()      Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0}  lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value'    Traceback (most recent call last):    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/processCcdDecam.py"", line 77, in run      mi = exp.getMaskedImage()    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 41, in __getattribute__      subject = oga(self, '__subject__')    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 136, in __subject__      set_cache(self, get_callback(self)())    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 242, in <lambda>      innerCallback(), dataId)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 236, in <lambda>      location, dataId)    File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/decamMapper.py"", line 118, in bypass_instcal      wcs         = afwImage.makeWcs(md)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/afw/10.1-26-g9124caf+1/python/lsst/afw/image/imageLib.py"", line 8706, in makeWcs      return _imageLib.makeWcs(*args)  RuntimeError:     File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs()      Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0}  lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value'  {code}    ",2,DM-3196,datamanagement,"makewcs choke decam image 10.1 10.0 processccddecam.py process decam image completion wcs read correctly different question fail makewcs traceback suspect change behavior relate dm-2883 dm-2967 repository datum code reproduce http://www.astro.washington.edu/users/yusra/reproduce/reproducemakewcserr.tar.gz apology size attachment document describe wcs representation image community pipeline courtesy francisco forster advise ticket capture change afw code d-108 179 166 118 decam yusra$ processccddecam.py newtestrepo/ --id visit=0232847 ccdnum=10 calibrate.dophotocal false calibrate.doastrometry false calibrate.measurepsf.starselector.name=""secondmoment dowritecalibratematche false --clobber config loading config overrride file /users yusra lsst_devel lsst repos obs_decam_ya config processccddecam.py config override file exist /users yusra lsst_devel lsst repos obs_decam_ya config decam processccddecam.py input=/user yusra decam newtestrepo calib output cameramapper loading registry registry /users yusra decam newtestrepo registry.sqlite3 processccddecam processing visit 232847 ccdnum 10 makewcs warning strip pvi_j key projection ra tpv dec tpv processccddecam fatal fail dataid={'visit 232847 ccdnum 10 file src image wcs.cc line 130 void lsst::afw::image::wcs::_initwcs fail setup wcs structure wcsset status invalid parameter value lsst::pex::exceptions::runtimeerror fail setup wcs structure wcsset status invalid parameter value traceback recent file /users yusra lsst_devel lsst dms5 darwinx86 pipe_base/10.1 g18c2ba7 49 python lsst pipe base cmdlinetask.py line 320 result task.run(dataref kwargs file /users yusra lsst_devel lsst dms5 darwinx86 pipe_base/10.1 g18c2ba7 49 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /users yusra lsst_devel lsst repos obs_decam_ya python lsst obs decam processccddecam.py line 77 run mi exp.getmaskedimage file /users yusra lsst_devel lsst dms5 darwinx86 daf_persistence/10.1 g6edbc00 28 python lsst daf persistence readproxy.py line 41 getattribute subject oga(self subject file /users yusra lsst_devel lsst dms5 darwinx86 daf_persistence/10.1 g6edbc00 28 python lsst daf persistence readproxy.py line 136 subject get_callback(self file /users yusra lsst_devel lsst dms5 darwinx86 daf_persistence/10.1 g6edbc00 28 python lsst daf persistence butler.py line 242 innercallback dataid file /users yusra lsst_devel lsst dms5 darwinx86 daf_persistence/10.1 g6edbc00 28 python lsst daf persistence butler.py line 236 location dataid file /users yusra lsst_devel lsst repos obs_decam_ya python lsst obs decam decammapper.py line 118 bypass_instcal wcs afwimage.makewcs(md file /users yusra lsst_devel lsst dms5 darwinx86 afw/10.1 26 g9124caf+1 python lsst afw image imagelib.py line 8706 makewcs return runtimeerror file src image wcs.cc line 130 void lsst::afw::image::wcs::_initwcs fail setup wcs structure wcsset status invalid parameter value lsst::pex::exceptions::runtimeerror fail setup wcs structure wcsset status invalid parameter value code","makeWcs() chokes on decam images in 10.1 In 10.0, processCcdDecam.py could process decam images to completion (whether the WCS was read correctly is a different question). Now it fails on makeWcs() (see traceback below), and I suspect this change in behavior is related to DM-2883 and DM-2967. Repository with both data and code to reproduce: http://www.astro.washington.edu/users/yusra/reproduce/reproduceMakeWcsErr.tar.gz (apologies for the size) The attachment is a document describing the WCS representation in the images from the community pipeline, courtesy of Francisco Forster. Please advise. This ticket captures any changes made to afw. {code} D-108-179-166-118:decam yusra$ processCcdDecam.py newTestRepo/ --id visit=0232847 ccdnum=10 --config calibrate.doPhotoCal=False calibrate.doAstrometry=False calibrate.measurePsf.starSelector.name=""secondMoment"" doWriteCalibrateMatches=False --clobber-config : Loading config overrride file '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/processCcdDecam.py' : Config override file does not exist: '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/decam/processCcdDecam.py' : input=/Users/yusra/decam/newTestRepo : calib=None : output=None CameraMapper: Loading registry registry from /Users/yusra/decam/newTestRepo/registry.sqlite3 processCcdDecam: Processing {'visit': 232847, 'ccdnum': 10} makeWcs WARNING: Stripping PVi_j keys from projection RA---TPV/DEC--TPV processCcdDecam FATAL: Failed on dataId={'visit': 232847, 'ccdnum': 10}: File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs() Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0} lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value' Traceback (most recent call last): File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__ result = task.run(dataRef, **kwargs) File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/processCcdDecam.py"", line 77, in run mi = exp.getMaskedImage() File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 41, in __getattribute__ subject = oga(self, '__subject__') File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 136, in __subject__ set_cache(self, get_callback(self)()) File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 242, in  innerCallback(), dataId) File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 236, in  location, dataId) File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/decamMapper.py"", line 118, in bypass_instcal wcs = afwImage.makeWcs(md) File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/afw/10.1-26-g9124caf+1/python/lsst/afw/image/imageLib.py"", line 8706, in makeWcs return _imageLib.makeWcs(*args) RuntimeError: File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs() Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0} lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value' {code}"
"Standardize Qserv install procedure: step 1 build docker container for master/worker instance and development version  - shmux could be used for parallel ssh (remove Qserv builtin one)  - look at ""serf and consul"" (See Confluence pages)  - improve doc: http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/index.html  - run multiple instances/versions of Qserv using different run dir/ports and the same data",8,DM-3199,datamanagement,standardize qserv install procedure step build docker container master worker instance development version shmux parallel ssh remove qserv builtin look serf consul confluence page improve doc http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/how-to/index.html run multiple instance version qserv different run dir port datum,"Standardize Qserv install procedure: step 1 build docker container for master/worker instance and development version - shmux could be used for parallel ssh (remove Qserv builtin one) - look at ""serf and consul"" (See Confluence pages) - improve doc: http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/index.html - run multiple instances/versions of Qserv using different run dir/ports and the same data"
"Include reference magnitude errors in PhotoCal PhotoCal task currently ignores the uncertainties on reference sources, which can lead to problems when the reference and measured catalogs have relatively little overlap or otherwise disagree on how trustworthy a source is.",3,DM-3202,datamanagement,include reference magnitude error photocal photocal task currently ignore uncertainty reference source lead problem reference measured catalog relatively little overlap disagree trustworthy source,"Include reference magnitude errors in PhotoCal PhotoCal task currently ignores the uncertainties on reference sources, which can lead to problems when the reference and measured catalogs have relatively little overlap or otherwise disagree on how trustworthy a source is."
W16 Data Access and Db Release Documentation Write Release documentation covering Data Access and Database work.,5,DM-3204,datamanagement,w16 data access db release documentation write release documentation cover data access database work,W16 Data Access and Db Release Documentation Write Release documentation covering Data Access and Database work.
"Revisit cost of replicating non-partitioned tables on all nodes Revisit size of all non-partitioned tables, and cost of replicating them on all worker nodes.",4,DM-3205,datamanagement,revisit cost replicate non partitioned table nodes revisit size non partitioned table cost replicate worker node,"Revisit cost of replicating non-partitioned tables on all nodes Revisit size of all non-partitioned tables, and cost of replicating them on all worker nodes."
"Estimate I/O load for non-partitioned tables Estimate realistic IO load from user queries on non-partitioned tables. Consider whether there might be hot spots (eg., maybe a small subset of columns from exposure is used very often. If it is it, maybe it'd be worth replicating only these columns across all worker nodes and serve the rest from one shared file system).",6,DM-3206,datamanagement,estimate load non partitioned table estimate realistic io load user query non partitioned table consider hot spot eg maybe small subset column exposure maybe worth replicate column worker node serve rest share file system,"Estimate I/O load for non-partitioned tables Estimate realistic IO load from user queries on non-partitioned tables. Consider whether there might be hot spots (eg., maybe a small subset of columns from exposure is used very often. If it is it, maybe it'd be worth replicating only these columns across all worker nodes and serve the rest from one shared file system)."
"Add debugging for astrometry.net solver To be able to debug astrometric matching, it helps to be able to visualise the source positions, the distorted source positions, and the reference positions.  This is a pull request to add these.",1,DM-3209,datamanagement,add debug astrometry.net solver able debug astrometric matching help able visualise source position distorted source position reference position pull request add,"Add debugging for astrometry.net solver To be able to debug astrometric matching, it helps to be able to visualise the source positions, the distorted source positions, and the reference positions. This is a pull request to add these."
"ChebyshevBoundedField should use _ not . as field separators for persistence ChebyshevBoundedField uses ""."" instead of ""\_"" as field separators in its afw table persistence. This is the old way of doing things, and unfortunately causes errors when reading in older versions of tables, becaus afw converts ""."" to ""_"" in that situation.    This shows up as a unit test failure in DM-2981 (brought over from HSC) when an older version table is read in.    It is an open question whether to fix this as part of DM-2981 (which conveniently has a test that shows the problem, though not intentionally so) or separately, in which case a new test is wanted. In the former case I'm happy to do the work so I can finish DM-2981.    Many thanks to Jim Bosch for diagnosing the problem.",1,DM-3214,datamanagement,chebyshevboundedfield use field separator persistence chebyshevboundedfield use instead field separator afw table persistence old way thing unfortunately cause error read old version table becaus afw convert situation show unit test failure dm-2981 bring hsc old version table read open question fix dm-2981 conveniently test show problem intentionally separately case new test want case happy work finish dm-2981 thank jim bosch diagnose problem,"ChebyshevBoundedField should use _ not . as field separators for persistence ChebyshevBoundedField uses ""."" instead of ""\_"" as field separators in its afw table persistence. This is the old way of doing things, and unfortunately causes errors when reading in older versions of tables, becaus afw converts ""."" to ""_"" in that situation. This shows up as a unit test failure in DM-2981 (brought over from HSC) when an older version table is read in. It is an open question whether to fix this as part of DM-2981 (which conveniently has a test that shows the problem, though not intentionally so) or separately, in which case a new test is wanted. In the former case I'm happy to do the work so I can finish DM-2981. Many thanks to Jim Bosch for diagnosing the problem."
"Extrapolate to the current document  Discussed use  cases in the context of the TOWG, and also with the site and telescope group.  A picture of the  structure of IT operations has (I believe consensus emerged) that is the  four layers ITIL cake    Service Design (cataloged, budget, availability, etc).  Service Transition (The work of inserting Charge into the system).  Service Delivery  (the work of running the as-is system of servinces)  ITC -- The work of providing the Facility, Hardware and networking.     I also obtained the ability to interact with EA. (but still working to master it and its concepts)",8,DM-3217,datamanagement,extrapolate current document discuss use case context towg site telescope group picture structure operation believe consensus emerge layer itil cake service design catalog budget availability etc service transition work insert charge system service delivery work run system servince itc work provide facility hardware networking obtain ability interact ea work master concept,"Extrapolate to the current document Discussed use cases in the context of the TOWG, and also with the site and telescope group. A picture of the structure of IT operations has (I believe consensus emerged) that is the four layers ITIL cake Service Design (cataloged, budget, availability, etc). Service Transition (The work of inserting Charge into the system). Service Delivery (the work of running the as-is system of servinces) ITC -- The work of providing the Facility, Hardware and networking. I also obtained the ability to interact with EA. (but still working to master it and its concepts)"
unable to create public images Errors are returned when attempting to upload an image marked as public.,1,DM-3218,datamanagement,unable create public image error return attempt upload image mark public,unable to create public images Errors are returned when attempting to upload an image marked as public.
"Improve czar-worker communication debugging Add features to make it easier to debug communication problems. Particularly, record the source of a message, and remove extraneous messages.",2,DM-3223,datamanagement,improve czar worker communication debug add feature easy debug communication problem particularly record source message remove extraneous message,"Improve czar-worker communication debugging Add features to make it easier to debug communication problems. Particularly, record the source of a message, and remove extraneous messages."
"openstack API endpoint is broken Similar to what was observed in DM-3226, the referral endspoint returned by   {code:java}  https://nebulous.ncsa.illinois.edu:5000  {code}  are not FQDNs.  This fundamentally breaks any attempt to use the API one step past authenticating with keystone.    This is an example HTTP response:    {code:java}  HTTP/1.1 200 OK  Date: Mon, 27 Jul 2015 23:11:02 GMT  Server: Apache/2.4.10 (Ubuntu)  Vary: X-Auth-Token  X-Distribution: Ubuntu  x-openstack-request-id: req-ac7bb613-86ef-43ab-a663-75c2ed3fb124  Content-Length: 1656  Content-Type: application/json    {""access"": {""token"": {""issued_at"": ""2015-07-27T23:11:02.342216"", ""expires"": ""2015-07-28T00:11:02Z"", ""id"": ""99b843d4baf94569a0d34ca4fecb470c"", ""tenant"": {""description"": null, ""enabled"": true, ""id"": ""d1f16653856540d386224fb057b5b00c"", ""name"": ""LSST""}, ""audit_ids"": [""fAP8851vTQi1n5pYmNoIjw""]}, ""serviceCatalog"": [{""endpoints"": [{""adminURL"": ""http://nebula:9292"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9292"", ""id"": ""49365a8e8fe743af9d517e84a98e3ee9"", ""publicURL"": ""http://nebula:9292""}], ""endpoints_links"": [], ""type"": ""image"", ""name"": ""glance""}, {""endpoints"": [{""adminURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""id"": ""c1e31df3656042ef9c5502efd7d574f2"", ""publicURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c""}], ""endpoints_links"": [], ""type"": ""compute"", ""name"": ""nova""}, {""endpoints"": [{""adminURL"": ""http://nebula:9696"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9696"", ""id"": ""266c9dc8e0344f8fa3f078652e868443"", ""publicURL"": ""http://nebula:9696""}], ""endpoints_links"": [], ""type"": ""network"", ""name"": ""neutron""}, {""endpoints"": [{""adminURL"": ""http://nebula:35357/v2.0"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:5000/v2.0"", ""id"": ""5d474008bcee4f44800546e3f3302404"", ""publicURL"": ""http://nebula:5000/v2.0""}], ""endpoints_links"": [], ""type"": ""identity"", ""name"": ""keystone""}], ""user"": {""username"": ""jhoblitt"", ""roles_links"": [], ""id"": ""6ea0c8e153b04ae29572c5fd877b6ac3"", ""roles"": [{""name"": ""user""}], ""name"": ""jhoblitt""}, ""metadata"": {""is_admin"": 0, ""roles"": [""142761bd922e453294e9b7086a227cbc""]}}}    {code}  ",1,DM-3227,datamanagement,openstack api endpoint break similar observe dm-3226 referral endspoint return code java https://nebulous.ncsa.illinois.edu:5000 code fqdn fundamentally break attempt use api step past authenticating keystone example http response code java http/1.1 200 ok date mon 27 jul 2015 23:11:02 gmt server apache/2.4.10 ubuntu vary auth token distribution ubuntu openstack request id req ac7bb613 86ef-43ab a663 75c2ed3fb124 content length 1656 content type application json access token issued_at 2015 07 27t23:11:02.342216 expire 2015 07 28t00:11:02z 99b843d4baf94569a0d34ca4fecb470c tenant description null enable true d1f16653856540d386224fb057b5b00c lsst audit_id fap8851vtqi1n5pymnoijw servicecatalog endpoint adminurl http://nebula:9292 region regionone internalurl http://nebula:9292 49365a8e8fe743af9d517e84a98e3ee9 publicurl http://nebula:9292 endpoints_links type image glance endpoint adminurl http://nebula:8774 v2 d1f16653856540d386224fb057b5b00c region regionone internalurl http://nebula:8774 v2 d1f16653856540d386224fb057b5b00c c1e31df3656042ef9c5502efd7d574f2 publicurl http://nebula:8774 v2 d1f16653856540d386224fb057b5b00c endpoints_links type compute nova endpoint adminurl http://nebula:9696 region regionone internalurl http://nebula:9696 266c9dc8e0344f8fa3f078652e868443 publicurl http://nebula:9696 endpoints_links type network neutron endpoint adminurl http://nebula:35357 v2.0 region regionone internalurl http://nebula:5000 v2.0 5d474008bcee4f44800546e3f3302404 publicurl http://nebula:5000 v2.0 endpoints_links type identity keystone user username jhoblitt roles_links 6ea0c8e153b04ae29572c5fd877b6ac3 role user jhoblitt metadata is_admin role 142761bd922e453294e9b7086a227cbc code,"openstack API endpoint is broken Similar to what was observed in DM-3226, the referral endspoint returned by {code:java} https://nebulous.ncsa.illinois.edu:5000 {code} are not FQDNs. This fundamentally breaks any attempt to use the API one step past authenticating with keystone. This is an example HTTP response: {code:java} HTTP/1.1 200 OK Date: Mon, 27 Jul 2015 23:11:02 GMT Server: Apache/2.4.10 (Ubuntu) Vary: X-Auth-Token X-Distribution: Ubuntu x-openstack-request-id: req-ac7bb613-86ef-43ab-a663-75c2ed3fb124 Content-Length: 1656 Content-Type: application/json {""access"": {""token"": {""issued_at"": ""2015-07-27T23:11:02.342216"", ""expires"": ""2015-07-28T00:11:02Z"", ""id"": ""99b843d4baf94569a0d34ca4fecb470c"", ""tenant"": {""description"": null, ""enabled"": true, ""id"": ""d1f16653856540d386224fb057b5b00c"", ""name"": ""LSST""}, ""audit_ids"": [""fAP8851vTQi1n5pYmNoIjw""]}, ""serviceCatalog"": [{""endpoints"": [{""adminURL"": ""http://nebula:9292"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9292"", ""id"": ""49365a8e8fe743af9d517e84a98e3ee9"", ""publicURL"": ""http://nebula:9292""}], ""endpoints_links"": [], ""type"": ""image"", ""name"": ""glance""}, {""endpoints"": [{""adminURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""id"": ""c1e31df3656042ef9c5502efd7d574f2"", ""publicURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c""}], ""endpoints_links"": [], ""type"": ""compute"", ""name"": ""nova""}, {""endpoints"": [{""adminURL"": ""http://nebula:9696"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9696"", ""id"": ""266c9dc8e0344f8fa3f078652e868443"", ""publicURL"": ""http://nebula:9696""}], ""endpoints_links"": [], ""type"": ""network"", ""name"": ""neutron""}, {""endpoints"": [{""adminURL"": ""http://nebula:35357/v2.0"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:5000/v2.0"", ""id"": ""5d474008bcee4f44800546e3f3302404"", ""publicURL"": ""http://nebula:5000/v2.0""}], ""endpoints_links"": [], ""type"": ""identity"", ""name"": ""keystone""}], ""user"": {""username"": ""jhoblitt"", ""roles_links"": [], ""id"": ""6ea0c8e153b04ae29572c5fd877b6ac3"", ""roles"": [{""name"": ""user""}], ""name"": ""jhoblitt""}, ""metadata"": {""is_admin"": 0, ""roles"": [""142761bd922e453294e9b7086a227cbc""]}}} {code}"
evaluate NCSA OpenStack against SQRE requirements and provide feedback - part 1 See also https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=LDMDG&title=NCSA+Nebula+OpenStack+Issues,2,DM-3228,datamanagement,evaluate ncsa openstack sqre requirement provide feedback https://confluence.lsstcorp.org/pages/viewpage.action?spacekey=ldmdg&title=ncsa+nebula+openstack+issue,evaluate NCSA OpenStack against SQRE requirements and provide feedback - part 1 See also https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=LDMDG&title=NCSA+Nebula+OpenStack+Issues
"Fix problems with no-result queries on multi-node setup For queries like:        select * from Object where id = <non existent id>    qserv can't map it to any chunk, and it ends up executing      SELECT *     FROM qservTest_case01_qserv.Object_1234567890 AS QST_1_     WHERE objectId=<non existent id>    the chunk 1234567890 is a special chunk and it exists on all nodes.    And that fails with:    (build/qdisp/QueryResource.cc:61) - Error provisioning, msg=Unable to  write  file; multiple files exist. code=2 ",1,DM-3237,datamanagement,fix problem result query multi node setup query like select object qserv map chunk end execute select qservtest_case01_qserv object_1234567890 qst_1 objectid= chunk 1234567890 special chunk exist node fail build qdisp queryresource.cc:61 error provisioning msg unable write file multiple file exist code=2,"Fix problems with no-result queries on multi-node setup For queries like: select * from Object where id =  qserv can't map it to any chunk, and it ends up executing SELECT * FROM qservTest_case01_qserv.Object_1234567890 AS QST_1_ WHERE objectId= the chunk 1234567890 is a special chunk and it exists on all nodes. And that fails with: (build/qdisp/QueryResource.cc:61) - Error provisioning, msg=Unable to write file; multiple files exist. code=2"
Include polygon bounds in CoaddPsf logic This is a port of [HSC-974|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-974]. Original description:    The {{CoaddPsf}} class should use the polygon bounding areas that were added to {{Exposure}} and {{ExposureRecord}} in DM-2981 (was: HSC-973) when determining which PSF images to coadd.,1,DM-3243,datamanagement,include polygon bound coaddpsf logic port hsc-974|https://hsc jira.astro.princeton.edu jira browse hsc-974 original description coaddpsf class use polygon bounding area add exposure exposurerecord dm-2981 hsc-973 determine psf image coadd,Include polygon bounds in CoaddPsf logic This is a port of [HSC-974|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-974]. Original description: The {{CoaddPsf}} class should use the polygon bounding areas that were added to {{Exposure}} and {{ExposureRecord}} in DM-2981 (was: HSC-973) when determining which PSF images to coadd.
Add support for passing query classification info from user to czar We need to be able to pass information from user about query type (sync/async). This will require tweaking the parser.  ,8,DM-3245,datamanagement,add support pass query classification info user czar need able pass information user query type sync async require tweak parser,Add support for passing query classification info from user to czar We need to be able to pass information from user about query type (sync/async). This will require tweaking the parser.
"Add support for configuring async queries Extend Qserv configuration to allow a DBA to specify (a) where results from async queries should be stored and (b) what rules to apply when purging old results.    Note that we need to think about the purging rules, it is not immediately obvious what would make most sense.",5,DM-3247,datamanagement,add support configure async query extend qserv configuration allow dba specify result async query store rule apply purge old result note need think purge rule immediately obvious sense,"Add support for configuring async queries Extend Qserv configuration to allow a DBA to specify (a) where results from async queries should be stored and (b) what rules to apply when purging old results. Note that we need to think about the purging rules, it is not immediately obvious what would make most sense."
"Revisit and document user-facing aspects of async queries Outline all aspects of async queries that are affecting users, discuss with the DM team, and document. This includes things like:   * managing async queries (checking status, terminating)   * retrieving results from async queries   * managing query results (purging policies etc)   * probably more, need to think about it...",8,DM-3249,datamanagement,revisit document user face aspect async query outline aspect async query affect user discuss dm team document include thing like manage async query check status terminate retrieve result async query manage query result purge policy etc probably need think,"Revisit and document user-facing aspects of async queries Outline all aspects of async queries that are affecting users, discuss with the DM team, and document. This includes things like: * managing async queries (checking status, terminating) * retrieving results from async queries * managing query results (purging policies etc) * probably more, need to think about it..."
Unify KVInterface python and c++ interfaces Swig the C++ mysql-based KvInterface implementation.   ,8,DM-3253,datamanagement,unify kvinterface python c++ interface swig c++ mysql base kvinterface implementation,Unify KVInterface python and c++ interfaces Swig the C++ mysql-based KvInterface implementation.
Fiber path Gate to pachon Discussion on fiber path,5,DM-3256,datamanagement,fiber path gate pachon discussion fiber path,Fiber path Gate to pachon Discussion on fiber path
"Port flux.scaled from HSC [HSC-1295|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1295] introduces {{flux.scaled}}, which measures the flux within a circular aperture that is set from the size of the PSF, scaled by some factor.  Stephen Gwyn recommends using this as our fiducial calibration flux.",2,DM-3257,datamanagement,port flux.scale hsc hsc-1295|https://hsc jira.astro.princeton.edu jira browse hsc-1295 introduce flux.scale measure flux circular aperture set size psf scale factor stephen gwyn recommend fiducial calibration flux,"Port flux.scaled from HSC [HSC-1295|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1295] introduces {{flux.scaled}}, which measures the flux within a circular aperture that is set from the size of the PSF, scaled by some factor. Stephen Gwyn recommends using this as our fiducial calibration flux."
CoaddPsf.getAveragePosition() is not a valid position This is a port of [HSC-1138|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1138] to LSST. That is an aggregate of two related minor fixes:    * {{CoaddInputRecorder}} should default to {{saveVisitGoodPix=True}} so that average positions in the {{CoaddPsf}} can be properly weighted;  * {{computeAveragePosition}} and {{doComputeKernelImage}} should be consistent about the data included when determining whether a source is off image.,1,DM-3258,datamanagement,coaddpsf.getaverageposition valid position port hsc-1138|https://hsc jira.astro.princeton.edu jira browse hsc-1138 lsst aggregate relate minor fix coaddinputrecorder default savevisitgoodpix true average position coaddpsf properly weight computeaverageposition docomputekernelimage consistent datum include determine source image,CoaddPsf.getAveragePosition() is not a valid position This is a port of [HSC-1138|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1138] to LSST. That is an aggregate of two related minor fixes: * {{CoaddInputRecorder}} should default to {{saveVisitGoodPix=True}} so that average positions in the {{CoaddPsf}} can be properly weighted; * {{computeAveragePosition}} and {{doComputeKernelImage}} should be consistent about the data included when determining whether a source is off image.
"Define polygon bounds for CCDs based on vignetted regions This is a port of [HSC-976|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-976] to LSST. The original issue description was:    We should set the polygon bounds (added in DM-2981 [was HSC-973]) for HSC CCD exposures to cover the non-vignetted regions. This should probably be done in ISR or some other camera-specific location.    Note that, contrary to the description in DM-2981, this functionality was not included there.",1,DM-3259,datamanagement,define polygon bound ccd base vignette region port hsc-976|https://hsc jira.astro.princeton.edu jira browse hsc-976 lsst original issue description set polygon bound add dm-2981 hsc-973 hsc ccd exposure cover non vignetted region probably isr camera specific location note contrary description dm-2981 functionality include,"Define polygon bounds for CCDs based on vignetted regions This is a port of [HSC-976|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-976] to LSST. The original issue description was: We should set the polygon bounds (added in DM-2981 [was HSC-973]) for HSC CCD exposures to cover the non-vignetted regions. This should probably be done in ISR or some other camera-specific location. Note that, contrary to the description in DM-2981, this functionality was not included there."
Explore how to run multi-node tests Not testing qserv code often enough in multi-node environment led to introducing many problems over the past two years since we last run large scale test. It should be simple for developer to run a multi-node test. This story covers work related to understanding how to run integration test on multi-node.,4,DM-3262,datamanagement,explore run multi node test test qserv code multi node environment lead introduce problem past year run large scale test simple developer run multi node test story cover work relate understand run integration test multi node,Explore how to run multi-node tests Not testing qserv code often enough in multi-node environment led to introducing many problems over the past two years since we last run large scale test. It should be simple for developer to run a multi-node test. This story covers work related to understanding how to run integration test on multi-node.
"Audit & cherry-pick HSC-1126 fixes [HSC-1126|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1126] contains a number of unrelated bug fixes. Given the nature of that ticket, it's not immediately clear which might already have been ported to LSST, which don't apply, and, of the others, what dependencies they have on code which might still be in the queue for merger.    We need to dig through that ticket and ensure that everything is properly merged.",2,DM-3264,datamanagement,audit cherry pick hsc-1126 fix hsc-1126|https://hsc jira.astro.princeton.edu jira browse hsc-1126 contain number unrelated bug fix give nature ticket immediately clear port lsst apply dependency code queue merger need dig ticket ensure properly merge,"Audit & cherry-pick HSC-1126 fixes [HSC-1126|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1126] contains a number of unrelated bug fixes. Given the nature of that ticket, it's not immediately clear which might already have been ported to LSST, which don't apply, and, of the others, what dependencies they have on code which might still be in the queue for merger. We need to dig through that ticket and ensure that everything is properly merged."
"add task to meas_astrom to fit an aribtrary WCS with a TAN-SIP WCS Sometime in the past, Russell Owen wrote a method to take an arbitrary WCS and approximate it as a TanWcs (our implementation of FITS' TAN-SIP WCS formalism).  That method is currently just a utility function in the unit test testFitTanSipWcsHighOrder.py.  This issue will promote that method to a full-fledge task in meas_astrom.",2,DM-3265,datamanagement,add task meas_astrom fit aribtrary wcs tan sip wcs past russell owen write method arbitrary wcs approximate tanwcs implementation fits tan sip wcs formalism method currently utility function unit test testfittansipwcshighorder.py issue promote method fledge task meas_astrom,"add task to meas_astrom to fit an aribtrary WCS with a TAN-SIP WCS Sometime in the past, Russell Owen wrote a method to take an arbitrary WCS and approximate it as a TanWcs (our implementation of FITS' TAN-SIP WCS formalism). That method is currently just a utility function in the unit test testFitTanSipWcsHighOrder.py. This issue will promote that method to a full-fledge task in meas_astrom."
"assertWcsNearlyEqualOverBBox and friends is too hard to use as a free function assertWcsNearlyEqualOverBBox and similar functions elsewhere in afw were written to be methods of lsst.utils.tests.TestCase, so their first argument is a testCase. This is fine for use in unit tests, but a hassle to use as free functions because the user must provide a testCase argument (though it need only be a trivial class with a fail(self, msgStr) method). Worse, that minimal requirement is not documented, so technically providing a simple mock test case is unsafe.    I have two proposals:  - Document the fact that testCase need only support fail(self, msgStr). This makes it clear how to safely use these functions as free functions.  - Allow testCase to be None, in which case RuntimeError is raised. That makes these functions even easier to use as free functions.  ",1,DM-3347,datamanagement,assertwcsnearlyequaloverbbox friend hard use free function assertwcsnearlyequaloverbbox similar function afw write method lsst.utils.test testcase argument testcase fine use unit test hassle use free function user provide testcase argument need trivial class fail(self msgstr method bad minimal requirement document technically provide simple mock test case unsafe proposal document fact testcase need support fail(self msgstr make clear safely use function free function allow testcase case runtimeerror raise make function easy use free function,"assertWcsNearlyEqualOverBBox and friends is too hard to use as a free function assertWcsNearlyEqualOverBBox and similar functions elsewhere in afw were written to be methods of lsst.utils.tests.TestCase, so their first argument is a testCase. This is fine for use in unit tests, but a hassle to use as free functions because the user must provide a testCase argument (though it need only be a trivial class with a fail(self, msgStr) method). Worse, that minimal requirement is not documented, so technically providing a simple mock test case is unsafe. I have two proposals: - Document the fact that testCase need only support fail(self, msgStr). This makes it clear how to safely use these functions as free functions. - Allow testCase to be None, in which case RuntimeError is raised. That makes these functions even easier to use as free functions."
"Include translation of aliases in measurement calibration Tasks that calibrate measurement outputs should include transferring (and translating, as needed) the aliases in the original measurement catalog.  This should include transferring slots, which may involve ""renaming"" the original slots, as those names refer explicitly to raw measurements (i.e. ""slot_PsfFlux"" might become ""slot_PsfMag"").",3,DM-3348,datamanagement,include translation alias measurement calibration tasks calibrate measurement output include transfer translate need alias original measurement catalog include transfer slot involve rename original slot name refer explicitly raw measurement i.e. slot_psfflux slot_psfmag,"Include translation of aliases in measurement calibration Tasks that calibrate measurement outputs should include transferring (and translating, as needed) the aliases in the original measurement catalog. This should include transferring slots, which may involve ""renaming"" the original slots, as those names refer explicitly to raw measurements (i.e. ""slot_PsfFlux"" might become ""slot_PsfMag"")."
Add test case for ExposureRecord::contains In DM-3243 we ported from HSC the ability to take account of the associated {{validPolygon}} when checking whether a point falls within an {{Exposure}}. This functionality was not accompanied by an adequate unit test.,2,DM-3349,datamanagement,add test case exposurerecord::contains dm-3243 port hsc ability account associate validpolygon check point fall exposure functionality accompany adequate unit test,Add test case for ExposureRecord::contains In DM-3243 we ported from HSC the ability to take account of the associated {{validPolygon}} when checking whether a point falls within an {{Exposure}}. This functionality was not accompanied by an adequate unit test.
"Reproducing errors of the current obs_decam package Learning the stack and development worlflow by reproducing errors in obs_decam as DM-3196.  Changes from the stack need to be incorporated into obs_decam package to keep the package up to date, hence the errors. As a learning process I reproduced the errors and used her fix (afw branch u/yusra/DM-3196) to move on.  ",2,DM-3351,datamanagement,reproduce error current obs_decam package learn stack development worlflow reproduce error obs_decam dm-3196 change stack need incorporate obs_decam package package date error learning process reproduce error fix afw branch yusra dm-3196,"Reproducing errors of the current obs_decam package Learning the stack and development worlflow by reproducing errors in obs_decam as DM-3196. Changes from the stack need to be incorporated into obs_decam package to keep the package up to date, hence the errors. As a learning process I reproduced the errors and used her fix (afw branch u/yusra/DM-3196) to move on."
"Study basic afw  Learn the basic operation of the aft package about handling images, tables, etc.  ",6,DM-3352,datamanagement,study basic afw learn basic operation aft package handle image table etc,"Study basic afw Learn the basic operation of the aft package about handling images, tables, etc."
Read in the FITS cube that Herschel project produced IRSA needs to be able to read in the FITS cube generated by Herschel project. We need to support and guide the effort so the code is generic enough for non-Herschel data. ,2,DM-3354,datamanagement,read fits cube herschel project produce irsa need able read fits cube generate herschel project need support guide effort code generic non herschel datum,Read in the FITS cube that Herschel project produced IRSA needs to be able to read in the FITS cube generated by Herschel project. We need to support and guide the effort so the code is generic enough for non-Herschel data.
Support the FITS cube reader RSA needs to be able to read in the FITS cube generated by Herschel project. We need to guide the effort so the code is generic enough for non-Herschel data.,1,DM-3355,datamanagement,support fits cube reader rsa need able read fits cube generate herschel project need guide effort code generic non herschel datum,Support the FITS cube reader RSA needs to be able to read in the FITS cube generated by Herschel project. We need to guide the effort so the code is generic enough for non-Herschel data.
Fix Firefly build script so it'll work with latest version of gradle Firefly build was failing when using gradle version 2.5.  Minor changes to the dependencies declaration fixed it.,1,DM-3356,datamanagement,fix firefly build script work late version gradle firefly build fail gradle version 2.5 minor change dependency declaration fix,Fix Firefly build script so it'll work with latest version of gradle Firefly build was failing when using gradle version 2.5. Minor changes to the dependencies declaration fixed it.
"Add mysql-based test to multi-node integration test At the moment multi-node integration test runs only on multi-node using Qserv, it does not run on plain mysql, and thus we can't validate results. The story involves tweaking qserv_testdata such that we can run mysql test on the czar, and compare results from mysql and qserv.",5,DM-3358,datamanagement,add mysql base test multi node integration test moment multi node integration test run multi node qserv run plain mysql validate result story involve tweak qserv_testdata run mysql test czar compare result mysql qserv,"Add mysql-based test to multi-node integration test At the moment multi-node integration test runs only on multi-node using Qserv, it does not run on plain mysql, and thus we can't validate results. The story involves tweaking qserv_testdata such that we can run mysql test on the czar, and compare results from mysql and qserv."
"Debug problem with joins in multi-node tests We seen to have problems with joins:  {quote}  SELECT o.deepSourceId, s.objectId, s.id, o.ra, o.decl FROM Object o, Source s WHERE o.deepSourceId=s.objectId;  {quote}    cluster seems to have hung. I can send new queries to the czar, and they show up in the czar's log, but they don't get answered (they can be cancelled).  Cancelling the join works(at least for the czar) but no further queries work.",4,DM-3363,datamanagement,debug problem join multi node test see problem join quote select o.deepsourceid s.objectid s.id o.ra o.decl object source o.deepsourceid s.objectid quote cluster hang send new query czar czar log answer cancel cancel join works(at czar query work,"Debug problem with joins in multi-node tests We seen to have problems with joins: {quote} SELECT o.deepSourceId, s.objectId, s.id, o.ra, o.decl FROM Object o, Source s WHERE o.deepSourceId=s.objectId; {quote} cluster seems to have hung. I can send new queries to the czar, and they show up in the czar's log, but they don't get answered (they can be cancelled). Cancelling the join works(at least for the czar) but no further queries work."
Produce Data Access & DB team S15 Release docs Complete these documents:  * https://confluence.lsstcorp.org/display/DM/Summer+2015+Qserv+Release  * https://confluence.lsstcorp.org/display/DM/Summer+2015+WebServ+Release,4,DM-3366,datamanagement,produce data access db team s15 release doc complete document https://confluence.lsstcorp.org/display/dm/summer+2015+qserv+release https://confluence.lsstcorp.org/display/dm/summer+2015+webserv+release,Produce Data Access & DB team S15 Release docs Complete these documents: * https://confluence.lsstcorp.org/display/DM/Summer+2015+Qserv+Release * https://confluence.lsstcorp.org/display/DM/Summer+2015+WebServ+Release
"Add multi-process python runner script for Galaxy Shear Experiments The current runner scripts are in tcsh and bash.  There is no good excuse for this, except that it was easy to implement.  Since we need both multi-threading and better parameter parsing, this will be replaced with a python script.",2,DM-3367,datamanagement,add multi process python runner script galaxy shear experiment current runner script tcsh bash good excuse easy implement need multi threading well parameter parsing replace python script,"Add multi-process python runner script for Galaxy Shear Experiments The current runner scripts are in tcsh and bash. There is no good excuse for this, except that it was easy to implement. Since we need both multi-threading and better parameter parsing, this will be replaced with a python script."
"Port HSC MPI driver for single-visit processing Transfer the {{reduceFrames.py}} script and the {{ProcessExposureTask}} it utilizes from hscPipe to a new package in the LSST stack (RFC-68 proposes calling this new package {{pool_tasks}}, but this isn't set in stone).    We should probably rename either the driver script or the task (or both), so they agree; the lack of consistency is a historical artifact on the HSC side, and I think it's time to change that.",6,DM-3368,datamanagement,port hsc mpi driver single visit processing transfer reduceframes.py script processexposuretask utilize hscpipe new package lsst stack rfc-68 propose call new package pool_tasks set stone probably rename driver script task agree lack consistency historical artifact hsc think time change,"Port HSC MPI driver for single-visit processing Transfer the {{reduceFrames.py}} script and the {{ProcessExposureTask}} it utilizes from hscPipe to a new package in the LSST stack (RFC-68 proposes calling this new package {{pool_tasks}}, but this isn't set in stone). We should probably rename either the driver script or the task (or both), so they agree; the lack of consistency is a historical artifact on the HSC side, and I think it's time to change that."
"Port HSC MPI driver for coaddition Port the HSC driver for coaddition, {{stack.py}} from hscPipe to a new LSST package (the same as DM-3368).    In the process, we should remove the inclusion of {{ProcessCoaddTask}}, and instead run detection and background subtraction only.    I think it might be time to consider renaming this task as well; I find it a little unfortunate we use ""coadd"" everywhere else but ""stack"" here.",4,DM-3369,datamanagement,port hsc mpi driver coaddition port hsc driver coaddition stack.py hscpipe new lsst package dm-3368 process remove inclusion processcoaddtask instead run detection background subtraction think time consider rename task find little unfortunate use coadd stack,"Port HSC MPI driver for coaddition Port the HSC driver for coaddition, {{stack.py}} from hscPipe to a new LSST package (the same as DM-3368). In the process, we should remove the inclusion of {{ProcessCoaddTask}}, and instead run detection and background subtraction only. I think it might be time to consider renaming this task as well; I find it a little unfortunate we use ""coadd"" everywhere else but ""stack"" here."
"Port HSC MPI driver for multi-band coadd processing Port the HSC MPI driver of multi-band coadd processing, multiBand.py, from hscPipe to a new LSST package (the same as in DM-3368).",4,DM-3370,datamanagement,port hsc mpi driver multi band coadd process port hsc mpi driver multi band coadd processing multiband.py hscpipe new lsst package dm-3368,"Port HSC MPI driver for multi-band coadd processing Port the HSC MPI driver of multi-band coadd processing, multiBand.py, from hscPipe to a new LSST package (the same as in DM-3368)."
"Port HSC --rerun option for CmdLineTask Port the HSC side's {{--rerun}} option for specifying processing inputs and outputs.    This work should be preceded by an RFC; we've proposed implementing this option on the LSST side in the past, and it was met with some resistance as it isn't strictly necessary.  We've since found it extremely convenience on the HSC side, and I think it's very much worth porting.",4,DM-3371,datamanagement,port hsc option cmdlinetask port hsc --rerun option specify processing input output work precede rfc propose implement option lsst past meet resistance strictly necessary find extremely convenience hsc think worth porting,"Port HSC --rerun option for CmdLineTask Port the HSC side's {{--rerun}} option for specifying processing inputs and outputs. This work should be preceded by an RFC; we've proposed implementing this option on the LSST side in the past, and it was met with some resistance as it isn't strictly necessary. We've since found it extremely convenience on the HSC side, and I think it's very much worth porting."
"Port, replace, or defer HSC-side provenance of EUPS products The HSC pipeline checks that setup EUPS products are identical between runs with the same output directory, in the same way configuration is checked in both the LSST and HSC pipelines.    The implementation is a bit messy, and it's not strictly necessary, so it's not clear we should port this over as-is, or just wait for a better implementation to be provided by the Process Middleware team.  We should at least RFC this question now.",6,DM-3372,datamanagement,port replace defer hsc provenance eups product hsc pipeline check setup eups product identical run output directory way configuration check lsst hsc pipeline implementation bit messy strictly necessary clear port wait well implementation provide process middleware team rfc question,"Port, replace, or defer HSC-side provenance of EUPS products The HSC pipeline checks that setup EUPS products are identical between runs with the same output directory, in the same way configuration is checked in both the LSST and HSC pipelines. The implementation is a bit messy, and it's not strictly necessary, so it's not clear we should port this over as-is, or just wait for a better implementation to be provided by the Process Middleware team. We should at least RFC this question now."
"Port HSC code for generation of calibration products Port HSC code for building calibration products (flats, bias frames, etc.).",6,DM-3373,datamanagement,port hsc code generation calibration product port hsc code build calibration product flat bias frame etc,"Port HSC code for generation of calibration products Port HSC code for building calibration products (flats, bias frames, etc.)."
add realistic Footprints to measurement code The current measurement code for the galaxy shear simulations uses the full postage stamp bounding box for the Footprint.  We need to use more realistic Footprints for some of the tests we want to run.  That probably involves running {{SourceDetectionTask}} and somehow combining that with the input-catalog based iterating already in the measurement code.,4,DM-3374,datamanagement,add realistic footprints measurement code current measurement code galaxy shear simulation use postage stamp bounding box footprint need use realistic footprints test want run probably involve run sourcedetectiontask combine input catalog base iterate measurement code,add realistic Footprints to measurement code The current measurement code for the galaxy shear simulations uses the full postage stamp bounding box for the Footprint. We need to use more realistic Footprints for some of the tests we want to run. That probably involves running {{SourceDetectionTask}} and somehow combining that with the input-catalog based iterating already in the measurement code.
"Test shear bias vs. CModel region.nGrowFootprint One piece of how the CModel code chooses its fit region size is via nGrowFootprint, which is used to grow the original detection Footprint.  We should test how changing this parameter affects the _m_ and _c_ shear biases between input and recovered shear.  They should decrease for larger nGrowFootprint values, and eventually plateau.  We want to find the point where this happens, and see how the parameter affects both the fit region area and the shear biases before this threshold.    It may be necessary to make a small modification to the CModel code to output the fit region area to complete this test.",4,DM-3375,datamanagement,test shear bias vs. cmodel region.ngrowfootprint piece cmodel code choose fit region size ngrowfootprint grow original detection footprint test change parameter affect shear bias input recover shear decrease large ngrowfootprint value eventually plateau want find point happen parameter affect fit region area shear bias threshold necessary small modification cmodel code output fit region area complete test,"Test shear bias vs. CModel region.nGrowFootprint One piece of how the CModel code chooses its fit region size is via nGrowFootprint, which is used to grow the original detection Footprint. We should test how changing this parameter affects the _m_ and _c_ shear biases between input and recovered shear. They should decrease for larger nGrowFootprint values, and eventually plateau. We want to find the point where this happens, and see how the parameter affects both the fit region area and the shear biases before this threshold. It may be necessary to make a small modification to the CModel code to output the fit region area to complete this test."
"Test shear bias vs. CModel region.nInitialRadii Like DM-3375, but testing the region.nInitialRadii parameter instead.  This parameter sets the fit region using a multiple of the half-light ellipse from an initial approximate fit.  The full fit region is formed as the union of this with the grown detection Footprint, so it may be necessary to set nGrowFootprint to a negative number to see any affect from this parameter.",4,DM-3376,datamanagement,test shear bias vs. cmodel region.ninitialradii like dm-3375 test region.ninitialradii parameter instead parameter set fit region multiple half light ellipse initial approximate fit fit region form union grown detection footprint necessary set ngrowfootprint negative number affect parameter,"Test shear bias vs. CModel region.nInitialRadii Like DM-3375, but testing the region.nInitialRadii parameter instead. This parameter sets the fit region using a multiple of the half-light ellipse from an initial approximate fit. The full fit region is formed as the union of this with the grown detection Footprint, so it may be necessary to set nGrowFootprint to a negative number to see any affect from this parameter."
"NSF Cyber Summit talk NSF Cyber Security Summit talk:  a case study of LSST cyber security.  Talk goes over challenges and successes with LSST's security program.  Talk is divided into four sections:  security plan, data security, user access, and security for the observation site.",7,DM-3378,datamanagement,nsf cyber summit talk nsf cyber security summit talk case study lsst cyber security talk go challenge success lsst security program talk divide section security plan datum security user access security observation site,"NSF Cyber Summit talk NSF Cyber Security Summit talk: a case study of LSST cyber security. Talk goes over challenges and successes with LSST's security program. Talk is divided into four sections: security plan, data security, user access, and security for the observation site."
Port HSC hooks for simulated source injection Port HSC hooks injecting simulated sources into real images to test processing.    This includes the code in {{fakes.py}} in pipe_tasks and its callers.  The pipeline does not include code for actually adding the fake sources; it just provides a callback interface that is implemented by third-party plugins such as https://github.com/clackner2007/fake-sources.,4,DM-3380,datamanagement,port hsc hook simulated source injection port hsc hook inject simulate source real image test processing include code fakes.py pipe_task caller pipeline include code actually add fake source provide callback interface implement party plugin https://github.com/clackner2007/fake-source,Port HSC hooks for simulated source injection Port HSC hooks injecting simulated sources into real images to test processing. This includes the code in {{fakes.py}} in pipe_tasks and its callers. The pipeline does not include code for actually adding the fake sources; it just provides a callback interface that is implemented by third-party plugins such as https://github.com/clackner2007/fake-sources.
"Add test cases for thresholding In DM-3136 changes were made to the way thresholds are handled in detection ([{{a4b011d}}|https://github.com/lsst/meas_algorithms/commit/a4b011dd0775908c925ad9f40f802f9ed8723ef9] and [{{74c2ed0}}|https://github.com/lsst/meas_algorithms/commit/74c2ed0b79afce4c94b0db5f1e168c28ba1aa15b]). These were not accompanied by test cases, but they should be.",2,DM-3381,datamanagement,add test case thresholde dm-3136 change way threshold handle detection a4b011d}}|https://github.com lsst meas_algorithms commit a4b011dd0775908c925ad9f40f802f9ed8723ef9 74c2ed0}}|https://github.com lsst meas_algorithms commit/74c2ed0b79afce4c94b0db5f1e168c28ba1aa15b accompany test case,"Add test cases for thresholding In DM-3136 changes were made to the way thresholds are handled in detection ([{{a4b011d}}|https://github.com/lsst/meas_algorithms/commit/a4b011dd0775908c925ad9f40f802f9ed8723ef9] and [{{74c2ed0}}|https://github.com/lsst/meas_algorithms/commit/74c2ed0b79afce4c94b0db5f1e168c28ba1aa15b]). These were not accompanied by test cases, but they should be."
security playbook Practical document for handling and responding to incidents.,1,DM-3382,datamanagement,security playbook practical document handle respond incident,security playbook Practical document for handling and responding to incidents.
Meeting with on HTCondor Attended meeting with Miron Livny.,1,DM-3383,datamanagement,meet htcondor attended meeting miron livny,Meeting with on HTCondor Attended meeting with Miron Livny.
"Port HSC improvements to HSM moments code The HSM shear estimation has received several improvements and important bugfixes on the HSC side that need to be ported to LSST.  This is complicated by the fact that much of the code has been entirely rewritten on the LSST side to work within the new measurement framework, but we've also synchronized this package with the HSC side much more frequently than with other packages.",4,DM-3384,datamanagement,port hsc improvement hsm moment code hsm shear estimation receive improvement important bugfixe hsc need port lsst complicate fact code entirely rewrite lsst work new measurement framework synchronize package hsc frequently package,"Port HSC improvements to HSM moments code The HSM shear estimation has received several improvements and important bugfixes on the HSC side that need to be ported to LSST. This is complicated by the fact that much of the code has been entirely rewritten on the LSST side to work within the new measurement framework, but we've also synchronized this package with the HSC side much more frequently than with other packages."
"Make use of good pixel count when building CoaddPsfs When building a CoaddPsf we have the ability to take account of the number of pixels contributed by the inputs (see http://ls.st/paj and DM-3258). However, the {{CoaddPsf}} constructor fails to use this information. It should copy this field when copying the provided {{ExposureCatalog}}, so that {{computeAveragePosition}} can use it.",1,DM-3387,datamanagement,use good pixel count build coaddpsfs build coaddpsf ability account number pixel contribute input http://ls.st/paj dm-3258 coaddpsf constructor fail use information copy field copy provide exposurecatalog computeaverageposition use,"Make use of good pixel count when building CoaddPsfs When building a CoaddPsf we have the ability to take account of the number of pixels contributed by the inputs (see http://ls.st/paj and DM-3258). However, the {{CoaddPsf}} constructor fails to use this information. It should copy this field when copying the provided {{ExposureCatalog}}, so that {{computeAveragePosition}} can use it."
"Re-generate data for large scale tests at in2p3 Sources were incorrectly duplicated, need to be redone",3,DM-3390,datamanagement,generate datum large scale test in2p3 source incorrectly duplicate need redo,"Re-generate data for large scale tests at in2p3 Sources were incorrectly duplicated, need to be redone"
"Refactor Zscale.java class  In early this year, the decision all data types would be converted to float in FitsRead.  Thus,the bitpixel is not relevant.  In Zscale, it still uses bitpixel to test the data type.  It should be refactored in the same manner as FitsRead etc. ",2,DM-3391,datamanagement,refactor zscale.java class early year decision datum type convert float fitsread bitpixel relevant zscale use bitpixel test data type refactore manner fitsread etc,"Refactor Zscale.java class In early this year, the decision all data types would be converted to float in FitsRead. Thus,the bitpixel is not relevant. In Zscale, it still uses bitpixel to test the data type. It should be refactored in the same manner as FitsRead etc."
"Fix precision related problem in UDFs SciSQL udfs seem to have a subtle precision problem. The following query that is not relying on scisql returns one row:    {quote}  select ra, decl, deepSourceId   FROM Object o   WHERE decl between 0.992 and 0.993 and ra between 19.171 and 19.172;  +------------------+-------------------+------------------+  | ra               | decl              | deepSourceId     |  +------------------+-------------------+------------------+  | 19.1719166801441 | 0.992087616433663 | 4368217963236477 |  +------------------+-------------------+------------------+  1 row in set (2 min 9.49 sec)  {quote}    But equivalent scisql-based query:    {quote}  select ra, decl, deepSourceId   FROM Object o   WHERE qserv_areaspec_box(0.992, 19.171, 0.993, 19.172);  {quote}    will fail to find that row.    If we relax the search criteria just a little bit, it finds some other row, but still not the one with decl = 0.992087616433663    {quote}  select ra, decl, deepSourceId FROM Object o WHERE qserv_areaspec_box(0.99, 19.171, 0.999, 19.172);  +-------------------+-----------------+------------------+  | ra                | decl            | deepSourceId     |  +-------------------+-----------------+------------------+  | 0.994098536926311 | 19.171425377618 | 4372684729224984 |  +-------------------+-----------------+------------------+  {quote}  ",4,DM-3392,datamanagement,fix precision relate problem udfs scisql udfs subtle precision problem follow query rely scisql return row quote select ra decl deepsourceid object decl 0.992 0.993 ra 19.171 19.172 ------------------+-------------------+------------------+ ra decl deepsourceid ------------------+-------------------+------------------+ 19.1719166801441 0.992087616433663 4368217963236477 ------------------+-------------------+------------------+ row set min 9.49 sec quote equivalent scisql base query quote select ra decl deepsourceid object qserv_areaspec_box(0.992 19.171 0.993 19.172 quote fail find row relax search criterion little bit find row decl 0.992087616433663 quote select ra decl deepsourceid object qserv_areaspec_box(0.99 19.171 0.999 19.172 -------------------+-----------------+------------------+ ra decl deepsourceid -------------------+-----------------+------------------+ 0.994098536926311 19.171425377618 4372684729224984 -------------------+-----------------+------------------+ quote,"Fix precision related problem in UDFs SciSQL udfs seem to have a subtle precision problem. The following query that is not relying on scisql returns one row: {quote} select ra, decl, deepSourceId FROM Object o WHERE decl between 0.992 and 0.993 and ra between 19.171 and 19.172; +------------------+-------------------+------------------+ | ra | decl | deepSourceId | +------------------+-------------------+------------------+ | 19.1719166801441 | 0.992087616433663 | 4368217963236477 | +------------------+-------------------+------------------+ 1 row in set (2 min 9.49 sec) {quote} But equivalent scisql-based query: {quote} select ra, decl, deepSourceId FROM Object o WHERE qserv_areaspec_box(0.992, 19.171, 0.993, 19.172); {quote} will fail to find that row. If we relax the search criteria just a little bit, it finds some other row, but still not the one with decl = 0.992087616433663 {quote} select ra, decl, deepSourceId FROM Object o WHERE qserv_areaspec_box(0.99, 19.171, 0.999, 19.172); +-------------------+-----------------+------------------+ | ra | decl | deepSourceId | +-------------------+-----------------+------------------+ | 0.994098536926311 | 19.171425377618 | 4372684729224984 | +-------------------+-----------------+------------------+ {quote}"
"Fix column names in query result The following shows the problem (See the column names in the results, they are not what user will expect). It happens for all aggregates: min, max, avg, count etc    {code}  select min(ra_PS), min(decl_PS), max(ra_PS), max(decl_PS), avg(ra_PS) from Object;  +----------------+---------------+---------------+---------------+-------------------------------+  | MIN(QS1_MIN)   | MIN(QS2_MIN)  | MAX(QS3_MAX)  | MAX(QS4_MAX)  | (SUM(QS6_SUM)/SUM(QS5_COUNT)) |  +-----------------+--------------+---------------+---------------+-------------------------------+  | 0.041714119635 | -6.1011707745 | 359.938579891 | 3.89870649736 |                 112.537203939 |  +----------------+---------------+---------------+---------------+-------------------------------+  {code}",4,DM-3393,datamanagement,fix column name query result following show problem column name result user expect happen aggregate min max avg count etc code select min(ra_ps min(decl_ps max(ra_ps max(decl_ps avg(ra_ps object ----------------+---------------+---------------+---------------+-------------------------------+ min(qs1_min min(qs2_min max(qs3_max max(qs4_max sum(qs6_sum)/sum(qs5_count -----------------+--------------+---------------+---------------+-------------------------------+ 0.041714119635 -6.1011707745 359.938579891 3.89870649736 112.537203939 ----------------+---------------+---------------+---------------+-------------------------------+ code,"Fix column names in query result The following shows the problem (See the column names in the results, they are not what user will expect). It happens for all aggregates: min, max, avg, count etc {code} select min(ra_PS), min(decl_PS), max(ra_PS), max(decl_PS), avg(ra_PS) from Object; +----------------+---------------+---------------+---------------+-------------------------------+ | MIN(QS1_MIN) | MIN(QS2_MIN) | MAX(QS3_MAX) | MAX(QS4_MAX) | (SUM(QS6_SUM)/SUM(QS5_COUNT)) | +-----------------+--------------+---------------+---------------+-------------------------------+ | 0.041714119635 | -6.1011707745 | 359.938579891 | 3.89870649736 | 112.537203939 | +----------------+---------------+---------------+---------------+-------------------------------+ {code}"
Discourse evaluation (Part 2) Work in support of evaluation Discourse as a DM platform for internal and external interactions.,7,DM-3396,datamanagement,discourse evaluation work support evaluation discourse dm platform internal external interaction,Discourse evaluation (Part 2) Work in support of evaluation Discourse as a DM platform for internal and external interactions.
Find and evaluate multi-user password wallet for SQuaRE Work to find and evaluate an off-the-shelf solution for sharing web services passwords between the SQuaRE group.,1,DM-3397,datamanagement,find evaluate multi user password wallet square work find evaluate shelf solution share web service password square group,Find and evaluate multi-user password wallet for SQuaRE Work to find and evaluate an off-the-shelf solution for sharing web services passwords between the SQuaRE group.
"Fix problem with default_engine Fix the problem:    {quote}  08/04/2015 05:39:47 werkzeug INFO: 141.142.237.30 - - [04/Aug/2015 17:39:47] ""GET /meta/v0/ HTTP/1.1"" 200 -  08/04/2015 05:39:49 __main__ ERROR: Exception on /meta/v0/db [GET]  Traceback (most recent call last):    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1817, in wsgi_app      response = self.full_dispatch_request()    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1477, in full_dispatch_request      rv = self.handle_user_exception(e)    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1381, in handle_user_exception      reraise(exc_type, exc_value, tb)    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1475, in full_dispatch_request      rv = self.dispatch_request()    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1461, in dispatch_request      return self.view_functions[rule.endpoint](**req.view_args)    File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 59, in getDb      return _resultsOf(text(query), scalar=True)    File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 122, in _resultsOf      engine = current_app.config[""default_engine""]  KeyError: 'default_engine'    {quote}",1,DM-3398,datamanagement,"fix problem default_engine fix problem quote 08/04/2015 05:39:47 werkzeug info 141.142.237.30 04 aug/2015 17:39:47 /meta v0/ http/1.1 200 08/04/2015 05:39:49 main error exception v0 db traceback recent file /home becla stack linux64 flask/0.10.1 lib python flask-0.10.1 py2.7.egg flask app.py line 1817 wsgi_app response self.full_dispatch_request file /home becla stack linux64 flask/0.10.1 lib python flask-0.10.1 py2.7.egg flask app.py line 1477 full_dispatch_request rv self.handle_user_exception(e file /home becla stack linux64 flask/0.10.1 lib python flask-0.10.1 py2.7.egg flask app.py line 1381 handle_user_exception reraise(exc_type exc_value tb file /home becla stack linux64 flask/0.10.1 lib python flask-0.10.1 py2.7.egg flask app.py line 1475 full_dispatch_requ rv self.dispatch_requ file /home becla stack linux64 flask/0.10.1 lib python flask-0.10.1 py2.7.egg flask app.py line 1461 dispatch_requ return self.view_functions[rule.endpoint](**req.view_args file /nfs home becla stack repos dax_metaserv python lsst dax metaserv metarest_v0.py line 59 getdb return resultsof(text(query scalar true file /nfs home becla stack repos dax_metaserv python lsst dax metaserv metarest_v0.py line 122 resultsof engine current_app.config[""default_engine keyerror default_engine quote","Fix problem with default_engine Fix the problem: {quote} 08/04/2015 05:39:47 werkzeug INFO: 141.142.237.30 - - [04/Aug/2015 17:39:47] ""GET /meta/v0/ HTTP/1.1"" 200 - 08/04/2015 05:39:49 __main__ ERROR: Exception on /meta/v0/db [GET] Traceback (most recent call last): File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1817, in wsgi_app response = self.full_dispatch_request() File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1477, in full_dispatch_request rv = self.handle_user_exception(e) File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1381, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1475, in full_dispatch_request rv = self.dispatch_request() File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1461, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 59, in getDb return _resultsOf(text(query), scalar=True) File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 122, in _resultsOf engine = current_app.config[""default_engine""] KeyError: 'default_engine' {quote}"
"Eliminate circular aliases in slot centroid definition [~smonkewitz] has discovered that our schema aliases for even the default configuration of measurement algorithms involve cycles, because the slot centroid algorithm contains a reference to its own flag.  Fixing this should just involve an extra check in {{SafeCentroidExtractor}}.",1,DM-3400,datamanagement,eliminate circular alias slot centroid definition ~smonkewitz discover schema alias default configuration measurement algorithm involve cycle slot centroid algorithm contain reference flag fix involve extra check safecentroidextractor,"Eliminate circular aliases in slot centroid definition [~smonkewitz] has discovered that our schema aliases for even the default configuration of measurement algorithms involve cycles, because the slot centroid algorithm contains a reference to its own flag. Fixing this should just involve an extra check in {{SafeCentroidExtractor}}."
"Explicitly disallow alias cycles in Schemas The current guard against cycles is lazy and incomplete, as it seemed unlikely we'd ever have them.  That's already been disproven (DM-3400), so it seems prudent to fix the guard code now.",6,DM-3401,datamanagement,explicitly disallow alia cycle schemas current guard cycle lazy incomplete unlikely disproven dm-3400 prudent fix guard code,"Explicitly disallow alias cycles in Schemas The current guard against cycles is lazy and incomplete, as it seemed unlikely we'd ever have them. That's already been disproven (DM-3400), so it seems prudent to fix the guard code now."
Port HSC updates to ingestImages.py ingestImages.py provides a camera-agnostic manner of creating a data repository (including a registry).  The HSC fork contains multiple improvements not present on the LSST side.  We need these in order to ingest the HSC data.,2,DM-3404,datamanagement,port hsc update ingestimages.py ingestimages.py provide camera agnostic manner create data repository include registry hsc fork contain multiple improvement present lsst need order ingest hsc datum,Port HSC updates to ingestImages.py ingestImages.py provides a camera-agnostic manner of creating a data repository (including a registry). The HSC fork contains multiple improvements not present on the LSST side. We need these in order to ingest the HSC data.
"Clarify status of LSE-77 Work on LSE-77 _per se_ appears not to have kept up with the status of the substance of the interface requirements, as represented in, for instance, LSE-239.  The action here is to see what change request actions may be appropriate for LSE-77 at this point.",1,DM-3414,datamanagement,clarify status lse-77 work lse-77 se appear keep status substance interface requirement represent instance lse-239 action change request action appropriate lse-77 point,"Clarify status of LSE-77 Work on LSE-77 _per se_ appears not to have kept up with the status of the substance of the interface requirements, as represented in, for instance, LSE-239. The action here is to see what change request actions may be appropriate for LSE-77 at this point."
"Fix overestimation of aperture correction error We're overestimating the aperture correction errors by including photon noise twice: both in the aperture correction errors and the original measurement errors.    This is a migration of [HSC-1277|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1277] to LSST, which is where it should be fixed.",8,DM-3417,datamanagement,fix overestimation aperture correction error overestimate aperture correction error include photon noise twice aperture correction error original measurement error migration hsc-1277|https://hsc jira.astro.princeton.edu jira browse hsc-1277 lsst fix,"Fix overestimation of aperture correction error We're overestimating the aperture correction errors by including photon noise twice: both in the aperture correction errors and the original measurement errors. This is a migration of [HSC-1277|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1277] to LSST, which is where it should be fixed."
Evaluate changes to LSE-209 and LSE-70 Action item from    https://confluence.lsstcorp.org/display/SYSENG/2015+July+08-10+CCS-DAQ-OCS-DM+Workshop+IV    Send initial feedback on LSE-70 and LSE-209,4,DM-3418,datamanagement,evaluate change lse-209 lse-70 action item https://confluence.lsstcorp.org/display/syseng/2015+july+08-10+ccs-daq-ocs-dm+workshop+iv send initial feedback lse-70 lse-209,Evaluate changes to LSE-209 and LSE-70 Action item from https://confluence.lsstcorp.org/display/SYSENG/2015+July+08-10+CCS-DAQ-OCS-DM+Workshop+IV Send initial feedback on LSE-70 and LSE-209
obs_decam unit test for reading data  The unit test wasn't working before and I edited the unit test of reading raw data. This got included with DM-3462.   This unit test needs testdata_decam to be setup.      The test fails with the stack b1597 at makeWcs (DM-3196).   The afw branch u/yusra/DM-3196 is a temporary fix before DM-3196 is resolved.      ,2,DM-3419,datamanagement,obs_decam unit test read datum unit test work edit unit test read raw datum get include dm-3462 unit test need testdata_decam setup test fail stack b1597 makewcs dm-3196 afw branch yusra dm-3196 temporary fix dm-3196 resolve,obs_decam unit test for reading data The unit test wasn't working before and I edited the unit test of reading raw data. This got included with DM-3462. This unit test needs testdata_decam to be setup. The test fails with the stack b1597 at makeWcs (DM-3196). The afw branch u/yusra/DM-3196 is a temporary fix before DM-3196 is resolved.
Create testdata_decam  Create a new testdata_decam repo with public instrument calibrated data.        The 435MB file can be downloaded from http://uofi.box.com/testdata-decam,2,DM-3421,datamanagement,create testdata_decam create new testdata_decam repo public instrument calibrate datum 435 mb file download http://uofi.box.com/testdata-decam,Create testdata_decam Create a new testdata_decam repo with public instrument calibrated data. The 435MB file can be downloaded from http://uofi.box.com/testdata-decam
"DLP/LDM-240 support chages - Part II Service feature requests and bugfixes from JK, T/CAMs including format changes on sqre-jirakit.",2,DM-3426,datamanagement,dlp ldm-240 support chage ii service feature request bugfixe jk cam include format change sqre jirakit,"DLP/LDM-240 support chages - Part II Service feature requests and bugfixes from JK, T/CAMs including format changes on sqre-jirakit."
Set up new desktop and install the stack - Set up software on the new iMac  - Installed the lsst stack with eups and lsstsw  - Noticed a compiler requirement DM-3405  ,2,DM-3428,datamanagement,set new desktop install stack set software new imac installed lsst stack eup lsstsw notice compiler requirement dm-3405,Set up new desktop and install the stack - Set up software on the new iMac - Installed the lsst stack with eups and lsstsw - Noticed a compiler requirement DM-3405
Learn the development workflow and obs_decam status update  - Learn git workflow and branching of the stack.    - Had a long chat with Yusra about obs_decam   - Reproduced obs_decam issues with different builds of the stack. There was a measurement failures that got solved with the newer builds of the stack. ,4,DM-3429,datamanagement,learn development workflow obs_decam status update learn git workflow branch stack long chat yusra obs_decam reproduce obs_decam issue different build stack measurement failure get solve new build stack,Learn the development workflow and obs_decam status update - Learn git workflow and branching of the stack. - Had a long chat with Yusra about obs_decam - Reproduced obs_decam issues with different builds of the stack. There was a measurement failures that got solved with the newer builds of the stack.
"Debug problem with timeout If I run 4 simultaneous large queries: 3 object scans and 1 source scan, Xrootd silently died on 2 machines. Below I pasted the tail of the log files      ccqserv108    0808 00:22:33.824 [0x7fb739583700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0808 00:22:33.824 [0x7fb74a00e700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (256, (more))  0808 00:22:33.826 [0x7fb74a00e700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (62, (last))  0808 00:22:33.826 [0x7fb74a00e700] INFO  root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream  0808 00:22:34.468 [0x7fb739583700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=8  0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1  0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader  0808 00:22:34.469 [0x7fb739583700] INFO  root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:34.469 [0x7fb739583700] INFO  root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0    -----    ccqserv124    0808 00:22:25.329 [0x7f25a67bf700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:234) - ChunkDisk registering for 2044 : SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_ p=0x7f257003d0b8  0808 00:22:25.329 [0x7f25a67bf700] INFO  Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=7 chunk=2044 db=LSST entry time=Fri Aug  7 23:52:06 2015   frag: q=SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_, sc= rt=r_7bff268d0e369dda8fa314132538a96ad_2044_0  0808 00:22:25.329 [0x7f25a67bf700] INFO  Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_b762c96f418726ae3457c74c0350d0c4  0808 00:22:25.329 [0x7f25a67bf700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0808 00:22:25.330 [0x7f25a50b6700] INFO  root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (44, (last))  0808 00:22:25.330 [0x7f25a50b6700] INFO  root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=81  0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1  0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48]  0808 00:22:26.218 [0x7f25a67bf700] INFO  root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0 ",4,DM-3432,datamanagement,debug problem timeout run simultaneous large query object scan source scan xrootd silently die machine paste tail log file ccqserv108 0808 00:22:33.824 0x7fb739583700 warn foreman build wdb queryaction.cc:109 queryaction overriding dbname lsst 0808 00:22:33.824 0x7fb74a00e700 info root build xrdsvc channelstream.cc:122 return buffer 256 0808 00:22:33.826 0x7fb74a00e700 info root build xrdsvc channelstream.cc:122 return buffer 62 0808 00:22:33.826 0x7fb74a00e700 info root build xrdsvc ssisession.cc:153 requestfinished type isstream 0808 00:22:34.468 0x7fb739583700 info root build wdb queryaction.cc:261 fillrows size=8 0808 00:22:34.469 0x7fb739583700 debug root build wdb queryaction.cc:288 transmit last=1 0808 00:22:34.469 0x7fb739583700 debug root build wdb queryaction.cc:307 transmitheader 0808 00:22:34.469 0x7fb739583700 info root build proto protoheaderwrap.cc:52 msgbuf size=256 0]=40 1]=13 2]=2 3]=0 4]=0 251]=48 252]=48 253]=48 254]=48 255]=48 0808 00:22:34.469 0x7fb739583700 info root build xrdsvc ssisession_replychannel.cc:85 sendstream check stream len=256 last=0 ccqserv124 0808 00:22:25.329 0x7f25a67bf700 debug scansched build wsched chunkdisk.cc:234 chunkdisk register 2044 select min(ra qs1_min max(ra qs2_max min(decl qs3_min max(decl qs4_max lsst.object_2044 qst_1 p=0x7f257003d0b8 0808 00:22:25.329 0x7f25a67bf700 info foreman build wcontrol foreman.cc:296 runner run task msg session=7 chunk=2044 db lsst entry time fri aug 23:52:06 2015 frag select min(ra qs1_min max(ra qs2_max min(decl qs3_min max(decl qs4_max lsst.object_2044 qst_1 sc= rt r_7bff268d0e369dda8fa314132538a96ad_2044_0 0808 00:22:25.329 0x7f25a67bf700 info foreman build wdb queryaction.cc:177 exec flight db q_b762c96f418726ae3457c74c0350d0c4 0808 00:22:25.329 0x7f25a67bf700 warn foreman build wdb queryaction.cc:109 queryaction overriding dbname lsst 0808 00:22:25.330 0x7f25a50b6700 info root build xrdsvc channelstream.cc:122 return buffer 44 0808 00:22:25.330 0x7f25a50b6700 info root build xrdsvc ssisession.cc:153 requestfinished type isstream 0808 00:22:26.218 0x7f25a67bf700 info root build wdb queryaction.cc:261 size=81 0808 00:22:26.218 0x7f25a67bf700 debug root build wdb queryaction.cc:288 transmit last=1 0808 00:22:26.218 0x7f25a67bf700 debug root build wdb queryaction.cc:307 transmitheader 0808 00:22:26.218 0x7f25a67bf700 info root build proto protoheaderwrap.cc:52 msgbuf size=256 0]=40 1]=13 2]=2 3]=0 4]=0 251]=48 252]=48 253]=48 254]=48 255]=48 0808 00:22:26.218 0x7f25a67bf700 info root build xrdsvc ssisession_replychannel.cc:85 sendstream check stream len=256 last=0,"Debug problem with timeout If I run 4 simultaneous large queries: 3 object scans and 1 source scan, Xrootd silently died on 2 machines. Below I pasted the tail of the log files ccqserv108 0808 00:22:33.824 [0x7fb739583700] WARN Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST 0808 00:22:33.824 [0x7fb74a00e700] INFO root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (256, (more)) 0808 00:22:33.826 [0x7fb74a00e700] INFO root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (62, (last)) 0808 00:22:33.826 [0x7fb74a00e700] INFO root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream 0808 00:22:34.468 [0x7fb739583700] INFO root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=8 0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1 0808 00:22:34.469 [0x7fb739583700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader 0808 00:22:34.469 [0x7fb739583700] INFO root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48] 0808 00:22:34.469 [0x7fb739583700] INFO root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0 ----- ccqserv124 0808 00:22:25.329 [0x7f25a67bf700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:234) - ChunkDisk registering for 2044 : SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_ p=0x7f257003d0b8 0808 00:22:25.329 [0x7f25a67bf700] INFO Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=7 chunk=2044 db=LSST entry time=Fri Aug 7 23:52:06 2015 frag: q=SELECT MIN(ra) AS QS1_MIN,MAX(ra) AS QS2_MAX,MIN(decl) AS QS3_MIN,MAX(decl) AS QS4_MAX FROM LSST.Object_2044 AS QST_1_, sc= rt=r_7bff268d0e369dda8fa314132538a96ad_2044_0 0808 00:22:25.329 [0x7f25a67bf700] INFO Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_b762c96f418726ae3457c74c0350d0c4 0808 00:22:25.329 [0x7f25a67bf700] WARN Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST 0808 00:22:25.330 [0x7f25a50b6700] INFO root (build/xrdsvc/ChannelStream.cc:122) - returning buffer (44, (last)) 0808 00:22:25.330 [0x7f25a50b6700] INFO root (build/xrdsvc/SsiSession.cc:153) - RequestFinished type=isStream 0808 00:22:26.218 [0x7f25a67bf700] INFO root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=81 0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:288) - _transmit last=1 0808 00:22:26.218 [0x7f25a67bf700] DEBUG root (build/wdb/QueryAction.cc:307) - _transmitHeader 0808 00:22:26.218 [0x7f25a67bf700] INFO root (build/proto/ProtoHeaderWrap.cc:52) - msgBuf size=256 -> [[0]=40, [1]=13, [2]=2, [3]=0, [4]=0, ..., [251]=48, [252]=48, [253]=48, [254]=48, [255]=48] 0808 00:22:26.218 [0x7f25a67bf700] INFO root (build/xrdsvc/SsiSession_ReplyChannel.cc:85) - sendStream, checking stream 0 len=256 last=0"
"Fix socket timeout problem in xrootd framework When we run a query that take long time, client times out, it closes the socket, which triggers cancellation on the server side.",4,DM-3433,datamanagement,fix socket timeout problem xrootd framework run query long time client time close socket trigger cancellation server,"Fix socket timeout problem in xrootd framework When we run a query that take long time, client times out, it closes the socket, which triggers cancellation on the server side."
"Qserv - webserv integration Setup Qserv and configure webserv to talk to Qserv. Verify all works, and fix discovered problems.",2,DM-3436,datamanagement,qserv webserv integration setup qserv configure webserv talk qserv verify work fix discover problem,"Qserv - webserv integration Setup Qserv and configure webserv to talk to Qserv. Verify all works, and fix discovered problems."
"Add column names metadata to db query results Per discussion at data access meeting Aug 10, it'd be good to send column names with the query results.",2,DM-3437,datamanagement,add column name metadata db query result discussion datum access meeting aug 10 good send column name query result,"Add column names metadata to db query results Per discussion at data access meeting Aug 10, it'd be good to send column names with the query results."
Revisit KPIs for Qserv Need to come up with KPIs for Qserv  ,2,DM-3438,datamanagement,revisit kpis qserv need come kpi qserv,Revisit KPIs for Qserv Need to come up with KPIs for Qserv
"Rename ingest.py to reduce confusion with database The {{ingestImages.py}} bin script provides a camera-agnostic manner of creating a data repository (including a registry).  The back-end code resides in pipe_tasks under the name {{ingest.py}}, and the {{IngestTask._DefaultName = ""ingest""}}, which means that configuration files in obs packages are also named {{ingest.py}}.  This choice of name was unfortunate, as it may be confused with ingest of sources into the database.  We should change the name to reduce this confusion, perhaps {{ingestImages.py}} like the bin script.",2,DM-3439,datamanagement,rename ingest.py reduce confusion database ingestimages.py bin script provide camera agnostic manner create data repository include registry end code reside pipe_task ingest.py ingesttask._defaultname ingest mean configuration file obs package name ingest.py choice unfortunate confuse ingest source database change reduce confusion ingestimages.py like bin script,"Rename ingest.py to reduce confusion with database The {{ingestImages.py}} bin script provides a camera-agnostic manner of creating a data repository (including a registry). The back-end code resides in pipe_tasks under the name {{ingest.py}}, and the {{IngestTask._DefaultName = ""ingest""}}, which means that configuration files in obs packages are also named {{ingest.py}}. This choice of name was unfortunate, as it may be confused with ingest of sources into the database. We should change the name to reduce this confusion, perhaps {{ingestImages.py}} like the bin script."
"add meas_extensions_photometryKron to lsstsw, lsst_distrib meas_extensions_photometryKron should be added to the CI system, since we are trying to keep it updated.    This is blocked by DM-2429 because that includes a fix for a unit test (which the CI system would have caught).",1,DM-3440,datamanagement,add meas_extensions_photometrykron lsstsw lsst_distrib meas_extensions_photometrykron add ci system try update block dm-2429 include fix unit test ci system catch,"add meas_extensions_photometryKron to lsstsw, lsst_distrib meas_extensions_photometryKron should be added to the CI system, since we are trying to keep it updated. This is blocked by DM-2429 because that includes a fix for a unit test (which the CI system would have caught)."
"Processing y-band HSC data fails in loading reference sources {code}  processCcd.py /lsst3/HSC/data/ --output /raid/price/test --id visit=904400 ccd=50  [...]  processCcd.calibrate.astrometry.solver.loadAN: Loading reference objects using center (1023.5, 2091) pix = Fk5Coord(319.8934727, -0.0006943, 2000.00) sky and radius 0.111920792477 deg  processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}: Could not find flux field(s) y_camFlux, y_flux  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processCcd.py"", line 85, in run      result = self.process(sensorRef, postIsrExposure)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processImage.py"", line 160, in process      calib = self.calibrate.run(inputExposure, idFactory=idFactory)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/calibrate.py"", line 457, in run      astromRet = self.astrometry.run(exposure, sources1)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 177, in run      results = self.astrometry(sourceCat=sourceCat, exposure=exposure, bbox=bbox)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 292, in astrometry      astrom = self.solver.determineWcs(sourceCat=sourceCat, exposure=exposure, bbox=bbox)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 409, in determineWcs      return self.determineWcs2(sourceCat=sourceCat, **margs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 437, in determineWcs2      astrom = self.useKnownWcs(sourceCat, wcs=wcs, **kw)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 308, in useKnownWcs      calib = None,    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 173, in loadPixelBox      loadRes = self.loadSkyCircle(ctrCoord, maxRadius, filterName)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/loadAstrometryNetObjects.py"", line 141, in loadSkyCircle      fluxField = getRefFluxField(schema=refCat.schema, filterName=filterName)    File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 40, in getRefFluxField      raise RuntimeError(""Could not find flux field(s) %s"" % ("", "".join(fluxFieldList)))  RuntimeError: Could not find flux field(s) y_camFlux, y_flux  {code}    We should be able to fix this by setting config parameters (e.g., {{calibrate.astrometry.solver.defaultFilter}} or {{calibrate.astrometry.solver.filterMap}}), but how do we keep that synched with the choice of reference catalog?  And once we get past astrometry, we also have the same problem in photocal.",2,DM-3442,datamanagement,"process band hsc datum fail loading reference source code processccd.py hsc data/ --output /raid price test --id visit=904400 ccd=50 ... processccd.calibrate.astrometry.solver.loadan loading reference object center 1023.5 2091 pix fk5coord(319.8934727 -0.0006943 2000.00 sky radius 0.111920792477 deg processccd fatal fail dataid={'taiobs 2013 11 03 point 672 visit 904400 dateobs 2013 11 03 filter hsc field stripe82l ccd 50 exptime 30.0 find flux field(s y_camflux traceback recent file /home lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 15 python lsst pipe base cmdlinetask.py line 320 result task.run(dataref kwargs file /home lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 15 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home lsstsw stack linux64 pipe_tasks/10.1 28 gf9582e4 python lsst pipe task processccd.py line 85 run result self.process(sensorref postisrexposure file /home lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 15 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home lsstsw stack linux64 pipe_tasks/10.1 28 gf9582e4 python lsst pipe task processimage.py line 160 process calib self.calibrate.run(inputexposure idfactory idfactory file /home lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 15 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home lsstsw stack linux64 pipe_tasks/10.1 28 gf9582e4 python lsst pipe task calibrate.py line 457 run astromret self.astrometry.run(exposure sources1 file /home lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 15 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home lsstsw stack linux64 meas_astrom/10.1 19 g6e01b25 python lsst meas astrom anetastrometry.py line 177 run result self.astrometry(sourcecat sourcecat exposure exposure bbox bbox file /home lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 15 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home lsstsw stack linux64 meas_astrom/10.1 19 g6e01b25 python lsst meas astrom anetastrometry.py line 292 astrometry astrom self.solver.determinewcs(sourcecat sourcecat exposure exposure bbox bbox file /home lsstsw stack linux64 meas_astrom/10.1 19 g6e01b25 python lsst meas astrom anetbasicastrometry.py line 409 determinewcs return self.determinewcs2(sourcecat sourcecat margs file /home lsstsw stack linux64 meas_astrom/10.1 19 g6e01b25 python lsst meas astrom anetbasicastrometry.py line 437 astrom self.useknownwcs(sourcecat wcs wcs kw file /home lsstsw stack linux64 meas_astrom/10.1 19 g6e01b25 python lsst meas astrom anetbasicastrometry.py line 308 useknownwcs calib file /home lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 15 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home lsstsw stack linux64 meas_algorithms/10.1 15 g0d3ecf6 python lsst meas algorithm loadreferenceobjects.py line 173 loadpixelbox loadres self.loadskycircle(ctrcoord maxradius filtername file /home lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 15 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home lsstsw stack linux64 meas_astrom/10.1 19 g6e01b25 python lsst meas astrom loadastrometrynetobjects.py line 141 loadskycircle fluxfield getreffluxfield(schema refcat.schema filtername filtername file /home lsstsw stack linux64 meas_algorithms/10.1 15 g0d3ecf6 python lsst meas algorithm loadreferenceobjects.py line 40 getreffluxfield raise runtimeerror(""could find flux field(s .join(fluxfieldlist runtimeerror find flux field(s y_camflux y_flux code able fix set config parameter e.g. calibrate.astrometry.solver.defaultfilter calibrate.astrometry.solver.filtermap synche choice reference catalog past astrometry problem photocal","Processing y-band HSC data fails in loading reference sources {code} processCcd.py /lsst3/HSC/data/ --output /raid/price/test --id visit=904400 ccd=50 [...] processCcd.calibrate.astrometry.solver.loadAN: Loading reference objects using center (1023.5, 2091) pix = Fk5Coord(319.8934727, -0.0006943, 2000.00) sky and radius 0.111920792477 deg processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}: Could not find flux field(s) y_camFlux, y_flux Traceback (most recent call last): File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__ result = task.run(dataRef, **kwargs) File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processCcd.py"", line 85, in run result = self.process(sensorRef, postIsrExposure) File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processImage.py"", line 160, in process calib = self.calibrate.run(inputExposure, idFactory=idFactory) File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/calibrate.py"", line 457, in run astromRet = self.astrometry.run(exposure, sources1) File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 177, in run results = self.astrometry(sourceCat=sourceCat, exposure=exposure, bbox=bbox) File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 292, in astrometry astrom = self.solver.determineWcs(sourceCat=sourceCat, exposure=exposure, bbox=bbox) File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 409, in determineWcs return self.determineWcs2(sourceCat=sourceCat, **margs) File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 437, in determineWcs2 astrom = self.useKnownWcs(sourceCat, wcs=wcs, **kw) File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 308, in useKnownWcs calib = None, File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 173, in loadPixelBox loadRes = self.loadSkyCircle(ctrCoord, maxRadius, filterName) File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/loadAstrometryNetObjects.py"", line 141, in loadSkyCircle fluxField = getRefFluxField(schema=refCat.schema, filterName=filterName) File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 40, in getRefFluxField raise RuntimeError(""Could not find flux field(s) %s"" % ("", "".join(fluxFieldList))) RuntimeError: Could not find flux field(s) y_camFlux, y_flux {code} We should be able to fix this by setting config parameters (e.g., {{calibrate.astrometry.solver.defaultFilter}} or {{calibrate.astrometry.solver.filterMap}}), but how do we keep that synched with the choice of reference catalog? And once we get past astrometry, we also have the same problem in photocal."
"Add support for parsing user log files In order to get timings for jobs executed, add support for non-dagman generated log files.  These are the user log files that HTCondor writes out for each individual job.",6,DM-3444,datamanagement,add support parse user log file order timing job execute add support non dagman generate log file user log file htcondor write individual job,"Add support for parsing user log files In order to get timings for jobs executed, add support for non-dagman generated log files. These are the user log files that HTCondor writes out for each individual job."
"Handle problems with connecting to mysql in czar We observed in S15 tests that if we run too many queries, czar is running out of connections to mysql and as a result through exception that is uncaught (and dies). We triggered this by starting 110 queries. To ""fix"" this problem we increased the max_connections in etc/my.cnf from 256 to 512. So, most likely the easiest way to reproduce it would be to set the max_connections to a very small number.     This story involves handling the ""uncaught exception"" gracefully.",3,DM-3449,datamanagement,handle problem connect mysql czar observe s15 test run query czar run connection mysql result exception uncaught die trigger start 110 query fix problem increase max_connection etc my.cnf 256 512 likely easy way reproduce set max_connection small number story involve handle uncaught exception gracefully,"Handle problems with connecting to mysql in czar We observed in S15 tests that if we run too many queries, czar is running out of connections to mysql and as a result through exception that is uncaught (and dies). We triggered this by starting 110 queries. To ""fix"" this problem we increased the max_connections in etc/my.cnf from 256 to 512. So, most likely the easiest way to reproduce it would be to set the max_connections to a very small number. This story involves handling the ""uncaught exception"" gracefully."
"Tweaks to configurations discovered during S15 tests Apply tweaks we found useful when running large scale tests. This includes:  # etc/my.cnf: change max_connections to 512  # add"":  {quote}export XRD_REQUESTTIMEOUT=64000  export XRD_STREAMTIMEOUT=64000  export XRD_DATASERVERTTL=64000  export XRD_TIMEOUTRESOLUTION=64000{quote}  to init.d/qserv-czar  # add :  {quote}ulimit -c unlimited{quote}  to all startup scripts in init.d. This will make sure core file is always dumped when we have problems.",1,DM-3450,datamanagement,tweak configuration discover s15 test apply tweak find useful run large scale test include etc my.cnf change max_connection 512 add quote}export xrd_requesttimeout=64000 export xrd_streamtimeout=64000 export xrd_dataserverttl=64000 export xrd_timeoutresolution=64000{quote init.d qserv czar add quote}ulimit unlimited{quote startup script init.d sure core file dump problem,"Tweaks to configurations discovered during S15 tests Apply tweaks we found useful when running large scale tests. This includes: # etc/my.cnf: change max_connections to 512 # add"": {quote}export XRD_REQUESTTIMEOUT=64000 export XRD_STREAMTIMEOUT=64000 export XRD_DATASERVERTTL=64000 export XRD_TIMEOUTRESOLUTION=64000{quote} to init.d/qserv-czar # add : {quote}ulimit -c unlimited{quote} to all startup scripts in init.d. This will make sure core file is always dumped when we have problems."
"Resolve problem with running many simultaneous queries When we run with 110 simultaneous queries, czar fails with ""uncaught exception""",2,DM-3451,datamanagement,resolve problem run simultaneous query run 110 simultaneous query czar fail uncaught exception,"Resolve problem with running many simultaneous queries When we run with 110 simultaneous queries, czar fails with ""uncaught exception"""
"Integrate pipelines with MySQL and Qserv Load data produced by pipelines into MySQL (on lsst10), and Qserv",5,DM-3452,datamanagement,integrate pipeline mysql qserv load datum produce pipeline mysql lsst10 qserv,"Integrate pipelines with MySQL and Qserv Load data produced by pipelines into MySQL (on lsst10), and Qserv"
"AstrometryTask.run return not consistent with ANetAstrometryTask ANetAstrometryTask.run returns matchMetadata but AstrometryTask.run returns matchMeta. The two must agree. It turns out that matchMeta is more widely used, so I'll standardize on that.",1,DM-3453,datamanagement,astrometrytask.run return consistent anetastrometrytask anetastrometrytask.run return matchmetadata astrometrytask.run return matchmeta agree turn matchmeta widely standardize,"AstrometryTask.run return not consistent with ANetAstrometryTask ANetAstrometryTask.run returns matchMetadata but AstrometryTask.run returns matchMeta. The two must agree. It turns out that matchMeta is more widely used, so I'll standardize on that."
"ProcessImageTask.matchSources fails if using ANetAstrometryTask ProcessImageTask.matchSources fails when using ANetAstrometryTask with the following error:  {code}  processCcd.calibrate.astrometry: Applying distortion correction  processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}:     File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem<T> lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string<char>]      Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0}  lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.'    Traceback (most recent call last):    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processCcd.py"", line 85, in run      result = self.process(sensorRef, postIsrExposure)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 219, in process      srcMatches, srcMatchMeta = self.matchSources(calExposure, sources)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 250, in matchSources      astromRet = astrometry.loadAndMatch(exposure=exposure, sourceCat=sources)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 321, in loadAndMatch      with self.distortionContext(sourceCat=sourceCat, exposure=exposure) as bbox:    File ""/ssd/rowen/lsstsw/anaconda/lib/python2.7/contextlib.py"", line 17, in __enter__      return self.gen.next()    File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 295, in distortionContext      sourceCat.table.defineCentroid(self.distortedName)    File ""/ssd/rowen/lsstsw/stack/Linux64/afw/10.1-37-gaedf466/python/lsst/afw/table/tableLib.py"", line 8887, in defineCentroid      return _tableLib.SourceTable_defineCentroid(self, *args)  NotFoundError:     File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem<T> lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string<char>]      Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0}  lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.'  {code}  This is probably a result of DM-2939. The basic problem is that the distortion context in ANetAstrometryTask should not be run at that point in processing. [~price] suggests that a simple clean fix is to make the distortion context a no-op if the WCS already contains distortion, if that works. This is what I will try first.",1,DM-3455,datamanagement,processimagetask.matchsource fail anetastrometrytask processimagetask.matchsource fail anetastrometrytask follow error code processccd.calibrate.astrometry apply distortion correction processccd fatal fail dataid={'taiobs 2013 11 03 point 672 visit 904400 dateobs 2013 11 03 filter hsc field stripe82l ccd 50 exptime 30.0 file src table schema.cc line 239 lsst::afw::table::schemaitem lsst::afw::table::detail::schemaimpl::find(const string const double std::stre std::basic_stre field subfield withname astrom_distorted_x find type lsst::pex::exceptions::notfounderror field subfield withname astrom_distorted_x find type traceback recent file /ssd rowen lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 python lsst pipe base cmdlinetask.py line 320 result task.run(dataref kwargs file /ssd rowen lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /ssd rowen lsstsw stack linux64 pipe_task ticket dm-3453 g086c9ddd0a python lsst pipe task processccd.py line 85 run result self.process(sensorref postisrexposure file /ssd rowen lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /ssd rowen lsstsw stack linux64 pipe_task ticket dm-3453 g086c9ddd0a python lsst pipe task processimage.py line 219 process srcmatches srcmatchmeta self.matchsources(calexposure source file /ssd rowen lsstsw stack linux64 pipe_task ticket dm-3453 g086c9ddd0a python lsst pipe task processimage.py line 250 matchsource astromret astrometry.loadandmatch(exposure exposure sourcecat source file /ssd rowen lsstsw stack linux64 pipe_base/10.1 g6ba0cc7 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /ssd rowen lsstsw stack linux64 meas_astrom ticket dm-3453 gbb2ad1f49c python lsst meas astrom anetastrometry.py line 321 loadandmatch self.distortioncontext(sourcecat sourcecat exposure exposure bbox file /ssd rowen lsstsw anaconda lib python2.7 contextlib.py line 17 enter return self.gen.next file /ssd rowen lsstsw stack linux64 meas_astrom ticket dm-3453 gbb2ad1f49c python lsst meas astrom anetastrometry.py line 295 distortioncontext sourcecat.table.definecentroid(self.distortedname file /ssd rowen lsstsw stack linux64 afw/10.1 37 gaedf466 python lsst afw table tablelib.py line 8887 definecentroid return sourcetable_definecentroid(self args notfounderror file src table schema.cc line 239 lsst::afw::table::schemaitem lsst::afw::table::detail::schemaimpl::find(const string const double std::stre std::basic_stre field subfield withname astrom_distorted_x find type lsst::pex::exceptions::notfounderror field subfield withname astrom_distorted_x find type code probably result dm-2939 basic problem distortion context anetastrometrytask run point processing ~price suggest simple clean fix distortion context op wcs contain distortion work try,"ProcessImageTask.matchSources fails if using ANetAstrometryTask ProcessImageTask.matchSources fails when using ANetAstrometryTask with the following error: {code} processCcd.calibrate.astrometry: Applying distortion correction processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}: File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string] Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0} lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.' Traceback (most recent call last): File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__ result = task.run(dataRef, **kwargs) File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processCcd.py"", line 85, in run result = self.process(sensorRef, postIsrExposure) File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 219, in process srcMatches, srcMatchMeta = self.matchSources(calExposure, sources) File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 250, in matchSources astromRet = astrometry.loadAndMatch(exposure=exposure, sourceCat=sources) File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 321, in loadAndMatch with self.distortionContext(sourceCat=sourceCat, exposure=exposure) as bbox: File ""/ssd/rowen/lsstsw/anaconda/lib/python2.7/contextlib.py"", line 17, in __enter__ return self.gen.next() File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 295, in distortionContext sourceCat.table.defineCentroid(self.distortedName) File ""/ssd/rowen/lsstsw/stack/Linux64/afw/10.1-37-gaedf466/python/lsst/afw/table/tableLib.py"", line 8887, in defineCentroid return _tableLib.SourceTable_defineCentroid(self, *args) NotFoundError: File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string] Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0} lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.' {code} This is probably a result of DM-2939. The basic problem is that the distortion context in ANetAstrometryTask should not be run at that point in processing. [~price] suggests that a simple clean fix is to make the distortion context a no-op if the WCS already contains distortion, if that works. This is what I will try first."
"Fix problems with talking from webserv to qserv Flask or sqlalchemy which are part of webserv are producing some extra queries that are confusing qserv. So basically, at the moment even the simplest query run via webserv that is directed to qserv fails.",2,DM-3456,datamanagement,fix problem talk webserv qserv flask sqlalchemy webserv produce extra query confuse qserv basically moment simple query run webserv direct qserv fail,"Fix problems with talking from webserv to qserv Flask or sqlalchemy which are part of webserv are producing some extra queries that are confusing qserv. So basically, at the moment even the simplest query run via webserv that is directed to qserv fails."
Implement prototype stack documentation with Sphinx Implement a minimally-viable Sphinx documentation repository for the LSST stack. The code is available at https://github.com/lsst-sqre/lsst_stack_docs,5,DM-3457,datamanagement,implement prototype stack documentation sphinx implement minimally viable sphinx documentation repository lsst stack code available https://github.com/lsst-sqre/lsst_stack_doc,Implement prototype stack documentation with Sphinx Implement a minimally-viable Sphinx documentation repository for the LSST stack. The code is available at https://github.com/lsst-sqre/lsst_stack_docs
Research existing sphinx doc implementations Examine how python packages such as astropy structure and implement their sphinx docs.,2,DM-3458,datamanagement,research exist sphinx doc implementation examine python package astropy structure implement sphinx doc,Research existing sphinx doc implementations Examine how python packages such as astropy structure and implement their sphinx docs.
"make forced and SFM interfaces more consistent From [~rowen]:  {quote}  SimpleMeasurementTask.run and ForcedMeasurementTask.run now both take a source catalog, but the two use the opposite order for the first two arguments (one has the catalog first, the other has the exposure first)  {quote}",1,DM-3459,datamanagement,force sfm interface consistent ~rowen quote simplemeasurementtask.run forcedmeasurementtask.run source catalog use opposite order argument catalog exposure quote,"make forced and SFM interfaces more consistent From [~rowen]: {quote} SimpleMeasurementTask.run and ForcedMeasurementTask.run now both take a source catalog, but the two use the opposite order for the first two arguments (one has the catalog first, the other has the exposure first) {quote}"
"applyApCorr mis-handles missing data In ApplyApCorrTask.run the following lines do not behave as expected because get returns None if the data is missing, rather than raising an exception:  {code}              try:                  apCorrModel = apCorrMap.get(apCorrInfo.fluxName)                  apCorrSigmaModel = apCorrMap.get(apCorrInfo.fluxSigmaName)              except Exception:  {code}",1,DM-3460,datamanagement,applyapcorr mis handle miss datum applyapcorrtask.run follow line behave expect return data miss raise exception code try apcorrmodel apcorrmap.get(apcorrinfo.fluxname apcorrsigmamodel apcorrmap.get(apcorrinfo.fluxsigmaname exception code,"applyApCorr mis-handles missing data In ApplyApCorrTask.run the following lines do not behave as expected because get returns None if the data is missing, rather than raising an exception: {code} try: apCorrModel = apCorrMap.get(apCorrInfo.fluxName) apCorrSigmaModel = apCorrMap.get(apCorrInfo.fluxSigmaName) except Exception: {code}"
"psfex lapack symbols may collide with built in lapack On my Mac meas_extensions_psfex fails to build due to the numpy config test failing. ""import numpy"" fails with:  {code}  dlopen(/Users/rowen/LSST/lsstsw/anaconda/lib/python2.7/site-packages/numpy/linalg/lapack_lite.so, 2): can't resolve symbol __NSConcreteStackBlock in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib because dependent dylib #1 could not be loaded in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib  {code}  Our best guess (see discussion in Data Management 2015-08-14 at approx. 1:57 PM Pacific time) is that the special lapack functions in psfex are colliding with the lapack that anaconda uses.    In case it helps I see this on OS X 10.9.5. I do not see it on lsst-dev.",2,DM-3463,datamanagement,psfex lapack symbol collide build lapack mac meas_extensions_psfex fail build numpy config test fail import numpy fail code dlopen(/user rowen lsst lsstsw anaconda lib python2.7 site package numpy linalg lapack_lite.so resolve symbol /system library frameworks accelerate.framework versions frameworks veclib.framework versions libvmisc.dylib dependent dylib load /system library frameworks accelerate.framework versions frameworks veclib.framework versions libvmisc.dylib code good guess discussion data management 2015 08 14 approx 1:57 pm pacific time special lapack function psfex collide lapack anaconda use case help os 10.9.5 lsst dev,"psfex lapack symbols may collide with built in lapack On my Mac meas_extensions_psfex fails to build due to the numpy config test failing. ""import numpy"" fails with: {code} dlopen(/Users/rowen/LSST/lsstsw/anaconda/lib/python2.7/site-packages/numpy/linalg/lapack_lite.so, 2): can't resolve symbol __NSConcreteStackBlock in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib because dependent dylib #1 could not be loaded in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib {code} Our best guess (see discussion in Data Management 2015-08-14 at approx. 1:57 PM Pacific time) is that the special lapack functions in psfex are colliding with the lapack that anaconda uses. In case it helps I see this on OS X 10.9.5. I do not see it on lsst-dev."
"drawing text to ds9 fails if size or the font family is set Commands like  {code}  ds9.dot('xxxx', 100, 100, size=3)  ds9.dot('xxxx', 100, 120, fontFamily=""times"")  {code}  silently fail.  The problem is that commands like  {code}  xpaset -p ds9 regions command '{text 100 100 # text=xxxx color=red font=""times 12""}'  {code}  fail; you need to say {{font=times 12 normal}}",1,DM-3468,datamanagement,"draw text ds9 fail size font family set command like code ds9.dot('xxxx 100 100 size=3 ds9.dot('xxxx 100 120 fontfamily=""times code silently fail problem command like code xpaset ds9 region command text 100 100 text xxxx color red font=""time 12 code fail need font time 12 normal","drawing text to ds9 fails if size or the font family is set Commands like {code} ds9.dot('xxxx', 100, 100, size=3) ds9.dot('xxxx', 100, 120, fontFamily=""times"") {code} silently fail. The problem is that commands like {code} xpaset -p ds9 regions command '{text 100 100 # text=xxxx color=red font=""times 12""}' {code} fail; you need to say {{font=times 12 normal}}"
"Install/deploy SUI web application at NCSA For summer 15 release,  we will deploya SUI web app on NCSA accessible to DM team.    - work with NCSA to have a server setup  - install necessary software packages  - install SUI software  - deploy the system and test ",5,DM-3470,datamanagement,install deploy sui web application ncsa summer 15 release deploya sui web app ncsa accessible dm team work ncsa server setup install necessary software package install sui software deploy system test,"Install/deploy SUI web application at NCSA For summer 15 release, we will deploya SUI web app on NCSA accessible to DM team. - work with NCSA to have a server setup - install necessary software packages - install SUI software - deploy the system and test"
"Debug problems with near neighbor queries The following near neighbor query:    {quote}  select o1.ra as ra1, o2.ra as ra2, o1.decl as decl1, o2.decl as decl2,   scisql_angSep(o1.ra, o1.decl,o2.ra, o2.decl) AS theDistance   from Object o1, Object o2   where qserv_areaspec_box(90.299197, -66.468216, 98.762526, -56.412851)   and scisql_angSep(o1.ra, o1.decl, o2.ra, o2.decl) < 0.015  {quote}    fails on the IN2P3 cluster.",4,DM-3474,datamanagement,debug problem near neighbor query follow near neighbor query quote select o1.ra ra1 o2.ra ra2 o1.decl decl1 o2.decl decl2 scisql_angsep(o1.ra o1.decl o2.ra o2.decl thedistance object o1 object o2 qserv_areaspec_box(90.299197 -66.468216 98.762526 scisql_angsep(o1.ra o1.decl o2.ra o2.decl 0.015 quote fail in2p3 cluster,"Debug problems with near neighbor queries The following near neighbor query: {quote} select o1.ra as ra1, o2.ra as ra2, o1.decl as decl1, o2.decl as decl2, scisql_angSep(o1.ra, o1.decl,o2.ra, o2.decl) AS theDistance from Object o1, Object o2 where qserv_areaspec_box(90.299197, -66.468216, 98.762526, -56.412851) and scisql_angSep(o1.ra, o1.decl, o2.ra, o2.decl) < 0.015 {quote} fails on the IN2P3 cluster."
"Design SQL API for getting query type When unsure, user should be able to check what type of query a given query is (for example, is the query ""select * from Object where qserv_area_spec(1, 2, 10, 5)"" considered interactive or async? This story involves deciding how the SQL API will look like.",2,DM-3477,datamanagement,design sql api get query type unsure user able check type query give query example query select object qserv_area_spec(1 10 consider interactive async story involve decide sql api look like,"Design SQL API for getting query type When unsure, user should be able to check what type of query a given query is (for example, is the query ""select * from Object where qserv_area_spec(1, 2, 10, 5)"" considered interactive or async? This story involves deciding how the SQL API will look like."
"Design API for passing query type User should be able to pass hint with a query indicating what query type it is. Based on that the return result will either be the query result, or queryId. This story involves designing the API  (sql and RESTful).",3,DM-3478,datamanagement,design api pass query type user able pass hint query indicate query type base return result query result queryid story involve design api sql restful,"Design API for passing query type User should be able to pass hint with a query indicating what query type it is. Based on that the return result will either be the query result, or queryId. This story involves designing the API (sql and RESTful)."
Design RESTful APIs for async queries in WebServ Need RESTul API for:   * retrieving partial results while query is running   * killing async queries,2,DM-3479,datamanagement,design restful api async query webserv need restul api retrieve partial result query run kill async query,Design RESTful APIs for async queries in WebServ Need RESTul API for: * retrieving partial results while query is running * killing async queries
"Design SQL APIs for async queries Need SQL API for:   * submitting async query, note that we should be able to specify where the results are going / what is the format of the results   * retrieving status of async query   * retrieving results of async query   * retrieving partial results of async query while it is running  ",5,DM-3480,datamanagement,design sql api async query need sql api submit async query note able specify result go format result retrieve status async query retrieve result async query retrieve partial result async query run,"Design SQL APIs for async queries Need SQL API for: * submitting async query, note that we should be able to specify where the results are going / what is the format of the results * retrieving status of async query * retrieving results of async query * retrieving partial results of async query while it is running"
adapt sandbox-jenkins-demo to changes in jfryman/nginx 0.2.7 Changes in the way  jfryman/nginx 0.2.7 handles tls cert files since 0.2.6 have run awful of selinux permissions issues.,2,DM-3481,datamanagement,adapt sandbox jenkin demo change jfryman nginx 0.2.7 change way jfryman nginx 0.2.7 handle tls cert file 0.2.6 run awful selinux permission issue,adapt sandbox-jenkins-demo to changes in jfryman/nginx 0.2.7 Changes in the way jfryman/nginx 0.2.7 handles tls cert files since 0.2.6 have run awful of selinux permissions issues.
Attending Cyber Security Summit Attending NSF Cyber Security Summit in my capacity as LSST ISO.,3,DM-3482,datamanagement,attend cyber security summit attending nsf cyber security summit capacity lsst iso,Attending Cyber Security Summit Attending NSF Cyber Security Summit in my capacity as LSST ISO.
"Calibration transformation should not fail on negative flux Before database ingest, measured source fluxes are converted to magnitudes as per DM-2305. The default behaviour of {{afw::image::Calib}} is to throw when a negative flux is encountered, which derails the whole transformation procedure. Better is to return a NaN.",1,DM-3483,datamanagement,calibration transformation fail negative flux database ingest measure source flux convert magnitude dm-2305 default behaviour afw::image::calib throw negative flux encounter derail transformation procedure well return nan.,"Calibration transformation should not fail on negative flux Before database ingest, measured source fluxes are converted to magnitudes as per DM-2305. The default behaviour of {{afw::image::Calib}} is to throw when a negative flux is encountered, which derails the whole transformation procedure. Better is to return a NaN."
"Design RESTful API for getting query type When unsure, user should be able to check what type of query a given query is (for example, is the query ""select * from Object where qserv_area_spec(1, 2, 10, 5)"" considered interactive or async? This story involves deciding how the RESTful API will look like.",2,DM-3484,datamanagement,design restful api get query type unsure user able check type query give query example query select object qserv_area_spec(1 10 consider interactive async story involve decide restful api look like,"Design RESTful API for getting query type When unsure, user should be able to check what type of query a given query is (for example, is the query ""select * from Object where qserv_area_spec(1, 2, 10, 5)"" considered interactive or async? This story involves deciding how the RESTful API will look like."
Debug problem with large results set Query returning 2 billion rows causes problems for czar - czar is using nearly 16 GB or memory. Need to understand why RAM usage in czar is correlated with result size.,1,DM-3488,datamanagement,debug problem large result set query return billion row cause problem czar czar nearly 16 gb memory need understand ram usage czar correlate result size,Debug problem with large results set Query returning 2 billion rows causes problems for czar - czar is using nearly 16 GB or memory. Need to understand why RAM usage in czar is correlated with result size.
"replace Calib negative flux behavior methods with context manager DM-3483 adds a nice context manager for handling the behavior of Calib objects when encountering negative fluxes.  This could be even nicer if we moved it into afw and integrated it with Calib itself, replacing the existing static methods (which are bad because they use global variables).    This will be an API change, and will require an RFC.",2,DM-3489,datamanagement,replace calib negative flux behavior method context manager dm-3483 add nice context manager handle behavior calib object encounter negative flux nice move afw integrate calib replace exist static method bad use global variable api change require rfc,"replace Calib negative flux behavior methods with context manager DM-3483 adds a nice context manager for handling the behavior of Calib objects when encountering negative fluxes. This could be even nicer if we moved it into afw and integrated it with Calib itself, replacing the existing static methods (which are bad because they use global variables). This will be an API change, and will require an RFC."
"Quick-and-dirty n-way spatial matching This issue will add limited N-way spatial matching of multiple catalogs with identical schemas, sufficient for measuring FY15 KPMs.  It will be a simple wrapper on our existing 2-way matching code in afw, and will not be intended for long term use (as it won't be an efficient algorithm or an ideal interrface).",2,DM-3490,datamanagement,quick dirty way spatial match issue add limited way spatial matching multiple catalog identical schema sufficient measure fy15 kpm simple wrapper exist way matching code afw intend long term use will efficient algorithm ideal interrface,"Quick-and-dirty n-way spatial matching This issue will add limited N-way spatial matching of multiple catalogs with identical schemas, sufficient for measuring FY15 KPMs. It will be a simple wrapper on our existing 2-way matching code in afw, and will not be intended for long term use (as it won't be an efficient algorithm or an ideal interrface)."
"Update ip_diffim to use the new NO_DATA flag instead of EDGE In ip_diffImm some uses of EDGE were converted to or supplemented with NO_DATA, but others were not. This ticket handles the missing instances.",1,DM-3491,datamanagement,update ip_diffim use new no_data flag instead edge ip_diffimm use edge convert supplement no_data ticket handle miss instance,"Update ip_diffim to use the new NO_DATA flag instead of EDGE In ip_diffImm some uses of EDGE were converted to or supplemented with NO_DATA, but others were not. This ticket handles the missing instances."
"Correct for distortion in matchOptimisticB astrometry matcher matchOptimisticB does not correct for distortion, although an estimate of the distortion is available.  We suspect that doing the matching on the celestial sphere might be ideal, but matching on a tangent plane has worked for HSC.",5,DM-3492,datamanagement,correct distortion matchoptimisticb astrometry matcher matchoptimisticb correct distortion estimate distortion available suspect matching celestial sphere ideal match tangent plane work hsc,"Correct for distortion in matchOptimisticB astrometry matcher matchOptimisticB does not correct for distortion, although an estimate of the distortion is available. We suspect that doing the matching on the celestial sphere might be ideal, but matching on a tangent plane has worked for HSC."
"Fix crosstalk following ds9 interface changes crosstalk.py in obs_subaru uses ds9 without actually displaying anything, which causes trouble if display_ds9 is not setup.",1,DM-3493,datamanagement,fix crosstalk follow ds9 interface change crosstalk.py obs_subaru use ds9 actually display cause trouble display_ds9 setup,"Fix crosstalk following ds9 interface changes crosstalk.py in obs_subaru uses ds9 without actually displaying anything, which causes trouble if display_ds9 is not setup."
"lsst.afw.display.setMaskTransparency doc doesn't match code The docstring for {{setMaskTransparency}} says ""Specify display's mask transparency (percent); or *None to not set it when loading masks*"", but (from inspection of the code), it doesn't do anything if {{transparency}} is {{None}}.  ",1,DM-3494,datamanagement,lsst.afw.display.setmasktransparency doc match code docstring setmasktransparency say specify display mask transparency percent set load mask inspection code transparency,"lsst.afw.display.setMaskTransparency doc doesn't match code The docstring for {{setMaskTransparency}} says ""Specify display's mask transparency (percent); or *None to not set it when loading masks*"", but (from inspection of the code), it doesn't do anything if {{transparency}} is {{None}}."
Add support for images produced by pipelines in end-to-end integration test We need to add support for images that are produced by pipelines which are run as part of the end-to-end integration tests - ImgServ should be able to serve these images.,6,DM-3496,datamanagement,add support image produce pipeline end end integration test need add support image produce pipeline run end end integration test imgserv able serve image,Add support for images produced by pipelines in end-to-end integration test We need to add support for images that are produced by pipelines which are run as part of the end-to-end integration tests - ImgServ should be able to serve these images.
"Modify czar to use per query CSS metadata Czar is currently caching CSS information, the snapshot is taken when czar starts. Once we start dealing with more dynamic system where databases and tables can come and go anytime while the system is up (in particular, L3 databases and tables), the metadata in CSS would need to be refreshed per query to stay up to date",6,DM-3505,datamanagement,modify czar use query css metadata czar currently cache css information snapshot take czar start start deal dynamic system database table come anytime system particular l3 database table metadata css need refresh query stay date,"Modify czar to use per query CSS metadata Czar is currently caching CSS information, the snapshot is taken when czar starts. Once we start dealing with more dynamic system where databases and tables can come and go anytime while the system is up (in particular, L3 databases and tables), the metadata in CSS would need to be refreshed per query to stay up to date"
"Expose column metadata via metaserv We need to expose via RESTful APIs information about columns such as units, ucds, column description etc. It will require querying information_schema with information from our DDT tables. This should include exposing information which columns are the ra/decl columns used for indexing.",6,DM-3512,datamanagement,expose column metadata metaserv need expose restful api information column unit ucds column description etc require query information_schema information ddt table include expose information column ra decl column indexing,"Expose column metadata via metaserv We need to expose via RESTful APIs information about columns such as units, ucds, column description etc. It will require querying information_schema with information from our DDT tables. This should include exposing information which columns are the ra/decl columns used for indexing."
"Add support for row counts in Metaserv SUI software would like to frequently check what the row counts for our tables. This story involves caching the information about the row count information in the metaserv.     Note that for L3 tables, that might change, we will need to intercept queries that alter these tables, and update the cache.This is beyond the scope of this story.",4,DM-3513,datamanagement,add support row count metaserv sui software like frequently check row count table story involve cache information row count information metaserv note l3 table change need intercept query alter table update cache scope story,"Add support for row counts in Metaserv SUI software would like to frequently check what the row counts for our tables. This story involves caching the information about the row count information in the metaserv. Note that for L3 tables, that might change, we will need to intercept queries that alter these tables, and update the cache.This is beyond the scope of this story."
Add flag to MetaServ showing if qserv_area_spec is available SUI software needs to know which tables support spatial queries. This story involves exposing this information via Metaserv (the information is available via CSS) ,3,DM-3514,datamanagement,add flag metaserv show qserv_area_spec available sui software need know table support spatial query story involve expose information metaserv information available css,Add flag to MetaServ showing if qserv_area_spec is available SUI software needs to know which tables support spatial queries. This story involves exposing this information via Metaserv (the information is available via CSS)
"Refactor DipoleMeasurement: Dipole classification to plugin DipoleMeasurementTask currently runs dipole measurement plugins and runs its own implemented dipole classification method. This ticket translates dipole classification to a plugin itself to simplify DipoleMeasurementTask.  Instead of inheriting from SingleFrameMeasurementTask, DipoleMeasurementTask will run SingleFrameMeasurementTask setting the appropriate default slots and plugins (including dipole classification plugin).      ",6,DM-3515,datamanagement,refactor dipolemeasurement dipole classification plugin dipolemeasurementtask currently run dipole measurement plugin run implement dipole classification method ticket translate dipole classification plugin simplify dipolemeasurementtask instead inherit singleframemeasurementtask dipolemeasurementtask run singleframemeasurementtask set appropriate default slot plugin include dipole classification plugin,"Refactor DipoleMeasurement: Dipole classification to plugin DipoleMeasurementTask currently runs dipole measurement plugins and runs its own implemented dipole classification method. This ticket translates dipole classification to a plugin itself to simplify DipoleMeasurementTask. Instead of inheriting from SingleFrameMeasurementTask, DipoleMeasurementTask will run SingleFrameMeasurementTask setting the appropriate default slots and plugins (including dipole classification plugin)."
"prune stale obs_subaru dependencies obs_subaru has some Eups dependencies that should be removed:   - meas_extensions_multiShapelet (removed from the LSST stack, content moved to meas_modelfit)   - meas_multifit (renamed to meas_modelfit)   - fitsthumb (need to cherry-pick code from HSC to replace it)   - pyfits (investigate what we use it for; it might not be needed)   - psycopg2 (I don't think we'll ever use this on the LSST side; we should add it back to the HSC side after we re-fork)",4,DM-3518,datamanagement,prune stale obs_subaru dependency obs_subaru eups dependency remove meas_extensions_multishapelet remove lsst stack content move meas_modelfit meas_multifit rename meas_modelfit fitsthumb need cherry pick code hsc replace pyfit investigate use need psycopg2 think use lsst add hsc fork,"prune stale obs_subaru dependencies obs_subaru has some Eups dependencies that should be removed: - meas_extensions_multiShapelet (removed from the LSST stack, content moved to meas_modelfit) - meas_multifit (renamed to meas_modelfit) - fitsthumb (need to cherry-pick code from HSC to replace it) - pyfits (investigate what we use it for; it might not be needed) - psycopg2 (I don't think we'll ever use this on the LSST side; we should add it back to the HSC side after we re-fork)"
"Releasing un-acquired resources bug Running a mix of queries: 75 low volume and 10 high volume that include near neighbor failed at some point with    {quote}  terminate called after throwing an instance of 'lsst::qserv::Bug'    what():  ChunkResource ChunkEntry::release: Error releasing un-acquired resource  {quote}    Stack trace:    {quote}  #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  Missing separate debuginfos, use: debuginfo-install expat-2.1.0-8.el7.x86_64 glibc-2.17-78.el7.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.12.2-14.el7.x86_64 libcom_err-1.42.9-7.el7.x86_64 libgcc-4.8.3-9.el7.x86_64 libicu-50.1.2-11.el7.x86_64 libselinux-2.2.2-6.el7.x86_64 libstdc++-4.8.3-9.el7.x86_64 nss-softokn-freebl-3.16.2.3-9.el7.x86_64 openssl-libs-1.0.1e-42.el7_1.9.x86_64 pcre-8.32-14.el7.x86_64 xz-libs-5.1.2-9alpha.el7.x86_64 zlib-1.2.7-13.el7.x86_64  (gdb) where  #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  #1  0x00007fb6b0ccfcf8 in abort () from /lib64/libc.so.6  #2  0x00007fb6b15d29b5 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6  #3  0x00007fb6b15d0926 in ?? () from /lib64/libstdc++.so.6  #4  0x00007fb6b15cf8e9 in ?? () from /lib64/libstdc++.so.6  #5  0x00007fb6b15d0554 in __gxx_personality_v0 () from /lib64/libstdc++.so.6  #6  0x00007fb6b1069913 in ?? () from /lib64/libgcc_s.so.1  #7  0x00007fb6b1069e47 in _Unwind_Resume () from /lib64/libgcc_s.so.1  #8  0x00007fb6ab554247 in lsst::qserv::wdb::ChunkResourceMgr::Impl::release (this=0x21d1cc0, i=...) at build/wdb/ChunkResource.cc:398  #9  0x00007fb6ab552696 in lsst::qserv::wdb::ChunkResource::~ChunkResource (this=0x7fb68a5f9b70, __in_chrg=<optimized out>)      at build/wdb/ChunkResource.cc:131  #10 0x00007fb6ab560f0f in lsst::qserv::wdb::QueryAction::Impl::_dispatchChannel (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:392  #11 0x00007fb6ab55f5ab in lsst::qserv::wdb::QueryAction::Impl::act (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:187  #12 0x00007fb6ab562084 in lsst::qserv::wdb::QueryAction::operator() (this=0x7fb658050548) at build/wdb/QueryAction.cc:450  #13 0x00007fb6ab544f46 in lsst::qserv::wcontrol::ForemanImpl::Runner::operator() (this=0x7fb67400fa20) at build/wcontrol/Foreman.cc:302  #14 0x00007fb6ab551cf0 in std::_Bind_simple<lsst::qserv::wcontrol::ForemanImpl::Runner ()>::_M_invoke<>(std::_Index_tuple<>) (      this=0x7fb67400fa20) at /usr/include/c++/4.8.2/functional:1732  #15 0x00007fb6ab551a8b in std::_Bind_simple<lsst::qserv::wcontrol::ForemanImpl::Runner ()>::operator()() (this=0x7fb67400fa20)      at /usr/include/c++/4.8.2/functional:1720  {quote}    Tail of log file from xrootd log:    {quote}  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:139) - _getNextTasks(1)>->->  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:151) - Returning 1 to launch  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:154) - _getNextTasks <<<<<  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:172) - _getNextTasks(29)>->->  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:199) - ChunkDisk busyness: yes  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:171) - ChunkDisk getNext: current= (scan=10436,  cached=8360,8259,) candidate=10301  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:184) - ChunkDisk denying task  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:196) - _getNextTasks <<<<<  0821 19:08:58.531 [0x7fb68a6fb700] INFO  root (build/xrdsvc/SsiSession.cc:120) - Enqueued TaskMsg for Resource(/chk/LSST/2732) in 0.001016 seconds  0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:175) - Registered runner 0x7fb66c141ab0  0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:209) - Started task Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0   0821 19:08:58.531 [0x7fb6895f8700] INFO  Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0   0821 19:08:58.531 [0x7fb6895f8700] INFO  Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_fd51ad249f62fb765e173d7b3cae5d94  0821 19:08:58.531 [0x7fb6895f8700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=106  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=210  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=316    ...(thousands of _fillRows lines)    terminate called after throwing an instance of 'lsst::qserv::Bug'    what():  ChunkResource ChunkEntry::release: Error releasing un-acquired resource  {quote}  ",3,DM-3522,datamanagement,"release un acquire resource bug run mix query 75 low volume 10 high volume include near neighbor fail point quote terminate call throw instance lsst::qserv::bug chunkresource chunkentry::release error release un acquire resource quote stack trace quote 0x00007fb6b0cce5e9 raise libc.so.6 miss separate debuginfos use debuginfo install expat-2.1.0 8.el7.x86_64 glibc-2.17 78.el7.x86_64 keyutil libs-1.5.8 3.el7.x86_64 krb5 libs-1.12.2 14.el7.x86_64 libcom_err-1.42.9 7.el7.x86_64 libgcc-4.8.3 9.el7.x86_64 libicu-50.1.2 11.el7.x86_64 libselinux-2.2.2 6.el7.x86_64 libstdc++-4.8.3 9.el7.x86_64 nss softokn freebl-3.16.2.3 9.el7.x86_64 openssl libs-1.0.1e-42.el7_1.9.x86_64 pcre-8.32 14.el7.x86_64 xz libs-5.1.2 9alpha.el7.x86_64 zlib-1.2.7 13.el7.x86_64 gdb 0x00007fb6b0cce5e9 raise libc.so.6 0x00007fb6b0ccfcf8 abort libc.so.6 0x00007fb6b15d29b5 gnu_cxx::__verbose_terminate_handler libstdc++.so.6 0x00007fb6b15d0926 libstdc++.so.6 0x00007fb6b15cf8e9 libstdc++.so.6 0x00007fb6b15d0554 gxx_personality_v0 libstdc++.so.6 0x00007fb6b1069913 libgcc_s.so.1 0x00007fb6b1069e47 libgcc_s.so.1 0x00007fb6ab554247 lsst::qserv::wdb::chunkresourcemgr::impl::release this=0x21d1cc0 i= build wdb chunkresource.cc:398 0x00007fb6ab552696 lsst::qserv::wdb::chunkresource::~chunkresource this=0x7fb68a5f9b70 in_chrg= build wdb chunkresource.cc:131 10 0x00007fb6ab560f0f lsst::qserv::wdb::queryaction::impl::_dispatchchannel this=0x7fb65848c4d0 build wdb queryaction.cc:392 11 lsst::qserv::wdb::queryaction::impl::act this=0x7fb65848c4d0 build wdb queryaction.cc:187 12 0x00007fb6ab562084 lsst::qserv::wdb::queryaction::operator this=0x7fb658050548 build wdb queryaction.cc:450 13 0x00007fb6ab544f46 lsst::qserv::wcontrol::foremanimpl::runner::operator this=0x7fb67400fa20 build wcontrol foreman.cc:302 14 0x00007fb6ab551cf0 std::_bind_simple::_m_invoke<>(std::_index_tuple this=0x7fb67400fa20 /usr include c++/4.8.2 functional:1732 15 0x00007fb6ab551a8b std::_bind_simple::operator this=0x7fb67400fa20 /usr include c++/4.8.2 functional:1720 quote tail log file xrootd log quote 0821 19:08:58.530 0x7fb68a6fb700 debug groupsched build wsched groupscheduler.cc:139 getnexttasks(1)>->- 0821 19:08:58.530 0x7fb68a6fb700 debug groupsched build wsched groupscheduler.cc:151 return launch 0821 19:08:58.530 0x7fb68a6fb700 debug groupsched build wsched groupscheduler.cc:154 getnexttask 0821 19:08:58.530 0x7fb68a6fb700 debug scansched build wsched scanscheduler.cc:172 getnexttasks(29)>->- 0821 19:08:58.530 0x7fb68a6fb700 debug scansched build wsched chunkdisk.cc:199 chunkdisk busyness yes 0821 19:08:58.530 0x7fb68a6fb700 debug scansched build wsched chunkdisk.cc:171 chunkdisk getnext current= scan=10436 cached=8360,8259 candidate=10301 0821 19:08:58.530 0x7fb68a6fb700 debug scansched build wsched chunkdisk.cc:184 chunkdisk deny task 0821 19:08:58.530 0x7fb68a6fb700 debug scansched build wsched scanscheduler.cc:196 getnexttask 0821 19:08:58.531 0x7fb68a6fb700 info root build xrdsvc ssisession.cc:120 enqueued taskmsg resource(/chk lsst/2732 0.001016 second 0821 19:08:58.531 0x7fb6895f8700 debug foreman build wcontrol foreman.cc:175 register runner 0x7fb66c141ab0 0821 19:08:58.531 0x7fb6895f8700 debug foreman build wcontrol foreman.cc:209 start task task msg session=434445 chunk=2732 db lsst entry time= frag select o.deepsourceid o.ra o.decl s.coord_ra s.coord_decl s.parent lsst.object_2732 lsst.source_2732 scisql_s2ptinbox(o.ra o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 scisql_s2ptinbox(s.coord_ra s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 o.deepsourceid s.objectid sc= rt r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0 0821 19:08:58.531 0x7fb6895f8700 info foreman build wcontrol foreman.cc:296 runner run task msg session=434445 chunk=2732 db lsst entry time= frag select o.deepsourceid o.ra o.decl s.coord_ra s.coord_decl s.parent lsst.object_2732 lsst.source_2732 scisql_s2ptinbox(o.ra o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 scisql_s2ptinbox(s.coord_ra s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 o.deepsourceid s.objectid sc= rt r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0 0821 19:08:58.531 0x7fb6895f8700 info foreman build wdb queryaction.cc:177 exec flight db q_fd51ad249f62fb765e173d7b3cae5d94 0821 19:08:58.531 0x7fb6895f8700 warn foreman build wdb queryaction.cc:109 queryaction overriding dbname lsst 0821 19:08:58.718 0x7fb6a0d8e700 info root build wdb queryaction.cc:261 fillrow size=106 0821 19:08:58.718 0x7fb6a0d8e700 info root build wdb queryaction.cc:261 fillrows size=210 0821 19:08:58.718 0x7fb6a0d8e700 info root build wdb queryaction.cc:261 size=316 thousand fillrow line terminate call throw instance lsst::qserv::bug chunkresource chunkentry::release error release un acquire resource quote","Releasing un-acquired resources bug Running a mix of queries: 75 low volume and 10 high volume that include near neighbor failed at some point with {quote} terminate called after throwing an instance of 'lsst::qserv::Bug' what(): ChunkResource ChunkEntry::release: Error releasing un-acquired resource {quote} Stack trace: {quote} #0 0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6 Missing separate debuginfos, use: debuginfo-install expat-2.1.0-8.el7.x86_64 glibc-2.17-78.el7.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.12.2-14.el7.x86_64 libcom_err-1.42.9-7.el7.x86_64 libgcc-4.8.3-9.el7.x86_64 libicu-50.1.2-11.el7.x86_64 libselinux-2.2.2-6.el7.x86_64 libstdc++-4.8.3-9.el7.x86_64 nss-softokn-freebl-3.16.2.3-9.el7.x86_64 openssl-libs-1.0.1e-42.el7_1.9.x86_64 pcre-8.32-14.el7.x86_64 xz-libs-5.1.2-9alpha.el7.x86_64 zlib-1.2.7-13.el7.x86_64 (gdb) where #0 0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6 #1 0x00007fb6b0ccfcf8 in abort () from /lib64/libc.so.6 #2 0x00007fb6b15d29b5 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6 #3 0x00007fb6b15d0926 in ?? () from /lib64/libstdc++.so.6 #4 0x00007fb6b15cf8e9 in ?? () from /lib64/libstdc++.so.6 #5 0x00007fb6b15d0554 in __gxx_personality_v0 () from /lib64/libstdc++.so.6 #6 0x00007fb6b1069913 in ?? () from /lib64/libgcc_s.so.1 #7 0x00007fb6b1069e47 in _Unwind_Resume () from /lib64/libgcc_s.so.1 #8 0x00007fb6ab554247 in lsst::qserv::wdb::ChunkResourceMgr::Impl::release (this=0x21d1cc0, i=...) at build/wdb/ChunkResource.cc:398 #9 0x00007fb6ab552696 in lsst::qserv::wdb::ChunkResource::~ChunkResource (this=0x7fb68a5f9b70, __in_chrg=) at build/wdb/ChunkResource.cc:131 #10 0x00007fb6ab560f0f in lsst::qserv::wdb::QueryAction::Impl::_dispatchChannel (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:392 #11 0x00007fb6ab55f5ab in lsst::qserv::wdb::QueryAction::Impl::act (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:187 #12 0x00007fb6ab562084 in lsst::qserv::wdb::QueryAction::operator() (this=0x7fb658050548) at build/wdb/QueryAction.cc:450 #13 0x00007fb6ab544f46 in lsst::qserv::wcontrol::ForemanImpl::Runner::operator() (this=0x7fb67400fa20) at build/wcontrol/Foreman.cc:302 #14 0x00007fb6ab551cf0 in std::_Bind_simple::_M_invoke<>(std::_Index_tuple<>) ( this=0x7fb67400fa20) at /usr/include/c++/4.8.2/functional:1732 #15 0x00007fb6ab551a8b in std::_Bind_simple::operator()() (this=0x7fb67400fa20) at /usr/include/c++/4.8.2/functional:1720 {quote} Tail of log file from xrootd log: {quote} 0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:139) - _getNextTasks(1)>->-> 0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:151) - Returning 1 to launch 0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:154) - _getNextTasks <<<<< 0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:172) - _getNextTasks(29)>->-> 0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:199) - ChunkDisk busyness: yes 0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:171) - ChunkDisk getNext: current= (scan=10436, cached=8360,8259,) candidate=10301 0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:184) - ChunkDisk denying task 0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:196) - _getNextTasks <<<<< 0821 19:08:58.531 [0x7fb68a6fb700] INFO root (build/xrdsvc/SsiSession.cc:120) - Enqueued TaskMsg for Resource(/chk/LSST/2732) in 0.001016 seconds 0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:175) - Registered runner 0x7fb66c141ab0 0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:209) - Started task Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0 0821 19:08:58.531 [0x7fb6895f8700] INFO Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0 0821 19:08:58.531 [0x7fb6895f8700] INFO Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_fd51ad249f62fb765e173d7b3cae5d94 0821 19:08:58.531 [0x7fb6895f8700] WARN Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST 0821 19:08:58.718 [0x7fb6a0d8e700] INFO root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=106 0821 19:08:58.718 [0x7fb6a0d8e700] INFO root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=210 0821 19:08:58.718 [0x7fb6a0d8e700] INFO root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=316 ...(thousands of _fillRows lines) terminate called after throwing an instance of 'lsst::qserv::Bug' what(): ChunkResource ChunkEntry::release: Error releasing un-acquired resource {quote}"
Information categorization for docushare Project started to categorize documents on docushare as per LSST's information categorization policy.,3,DM-3533,datamanagement,information categorization docushare project start categorize document docushare lsst information categorization policy,Information categorization for docushare Project started to categorize documents on docushare as per LSST's information categorization policy.
Meet with Scott K for IdM project Scott K will come to NCSA to discuss initial IdM requirements for LSST.,1,DM-3534,datamanagement,meet scott idm project scott come ncsa discuss initial idm requirement lsst,Meet with Scott K for IdM project Scott K will come to NCSA to discuss initial IdM requirements for LSST.
NIST SP 800.82 investigation NIST SP 800.82 might contain useful information for securing the scada enclave at the observatory site.  Or it might even be the model we should use in full.,2,DM-3535,datamanagement,nist sp 800.82 investigation nist sp 800.82 contain useful information secure scada enclave observatory site model use,NIST SP 800.82 investigation NIST SP 800.82 might contain useful information for securing the scada enclave at the observatory site. Or it might even be the model we should use in full.
"Translate more mask pixel bits from instcal data quality mask of DECam data  The mask pixel bits are defined differently in DECam instcal data from different pipelines, see http://community.lsst.org/t/decam-data-quality-masks/133  for a summary.      The current mapping in DecamInstcalMapper is for the community pipeline Pre-V3.5.0.  More mask bits were defined in DESDM y1 products. This issue provides full mapping for DESDM y1 products so all questionable pixels are translated into the MaskedImage.       ",2,DM-3538,datamanagement,translate mask pixel bit instcal datum quality mask decam datum mask pixel bit define differently decam instcal datum different pipeline http://community.lsst.org/t/decam-data-quality-masks/133 summary current mapping decaminstcalmapper community pipeline pre v3.5.0 mask bit define desdm y1 product issue provide mapping desdm y1 product questionable pixel translate maskedimage,"Translate more mask pixel bits from instcal data quality mask of DECam data The mask pixel bits are defined differently in DECam instcal data from different pipelines, see http://community.lsst.org/t/decam-data-quality-masks/133 for a summary. The current mapping in DecamInstcalMapper is for the community pipeline Pre-V3.5.0. More mask bits were defined in DESDM y1 products. This issue provides full mapping for DESDM y1 products so all questionable pixels are translated into the MaskedImage."
"Fix bugs found in FitsRead  There was a bug found in writeFitsFile(OutputStream stream, FitsRead[] fitsReadAry, Fits refFits) method where it saved the data from refFits into the output Fits file instead of the data from the FitsRead object.    There is another possible bug under investigation.  The output Fits file created by FitsRead.createNewFits() can not be opened as an image.   ",4,DM-3539,datamanagement,fix bug find fitsread bug find writefitsfile(outputstream stream fitsread fitsreadary fits reffit method save datum reffit output fit file instead datum fitsread object possible bug investigation output fit file create fitsread.createnewfit open image,"Fix bugs found in FitsRead There was a bug found in writeFitsFile(OutputStream stream, FitsRead[] fitsReadAry, Fits refFits) method where it saved the data from refFits into the output Fits file instead of the data from the FitsRead object. There is another possible bug under investigation. The output Fits file created by FitsRead.createNewFits() can not be opened as an image."
"Refactor ChunkResource for testability While the wdb/testChunkResource.cc unit test appears to exercise the chunk resource management code, it does not seem to actually test it.    We should refactor the classes in the ChunkResource header and implementation files to make the sanity of the implementation checkable. In particular, I think that Backend from ChunkResource.cc should be a public interface (with a less generic name) that the unit test can implement, and that ChunkResourceMgr should be a concrete class implemented in terms of a user specifiable Backend.    This way the unit test can inject the dependency it wants (namely a mock backend that tracks sub-chunk tables as they are acquired and released), we don't pollute the actual implementation classes with ""I'm fake"" flags and alternate code paths for fake objects, and we can make the unit test actually perform validity tests.    The first cut at this should include a check for the problem described in DM-3522.",4,DM-3540,datamanagement,refactor chunkresource testability wdb testchunkresource.cc unit test appear exercise chunk resource management code actually test refactor class chunkresource header implementation file sanity implementation checkable particular think backend chunkresource.cc public interface generic unit test implement chunkresourcemgr concrete class implement term user specifiable backend way unit test inject dependency want mock backend track sub chunk table acquire release pollute actual implementation class fake flag alternate code path fake object unit test actually perform validity test cut include check problem describe dm-3522,"Refactor ChunkResource for testability While the wdb/testChunkResource.cc unit test appears to exercise the chunk resource management code, it does not seem to actually test it. We should refactor the classes in the ChunkResource header and implementation files to make the sanity of the implementation checkable. In particular, I think that Backend from ChunkResource.cc should be a public interface (with a less generic name) that the unit test can implement, and that ChunkResourceMgr should be a concrete class implemented in terms of a user specifiable Backend. This way the unit test can inject the dependency it wants (namely a mock backend that tracks sub-chunk tables as they are acquired and released), we don't pollute the actual implementation classes with ""I'm fake"" flags and alternate code paths for fake objects, and we can make the unit test actually perform validity tests. The first cut at this should include a check for the problem described in DM-3522."
Move LDM-151 to Sphinx/Read the Docs Move the LDM-151 (DM applications design document) to restructuredText (built with Sphinx) and published automatically via readthedocs.org.    See discussion at http://community.lsst.org/t/requesting-comments-for-design-documentation-format-for-dm/132?u=jsick    This is an experiment.,1,DM-3546,datamanagement,ldm-151 sphinx read docs ldm-151 dm application design document restructuredtext build sphinx publish automatically readthedocs.org discussion http://community.lsst.org/t/requesting-comments-for-design-documentation-format-for-dm/132?u=jsick experiment,Move LDM-151 to Sphinx/Read the Docs Move the LDM-151 (DM applications design document) to restructuredText (built with Sphinx) and published automatically via readthedocs.org. See discussion at http://community.lsst.org/t/requesting-comments-for-design-documentation-format-for-dm/132?u=jsick This is an experiment.
SsiService not being destroyed SsiService::~SsiService is not being called. ,1,DM-3547,datamanagement,ssiservice destroy ssiservice::~ssiservice call,SsiService not being destroyed SsiService::~SsiService is not being called.
"Attend SciPy 2015 tutorials Attend the tutorials at SciPy 2015 (July 6-7, 2015) in order to get hands-on experience with current scientific data analysis tools in the Python environment.    Planned attendance:   * Introduction to NumPy  * Building Python Data Applications with Blaze and Bokeh  * Efficient Python for High-Performance Parallel Computing  * Jupyter Advanced Topics Tutorial  ",4,DM-3551,datamanagement,attend scipy 2015 tutorial attend tutorial scipy 2015 july 2015 order hand experience current scientific datum analysis tool python environment planned attendance introduction numpy build python data applications blaze bokeh efficient python high performance parallel computing jupyter advanced topics tutorial,"Attend SciPy 2015 tutorials Attend the tutorials at SciPy 2015 (July 6-7, 2015) in order to get hands-on experience with current scientific data analysis tools in the Python environment. Planned attendance: * Introduction to NumPy * Building Python Data Applications with Blaze and Bokeh * Efficient Python for High-Performance Parallel Computing * Jupyter Advanced Topics Tutorial"
"Binary FITS table catalog upload We need to be able to upload catalogs in binary FITS table format.    We'll do it by converting the first table in the provided FITS into an IPAC table. Our upload should be handling the conversion.    Later, we should figure out how to handle multiple tables in one FITS.",4,DM-3554,datamanagement,binary fit table catalog upload need able upload catalog binary fits table format convert table provided fit ipac table upload handle conversion later figure handle multiple table fit,"Binary FITS table catalog upload We need to be able to upload catalogs in binary FITS table format. We'll do it by converting the first table in the provided FITS into an IPAC table. Our upload should be handling the conversion. Later, we should figure out how to handle multiple tables in one FITS."
"Ignore ""SELECT @@tx_isolation"" queries Looks like one of the queries we registered in webserv is: cursor.execute('SELECT @@tx_isolation') and that is bound to confuse Qserv. Need to suppress it at mysql proxy level.",1,DM-3555,datamanagement,ignore select @@tx_isolation query look like query register webserv @@tx_isolation bind confuse qserv need suppress mysql proxy level,"Ignore ""SELECT @@tx_isolation"" queries Looks like one of the queries we registered in webserv is: cursor.execute('SELECT @@tx_isolation') and that is bound to confuse Qserv. Need to suppress it at mysql proxy level."
"Attend SciPy 2015 conference Attend the SciPy 2015 conference, July 8-10, 2015, to learn about current trends in this community.    Notably, the conference covers a variety of interactive data analysis tools, including Jupyter/IPython, interactive graphics packages such as VisPy and Bokeh, and astronomical data handling tools such as astropy.",6,DM-3556,datamanagement,attend scipy 2015 conference attend scipy 2015 conference july 10 2015 learn current trend community notably conference cover variety interactive datum analysis tool include jupyter ipython interactive graphic package vispy bokeh astronomical datum handling tool astropy,"Attend SciPy 2015 conference Attend the SciPy 2015 conference, July 8-10, 2015, to learn about current trends in this community. Notably, the conference covers a variety of interactive data analysis tools, including Jupyter/IPython, interactive graphics packages such as VisPy and Bokeh, and astronomical data handling tools such as astropy."
"Document SciPy 2015 takeaways Write up what was learned, and recommendations, from SciPy 2015.",4,DM-3557,datamanagement,document scipy 2015 takeaway write learn recommendation scipy 2015,"Document SciPy 2015 takeaways Write up what was learned, and recommendations, from SciPy 2015."
"Experiment with Jupyter widget technology and Firefly Tools Based on DM-2047 work to date, investigate the feasibility of using the Jupyter widget interface to wrap up Firefly tools.",8,DM-3558,datamanagement,experiment jupyter widget technology firefly tools base dm-2047 work date investigate feasibility jupyter widget interface wrap firefly tool,"Experiment with Jupyter widget technology and Firefly Tools Based on DM-2047 work to date, investigate the feasibility of using the Jupyter widget interface to wrap up Firefly tools."
Make location of images more flexible It'd be useful to be able to point imgserv to any location containing images (in particular for various random tests that we will be running over the next few years). Right now this requires changing imgserv code. An idea tossed around: pass the location via URI (optional  parameter),4,DM-3571,datamanagement,location image flexible useful able point imgserv location contain image particular random test run year right require change imgserv code idea toss pass location uri optional parameter,Make location of images more flexible It'd be useful to be able to point imgserv to any location containing images (in particular for various random tests that we will be running over the next few years). Right now this requires changing imgserv code. An idea tossed around: pass the location via URI (optional parameter)
"Collect requirements for pipeline developer visualization tools This story covers a series of conversations with Robert Lupton, Jim Bosch, Paul Price, K-T Lim, and others going into details about how to make the visualization environment (based on Firefly tools) more useful for developers.",4,DM-3572,datamanagement,collect requirement pipeline developer visualization tool story cover series conversation robert lupton jim bosch paul price lim go detail visualization environment base firefly tool useful developer,"Collect requirements for pipeline developer visualization tools This story covers a series of conversations with Robert Lupton, Jim Bosch, Paul Price, K-T Lim, and others going into details about how to make the visualization environment (based on Firefly tools) more useful for developers."
Replace qservAdmin.py use with CssAccess WE have new CSS interface which unifies C++ and Python and it is time to replace qservAdmin.py with CssAcces in places like qserv-admin.py and data loader.  ,8,DM-3574,datamanagement,replace qservadmin.py use cssaccess new css interface unify c++ python time replace qservadmin.py cssacces place like qserv-admin.py datum loader,Replace qservAdmin.py use with CssAccess WE have new CSS interface which unifies C++ and Python and it is time to replace qservAdmin.py with CssAcces in places like qserv-admin.py and data loader.
"Catalog transformation should pass through all non-measurement fields We need to include fields added by other tasks (e.g. the deblender) or the source minimal schema (e.g. parent) in transformed catalogs.  I believe it should be safe to assume any such fields can simply be copied (i.e. they do not require any actual transformation - they're mostly flags), but we do need to make the list of fields to be copied by this mechanism dynamic.",3,DM-3590,datamanagement,catalog transformation pass non measurement field need include field add task e.g. deblender source minimal schema e.g. parent transform catalog believe safe assume field simply copy i.e. require actual transformation flag need list field copy mechanism dynamic,"Catalog transformation should pass through all non-measurement fields We need to include fields added by other tasks (e.g. the deblender) or the source minimal schema (e.g. parent) in transformed catalogs. I believe it should be safe to assume any such fields can simply be copied (i.e. they do not require any actual transformation - they're mostly flags), but we do need to make the list of fields to be copied by this mechanism dynamic."
"Submit change request for LSE-68, mid-phase-3 update Collect changes and submit a change request to LSE-68 for a mid-phase-3 update to the document, covering clarifications on guider data and image identification, among others.    Includes preliminary work to prepare for change request.",4,DM-3606,datamanagement,submit change request lse-68 mid phase-3 update collect change submit change request lse-68 mid phase-3 update document cover clarification guider datum image identification include preliminary work prepare change request,"Submit change request for LSE-68, mid-phase-3 update Collect changes and submit a change request to LSE-68 for a mid-phase-3 update to the document, covering clarifications on guider data and image identification, among others. Includes preliminary work to prepare for change request."
"Prepare document for CCB review of LCR-357 LCR-357 was an outline of work to be done, based on discussions with the Camera DAQ staff.  This task is to generate an actual proposed document change for the CCB.",6,DM-3607,datamanagement,prepare document ccb review lcr-357 lcr-357 outline work base discussion camera daq staff task generate actual propose document change ccb,"Prepare document for CCB review of LCR-357 LCR-357 was an outline of work to be done, based on discussions with the Camera DAQ staff. This task is to generate an actual proposed document change for the CCB."
"provide detailed information needed to DAX meta API SUIT needs certain specific information through DAX meta service when searching for meta data. For Example, what kind of table it is, does it have spatial index to search by position, which set of (ra, dec) columns is the primary one, etc?",1,DM-3608,datamanagement,provide detailed information need dax meta api suit need certain specific information dax meta service search meta datum example kind table spatial index search position set ra dec column primary etc,"provide detailed information needed to DAX meta API SUIT needs certain specific information through DAX meta service when searching for meta data. For Example, what kind of table it is, does it have spatial index to search by position, which set of (ra, dec) columns is the primary one, etc?"
The Alert subscription system requirement gathering (F16) Solidify the requirement for the alert subscription system. ,8,DM-3609,datamanagement,alert subscription system requirement gathering f16 solidify requirement alert subscription system,The Alert subscription system requirement gathering (F16) Solidify the requirement for the alert subscription system.
"CCB review and posting of final updated document Carry out the CCB review, respond to questions, support final implementation of updated document.",2,DM-3610,datamanagement,ccb review posting final update document carry ccb review respond question support final implementation update document,"CCB review and posting of final updated document Carry out the CCB review, respond to questions, support final implementation of updated document."
Prepare for Winter 2016 work on LSE-68 Use a session at the LSST 2015 all-hands meeting to prepare for LSE-68 work in the Winter 2016 cycle.,3,DM-3611,datamanagement,prepare winter 2016 work lse-68 use session lsst 2015 hand meeting prepare lse-68 work winter 2016 cycle,Prepare for Winter 2016 work on LSE-68 Use a session at the LSST 2015 all-hands meeting to prepare for LSE-68 work in the Winter 2016 cycle.
"option to plot error bars on XY plot When there are error or uncertainty for a data point, there should be option to plot the error bars in the XY plot. ",6,DM-3612,datamanagement,option plot error bar xy plot error uncertainty data point option plot error bar xy plot,"option to plot error bars on XY plot When there are error or uncertainty for a data point, there should be option to plot the error bars in the XY plot."
expose region overlay on image function through JavaScript API expose region overlay on image function through JavaScript API,5,DM-3615,datamanagement,expose region overlay image function javascript api expose region overlay image function javascript api,expose region overlay on image function through JavaScript API expose region overlay on image function through JavaScript API
Expose image XY readout at cursor point function in JavaScript API Expose image XY readout at cursor point function in JavaScript API,2,DM-3616,datamanagement,expose image xy readout cursor point function javascript api expose image xy readout cursor point function javascript api,Expose image XY readout at cursor point function in JavaScript API Expose image XY readout at cursor point function in JavaScript API
Fix bug related to restarting xrootd in wmgr Changes from DM-2930 are failing integration tests because wmgr is restarting xrootd and now we need to also restart mysqld if xrootd pid changes.,1,DM-3618,datamanagement,fix bug relate restart xrootd wmgr changes dm-2930 fail integration test wmgr restart xrootd need restart mysqld xrootd pid change,Fix bug related to restarting xrootd in wmgr Changes from DM-2930 are failing integration tests because wmgr is restarting xrootd and now we need to also restart mysqld if xrootd pid changes.
OCS-DM-CCS-DAQ workshop Prepare for and attend the OCS-DAQ-CCS-DM workshop in November 2014.  This is primarily intended to review the status of LSE-70 (the definition of the OCS interface to the subsystems) and therefore prepare for updates to LSE-72 in time.,6,DM-3620,datamanagement,ocs dm ccs daq workshop prepare attend ocs daq ccs dm workshop november 2014 primarily intend review status lse-70 definition ocs interface subsystem prepare update lse-72 time,OCS-DM-CCS-DAQ workshop Prepare for and attend the OCS-DAQ-CCS-DM workshop in November 2014. This is primarily intended to review the status of LSE-70 (the definition of the OCS interface to the subsystems) and therefore prepare for updates to LSE-72 in time.
OCS-DM-CCS-DAQ workshop II Prepare for and attend the OCS-DAQ-CCS-DM workshop in February 2015. This is primarily intended to continue to make progress on LSE-70 (the definition of the OCS interface to the subsystems) and therefore prepare for updates to LSE-72 in time.,6,DM-3621,datamanagement,ocs dm ccs daq workshop ii prepare attend ocs daq ccs dm workshop february 2015 primarily intend continue progress lse-70 definition ocs interface subsystem prepare update lse-72 time,OCS-DM-CCS-DAQ workshop II Prepare for and attend the OCS-DAQ-CCS-DM workshop in February 2015. This is primarily intended to continue to make progress on LSE-70 (the definition of the OCS interface to the subsystems) and therefore prepare for updates to LSE-72 in time.
"Investigate disk-only (no tape) data releases The question that is being raised is how much it would cost to keep more than the last two Data Releases on readily-accessible storage (i.e. spinning disk). This will require changing several numbers and formulas in LDM-141, the storage sizing  model, observing the results as they propagate through LDM-144, the cost model,  and checking to make sure that everything makes sense and nothing has been overlooked. It looks like an answer to this question will be needed somewhere between a month and three months from now.",2,DM-3624,datamanagement,investigate disk tape datum release question raise cost data releases readily accessible storage i.e. spinning disk require change number formula ldm-141 storage size model observe result propagate ldm-144 cost model check sure make sense overlook look like answer question need month month,"Investigate disk-only (no tape) data releases The question that is being raised is how much it would cost to keep more than the last two Data Releases on readily-accessible storage (i.e. spinning disk). This will require changing several numbers and formulas in LDM-141, the storage sizing model, observing the results as they propagate through LDM-144, the cost model, and checking to make sure that everything makes sense and nothing has been overlooked. It looks like an answer to this question will be needed somewhere between a month and three months from now."
"implement jenkins support for running builds in docker containers DM-3359 demonstrated the feasibility of running jenkins builds in one-off docker containers but did not cover investigation of user creation of containers nor did it include a puppetized deployment.    Per build containers are desirable for a number of reasons but the largest motivation is allowing DM developers to define their own jobs without disrupting the ""sanctity"" of the lsstsw build slaves.",7,DM-3627,datamanagement,implement jenkins support run build docker container dm-3359 demonstrate feasibility run jenkin build docker container cover investigation user creation container include puppetized deployment build container desirable number reason large motivation allow dm developer define job disrupt sanctity lsstsw build slave,"implement jenkins support for running builds in docker containers DM-3359 demonstrated the feasibility of running jenkins builds in one-off docker containers but did not cover investigation of user creation of containers nor did it include a puppetized deployment. Per build containers are desirable for a number of reasons but the largest motivation is allowing DM developers to define their own jobs without disrupting the ""sanctity"" of the lsstsw build slaves."
Change root to config in config override files Implement RFC-62 by using {{config}} rather than {{root}} in config override files for the root of the config.    Note that I propose not modifying astrometry_net_data configs because those are numerous and hidden. They have their own special loader in LoadAstrometryNetObjectsTask._readIndexFiles which could easily be updated later. if desired. An obvious time to make such a transition would be when overhauling the way this data is unpersisted.,2,DM-3630,datamanagement,change root config config override file implement rfc-62 config root config override file root config note propose modify astrometry_net_data config numerous hidden special loader loadastrometrynetobjectstask._readindexfile easily update later desire obvious time transition overhaul way datum unpersisted,Change root to config in config override files Implement RFC-62 by using {{config}} rather than {{root}} in config override files for the root of the config. Note that I propose not modifying astrometry_net_data configs because those are numerous and hidden. They have their own special loader in LoadAstrometryNetObjectsTask._readIndexFiles which could easily be updated later. if desired. An obvious time to make such a transition would be when overhauling the way this data is unpersisted.
"Management for Aug 23-29 Hiring -- make offer to candidate, and also review candidates and strategy.  Review proposal for FY 2015 spend  Management meetings.  Openstack meeting.",3,DM-3631,datamanagement,management aug 23 29 hiring offer candidate review candidate strategy review proposal fy 2015 spend management meeting openstack meeting,"Management for Aug 23-29 Hiring -- make offer to candidate, and also review candidates and strategy. Review proposal for FY 2015 spend Management meetings. Openstack meeting."
"Add readMatches back to meas_astrom Recent changes to meas_astrom accidentally removed a function readMatches (copied below). Please restore it, preferably in its own module (though if someday we have more small python functions we may want a utils.py module).    Also please include a unit test.    {code}  def readMatches(butler, dataId, sourcesName='icSrc', matchesName='icMatch', config=MeasAstromConfig(), sourcesFlags=afwTable.SOURCE_IO_NO_FOOTPRINTS):      """"""Read matches, sources and catalogue; combine.      @param butler Data butler      @param dataId Data identifier for butler      @param sourcesName Name for sources from butler      @param matchesName Name for matches from butler      @param sourcesFlags Flags to pass for source retrieval      @returns Matches      """"""      sources = butler.get(sourcesName, dataId, flags=sourcesFlags)      packedMatches = butler.get(matchesName, dataId)      astrom = Astrometry(config)      return astrom.joinMatchListWithCatalog(packedMatches, sources)  {code}  ",4,DM-3633,datamanagement,add readmatche recent change meas_astrom accidentally remove function readmatche copy restore preferably module someday small python function want utils.py module include unit test code def readmatches(butler dataid sourcesname='icsrc matchesname='icmatch config measastromconfig sourcesflag afwtable source_io_no_footprint read match source catalogue combine @param butler data butler @param dataid data identifier butler @param sourcesname source butler @param match butler @param flag pass source retrieval @return match source butler.get(sourcesname dataid flag sourcesflag packedmatche butler.get(matchesname dataid astrom astrometry(config return astrom.joinmatchlistwithcatalog(packedmatche source code,"Add readMatches back to meas_astrom Recent changes to meas_astrom accidentally removed a function readMatches (copied below). Please restore it, preferably in its own module (though if someday we have more small python functions we may want a utils.py module). Also please include a unit test. {code} def readMatches(butler, dataId, sourcesName='icSrc', matchesName='icMatch', config=MeasAstromConfig(), sourcesFlags=afwTable.SOURCE_IO_NO_FOOTPRINTS): """"""Read matches, sources and catalogue; combine. @param butler Data butler @param dataId Data identifier for butler @param sourcesName Name for sources from butler @param matchesName Name for matches from butler @param sourcesFlags Flags to pass for source retrieval @returns Matches """""" sources = butler.get(sourcesName, dataId, flags=sourcesFlags) packedMatches = butler.get(matchesName, dataId) astrom = Astrometry(config) return astrom.joinMatchListWithCatalog(packedMatches, sources) {code}"
"configDictField.py has code that relies on an undefined variable While taking a linter pass on {{pex_config}} I found that {{ConfigDict.\_\_setitem\_\_}} in {{configDictField.py}} has some code that uses an undefined variable {{value}}. See the else clause in:    {code}          if oldValue is None:                          if x == dtype:                  self._dict[k] = dtype(__name=name, __at=at, __label=label)              else:                  self._dict[k] = dtype(__name=name, __at=at, __label=label, **x._storage)              if setHistory:                  self.history.append((""Added item at key %s""%k, at, label))          else:              if value == dtype:                  value = dtype()              oldValue.update(__at=at, __label=label, **value._storage)              if setHistory:                  self.history.append((""Modified item at key %s""%k, at, label))  {code}",2,DM-3635,datamanagement,"configdictfield.py code rely undefined variable take linter pass pex_config find configdict.\_\_setitem\_\ configdictfield.py code use undefined variable value clause code oldvalue dtype self._dict[k dtype(__name label label self._dict[k dtype(__name label label x._storage sethistory self.history.append((""adde item key s""%k label value dtype value dtype oldvalue.update(__at label label value._storage sethistory self.history.append((""modifie item key s""%k label code","configDictField.py has code that relies on an undefined variable While taking a linter pass on {{pex_config}} I found that {{ConfigDict.\_\_setitem\_\_}} in {{configDictField.py}} has some code that uses an undefined variable {{value}}. See the else clause in: {code} if oldValue is None: if x == dtype: self._dict[k] = dtype(__name=name, __at=at, __label=label) else: self._dict[k] = dtype(__name=name, __at=at, __label=label, **x._storage) if setHistory: self.history.append((""Added item at key %s""%k, at, label)) else: if value == dtype: value = dtype() oldValue.update(__at=at, __label=label, **value._storage) if setHistory: self.history.append((""Modified item at key %s""%k, at, label)) {code}"
"continue L1 refined specifications  Re-synchronize with prior work after vacation.    Write up page both engineering and facility database ingest  and use, incorporate suggestion and comments from KT and GDF.    Clean up (better name entities, and move for better narrative flow) pages about the general thing that is called ""level 1"",  -- the context diagram and supporting prose for further descriptions,  Review internally with Jason, Steve and Margaret.    Create a revised  diagram that would change LDM-230 specifications. (all non crosstalk corrected data flow into a disk buffer, and and an new archive tasks empties it)  to make specifications consigned with the refined specifications on the page above.  Yet to write the prose....    These pages are in my personal pages in the LSST confluence area.        ",4,DM-3636,datamanagement,continue l1 refine specification synchronize prior work vacation write page engineering facility database ingest use incorporate suggestion comment kt gdf clean well entity well narrative flow page general thing call level context diagram support prose description review internally jason steve margaret create revise diagram change ldm-230 specification non crosstalk correct datum flow disk buffer new archive task empty specification consign refined specification page write prose page personal page lsst confluence area,"continue L1 refined specifications Re-synchronize with prior work after vacation. Write up page both engineering and facility database ingest and use, incorporate suggestion and comments from KT and GDF. Clean up (better name entities, and move for better narrative flow) pages about the general thing that is called ""level 1"", -- the context diagram and supporting prose for further descriptions, Review internally with Jason, Steve and Margaret. Create a revised diagram that would change LDM-230 specifications. (all non crosstalk corrected data flow into a disk buffer, and and an new archive tasks empties it) to make specifications consigned with the refined specifications on the page above. Yet to write the prose.... These pages are in my personal pages in the LSST confluence area."
preliminary detailed content required for Authorization and Authentication system for SUIT Provide the first draft  of detailed content required for authorization and authentication system from SUIT point of view to NCSA. ,6,DM-3637,datamanagement,preliminary detailed content require authorization authentication system suit provide draft detailed content require authorization authentication system suit point view ncsa,preliminary detailed content required for Authorization and Authentication system for SUIT Provide the first draft of detailed content required for authorization and authentication system from SUIT point of view to NCSA.
"RangeField mis-handles max < min RangeField contains the following bit of code to handle the case that max < min:  {code}           if min is not None and max is not None and min > max:              swap(min, max)  {code}    This is broken because there is no swap function and if there was it could not work in-place like this. However, rather than replace this with the standard {{min, max = max, min}} I suggest we raise an exception. If max < min then this probably indicates some kind of error or sloppiness that should not be silently ignored. If we insist on swapping the values then at least we should print a warning.    The fact that this bug has never been reported strongly suggests that we never do set min > max and thus that an exception will be fine.",1,DM-3638,datamanagement,rangefield mis handle max min rangefield contain following bit code handle case max min code min max min max swap(min max code break swap function work place like replace standard min max max min suggest raise exception max min probably indicate kind error sloppiness silently ignore insist swap value print warning fact bug report strongly suggest set min max exception fine,"RangeField mis-handles max < min RangeField contains the following bit of code to handle the case that max < min: {code} if min is not None and max is not None and min > max: swap(min, max) {code} This is broken because there is no swap function and if there was it could not work in-place like this. However, rather than replace this with the standard {{min, max = max, min}} I suggest we raise an exception. If max < min then this probably indicates some kind of error or sloppiness that should not be silently ignored. If we insist on swapping the values then at least we should print a warning. The fact that this bug has never been reported strongly suggests that we never do set min > max and thus that an exception will be fine."
"OCS-CCS-DAQ-DM teleconference, April 2015 Prepare for and attend a half-day teleconference on OCS issues.",2,DM-3639,datamanagement,ocs ccs daq dm teleconference april 2015 prepare attend half day teleconference ocs issue,"OCS-CCS-DAQ-DM teleconference, April 2015 Prepare for and attend a half-day teleconference on OCS issues."
"OCS-CCS-DAQ-DM workshop III, May 2015 Prepare for and attend an OCS-subsystems workshop at SLAC May 6-8, 2015.",6,DM-3640,datamanagement,ocs ccs daq dm workshop iii 2015 prepare attend ocs subsystem workshop slac 2015,"OCS-CCS-DAQ-DM workshop III, May 2015 Prepare for and attend an OCS-subsystems workshop at SLAC May 6-8, 2015."
"install DM stack, get familiar with the current DM task concept  install DM stack, get familiar with the current DM task concept.   This is for getting ready to use task with Firefly server side extension capability.  Here is the link to the tutorial.  https://confluence.lsstcorp.org/display/DM/Getting+started+with+stack+development",6,DM-3643,datamanagement,install dm stack familiar current dm task concept install dm stack familiar current dm task concept get ready use task firefly server extension capability link tutorial https://confluence.lsstcorp.org/display/dm/getting+started+with+stack+development,"install DM stack, get familiar with the current DM task concept install DM stack, get familiar with the current DM task concept. This is for getting ready to use task with Firefly server side extension capability. Here is the link to the tutorial. https://confluence.lsstcorp.org/display/DM/Getting+started+with+stack+development"
Support the design of Firefly core system using React and FLUX Working with Loi on the design of Firefly core system based React and FLUX frameowrk,4,DM-3644,datamanagement,support design firefly core system react flux working loi design firefly core system base react flux frameowrk,Support the design of Firefly core system using React and FLUX Working with Loi on the design of Firefly core system based React and FLUX frameowrk
Support the design of Firefly core system using React and FLUX  Working with Loi on the design of Firefly core system based React and FLUX frameowork  ,4,DM-3645,datamanagement,support design firefly core system react flux working loi design firefly core system base react flux frameowork,Support the design of Firefly core system using React and FLUX Working with Loi on the design of Firefly core system based React and FLUX frameowork
"LSE-72: OCS-CCS-DAQ-DM workshop, July 2015 Work associated with Workshop IV in the series, held at NCSA July 8-10, 2015.",2,DM-3646,datamanagement,lse-72 ocs ccs daq dm workshop july 2015 work associate workshop iv series hold ncsa july 10 2015,"LSE-72: OCS-CCS-DAQ-DM workshop, July 2015 Work associated with Workshop IV in the series, held at NCSA July 8-10, 2015."
on-going support to Camera team in UIUC Attend UIUC weekly meeting and give support as needed. ,2,DM-3650,datamanagement,go support camera team uiuc attend uiuc weekly meeting support need,on-going support to Camera team in UIUC Attend UIUC weekly meeting and give support as needed.
"MakeDiscreteSkyMapRunner.__call__ mis-handled returning results {{MakeDiscreteSkyMapRunner.\_\_call\_\_}} will fail if {{self.doReturnResults}} is {{True}} due to trying to reference undefined variables. This is at least approximately a copy of a problem that was fixed in pipe_base {{TaskRunner}}.    {{MakeDiscreteSkyMapRunner.\_\_call\_\_}} should be fixed in a similar way, and (like {{TaskRunner}}) changed to return a pipe_base {{Struct}}.  ",2,DM-3651,datamanagement,makediscreteskymaprunner.__call mis handle return result makediscreteskymaprunner.\_\_call\_\ fail self.doreturnresults true try reference undefined variable approximately copy problem fix pipe_base taskrunner makediscreteskymaprunner.\_\_call\_\ fix similar way like taskrunner change return pipe_base struct,"MakeDiscreteSkyMapRunner.__call__ mis-handled returning results {{MakeDiscreteSkyMapRunner.\_\_call\_\_}} will fail if {{self.doReturnResults}} is {{True}} due to trying to reference undefined variables. This is at least approximately a copy of a problem that was fixed in pipe_base {{TaskRunner}}. {{MakeDiscreteSkyMapRunner.\_\_call\_\_}} should be fixed in a similar way, and (like {{TaskRunner}}) changed to return a pipe_base {{Struct}}."
"SUIT design document outline Work with Gregory on the SUIT design document outline    1.  Requirements flow down, making sure that we design the system satisfying the current requirements.  2.  Use cases collection. at least one typical use case in each major science theme  3.  Levels of different users  ** novice: treat the web portal as a archive to get some information, don't know much about LSST  ** novice expert: has some ideas of what special functions they would like, has some knowledge of LSST data  ** domain expert: knows LSST data very well and want some special functions ready to use  ** savvy expert: knows LSST data very well and like to use API to their own programming    4. functions for all different levels of users  5 system design   ** system diagram  ** details of the different parts  *** Firefly server  *** Firefly client  *** Firefly server extension  *** Firefly JavaScript API  *** Firefly Python API  *** Firefly Python API, Jupyter notebook, and other Python applications  *** workspace and level3 data  *** SUI web portal sketch, workflow  ** dependency on other capabilities of other institutes    6. development and test plan,  timeline  7. deployment plan                ",1,DM-3652,datamanagement,suit design document outline work gregory suit design document outline requirement flow make sure design system satisfy current requirement use case collection typical use case major science theme level different user novice treat web portal archive information know lsst novice expert idea special function like knowledge lsst data domain expert know lsst datum want special function ready use savvy expert know lsst datum like use api programming function different level user system design system diagram detail different part firefly server firefly client firefly server extension firefly javascript api firefly python api firefly python api jupyter notebook python application workspace level3 datum sui web portal sketch workflow dependency capability institutes development test plan timeline deployment plan,"SUIT design document outline Work with Gregory on the SUIT design document outline 1. Requirements flow down, making sure that we design the system satisfying the current requirements. 2. Use cases collection. at least one typical use case in each major science theme 3. Levels of different users ** novice: treat the web portal as a archive to get some information, don't know much about LSST ** novice expert: has some ideas of what special functions they would like, has some knowledge of LSST data ** domain expert: knows LSST data very well and want some special functions ready to use ** savvy expert: knows LSST data very well and like to use API to their own programming 4. functions for all different levels of users 5 system design ** system diagram ** details of the different parts *** Firefly server *** Firefly client *** Firefly server extension *** Firefly JavaScript API *** Firefly Python API *** Firefly Python API, Jupyter notebook, and other Python applications *** workspace and level3 data *** SUI web portal sketch, workflow ** dependency on other capabilities of other institutes 6. development and test plan, timeline 7. deployment plan"
SUIT design document outline work with John Rector on SUIT design document outline,1,DM-3653,datamanagement,suit design document outline work john rector suit design document outline,SUIT design document outline work with John Rector on SUIT design document outline
"Summarize current LSE-75 status as intro for new T&S personnel With the arrival of new Telescope & Site personnel, especially the Telescope Scientist, [~sthomas], prepare a summary of the current state of LSE-75 and its open issues.",4,DM-3654,datamanagement,summarize current lse-75 status intro new t&s personnel arrival new telescope site personnel especially telescope scientist ~sthomas prepare summary current state lse-75 open issue,"Summarize current LSE-75 status as intro for new T&S personnel With the arrival of new Telescope & Site personnel, especially the Telescope Scientist, [~sthomas], prepare a summary of the current state of LSE-75 and its open issues."
Data loader doesn't work for match tables qserv-data-loader.py fails to load match tables:   - it does not invoke the correct partitioner executable for them   - not all CSS parameters required for match tables are passed down to the CSS update code,1,DM-3656,datamanagement,datum loader work match table qserv-data-loader.py fail load match table invoke correct partitioner executable css parameter require match table pass css update code,Data loader doesn't work for match tables qserv-data-loader.py fails to load match tables: - it does not invoke the correct partitioner executable for them - not all CSS parameters required for match tables are passed down to the CSS update code
"Create change request for LSE-75 Create a change request for LSE-75, the TCS - to - DM ICD.",2,DM-3657,datamanagement,create change request lse-75 create change request lse-75 tcs dm icd,"Create change request for LSE-75 Create a change request for LSE-75, the TCS - to - DM ICD."
"Discussions on LSE-75 with Telescope & Site personnel Pursue interactions with Telescope and Site personnel regarding LSE-75, and in particular the issues surrounding calibration data products for the wavefront and guider data analysis pipelines.    Covers work through the end of August 2015.",3,DM-3658,datamanagement,discussion lse-75 telescope site personnel pursue interaction telescope site personnel lse-75 particular issue surround calibration datum product wavefront guider datum analysis pipeline cover work end august 2015,"Discussions on LSE-75 with Telescope & Site personnel Pursue interactions with Telescope and Site personnel regarding LSE-75, and in particular the issues surrounding calibration data products for the wavefront and guider data analysis pipelines. Covers work through the end of August 2015."
"Initial discussions with Patrick Ingraham This story is a catch-all for preliminary conversations about LSE-140 with the new Calibration Instrumentation Scientist, Patrick Ingraham.",1,DM-3659,datamanagement,initial discussion patrick ingraham story catch preliminary conversation lse-140 new calibration instrumentation scientist patrick ingraham,"Initial discussions with Patrick Ingraham This story is a catch-all for preliminary conversations about LSE-140 with the new Calibration Instrumentation Scientist, Patrick Ingraham."
"Configure VMs to provide additional slots for task switching We have to set up a new set of slots that will be used to execute the overlapping thread of execution for alert production.   Because of limited resources at this time, this means reconfiguring the worker nodes to provide additional slots, and to split the worker nodes into two sets.    ",3,DM-3661,datamanagement,configure vms provide additional slot task switch set new set slot execute overlap thread execution alert production limited resource time mean reconfigure worker node provide additional slot split worker node set,"Configure VMs to provide additional slots for task switching We have to set up a new set of slots that will be used to execute the overlapping thread of execution for alert production. Because of limited resources at this time, this means reconfiguring the worker nodes to provide additional slots, and to split the worker nodes into two sets."
"Basis for HSC integration test We need to assemble the basis for an integration test using HSC data, to protect HSC processing and obs_subaru from upstream changes.    This includes the assembly of the required data, a basic mechanism to test the mechanics of data release production, sufficient for the SQuaRE team to take it and incorporate it into the Jenkins system.  Addition of any scientific validation is deferred for now.",4,DM-3663,datamanagement,basis hsc integration test need assemble basis integration test hsc datum protect hsc processing obs_subaru upstream change include assembly require datum basic mechanism test mechanic data release production sufficient square team incorporate jenkins system addition scientific validation defer,"Basis for HSC integration test We need to assemble the basis for an integration test using HSC data, to protect HSC processing and obs_subaru from upstream changes. This includes the assembly of the required data, a basic mechanism to test the mechanics of data release production, sufficient for the SQuaRE team to take it and incorporate it into the Jenkins system. Addition of any scientific validation is deferred for now."
"rename parameter vector methods in afw.geom.ellipses [~nlust] notes that the {{writeParameters}} and {{readParameters}} methods on the ellipse classes are confusingly named, especially when compared to similar methods on {{meas.modelfit.Model}}.",2,DM-3664,datamanagement,rename parameter vector method afw.geom.ellipse ~nlust note writeparameter readparameter method ellipse class confusingly name especially compare similar method meas.modelfit model,"rename parameter vector methods in afw.geom.ellipses [~nlust] notes that the {{writeParameters}} and {{readParameters}} methods on the ellipse classes are confusingly named, especially when compared to similar methods on {{meas.modelfit.Model}}."
"improve test coverage of CModel failure modes The CModel has a large number of failure modes, largely dealing with different kinds of problems in the inputs, and a correspondingly large number of flags.  It also has some fairly complex logic determining which flags can be set simultaneously.  All of these combinations need to be tested.    DM-1574 may be useful in capturing these conditions from runs on real data.",6,DM-3665,datamanagement,improve test coverage cmodel failure mode cmodel large number failure mode largely deal different kind problem input correspondingly large number flag fairly complex logic determining flag set simultaneously combination need test dm-1574 useful capture condition run real datum,"improve test coverage of CModel failure modes The CModel has a large number of failure modes, largely dealing with different kinds of problems in the inputs, and a correspondingly large number of flags. It also has some fairly complex logic determining which flags can be set simultaneously. All of these combinations need to be tested. DM-1574 may be useful in capturing these conditions from runs on real data."
"PSFEX does not build if PLplot is installed During the configure phase PSFEX checks for the presence of PLplot. If PLplot is found then the build fails (at least on a Mac using homebrew):  {code}  /bin/sh ../libtool  --tag=CC   --mode=link clang  -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include   -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib -lfftw3f -lm  -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd  libtool: link: clang -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o  ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib /Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib/libfftw3f.dylib -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd  Undefined symbols for architecture x86_64:    ""_plwid"", referenced from:        _cplot_drawloccoordgrid in cplot.o        _cplot_fwhm in cplot.o        _cplot_ellipticity in cplot.o        _cplot_moffatresi in cplot.o        _cplot_asymresi in cplot.o        _cplot_counts in cplot.o        _cplot_countfrac in cplot.o        ...  ld: symbol(s) not found for architecture x86_64  {code}    This particular error is caused by PSFEX using a deprecated PLplot API ({{plwid}}) that is not enabled by default and whose name is not translated to {{c_plwid}}. This PLplot change occurred in version 5.9.10 released in 2013. I assume upstream PSFEX has a fix for this.    Given that LSST does not need the PLplot functionality I think the simplest fix may well be to disable the test for PLplot in our version.    It seems likely that there will be a reasonable number of systems ""in the wild"" who will have PLplot installed so I'm inclined to think that this should be a blocker for the v11.0 release.    If we are lucky people will have all upgraded their PLplot installs to v5.11.0 because in that version PLplot change the name of the library from {{libplplotd}} to {{libplplot}} and PSFEX has hard-wired the former rather than using pkg-config. This results in configure not finding PLplot. I don't think this eventuality is likely though.  ",1,DM-3667,datamanagement,psfex build plplot instal configure phase psfex check presence plplot plplot find build fail mac homebrew code sh /libtool --tag cc link clang -g -o2 -i users timj work lsstsw src psfex lapack_functions include -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o libfits.a ./levmar liblevmar.a ./wcs libwcs_c.a -l users timj work lsstsw stack darwinx86 fftw/3.3.3 g8fdba61+da39a3ee5e lib -lm -l users timj work lsstsw src psfex lapack_functions lib -llapackstub -lf2c -lm -lplplotd libtool link clang -g -o2 -i users timj work lsstsw src psfex lapack_functions include -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o libfits.a ./levmar liblevmar.a ./wcs libwcs_c.a -l users timj work lsstsw stack darwinx86 fftw/3.3.3 g8fdba61+da39a3ee5e lib /users timj work lsstsw stack darwinx86 fftw/3.3.3 g8fdba61+da39a3ee5e lib libfftw3f.dylib -l users timj work lsstsw src psfex lapack_functions lib -llapackstub -lf2c -lm -lplplotd undefined symbol architecture x86_64 plwid reference cplot_drawloccoordgrid cplot.o cplot_fwhm cplot.o cplot_ellipticity cplot.o cplot_moffatresi cplot.o cplot_asymresi cplot.o cplot_counts cplot.o cplot_countfrac cplot.o ld symbol(s find architecture code particular error cause psfex deprecate plplot api plwid enable default translate c_plwid plplot change occur version 5.9.10 release 2013 assume upstream psfex fix give lsst need plplot functionality think simple fix disable test plplot version likely reasonable number system wild plplot instal inclined think blocker v11.0 release lucky people upgrade plplot install v5.11.0 version plplot change library libplplotd libplplot psfex hard wire pkg config result configure find plplot think eventuality likely,"PSFEX does not build if PLplot is installed During the configure phase PSFEX checks for the presence of PLplot. If PLplot is found then the build fails (at least on a Mac using homebrew): {code} /bin/sh ../libtool --tag=CC --mode=link clang -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib -lfftw3f -lm -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd libtool: link: clang -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib /Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib/libfftw3f.dylib -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd Undefined symbols for architecture x86_64: ""_plwid"", referenced from: _cplot_drawloccoordgrid in cplot.o _cplot_fwhm in cplot.o _cplot_ellipticity in cplot.o _cplot_moffatresi in cplot.o _cplot_asymresi in cplot.o _cplot_counts in cplot.o _cplot_countfrac in cplot.o ... ld: symbol(s) not found for architecture x86_64 {code} This particular error is caused by PSFEX using a deprecated PLplot API ({{plwid}}) that is not enabled by default and whose name is not translated to {{c_plwid}}. This PLplot change occurred in version 5.9.10 released in 2013. I assume upstream PSFEX has a fix for this. Given that LSST does not need the PLplot functionality I think the simplest fix may well be to disable the test for PLplot in our version. It seems likely that there will be a reasonable number of systems ""in the wild"" who will have PLplot installed so I'm inclined to think that this should be a blocker for the v11.0 release. If we are lucky people will have all upgraded their PLplot installs to v5.11.0 because in that version PLplot change the name of the library from {{libplplotd}} to {{libplplot}} and PSFEX has hard-wired the former rather than using pkg-config. This results in configure not finding PLplot. I don't think this eventuality is likely though."
"obs_test needs to override map_camera and std_camera The Butler can't get a camera unless the map_camera and std_camera are defined correctly.  In most cases the camera can be built by the map_camera method.  In the case of obs_test, the camera is built in the constructor of the Mapper, so std_camera should just return the camera attribute.",1,DM-3670,datamanagement,obs_test need override map_camera std_camera butler camera map_camera std_camera define correctly case camera build map_camera method case obs_t camera build constructor mapper std_camera return camera attribute,"obs_test needs to override map_camera and std_camera The Butler can't get a camera unless the map_camera and std_camera are defined correctly. In most cases the camera can be built by the map_camera method. In the case of obs_test, the camera is built in the constructor of the Mapper, so std_camera should just return the camera attribute."
Resourcing Verification runs   Identify required resources for Verification runs and communicate them to NCSA.   ,2,DM-3675,datamanagement,resource verification run identify require resource verification run communicate ncsa,Resourcing Verification runs Identify required resources for Verification runs and communicate them to NCSA.
HSC backport: Cleanup interpolation tasks and implement useFallbackValueAtEdge This is a port of the changesets from:  [HSC-756|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-756]  ,4,DM-3677,datamanagement,hsc backport cleanup interpolation task implement usefallbackvalueatedge port changeset hsc-756|https://hsc jira.astro.princeton.edu jira browse hsc-756,HSC backport: Cleanup interpolation tasks and implement useFallbackValueAtEdge This is a port of the changesets from: [HSC-756|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-756]
"HSC backport: Standalone updates to star object selection This involves pulling over the following standalone (i.e. non-ticket) HSC commits:  [Updated star selection algorithm.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/071fcadc016908a10583c746f0a8e79df2a45ead]    [Appropriate config parameter for a unit test of testPsfDetermination.py.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/e73c5e447ac0b8a71926d3e78fec30aad4beee91]    [Remove HSC specific codes.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/15bb812578531766199e9a1ee41cc707fb3d9873]  (Note, the above reverts some unwanted camera-specific clauses added in the first commit.  May just squash them to only add the desired features)    [ObjectSizeStarSelector: push non-fatal errors to DEBUG level|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/44f75bc60b41c5f77b323a8d9981048ef7e5f3c4]    [We don't use focal plane coordinates anywhere, and detector may be None|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/4413db4610e4793727e591f395f5ad8cd0cb6030]    [Fixed axis labels|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/67efacaccf8346fdfa1b450617aebabddb2b7ec0]    [Improved PSF debugging plots|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/b1bc91ed1538607eb90e070881a82498fd551909]    [Worked on star selector|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/6b36f4d757187d30142a7e026754a07ffeb8dea2]",1,DM-3678,datamanagement,hsc backport standalone update star object selection involve pull follow standalone i.e. non ticket hsc commit update star selection algorithm.|https://github.com hypersuprime cam meas_algorithms commit/071fcadc016908a10583c746f0a8e79df2a45ead appropriate config parameter unit test testpsfdetermination.py.|https://github.com hypersuprime cam meas_algorithms commit e73c5e447ac0b8a71926d3e78fec30aad4beee91 remove hsc specific codes.|https://github.com hypersuprime cam meas_algorithms commit/15bb812578531766199e9a1ee41cc707fb3d9873 note revert unwanted camera specific clause add commit squash add desire feature objectsizestarselector push non fatal error debug level|https://github.com hypersuprime cam meas_algorithms commit/44f75bc60b41c5f77b323a8d9981048ef7e5f3c4 use focal plane coordinate detector none|https://github.com hypersuprime cam meas_algorithms commit/4413db4610e4793727e591f395f5ad8cd0cb6030 fix axis labels|https://github.com hypersuprime cam meas_algorithms improved psf debug plots|https://github.com hypersuprime cam meas_algorithms commit b1bc91ed1538607eb90e070881a82498fd551909 work star selector|https://github.com hypersuprime cam meas_algorithms commit/6b36f4d757187d30142a7e026754a07ffeb8dea2,"HSC backport: Standalone updates to star object selection This involves pulling over the following standalone (i.e. non-ticket) HSC commits: [Updated star selection algorithm.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/071fcadc016908a10583c746f0a8e79df2a45ead] [Appropriate config parameter for a unit test of testPsfDetermination.py.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/e73c5e447ac0b8a71926d3e78fec30aad4beee91] [Remove HSC specific codes.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/15bb812578531766199e9a1ee41cc707fb3d9873] (Note, the above reverts some unwanted camera-specific clauses added in the first commit. May just squash them to only add the desired features) [ObjectSizeStarSelector: push non-fatal errors to DEBUG level|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/44f75bc60b41c5f77b323a8d9981048ef7e5f3c4] [We don't use focal plane coordinates anywhere, and detector may be None|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/4413db4610e4793727e591f395f5ad8cd0cb6030] [Fixed axis labels|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/67efacaccf8346fdfa1b450617aebabddb2b7ec0] [Improved PSF debugging plots|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/b1bc91ed1538607eb90e070881a82498fd551909] [Worked on star selector|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/6b36f4d757187d30142a7e026754a07ffeb8dea2]"
"Allow building/publishing components off branches other than master Support of xrootd within the stack is currently complicated by the fact that qserv depends on features that are not available on upstream master (only available on an upstream non-master branch).  Since we can currently only publish packages from master, this means that our lsst fork of xrootd cannot be a ""pure"" fork -- we end up merging/rebasing from an upstream branch, then force-pushing the downstream master.  Upstream and downstream xrootd repos thus have completely different branch topologies, labels, etc., and history of master in the lsst fork is being continually rewritten to carry local patches forward.  The processes of both adopting upstream changes into the lsst fork and the pushing lsst changes back upstream are cumbersome, confusing, and labor intensive.    It is proposed that we extend our tools to allow publishing components from branches other than master.  This would allow us to have xrootd for example be a ""pure"" fork of upstream -- we could then create our own branch based off any upstream branch, carry our downstream patches there, and release off of that.    This functionality could be used similarly for any of our current ""t&p"" components where it would be convenient to track the upstream repo directly and/or carry changes in git instead of in an agglomerated patch file (e.g. when we might want to update frequently and/or contribute general purpose changes back upstream regularly with pr's, etc.)",2,DM-3679,datamanagement,allow building publishing component branch master support xrootd stack currently complicate fact qserv depend feature available upstream master available upstream non master branch currently publish package master mean lsst fork xrootd pure fork end merge rebase upstream branch force push downstream master upstream downstream xrootd repos completely different branch topology label etc history master lsst fork continually rewrite carry local patch forward process adopt upstream change lsst fork push lsst change upstream cumbersome confusing labor intensive propose extend tool allow publishing component branch master allow xrootd example pure fork upstream create branch base upstream branch carry downstream patch release functionality similarly current t&p component convenient track upstream repo directly and/or carry change git instead agglomerated patch file e.g. want update frequently and/or contribute general purpose change upstream regularly pr etc,"Allow building/publishing components off branches other than master Support of xrootd within the stack is currently complicated by the fact that qserv depends on features that are not available on upstream master (only available on an upstream non-master branch). Since we can currently only publish packages from master, this means that our lsst fork of xrootd cannot be a ""pure"" fork -- we end up merging/rebasing from an upstream branch, then force-pushing the downstream master. Upstream and downstream xrootd repos thus have completely different branch topologies, labels, etc., and history of master in the lsst fork is being continually rewritten to carry local patches forward. The processes of both adopting upstream changes into the lsst fork and the pushing lsst changes back upstream are cumbersome, confusing, and labor intensive. It is proposed that we extend our tools to allow publishing components from branches other than master. This would allow us to have xrootd for example be a ""pure"" fork of upstream -- we could then create our own branch based off any upstream branch, carry our downstream patches there, and release off of that. This functionality could be used similarly for any of our current ""t&p"" components where it would be convenient to track the upstream repo directly and/or carry changes in git instead of in an agglomerated patch file (e.g. when we might want to update frequently and/or contribute general purpose changes back upstream regularly with pr's, etc.)"
"Fix PATH and compiler version detection in qserv scons In recently merged DM-3662 compiler version testing was done using OS tools with regular $PATH. This is inconsistent with other scons tools which reset PATH when executing actions.   We want to do two things:  - propagate PATH to the command execution  - Use scons tools to run ""$CXX --version"" instead of OS tools to keep things consistent",1,DM-3686,datamanagement,fix path compiler version detection qserv scon recently merge dm-3662 compiler version testing os tool regular path inconsistent scon tool reset path execute action want thing propagate path command execution use scon tool run cxx instead os tool thing consistent,"Fix PATH and compiler version detection in qserv scons In recently merged DM-3662 compiler version testing was done using OS tools with regular $PATH. This is inconsistent with other scons tools which reset PATH when executing actions. We want to do two things: - propagate PATH to the command execution - Use scons tools to run ""$CXX --version"" instead of OS tools to keep things consistent"
Revisit KPIs for Image Access Need to come up with KPIs for Image Query Access,4,DM-3687,datamanagement,revisit kpis image access need come kpi image query access,Revisit KPIs for Image Access Need to come up with KPIs for Image Query Access
lsst_dm_stack_demo failure Viz:    {code}  $ ./bin/demo.sh  [...]  $ bin/compare detected-sources.txt.expected detected-sources.txt  Failed (max difference 0.439326 over tolerance 0.004000) in column base_GaussianFlux_flux.  Failed (max difference 0.439326 over tolerance 0.004000) in column base_GaussianFlux_fluxSigma.  Failed (max difference 0.244707 over tolerance 0.004000) in column base_PsfFlux_flux.  Failed (max difference 0.244707 over tolerance 0.004000) in column base_PsfFlux_fluxSigma.  {code}    This is on OS X with {{lsst_apps}} {{w_2015_36}}.    ,2,DM-3688,datamanagement,lsst_dm_stack_demo failure viz code ./bin demo.sh ... bin compare detected-sources.txt.expected detected-sources.txt fail max difference 0.439326 tolerance 0.004000 column base_gaussianflux_flux fail max difference 0.439326 tolerance 0.004000 column base_gaussianflux_fluxsigma fail max difference 0.244707 tolerance 0.004000 column base_psfflux_flux fail max difference 0.244707 tolerance 0.004000 column base_psfflux_fluxsigma code os lsst_apps w_2015_36,lsst_dm_stack_demo failure Viz: {code} $ ./bin/demo.sh [...] $ bin/compare detected-sources.txt.expected detected-sources.txt Failed (max difference 0.439326 over tolerance 0.004000) in column base_GaussianFlux_flux. Failed (max difference 0.439326 over tolerance 0.004000) in column base_GaussianFlux_fluxSigma. Failed (max difference 0.244707 over tolerance 0.004000) in column base_PsfFlux_flux. Failed (max difference 0.244707 over tolerance 0.004000) in column base_PsfFlux_fluxSigma. {code} This is on OS X with {{lsst_apps}} {{w_2015_36}}.
Forward community.lsst.org (Discourse) notifications to existing mailman lists Setup a system to forward new post notifications from http://community.lsst.org categories to their appropriate legacy Mailman email list counterparts.    ||Discourse Source Category||Mailmain Forward List||  |DM Team||dm-staff|  |Announcements||dm-announce|  |DM Notifications||dm-devel|    Once this is implemented I will send a notice that all communications and replies should occur on discourse (these mailing lists should be read-only).    I will also send a notice that dm-users is deprecated.  ,7,DM-3690,datamanagement,forward community.lsst.org discourse notification exist mailman list setup system forward new post notification category appropriate legacy mailman email list counterpart ||discourse source category||mailmain forward list|| team||dm staff| |announcements||dm announce| |dm notifications||dm devel| implement send notice communication reply occur discourse mailing list read send notice dm user deprecate,Forward community.lsst.org (Discourse) notifications to existing mailman lists Setup a system to forward new post notifications from http://community.lsst.org categories to their appropriate legacy Mailman email list counterparts. ||Discourse Source Category||Mailmain Forward List|| |DM Team||dm-staff| |Announcements||dm-announce| |DM Notifications||dm-devel| Once this is implemented I will send a notice that all communications and replies should occur on discourse (these mailing lists should be read-only). I will also send a notice that dm-users is deprecated.
"CalibrateTask has outdated, incorrect code for handling aperture corrections The CFHT-specific CalibrateTask tries to apply aperture correction once just after measuring it (which is too early) and again later, at the right time. The error probably has no effect on the final results, but it is confusing and needlessly divergent from the standard CalibrateTask. The required changes are small. I plan to test by running [~boutigny]'s CFHT demo.",1,DM-3691,datamanagement,calibratetask outdate incorrect code handle aperture correction cfht specific calibratetask try apply aperture correction measure early later right time error probably effect final result confusing needlessly divergent standard calibratetask require change small plan test run ~boutigny cfht demo,"CalibrateTask has outdated, incorrect code for handling aperture corrections The CFHT-specific CalibrateTask tries to apply aperture correction once just after measuring it (which is too early) and again later, at the right time. The error probably has no effect on the final results, but it is confusing and needlessly divergent from the standard CalibrateTask. The required changes are small. I plan to test by running [~boutigny]'s CFHT demo."
HSC backport: Allow for some fraction of PSF Candidates to be reserved from the fitting This is a port of the changesets from [HSC-966|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-966].    It provides the ability to reserve some fraction of PSF candidates from the PSF fitting in order to check for overfitting and do cross validation.,1,DM-3692,datamanagement,hsc backport allow fraction psf candidates reserve fitting port changeset hsc-966|https://hsc jira.astro.princeton.edu jira browse hsc-966 provide ability reserve fraction psf candidate psf fit order check overfitting cross validation,HSC backport: Allow for some fraction of PSF Candidates to be reserved from the fitting This is a port of the changesets from [HSC-966|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-966]. It provides the ability to reserve some fraction of PSF candidates from the PSF fitting in order to check for overfitting and do cross validation.
"HSC backport: allow photometric and astrometric calibrations to be required This is a port of the standalone changesets:  [calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/e9db5c0dcdca20e8f7ba71f24f8b797e71699352]  [fixup! calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/c2d89396923f9d589822c043ed8753647e70f3f6]  (the above is a fixup, so will likely be squashed)  [make failure to match sources non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/cf5724b852937cfcef1b71b7a372552011fda670]  [calibrate: restore original Wcs after initial astrometry solution|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/ab6cb9e206d0456dc764c5ef78ac80ece937c610]  [move CalibrateTask from ProcessImageTask into ProcessCcdTask|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/08a8ec029dd52ac55e47b707a6905df061a40506]  [processCoadd: set detection to use the declared variances|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/9e8563fd8d630dad967786387b1f27b6bc7ee039]  [adapt to removal of CalibrateTask from ProcessImageTask in pipe_tasks|https://github.com/HyperSuprime-Cam/obs_subaru/commit/52733a7ab1731a15cbb93151851f57cec276f928]  and HSC tickets:  [HSC-1085: background not saved in processCcd|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1085] and  [HSC-1086: psf - catalog scatter is very large in some coadds|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1086]",2,DM-3693,datamanagement,hsc backport allow photometric astrometric calibration require port standalone changeset calibrate astrometry failure non fatal|https://github.com hypersuprime cam pipe_tasks commit e9db5c0dcdca20e8f7ba71f24f8b797e71699352 fixup calibrate astrometry failure non fatal|https://github.com hypersuprime cam pipe_tasks commit c2d89396923f9d589822c043ed8753647e70f3f6 fixup likely squash failure match source non fatal|https://github.com hypersuprime cam pipe_tasks commit cf5724b852937cfcef1b71b7a372552011fda670 calibrate restore original wcs initial astrometry solution|https://github.com hypersuprime cam pipe_tasks commit ab6cb9e206d0456dc764c5ef78ac80ece937c610 calibratetask processimagetask processccdtask|https://github.com hypersuprime cam pipe_tasks commit/08a8ec029dd52ac55e47b707a6905df061a40506 processcoadd set detection use declare variances|https://github.com hypersuprime cam pipe_tasks commit/9e8563fd8d630dad967786387b1f27b6bc7ee039 adapt removal calibratetask processimagetask pipe_tasks|https://github.com hypersuprime cam obs_subaru commit/52733a7ab1731a15cbb93151851f57cec276f928 hsc ticket hsc-1085 background save processccd|https://hsc jira.astro.princeton.edu jira browse hsc-1085 hsc-1086 psf catalog scatter large coadds|https://hsc jira.astro.princeton.edu jira browse hsc-1086,"HSC backport: allow photometric and astrometric calibrations to be required This is a port of the standalone changesets: [calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/e9db5c0dcdca20e8f7ba71f24f8b797e71699352] [fixup! calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/c2d89396923f9d589822c043ed8753647e70f3f6] (the above is a fixup, so will likely be squashed) [make failure to match sources non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/cf5724b852937cfcef1b71b7a372552011fda670] [calibrate: restore original Wcs after initial astrometry solution|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/ab6cb9e206d0456dc764c5ef78ac80ece937c610] [move CalibrateTask from ProcessImageTask into ProcessCcdTask|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/08a8ec029dd52ac55e47b707a6905df061a40506] [processCoadd: set detection to use the declared variances|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/9e8563fd8d630dad967786387b1f27b6bc7ee039] [adapt to removal of CalibrateTask from ProcessImageTask in pipe_tasks|https://github.com/HyperSuprime-Cam/obs_subaru/commit/52733a7ab1731a15cbb93151851f57cec276f928] and HSC tickets: [HSC-1085: background not saved in processCcd|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1085] and [HSC-1086: psf - catalog scatter is very large in some coadds|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1086]"
"Decrease buildbot  frequency Buildbot frequency is now down to two builds, one at 19:42 machine time (NCSA) and one at 1:42. This is to stop people needing buildbot runs to eups publish to have to wait before a CI build, since they are now done on[ https://ci.lsst.codes ]/ Jenkins.     ",1,DM-3694,datamanagement,decrease buildbot frequency buildbot frequency build 19:42 machine time ncsa 1:42 stop people need buildbot run eup publish wait ci build https://ci.lsst.code jenkins,"Decrease buildbot frequency Buildbot frequency is now down to two builds, one at 19:42 machine time (NCSA) and one at 1:42. This is to stop people needing buildbot runs to eups publish to have to wait before a CI build, since they are now done on[ https://ci.lsst.codes ]/ Jenkins."
"Add unit tests for secondary index qproc::testIndexMap.cc is very sketchy and doesn't perform any test for now (i.e. no BOOST_CHECK). It should be improved to really cover code related to secondary index. A mock secondary index is required here, i.e. qproc::FakeBacken should be strengthened.",8,DM-3695,datamanagement,add unit test secondary index qproc::testindexmap.cc sketchy perform test i.e. boost_check improve cover code relate secondary index mock secondary index require i.e. qproc::fakebacken strengthen,"Add unit tests for secondary index qproc::testIndexMap.cc is very sketchy and doesn't perform any test for now (i.e. no BOOST_CHECK). It should be improved to really cover code related to secondary index. A mock secondary index is required here, i.e. qproc::FakeBacken should be strengthened."
"Install squid proxy on cc-in2p3 build node Can we add this one to current sprint? It is required to access docker hub on in2p3 cluster.    I also need to automate/document it, test it on build nodes, and be reviewed by in2p3 sysadmins.    Cheers",4,DM-3697,datamanagement,install squid proxy cc in2p3 build node add current sprint require access docker hub in2p3 cluster need automate document test build nodes review in2p3 sysadmin cheer,"Install squid proxy on cc-in2p3 build node Can we add this one to current sprint? It is required to access docker hub on in2p3 cluster. I also need to automate/document it, test it on build nodes, and be reviewed by in2p3 sysadmins. Cheers"
"Replace --trace with --loglevel in pipe_base ArgumentParser Replace the --trace argument with an enhanced version of --loglevel that supports named values and numeric log levels (which are the negative of trace levels). This simplifies the interface for users and potentially reduces the log level/trace level confusion, though that won't fully happen until we finish replacing use of pex_logging Trace and Debug with Log.    This work was already done as part of DM-3532; it just needs to be copied with minor changes (since there are no named trace levels in pex_logging).",1,DM-3698,datamanagement,replace --trace pipe_base argumentparser replace --trace argument enhanced version support name value numeric log level negative trace level simplifie interface user potentially reduce log level trace level confusion will fully happen finish replace use pex_logge trace debug log work dm-3532 need copy minor change name trace level pex_logging,"Replace --trace with --loglevel in pipe_base ArgumentParser Replace the --trace argument with an enhanced version of --loglevel that supports named values and numeric log levels (which are the negative of trace levels). This simplifies the interface for users and potentially reduces the log level/trace level confusion, though that won't fully happen until we finish replacing use of pex_logging Trace and Debug with Log. This work was already done as part of DM-3532; it just needs to be copied with minor changes (since there are no named trace levels in pex_logging)."
Setup and conduct a conversation about DCR in the project Advertise and conduct a broadly advertised videocon on DCR in the context of diffim.  The result of this should be minutes.  Ideally we would come out of this meeting with a list of possible techniques for dealing with DCR (preferentially sorted by priority).,4,DM-3701,datamanagement,setup conduct conversation dcr project advertise conduct broadly advertise videocon dcr context diffim result minute ideally come meeting list possible technique deal dcr preferentially sort priority,Setup and conduct a conversation about DCR in the project Advertise and conduct a broadly advertised videocon on DCR in the context of diffim. The result of this should be minutes. Ideally we would come out of this meeting with a list of possible techniques for dealing with DCR (preferentially sorted by priority).
"Port suspect pixel flags to meas_base Pull HSC pixelFlags for suspect and suspect center over from {{meas_algorithms}} to {{meas_base}}. Additionally there are a few places in {{meas_base}} (& possibly in {{_algorithms}} as well) which set flags that have comments such as ""Set suspect flag if it was available"". Each of these places should be updated to use the ported bit. The relevant commit can be found at https://github.com/HyperSuprime-Cam/meas_algorithms/commit/21be65187c30302abb430d59fc5f67730ca7e0a1 and is discussed at https://dev.lsstcorp.org/trac/ticket/2838    It may also be necessary to add or update a unit test to make sure the flag is set appropriately.",4,DM-3703,datamanagement,port suspect pixel flag meas_base pull hsc pixelflag suspect suspect center meas_algorithms meas_base additionally place meas_base possibly algorithm set flag comment set suspect flag available place update use ported bit relevant commit find https://github.com/hypersuprime-cam/meas_algorithms/commit/21be65187c30302abb430d59fc5f67730ca7e0a1 discuss https://dev.lsstcorp.org/trac/ticket/2838 necessary add update unit test sure flag set appropriately,"Port suspect pixel flags to meas_base Pull HSC pixelFlags for suspect and suspect center over from {{meas_algorithms}} to {{meas_base}}. Additionally there are a few places in {{meas_base}} (& possibly in {{_algorithms}} as well) which set flags that have comments such as ""Set suspect flag if it was available"". Each of these places should be updated to use the ported bit. The relevant commit can be found at https://github.com/HyperSuprime-Cam/meas_algorithms/commit/21be65187c30302abb430d59fc5f67730ca7e0a1 and is discussed at https://dev.lsstcorp.org/trac/ticket/2838 It may also be necessary to add or update a unit test to make sure the flag is set appropriately."
"fix EventLog references in ctrl_orca There are a couple references to EventLog in ctrl_orca, which is an object that no longer exists.",4,DM-3706,datamanagement,fix eventlog reference ctrl_orca couple reference eventlog ctrl_orca object long exist,"fix EventLog references in ctrl_orca There are a couple references to EventLog in ctrl_orca, which is an object that no longer exists."
"qserv scons - do not copy files to variant_dir Some people are not happy with our current scons setup which copies source files from source directories to variant_dir, it makes it harder to trace errors using tools like eclipse or debug code. Would be nice to get rid of the extra copy, but we still want to have separate build directory (variant_dir). It should be simple enough, I think, but will need some testing of course.",2,DM-3707,datamanagement,qserv scon copy file variant_dir people happy current scon setup copy source file source directory variant_dir make hard trace error tool like eclipse debug code nice rid extra copy want separate build directory variant_dir simple think need testing course,"qserv scons - do not copy files to variant_dir Some people are not happy with our current scons setup which copies source files from source directories to variant_dir, it makes it harder to trace errors using tools like eclipse or debug code. Would be nice to get rid of the extra copy, but we still want to have separate build directory (variant_dir). It should be simple enough, I think, but will need some testing of course."
"Scons build of lapack_functions in PSFex fails if SCONSFLAGS are set The scons build system is unaware of extra flags which may be set in SCONSFLAGS environment variable, which are used from scons utils. This will cause the build to fail. The package needs to behave properly and build in the presence of these flags",2,DM-3749,datamanagement,scon build lapack_function psfex fail sconsflags set scon build system unaware extra flag set sconsflags environment variable scon util cause build fail package need behave properly build presence flag,"Scons build of lapack_functions in PSFex fails if SCONSFLAGS are set The scons build system is unaware of extra flags which may be set in SCONSFLAGS environment variable, which are used from scons utils. This will cause the build to fail. The package needs to behave properly and build in the presence of these flags"
"Expose table metadata via metaserv SUI team would find it useful to get counts of columns for a table, ideally, all counts of columns for all tables in a given database in one request. They'd also find it useful if we could send column description.",6,DM-3758,datamanagement,expose table metadata metaserv sui team find useful count column table ideally count column table give database request find useful send column description,"Expose table metadata via metaserv SUI team would find it useful to get counts of columns for a table, ideally, all counts of columns for all tables in a given database in one request. They'd also find it useful if we could send column description."
"Sizing model storage costing update The question that is being raised is how much it would cost to keep more than the last two Data Releases on readily-accessible storage (i.e. spinning disk). This will require changing several numbers and formulas in LDM-141, the storage sizing model, observing the results as they propagate through LDM-144, the cost model, and checking to make sure that everything makes sense and nothing has been overlooked.    Assignees: Jason Alt  Duration: September 2015",6,DM-3761,datamanagement,size model storage costing update question raise cost data releases readily accessible storage i.e. spinning disk require change number formula ldm-141 storage size model observe result propagate ldm-144 cost model check sure make sense overlook assignee jason alt duration september 2015,"Sizing model storage costing update The question that is being raised is how much it would cost to keep more than the last two Data Releases on readily-accessible storage (i.e. spinning disk). This will require changing several numbers and formulas in LDM-141, the storage sizing model, observing the results as they propagate through LDM-144, the cost model, and checking to make sure that everything makes sense and nothing has been overlooked. Assignees: Jason Alt Duration: September 2015"
"Czar dies when parser throws exception Running a query that mistakenly uses scisql_s2PtInBox instead of qserv_s2PtInBox    {code}select objectId, coord_ra, coord_dec   from smm_bremerton.deepCoadd_forced_src   where scisql_s2PtInBox(coord_ra, coord_dec, 320.05, 0.457, 320.06, 0.46){code}    kills czar, the error message in czar log file is:    {code}ERROR ccontrol.UserQueryFactory (build/ccontrol/UserQueryFactory.cc:117)   - Invalid query: ParseException:Parse error(ANTLR):unexpected token:  scisql_s2PtInBox:{code}    We need to change the code so that a random query does not kill czar. This story involves fixing czar so that it does not die when parser chokes on the syntax.  ",6,DM-3764,datamanagement,czar die parser throw exception run query mistakenly use scisql_s2ptinbox instead qserv_s2ptinbox code}select objectid coord_ra coord_dec smm_bremerton.deepcoadd_forced_src scisql_s2ptinbox(coord_ra coord_dec 320.05 0.457 320.06 0.46){code kill czar error message czar log file code}error ccontrol userqueryfactory build ccontrol userqueryfactory.cc:117 invalid query parseexception parse error(antlr):unexpecte token need change code random query kill czar story involve fix czar die parser choke syntax,"Czar dies when parser throws exception Running a query that mistakenly uses scisql_s2PtInBox instead of qserv_s2PtInBox {code}select objectId, coord_ra, coord_dec from smm_bremerton.deepCoadd_forced_src where scisql_s2PtInBox(coord_ra, coord_dec, 320.05, 0.457, 320.06, 0.46){code} kills czar, the error message in czar log file is: {code}ERROR ccontrol.UserQueryFactory (build/ccontrol/UserQueryFactory.cc:117) - Invalid query: ParseException:Parse error(ANTLR):unexpected token: scisql_s2PtInBox:{code} We need to change the code so that a random query does not kill czar. This story involves fixing czar so that it does not die when parser chokes on the syntax."
Resolve the issues found in the S15 end-to-end system exercise There are a few items we need to take care to finish the end-to-end system for S15. ,8,DM-3768,datamanagement,resolve issue find s15 end end system exercise item need care finish end end system s15,Resolve the issues found in the S15 end-to-end system exercise There are a few items we need to take care to finish the end-to-end system for S15.
access the database created and populated for Bremerton end-to-end system Collect the information for the tables populated for Bremerton end-to-end exercise. Use them in SUI/T so we can access them using the DAX API. ,2,DM-3769,datamanagement,access database create populate bremerton end end system collect information table populate bremerton end end exercise use sui access dax api,access the database created and populated for Bremerton end-to-end system Collect the information for the tables populated for Bremerton end-to-end exercise. Use them in SUI/T so we can access them using the DAX API.
"build the SUI system on NCSA to use the right database and tables Due to the changes of the database and tables, the system has to be rebuilt.",1,DM-3770,datamanagement,build sui system ncsa use right database table change database table system rebuild,"build the SUI system on NCSA to use the right database and tables Due to the changes of the database and tables, the system has to be rebuilt."
Resolve the issues accessing the newly populated tables There are several issues need to be resolved for the system to work properly. ,5,DM-3771,datamanagement,resolve issue access newly populate table issue need resolve system work properly,Resolve the issues accessing the newly populated tables There are several issues need to be resolved for the system to work properly.
Fix compiler detection for non-default gcc/g++ compiler {{scons CXX=g+\+-4.4}} launches {{g\+\+-4.4 --version}} which returns {{g++-4.4 (Debian 4.4.7-2) 4.4.7}}. Nevertheless the {{-4.4}} is not supported by Qserv compiler detection tool. Support will be added here,1,DM-3772,datamanagement,fix compiler detection non default gcc g++ compiler scon cxx g+\+-4.4 launch g\+\+-4.4 --version return g++-4.4 debian 4.4.7 4.4.7 -4.4 support qserv compiler detection tool support add,Fix compiler detection for non-default gcc/g++ compiler {{scons CXX=g+\+-4.4}} launches {{g\+\+-4.4 --version}} which returns {{g++-4.4 (Debian 4.4.7-2) 4.4.7}}. Nevertheless the {{-4.4}} is not supported by Qserv compiler detection tool. Support will be added here
add RUNID option to EventAppender A RUNID needs to be added as an option to EventAppender to allow event logging selectors to receive only events for a particular run.,3,DM-3773,datamanagement,add runid option eventappender runid need add option eventappender allow event log selector receive event particular run,add RUNID option to EventAppender A RUNID needs to be added as an option to EventAppender to allow event logging selectors to receive only events for a particular run.
"lsst_build's default ref from repos.yaml support is broken when building multiple packages A problem with the default ref in {{repos.yaml}} support implemented in DM-3679 was discovered last Friday, shortly after deploying this feature to the production CI systems.    The default ref for {{xrootd}} was changed/overridden in {{repos.yaml}} to {{legacy/master}}.  This worked as expected (and as was tested) when setting {{xrootd}} as the sole {{lsstswBuild.sh}} product or when running {{rebuild}} by hand.  However, when building any package that pulled in {{xrootd}} as a recursive dependency, the {{master}} branch was being used (this case had not been manually tested).",1,DM-3774,datamanagement,lsst_build default ref repos.yaml support break build multiple package problem default ref repos.yaml support implement dm-3679 discover friday shortly deploy feature production ci system default ref xrootd change overridden repos.yaml legacy master work expect test set xrootd sole lsstswbuild.sh product run rebuild hand build package pull xrootd recursive dependency master branch case manually test,"lsst_build's default ref from repos.yaml support is broken when building multiple packages A problem with the default ref in {{repos.yaml}} support implemented in DM-3679 was discovered last Friday, shortly after deploying this feature to the production CI systems. The default ref for {{xrootd}} was changed/overridden in {{repos.yaml}} to {{legacy/master}}. This worked as expected (and as was tested) when setting {{xrootd}} as the sole {{lsstswBuild.sh}} product or when running {{rebuild}} by hand. However, when building any package that pulled in {{xrootd}} as a recursive dependency, the {{master}} branch was being used (this case had not been manually tested)."
HSC backport: updates to tract and patch finding This is a port of the following HSC updates to how tracts and patches are found and listed given a set of coordinates.  These are all standalone commits (i.e. not associated with a ticket):  [Add findTract() and findTractPatchList() in ringsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/761e915dde25ce8ed5622c2d84b83793e9580fd7]  [move RingsSkyMap.findTractPatchList to BaseSkyMap.findClosestTractPatchlist|https://github.com/HyperSuprime-Cam/skymap/commit/56476142060bdb7d8c7fb59eacc383f0e0d5c85b]  [Small bug fix for RingsSkyMap.findTract().|https://github.com/HyperSuprime-Cam/skymap/commit/f202a7780ebb89166f03479d7447ace1555027c1]  [Add fast findTractPatchList() in RingsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/7e49c358501f95ce4c0e1aa8f48103a24391fc22]  [Fixed the problems regarding poles and RA wrap.|https://github.com/HyperSuprime-Cam/skymap/commit/841b0c9eda7462a7a4f182b7971d5e8e81478bfe]  [Add spaces around '+' and '-' to match LSST standard coding style.|https://github.com/HyperSuprime-Cam/skymap/commit/f7e2f036494afe382e653194c82bb15728c60fc3],1,DM-3775,datamanagement,hsc backport update tract patch find port follow hsc update tract patch find list give set coordinate standalone commit i.e. associate ticket add findtract findtractpatchlist ringsskymap.|https://github.com hypersuprime cam skymap commit/761e915dde25ce8ed5622c2d84b83793e9580fd7 ringsskymap.findtractpatchlist baseskymap.findclosesttractpatchlist|https://github.com hypersuprime cam skymap commit/56476142060bdb7d8c7fb59eacc383f0e0d5c85b small bug fix ringsskymap.findtract().|https://github.com hypersuprime cam skymap commit f202a7780ebb89166f03479d7447ace1555027c1 add fast findtractpatchlist ringsskymap.|https://github.com hypersuprime cam skymap commit/7e49c358501f95ce4c0e1aa8f48103a24391fc22 fix problem pole ra wrap.|https://github.com hypersuprime cam skymap commit/841b0c9eda7462a7a4f182b7971d5e8e81478bfe add space match lsst standard code style.|https://github.com hypersuprime cam skymap commit f7e2f036494afe382e653194c82bb15728c60fc3,HSC backport: updates to tract and patch finding This is a port of the following HSC updates to how tracts and patches are found and listed given a set of coordinates. These are all standalone commits (i.e. not associated with a ticket): [Add findTract() and findTractPatchList() in ringsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/761e915dde25ce8ed5622c2d84b83793e9580fd7] [move RingsSkyMap.findTractPatchList to BaseSkyMap.findClosestTractPatchlist|https://github.com/HyperSuprime-Cam/skymap/commit/56476142060bdb7d8c7fb59eacc383f0e0d5c85b] [Small bug fix for RingsSkyMap.findTract().|https://github.com/HyperSuprime-Cam/skymap/commit/f202a7780ebb89166f03479d7447ace1555027c1] [Add fast findTractPatchList() in RingsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/7e49c358501f95ce4c0e1aa8f48103a24391fc22] [Fixed the problems regarding poles and RA wrap.|https://github.com/HyperSuprime-Cam/skymap/commit/841b0c9eda7462a7a4f182b7971d5e8e81478bfe] [Add spaces around '+' and '-' to match LSST standard coding style.|https://github.com/HyperSuprime-Cam/skymap/commit/f7e2f036494afe382e653194c82bb15728c60fc3]
"LDM-144 Consistency Update Due to the 2 year gap between the original authoring date of LDM-144 and the recent update,  the 'costing only' update caused the document to be less self consistent than desired. This work is to do more than 'costing' updates such that the document is useful and on target to be more than a costing umbrella.",3,DM-3776,datamanagement,ldm-144 consistency update year gap original author date ldm-144 recent update cost update cause document self consistent desire work cost update document useful target cost umbrella,"LDM-144 Consistency Update Due to the 2 year gap between the original authoring date of LDM-144 and the recent update, the 'costing only' update caused the document to be less self consistent than desired. This work is to do more than 'costing' updates such that the document is useful and on target to be more than a costing umbrella."
Fix compiler warns in protobuf clients Google protobufs 2.6.1 includes a few unnecessary semicolons in some of its supplied header files; these generate a lot of compiler warnings when compiling client packages.    Proposed fix is to add a patch to our eups t&p protobufs package to remove the offending semicolons.,1,DM-3778,datamanagement,fix compiler warn protobuf client google protobufs 2.6.1 include unnecessary semicolon supply header file generate lot compiler warning compile client package propose fix add patch eup t&p protobufs package remove offend semicolon,Fix compiler warns in protobuf clients Google protobufs 2.6.1 includes a few unnecessary semicolons in some of its supplied header files; these generate a lot of compiler warnings when compiling client packages. Proposed fix is to add a patch to our eups t&p protobufs package to remove the offending semicolons.
"clean up gcc and eclipse code analyzer warns We've been ignoring some accumulating warns in the qserv build for some time now.  Now that it is possible to develop qserv in eclipse, it would be useful to address warns and analyzer issues so that we can start to notice when new ones pop up.",1,DM-3779,datamanagement,clean gcc eclipse code analyzer warn ignore accumulate warn qserv build time possible develop qserv eclipse useful address warn analyzer issue start notice new one pop,"clean up gcc and eclipse code analyzer warns We've been ignoring some accumulating warns in the qserv build for some time now. Now that it is possible to develop qserv in eclipse, it would be useful to address warns and analyzer issues so that we can start to notice when new ones pop up."
"Rationalize lsst/xrootd repo and maintenance procedures The procedure for pulling/pushing xrootd changes from/to the upstream official xrootd repo is cumbersome, confusing, and error-prone.    Buildbot now has support for releasing packages from branches other than master.  Given this, we can now reasonably replace our lsst/xrootd repo with a fresh genuine fork (shared history) of upstream, then carry our lsst-specific work forward on a dev-branch.  This will make it much easier to track and contribute to the xrootd project moving forward.    Existing legacy branches and tags are to be migrated to the fresh fork, so historical builds will not be broken.",1,DM-3780,datamanagement,rationalize lsst xrootd repo maintenance procedure procedure pull push xrootd change upstream official xrootd repo cumbersome confusing error prone buildbot support release package branch master give reasonably replace lsst xrootd repo fresh genuine fork share history upstream carry lsst specific work forward dev branch easy track contribute xrootd project move forward exist legacy branch tag migrate fresh fork historical build break,"Rationalize lsst/xrootd repo and maintenance procedures The procedure for pulling/pushing xrootd changes from/to the upstream official xrootd repo is cumbersome, confusing, and error-prone. Buildbot now has support for releasing packages from branches other than master. Given this, we can now reasonably replace our lsst/xrootd repo with a fresh genuine fork (shared history) of upstream, then carry our lsst-specific work forward on a dev-branch. This will make it much easier to track and contribute to the xrootd project moving forward. Existing legacy branches and tags are to be migrated to the fresh fork, so historical builds will not be broken."
Evaluate PASTRY DHT implementation The David Keller kademlia implementation used in the earlier prototype has some bugs/limitations.  Try to find a better off-the-shelf DHT and integrate with prototype framework.,6,DM-3791,datamanagement,evaluate pastry dht implementation david keller kademlia implementation early prototype bug limitation try find well shelf dht integrate prototype framework,Evaluate PASTRY DHT implementation The David Keller kademlia implementation used in the earlier prototype has some bugs/limitations. Try to find a better off-the-shelf DHT and integrate with prototype framework.
obs_test data mis-assembled obs_test images are mis-assembled and need to be regenerated. This may affect some existing unit tests that rely on the data.,2,DM-3792,datamanagement,obs_test datum mis assemble obs_test image mis assembled need regenerate affect exist unit test rely datum,obs_test data mis-assembled obs_test images are mis-assembled and need to be regenerated. This may affect some existing unit tests that rely on the data.
"remove install_name_tool fix to libpython2.7.dylib from anaconda package Now that SIM-1314 has been merged, we should be able to remove the    {code}  	if [[ $(uname -s) = Darwin* ]]; then  		#run install_name_tool on all of the libpythonX.X.dylib dynamic  		#libraries in anaconda  		for entry in $PREFIX/lib/libpython*.dylib  		do  			install_name_tool -id $entry $entry  		done  	fi  {code}    from eupspkg.cfg.sh in the EUPS anaconda package, and still have GalSim build correctly",1,DM-3796,datamanagement,remove install_name_tool fix libpython2.7.dylib anaconda package sim-1314 merge able remove code uname darwin run install_name_tool libpythonx.x.dylib dynamic library anaconda entry prefix lib libpython*.dylib install_name_tool -id entry entry fi code eupspkg.cfg.sh eups anaconda package galsim build correctly,"remove install_name_tool fix to libpython2.7.dylib from anaconda package Now that SIM-1314 has been merged, we should be able to remove the {code} if [[ $(uname -s) = Darwin* ]]; then #run install_name_tool on all of the libpythonX.X.dylib dynamic #libraries in anaconda for entry in $PREFIX/lib/libpython*.dylib do install_name_tool -id $entry $entry done fi {code} from eupspkg.cfg.sh in the EUPS anaconda package, and still have GalSim build correctly"
Enable SSL to community.lsst.org Enable SSL (https) for the Discourse site at community.lsst.org,1,DM-3797,datamanagement,enable ssl community.lsst.org enable ssl https discourse site community.lsst.org,Enable SSL to community.lsst.org Enable SSL (https) for the Discourse site at community.lsst.org
Update flag names and config override files to current conventions The {{deblend.masked}} and {{deblend.blendedness}} flag names in {{meas_deblender}} need to be updated to use underscores instead of periods.  Various flag names in the {{examples}} scripts also need updating to the underscore and camelCase format.    A search for these flags throughout the database revealed a number of config files that need updating to current conventions.  These are also included here.,1,DM-3798,datamanagement,update flag name config override file current convention deblend.maske deblend.blendedness flag name meas_deblender need update use underscore instead period flag name example script need update underscore camelcase format search flag database reveal number config file need update current convention include,Update flag names and config override files to current conventions The {{deblend.masked}} and {{deblend.blendedness}} flag names in {{meas_deblender}} need to be updated to use underscores instead of periods. Various flag names in the {{examples}} scripts also need updating to the underscore and camelCase format. A search for these flags throughout the database revealed a number of config files that need updating to current conventions. These are also included here.
"testProcessCcd.py computes values that are too different between MacOS and linux tests/testProcessCcd.py runs processCcd on visit 1 of obs_test's data repository. The result on MacOS is surprisingly different than on linux in at least one case: psfShape.getIxx() computes 2.71 on MacOS X and 2.65 on linux. Iyy and Ixy are likely different. It's worth checking all other computed values, as well. These differences likely indicate that something is wrong, e.g. in obs_test, processCcd, or the way the test runs processCcd.    This showed up as part of fixing DM-3792, but it is not clear if the changes on DM-3792 actually caused or increased the difference between MacOS and linux, or if the difference was always too large, but was masked by an intentionally generous tolerance in the unit test.",2,DM-3800,datamanagement,testprocessccd.py compute value different macos linux test testprocessccd.py run processccd visit obs_t data repository result macos surprisingly different linux case psfshape.getixx compute 2.71 macos 2.65 linux iyy ixy likely different worth check compute value difference likely indicate wrong e.g. obs_test processccd way test run processccd show fix dm-3792 clear change dm-3792 actually cause increase difference macos linux difference large mask intentionally generous tolerance unit test,"testProcessCcd.py computes values that are too different between MacOS and linux tests/testProcessCcd.py runs processCcd on visit 1 of obs_test's data repository. The result on MacOS is surprisingly different than on linux in at least one case: psfShape.getIxx() computes 2.71 on MacOS X and 2.65 on linux. Iyy and Ixy are likely different. It's worth checking all other computed values, as well. These differences likely indicate that something is wrong, e.g. in obs_test, processCcd, or the way the test runs processCcd. This showed up as part of fixing DM-3792, but it is not clear if the changes on DM-3792 actually caused or increased the difference between MacOS and linux, or if the difference was always too large, but was masked by an intentionally generous tolerance in the unit test."
"The gains in obs_test's amplifier table appear to be incorrect As of DM-3792 the gains in obs_test's camera's amplifier table were set to the values reported in the headers of the lsstSim raw data used to generate obs_test's raw data. (Before that one nominal gain was used for all amplifiers).    However, [~rhl] reports that these gains are incorrect. He measured the following gains by scaling the nominal gains by the median values in the bias-subtracted data:  {code}  amp   meas      curr  name  gain      gain  00    1.7741    1.7741  01    1.8998    1.65881  10    1.8130    1.74151  11    1.8903    1.67073  {code}    We could simply adopt these values, but I would like to understand why the gains are so far off from those reported by phoSim in the raw data.",4,DM-3801,datamanagement,gain obs_t amplifier table appear incorrect dm-3792 gain obs_t camera amplifier table set value report header lsstsim raw datum generate obs_test raw data nominal gain amplifier ~rhl report gain incorrect measure follow gain scale nominal gain median value bias subtract datum code amp meas curr gain gain 00 1.7741 1.7741 01 1.8998 1.65881 10 1.8130 1.74151 11 1.8903 1.67073 code simply adopt value like understand gain far report phosim raw data,"The gains in obs_test's amplifier table appear to be incorrect As of DM-3792 the gains in obs_test's camera's amplifier table were set to the values reported in the headers of the lsstSim raw data used to generate obs_test's raw data. (Before that one nominal gain was used for all amplifiers). However, [~rhl] reports that these gains are incorrect. He measured the following gains by scaling the nominal gains by the median values in the bias-subtracted data: {code} amp meas curr name gain gain 00 1.7741 1.7741 01 1.8998 1.65881 10 1.8130 1.74151 11 1.8903 1.67073 {code} We could simply adopt these values, but I would like to understand why the gains are so far off from those reported by phoSim in the raw data."
"The obs_test's sensor is shown 90 degrees rotated from that desired, in camera coords When plotting the obs_test sensor, e.g. using lsst.afw.cameraGeom.utils.plotFocalPlane, the image is a short, wide rectangle. This suggests that the camera coordinate frame is rotated 90 degrees from the CCD coordinate frame (which has 1018 pixels in X and 2000 pixels in Y).    We would prefer that the camera frame and CCD frame have the same orientation.",1,DM-3802,datamanagement,obs_t sensor show 90 degree rotate desire camera coord plot obs_test sensor e.g. lsst.afw.camerageom.utils.plotfocalplane image short wide rectangle suggest camera coordinate frame rotate 90 degree ccd coordinate frame 1018 pixel 2000 pixel prefer camera frame ccd frame orientation,"The obs_test's sensor is shown 90 degrees rotated from that desired, in camera coords When plotting the obs_test sensor, e.g. using lsst.afw.cameraGeom.utils.plotFocalPlane, the image is a short, wide rectangle. This suggests that the camera coordinate frame is rotated 90 degrees from the CCD coordinate frame (which has 1018 pixels in X and 2000 pixels in Y). We would prefer that the camera frame and CCD frame have the same orientation."
"Fix Qserv compiler warnings with clang Qserv triggers numerous warnings with clang on OS X. Full details are in the attached ticket, here we summarize the distinct warnings classes:    h5. Protobuf    {code}  /Users/timj/work/lsstsw/stack/DarwinX86/protobuf/2.6.1+fbf04ba888/include/google/protobuf/unknown_field_set.h:214:13: warning: anonymous types declared in an anonymous union        are an extension [-Wnested-anon-types]      mutable union {              ^  {code}    h5. Qserv    {code}  In file included from core/modules/sql/statement.cc:32:  core/modules/sql/Schema.h:74:1: warning: 'Schema' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Schema {  ^  core/modules/sql/statement.h:35:1: note: did you mean struct here?  class Schema; // Forward  ^~~~~  struct  {code}    {code}  core/modules/proto/WorkerResponse.h:34:1: warning: 'WorkerResponse' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct WorkerResponse {  ^  core/modules/ccontrol/MergingRequester.h:38:3: note: did you mean struct here?    class WorkerResponse;    ^~~~~    struct  {code}    {code}  In file included from core/modules/qana/QueryMapping.cc:46:  core/modules/qproc/ChunkSpec.h:51:1: warning: 'ChunkSpec' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ChunkSpec {  ^  core/modules/qana/QueryMapping.h:44:5: note: did you mean struct here?      class ChunkSpec;      ^~~~~      struct  {code}    {code}  core/modules/qana/TableInfo.h:186:1: warning: 'DirTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct DirTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:86:1: note: did you mean struct here?  class DirTableInfo;  ^~~~~  struct  core/modules/qana/TableInfo.h:221:1: warning: 'ChildTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ChildTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:87:1: note: did you mean struct here?  class ChildTableInfo;  ^~~~~  struct  core/modules/qana/TableInfo.h:260:1: warning: 'MatchTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct MatchTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:88:1: note: did you mean struct here?  class MatchTableInfo;  ^~~~~  struct  In file included from core/modules/qana/ColumnVertexMap.cc:36:  core/modules/qana/RelationGraph.h:513:1: warning: struct 'Vertex' was previously declared as a class [-Wmismatched-tags]  struct Vertex;  ^  core/modules/qana/ColumnVertexMap.h:44:7: note: previous use is here  class Vertex;        ^  In file included from core/modules/qana/ColumnVertexMap.cc:36:  core/modules/qana/RelationGraph.h:547:1: warning: 'Vertex' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Vertex {  ^  core/modules/qana/ColumnVertexMap.h:44:1: note: did you mean struct here?  class Vertex;  ^~~~~  struct  {code}    {code}  core/modules/wbase/Base.h:72:1: warning: 'ScriptMeta' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ScriptMeta {  ^  core/modules/wbase/Task.h:41:5: note: did you mean struct here?      class ScriptMeta;      ^~~~~      struct  {code}    {code}  In file included from core/modules/parser/BoolTermFactory.cc:46:  core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual]      virtual std::ostream& putStream(std::ostream& os) = 0;                            ^  core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none)      virtual std::ostream& putStream(std::ostream& os) const = 0;                            ^  {code}    {code}  core/modules/parser/FromFactory.cc:62:15: warning: unused function 'walkToSiblingBefore' [-Wunused-function]  inline RefAST walkToSiblingBefore(RefAST node, int typeId) {                ^  core/modules/parser/FromFactory.cc:72:1: warning: unused function 'getSiblingStringBounded' [-Wunused-function]  getSiblingStringBounded(RefAST left, RefAST right) {  ^  {code}    {code}  In file included from core/modules/wsched/ChunkDisk.cc:25:  core/modules/wsched/ChunkDisk.h:130:10: warning: private field '_completed' is not used [-Wunused-private-field]      bool _completed;           ^  {code}    {code}  In file included from core/modules/parser/PredicateFactory.cc:45:  core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual]      virtual std::ostream& putStream(std::ostream& os) = 0;                            ^  core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none)      virtual std::ostream& putStream(std::ostream& os) const = 0;                            ^  {code}    {code}  core/modules/parser/WhereFactory.cc:265:31: warning: binding reference member 'c' to stack allocated parameter 'c_' [-Wdangling-field]      PrintExcept(Check c_) : c(c_) {}                                ^~  core/modules/parser/WhereFactory.cc:291:28: note: in instantiation of member function 'lsst::qserv::parser::PrintExcept<lsst::qserv::parser::MetaCheck>::PrintExcept' requested        here      PrintExcept<MetaCheck> p(mc);                             ^  core/modules/parser/WhereFactory.cc:269:12: note: reference member declared here      Check& c;             ^  {code}    {code}  core/modules/rproc/ProtoRowBuffer.cc:44:11: warning: unused variable 'largeRowThreshold' [-Wunused-const-variable]  int const largeRowThreshold = 500*1024;            ^  {code}    {code}  core/modules/util/testIterableFormatter.cc:85:43: warning: suggest braces around initialization of subobject [-Wmissing-braces]      std::array<std::string, 6> iterable { ""1"", ""2"", ""3"", ""4"", ""5"", ""6""};                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                                            {                           }  {code}    {code}  In file included from core/modules/qdisp/XrdSsiMocks.cc:37:  core/modules/qdisp/XrdSsiMocks.h:64:16: warning: private field '_executive' is not used [-Wunused-private-field]      Executive *_executive;                 ^  {code}    {code}  core/modules/xrdoss/QservOss.cc:77:1: warning: unused function 'print' [-Wunused-function]  print(std::ostream& os, lsst::qserv::xrdoss::QservOss::StringSet const& h) {  ^  {code}    h5. OS X    {code}  core/modules/qdisp/QueryRequest.h:54:25: warning: 'lsst::qserv::qdisp::BadResponseError::what' hides overloaded virtual function [-Woverloaded-virtual]      virtual char const* what() throw() {                          ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function        'std::exception::what' declared here: different qualifiers (const vs none)      virtual const char* what() const _NOEXCEPT;                          ^  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:37:  core/modules/qdisp/QueryRequest.h:67:25: warning: 'lsst::qserv::qdisp::RequestError::what' hides overloaded virtual function [-Woverloaded-virtual]      virtual char const* what() throw() {                          ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function        'std::exception::what' declared here: different qualifiers (const vs none)      virtual const char* what() const _NOEXCEPT;                          ^  {code}    {code}  core/modules/proto/TaskMsgDigest.cc:55:5: warning: 'MD5' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      MD5(reinterpret_cast<unsigned char const*>(str.data()),      ^  /usr/include/openssl/md5.h:116:16: note: 'MD5' has been explicitly marked deprecated here  unsigned char *MD5(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  {code}    {code}  core/modules/util/StringHash.cc:78:24: warning: 'SHA1' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      return wrapHashHex<SHA1, SHA_DIGEST_LENGTH>(buffer, bufferSize);                         ^  /usr/include/openssl/sha.h:124:16: note: 'SHA1' has been explicitly marked deprecated here  unsigned char *SHA1(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  core/modules/util/StringHash.cc:83:24: warning: 'SHA256' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      return wrapHashHex<SHA256, SHA256_DIGEST_LENGTH>(buffer, bufferSize);                         ^  /usr/include/openssl/sha.h:150:16: note: 'SHA256' has been explicitly marked deprecated here  unsigned char *SHA256(const unsigned char *d, size_t n,unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  {code}    h5. Xrootd    {code}  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:33:  In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a        struct here but previously declared as a class [-Wmismatched-tags]  struct  XrdSsiRespInfo  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here?  class XrdSsiRespInfo;  ^~~~~  struct  {code}    {code}  core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual]      virtual int Opendir(const char *) { return XrdOssOK; }                  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function        'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1)  virtual int     Opendir(const char *, XrdOucEnv &)           {return -ENOTDIR;}                  ^  {code}    {code}  In file included from core/modules/xrdsvc/SsiSession.h:32:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of        non-void function [-Wreturn-type]                            }                            ^  {code}    h5. boost    {code}  /Users/timj/work/lsstsw/stack/DarwinX86/boost/1.55.0.1.lsst2+fbf04ba888/include/boost/regex/v4/regex_raw_buffer.hpp:132:7: warning: 'register' storage class specifier is        deprecated [-Wdeprecated-register]        register pointer result = end;        ^~~~~~~~~  {code}",1,DM-3803,datamanagement,fix qserv compiler warning clang qserv trigger numerous warning clang os x. detail attached ticket summarize distinct warning class h5 protobuf code /users timj work lsstsw stack darwinx86 protobuf/2.6.1+fbf04ba888 include google protobuf unknown_field_set.h:214:13 warn anonymous type declare anonymous union extension -wnested anon type mutable union code h5 qserv code file include core module sql statement.cc:32 core module sql schema.h:74:1 warning schema define struct previously declare class -wmismatched tag struct schema core module sql statement.h:35:1 note mean struct class schema forward ^~~~~ struct code code core module proto workerresponse.h:34:1 warning workerresponse define struct previously declare class -wmismatched tag struct workerresponse core module ccontrol mergingrequester.h:38:3 note mean struct class workerresponse ^~~~~ struct code code file include core module qana querymapping.cc:46 core module qproc chunkspec.h:51:1 warning chunkspec define struct previously declare class -wmismatched tag struct chunkspec core module qana querymapping.h:44:5 note mean struct class chunkspec ^~~~~ struct code code core module qana tableinfo.h:186:1 warn dirtableinfo define struct previously declare class -wmismatched tag struct dirtableinfo tableinfo core module qana tableinfo.h:86:1 note mean struct class dirtableinfo ^~~~~ struct core module qana tableinfo.h:221:1 warning childtableinfo define struct previously declare class -wmismatched tag struct childtableinfo tableinfo core module qana tableinfo.h:87:1 note mean struct class childtableinfo ^~~~~ struct core module qana tableinfo.h:260:1 warning matchtableinfo define struct previously declare class -wmismatched tag struct matchtableinfo tableinfo core module qana tableinfo.h:88:1 note mean struct class matchtableinfo ^~~~~ struct file include core module qana columnvertexmap.cc:36 core module qana relationgraph.h:513:1 warning struct vertex previously declare class -wmismatched tag struct vertex core module qana columnvertexmap.h:44:7 note previous use class vertex file include core module qana columnvertexmap.cc:36 core module qana relationgraph.h:547:1 warning vertex define struct previously declare class -wmismatched tag struct vertex core module qana columnvertexmap.h:44:1 note mean struct class vertex ^~~~~ struct code code core module wbase base.h:72:1 warning scriptmeta define struct previously declare class -wmismatched tag struct scriptmeta core module wbase task.h:41:5 note mean struct class scriptmeta ^~~~~ struct code code file include core module parser booltermfactory.cc:46 core module query predicate.h:86:27 warning lsst::qserv::query::genericpredicate::putstream hide overload virtual function -woverloade virtual virtual std::ostream putstream(std::ostream os core module query predicate.h:71:27 note hide overload virtual function lsst::qserv::query::predicate::putstream declare different qualifier const vs virtual std::ostream putstream(std::ostream os const code code core module parser fromfactory.cc:62:15 warning unused function walktosiblingbefore -wunused function inline refast walktosiblingbefore(refast node int typeid core module parser fromfactory.cc:72:1 warning unused function getsiblingstringbounde -wunused function getsiblingstringbounded(refast leave refast right code code file include core module wsche chunkdisk.cc:25 core module wsche chunkdisk.h:130:10 warning private field complete -wunused private field bool complete code code file include core module parser predicatefactory.cc:45 core module query predicate.h:86:27 warning lsst::qserv::query::genericpredicate::putstream hide overload virtual function -woverloade virtual virtual std::ostream putstream(std::ostream os core module query predicate.h:71:27 note hide overload virtual function lsst::qserv::query::predicate::putstream declare different qualifier const vs virtual std::ostream putstream(std::ostream os const code code core module parser wherefactory.cc:265:31 warning bind reference member stack allocate parameter -wdangling field printexcept(check c(c ^~ core module parser wherefactory.cc:291:28 note instantiation member function lsst::qserv::parser::printexcept::printexcept request printexcept p(mc core module parser wherefactory.cc:269:12 note reference member declare check code code core module rproc protorowbuffer.cc:44:11 warn unused variable largerowthreshold -wunused const variable int const largerowthreshold 500 1024 code code core module util testiterableformatter.cc:85:43 warning suggest brace initialization subobject -wmissing brace std::array iterable ^~~~~~~~~~~~~~~~~~~~~~~~~~~~ code code file include core module qdisp xrdssimocks.cc:37 core module qdisp xrdssimocks.h:64:16 warning private field executive -wunused private field executive executive code code core module xrdoss qservoss.cc:77:1 warn unused function print -wunused function print(std::ostream os lsst::qserv::xrdoss::qservoss::stringset const code h5 os code core module qdisp queryrequest.h:54:25 warning lsst::qserv::qdisp::badresponseerror::what hide overload virtual function -woverloade virtual virtual char const throw /applications xcode.app contents developer toolchains xcodedefault.xctoolchain usr bin/ /include c++/v1 exception:95:25 note hide overload virtual function std::exception::what declare different qualifier const vs virtual const char const noexcept file include core module qdisp executive.cc:64 file include core module qdisp xrdssimocks.h:37 core module qdisp queryrequest.h:67:25 warn lsst::qserv::qdisp::requesterror::what hide overload virtual function -woverloade virtual virtual char const throw /applications xcode.app contents developer toolchains xcodedefault.xctoolchain usr bin/ /include c++/v1 exception:95:25 note hide overload virtual function std::exception::what declare different qualifier const vs virtual const char const noexcept code code core module proto taskmsgdigest.cc:55:5 warning md5 deprecate deprecate os 10.7 -wdeprecated declaration md5(reinterpret_cast(str.data /usr include openssl md5.h:116:16 note md5 explicitly mark deprecate unsigned char md5(const unsigned char size_t unsigned char md deprecated_in_mac_os_x_version_10_7_and_later code code core module util stringhash.cc:78:24 warning sha1 deprecate deprecate os 10.7 -wdeprecated declaration return wraphashhex(buffer buffersize include openssl sha.h:124:16 note sha1 explicitly mark deprecate unsigned char sha1(const unsigned char size_t unsigned char md deprecated_in_mac_os_x_version_10_7_and_later core module util stringhash.cc:83:24 warning sha256 deprecate deprecate os 10.7 -wdeprecated declaration return wraphashhex(buffer buffersize include openssl sha.h:150:16 note sha256 explicitly mark deprecate unsigned char sha256(const unsigned char size_t unsigned char md deprecated_in_mac_os_x_version_10_7_and_later code h5 xrootd code file include core module qdisp executive.cc:64 file include core module qdisp xrdssimocks.h:33 file include /users timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdssi xrdssirequest.hh:37 /users timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdssi xrdssirespinfo.hh:43:1 warning xrdssirespinfo define struct previously declare class -wmismatched tag struct xrdssirespinfo timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdssi xrdssisession.hh:45:1 note mean struct class xrdssirespinfo ^~~~~ struct code code core module xrdoss qservoss.h:64:17 warning lsst::qserv::xrdoss::fakeossdf::opendir hide overload virtual function -woverloade virtual virtual int opendir(const char return xrdossok timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdoss xrdoss.hh:63:17 note hide overload virtual function xrdossdf::opendir declare different number parameter vs virtual int opendir(const char xrdoucenv return -enotdir code code file include core module xrdsvc ssisession.h:32 /users timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdssi xrdssiresponder.hh:177:27 warning control reach end non void function -wreturn type code h5 boost code /users timj work lsstsw stack darwinx86 boost/1.55.0.1.lsst2+fbf04ba888 include boost regex v4 regex_raw_buffer.hpp:132:7 warning register storage class specifi deprecate -wdeprecated register register pointer result end ^~~~~~~~~ code,"Fix Qserv compiler warnings with clang Qserv triggers numerous warnings with clang on OS X. Full details are in the attached ticket, here we summarize the distinct warnings classes: h5. Protobuf {code} /Users/timj/work/lsstsw/stack/DarwinX86/protobuf/2.6.1+fbf04ba888/include/google/protobuf/unknown_field_set.h:214:13: warning: anonymous types declared in an anonymous union are an extension [-Wnested-anon-types] mutable union { ^ {code} h5. Qserv {code} In file included from core/modules/sql/statement.cc:32: core/modules/sql/Schema.h:74:1: warning: 'Schema' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct Schema { ^ core/modules/sql/statement.h:35:1: note: did you mean struct here? class Schema; // Forward ^~~~~ struct {code} {code} core/modules/proto/WorkerResponse.h:34:1: warning: 'WorkerResponse' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct WorkerResponse { ^ core/modules/ccontrol/MergingRequester.h:38:3: note: did you mean struct here? class WorkerResponse; ^~~~~ struct {code} {code} In file included from core/modules/qana/QueryMapping.cc:46: core/modules/qproc/ChunkSpec.h:51:1: warning: 'ChunkSpec' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct ChunkSpec { ^ core/modules/qana/QueryMapping.h:44:5: note: did you mean struct here? class ChunkSpec; ^~~~~ struct {code} {code} core/modules/qana/TableInfo.h:186:1: warning: 'DirTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct DirTableInfo : TableInfo { ^ core/modules/qana/TableInfo.h:86:1: note: did you mean struct here? class DirTableInfo; ^~~~~ struct core/modules/qana/TableInfo.h:221:1: warning: 'ChildTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct ChildTableInfo : TableInfo { ^ core/modules/qana/TableInfo.h:87:1: note: did you mean struct here? class ChildTableInfo; ^~~~~ struct core/modules/qana/TableInfo.h:260:1: warning: 'MatchTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct MatchTableInfo : TableInfo { ^ core/modules/qana/TableInfo.h:88:1: note: did you mean struct here? class MatchTableInfo; ^~~~~ struct In file included from core/modules/qana/ColumnVertexMap.cc:36: core/modules/qana/RelationGraph.h:513:1: warning: struct 'Vertex' was previously declared as a class [-Wmismatched-tags] struct Vertex; ^ core/modules/qana/ColumnVertexMap.h:44:7: note: previous use is here class Vertex; ^ In file included from core/modules/qana/ColumnVertexMap.cc:36: core/modules/qana/RelationGraph.h:547:1: warning: 'Vertex' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct Vertex { ^ core/modules/qana/ColumnVertexMap.h:44:1: note: did you mean struct here? class Vertex; ^~~~~ struct {code} {code} core/modules/wbase/Base.h:72:1: warning: 'ScriptMeta' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct ScriptMeta { ^ core/modules/wbase/Task.h:41:5: note: did you mean struct here? class ScriptMeta; ^~~~~ struct {code} {code} In file included from core/modules/parser/BoolTermFactory.cc:46: core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual] virtual std::ostream& putStream(std::ostream& os) = 0; ^ core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none) virtual std::ostream& putStream(std::ostream& os) const = 0; ^ {code} {code} core/modules/parser/FromFactory.cc:62:15: warning: unused function 'walkToSiblingBefore' [-Wunused-function] inline RefAST walkToSiblingBefore(RefAST node, int typeId) { ^ core/modules/parser/FromFactory.cc:72:1: warning: unused function 'getSiblingStringBounded' [-Wunused-function] getSiblingStringBounded(RefAST left, RefAST right) { ^ {code} {code} In file included from core/modules/wsched/ChunkDisk.cc:25: core/modules/wsched/ChunkDisk.h:130:10: warning: private field '_completed' is not used [-Wunused-private-field] bool _completed; ^ {code} {code} In file included from core/modules/parser/PredicateFactory.cc:45: core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual] virtual std::ostream& putStream(std::ostream& os) = 0; ^ core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none) virtual std::ostream& putStream(std::ostream& os) const = 0; ^ {code} {code} core/modules/parser/WhereFactory.cc:265:31: warning: binding reference member 'c' to stack allocated parameter 'c_' [-Wdangling-field] PrintExcept(Check c_) : c(c_) {} ^~ core/modules/parser/WhereFactory.cc:291:28: note: in instantiation of member function 'lsst::qserv::parser::PrintExcept::PrintExcept' requested here PrintExcept p(mc); ^ core/modules/parser/WhereFactory.cc:269:12: note: reference member declared here Check& c; ^ {code} {code} core/modules/rproc/ProtoRowBuffer.cc:44:11: warning: unused variable 'largeRowThreshold' [-Wunused-const-variable] int const largeRowThreshold = 500*1024; ^ {code} {code} core/modules/util/testIterableFormatter.cc:85:43: warning: suggest braces around initialization of subobject [-Wmissing-braces] std::array iterable { ""1"", ""2"", ""3"", ""4"", ""5"", ""6""}; ^~~~~~~~~~~~~~~~~~~~~~~~~~~~ { } {code} {code} In file included from core/modules/qdisp/XrdSsiMocks.cc:37: core/modules/qdisp/XrdSsiMocks.h:64:16: warning: private field '_executive' is not used [-Wunused-private-field] Executive *_executive; ^ {code} {code} core/modules/xrdoss/QservOss.cc:77:1: warning: unused function 'print' [-Wunused-function] print(std::ostream& os, lsst::qserv::xrdoss::QservOss::StringSet const& h) { ^ {code} h5. OS X {code} core/modules/qdisp/QueryRequest.h:54:25: warning: 'lsst::qserv::qdisp::BadResponseError::what' hides overloaded virtual function [-Woverloaded-virtual] virtual char const* what() throw() { ^ /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function 'std::exception::what' declared here: different qualifiers (const vs none) virtual const char* what() const _NOEXCEPT; ^ In file included from core/modules/qdisp/Executive.cc:64: In file included from core/modules/qdisp/XrdSsiMocks.h:37: core/modules/qdisp/QueryRequest.h:67:25: warning: 'lsst::qserv::qdisp::RequestError::what' hides overloaded virtual function [-Woverloaded-virtual] virtual char const* what() throw() { ^ /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function 'std::exception::what' declared here: different qualifiers (const vs none) virtual const char* what() const _NOEXCEPT; ^ {code} {code} core/modules/proto/TaskMsgDigest.cc:55:5: warning: 'MD5' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations] MD5(reinterpret_cast(str.data()), ^ /usr/include/openssl/md5.h:116:16: note: 'MD5' has been explicitly marked deprecated here unsigned char *MD5(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER; ^ {code} {code} core/modules/util/StringHash.cc:78:24: warning: 'SHA1' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations] return wrapHashHex(buffer, bufferSize); ^ /usr/include/openssl/sha.h:124:16: note: 'SHA1' has been explicitly marked deprecated here unsigned char *SHA1(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER; ^ core/modules/util/StringHash.cc:83:24: warning: 'SHA256' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations] return wrapHashHex(buffer, bufferSize); ^ /usr/include/openssl/sha.h:150:16: note: 'SHA256' has been explicitly marked deprecated here unsigned char *SHA256(const unsigned char *d, size_t n,unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER; ^ {code} h5. Xrootd {code} In file included from core/modules/qdisp/Executive.cc:64: In file included from core/modules/qdisp/XrdSsiMocks.h:33: In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37: /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct XrdSsiRespInfo ^ /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here? class XrdSsiRespInfo; ^~~~~ struct {code} {code} core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual] virtual int Opendir(const char *) { return XrdOssOK; } ^ /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function 'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1) virtual int Opendir(const char *, XrdOucEnv &) {return -ENOTDIR;} ^ {code} {code} In file included from core/modules/xrdsvc/SsiSession.h:32: /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of non-void function [-Wreturn-type] } ^ {code} h5. boost {code} /Users/timj/work/lsstsw/stack/DarwinX86/boost/1.55.0.1.lsst2+fbf04ba888/include/boost/regex/v4/regex_raw_buffer.hpp:132:7: warning: 'register' storage class specifier is deprecated [-Wdeprecated-register] register pointer result = end; ^~~~~~~~~ {code}"
"Fix order of arguments change in meas_base SingleFrameMeasurement In sfm.py on line 271, a comment indicates that some code is a temporary work around until the switch from meas_algorithms to meas_base is complete. This work is complete, so this temporary workaround should be removed, or if it is decided it should be kept, the comment should be removed. See https://github.com/lsst/meas_base/blob/tickets/DM-2915/python/lsst/meas/base/sfm.py#L271",2,DM-3804,datamanagement,fix order argument change meas_base singleframemeasurement sfm.py line 271 comment indicate code temporary work switch meas_algorithm meas_base complete work complete temporary workaround remove decide keep comment remove https://github.com/lsst/meas_base/blob/tickets/dm-2915/python/lsst/meas/base/sfm.py#l271,"Fix order of arguments change in meas_base SingleFrameMeasurement In sfm.py on line 271, a comment indicates that some code is a temporary work around until the switch from meas_algorithms to meas_base is complete. This work is complete, so this temporary workaround should be removed, or if it is decided it should be kept, the comment should be removed. See https://github.com/lsst/meas_base/blob/tickets/DM-2915/python/lsst/meas/base/sfm.py#L271"
convert newinstall.sh to use miniconda instead of anaconda To match the conversion of lsstsw from anaconda -> miniconda to reduce the disk footprint and improve install times.,1,DM-3806,datamanagement,convert newinstall.sh use miniconda instead anaconda match conversion lsstsw anaconda miniconda reduce disk footprint improve install time,convert newinstall.sh to use miniconda instead of anaconda To match the conversion of lsstsw from anaconda -> miniconda to reduce the disk footprint and improve install times.
"Setup lsst_sphinx_kit package structure Setup the lsst_sphinx_kit package, including    * setup.py  * unit tests, tox and Travis CI  * README stub  * Sphinx stub and readthedocs",1,DM-3808,datamanagement,setup lsst_sphinx_kit package structure setup lsst_sphinx_kit package include setup.py unit test tox travis ci readme stub sphinx stub readthedoc,"Setup lsst_sphinx_kit package structure Setup the lsst_sphinx_kit package, including * setup.py * unit tests, tox and Travis CI * README stub * Sphinx stub and readthedocs"
HSC backport: Include documentation strings for config parameters when they are dumped This is a port of the following HSC tickets:  [HSC-1072|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1072]  and  [HSC-1175|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1175],4,DM-3811,datamanagement,hsc backport include documentation string config parameter dump port following hsc ticket hsc-1072|https://hsc jira.astro.princeton.edu jira browse hsc-1072 hsc-1175|https://hsc jira.astro.princeton.edu jira browse hsc-1175,HSC backport: Include documentation strings for config parameters when they are dumped This is a port of the following HSC tickets: [HSC-1072|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1072] and [HSC-1175|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1175]
"Intermittent build failures on v11 candidate with eups distrib We are seeing frequent intermittent failures on a variety of platforms when installing the v11 candidate with eups distrib install. Repeating the command works. It doesn't seem to be evenly distributed between packages: cfitsio, -meas_astrom-(?), meas_algorithms, and obs_lsstSim have been seen multiple times. It's been seen in the release verification CI (that uses the documented user facing process of eups distrib install instead of the factory CI which uses lsstsw) and in individual user ""manual"" builds on lsst-dev.     Test build key:  ||buildbot build # || eups tag || refs || comment ||  ||b1688 || t20150914-b1688 || tickets/DM-3829 v11_0_rc2 || ||  ||b1689 || t20150914-b1689 || tickets/DM-3829 tickets/DM-3815-intermittent-build-failure v11_0_rc2 || ||  ||b1690 || t20150914-b1690 || tickets/DM-3829 v11_0_rc2 master || modifications to tickets/DM-3829 ||  ||b1692 || t20150915-b1692 || tickets/DM-3829 ticket/DM-2752-egg-error v11_0_rc2 || ||",6,DM-3815,datamanagement,intermittent build failure v11 candidate eup distrib see frequent intermittent failure variety platform instal v11 candidate eup distrib install repeat command work evenly distribute package cfitsio -meas_astrom- meas_algorithms obs_lsstsim see multiple time see release verification ci use document user facing process eup distrib install instead factory ci use lsstsw individual user manual build lsst dev test build key ||buildbot build || eup tag || refs || comment || ||b1688 || t20150914 b1688 || ticket dm-3829 v11_0_rc2 || || ||b1689 || t20150914 b1689 || ticket dm-3829 ticket dm-3815 intermittent build failure v11_0_rc2 || || ||b1690 || t20150914 b1690 || ticket dm-3829 v11_0_rc2 master || modification ticket dm-3829 || ||b1692 || t20150915 b1692 || ticket dm-3829 ticket dm-2752 egg error v11_0_rc2 || ||,"Intermittent build failures on v11 candidate with eups distrib We are seeing frequent intermittent failures on a variety of platforms when installing the v11 candidate with eups distrib install. Repeating the command works. It doesn't seem to be evenly distributed between packages: cfitsio, -meas_astrom-(?), meas_algorithms, and obs_lsstSim have been seen multiple times. It's been seen in the release verification CI (that uses the documented user facing process of eups distrib install instead of the factory CI which uses lsstsw) and in individual user ""manual"" builds on lsst-dev. Test build key: ||buildbot build # || eups tag || refs || comment || ||b1688 || t20150914-b1688 || tickets/DM-3829 v11_0_rc2 || || ||b1689 || t20150914-b1689 || tickets/DM-3829 tickets/DM-3815-intermittent-build-failure v11_0_rc2 || || ||b1690 || t20150914-b1690 || tickets/DM-3829 v11_0_rc2 master || modifications to tickets/DM-3829 || ||b1692 || t20150915-b1692 || tickets/DM-3829 ticket/DM-2752-egg-error v11_0_rc2 || ||"
"levels in DecamMapper.paf is not quite right When ccdnum is not given as part of the dataId, instead of iterating over it, an error like this happens    {code:java}    RuntimeError: No unique lookup for ['ccdnum'] from {'visit': 205344}: 61 matches  {code}    Likely a problem in policy/DecamMapper.paf",1,DM-3816,datamanagement,level decammapper.paf right ccdnum give dataid instead iterate error like happen code java runtimeerror unique lookup ccdnum visit 205344 61 match code likely problem policy decammapper.paf,"levels in DecamMapper.paf is not quite right When ccdnum is not given as part of the dataId, instead of iterating over it, an error like this happens {code:java} RuntimeError: No unique lookup for ['ccdnum'] from {'visit': 205344}: 61 matches {code} Likely a problem in policy/DecamMapper.paf"
"Deploy docker images on ccqserv124/149 Creation of configured master and worker image will be improved here, and a deploymen tool (like swarm, of hand-made) will be used to deploy images over in2p3 cluster.",6,DM-3817,datamanagement,deploy docker image ccqserv124/149 creation configure master worker image improve deployman tool like swarm hand deploy image in2p3 cluster,"Deploy docker images on ccqserv124/149 Creation of configured master and worker image will be improved here, and a deploymen tool (like swarm, of hand-made) will be used to deploy images over in2p3 cluster."
Bi-weekly PO security meeting Bi-weekly meeting with PO on cyber security.,1,DM-3818,datamanagement,bi weekly po security meeting bi weekly meeting po cyber security,Bi-weekly PO security meeting Bi-weekly meeting with PO on cyber security.
IdM work Work for LSST Identity management and authentication.,2,DM-3819,datamanagement,idm work work lsst identity management authentication,IdM work Work for LSST Identity management and authentication.
NIST SP 800.82 investigation NIST SP 800.82 investigation for a more cohesive SCADA enclave security plan.,2,DM-3820,datamanagement,nist sp 800.82 investigation nist sp 800.82 investigation cohesive scada enclave security plan,NIST SP 800.82 investigation NIST SP 800.82 investigation for a more cohesive SCADA enclave security plan.
"Recent CModel bugfixes from HSC I've just fixed two rather critical bugs in the CModel code on the HSC side (they would have been introduced on the LSST side in the last transfer, DM-2977):   - The {{minInitialRadius}} configuration parameter had a default that is too small, causing many galaxies to be fit with point source models, leading to bad star/galaxy classifications.  This is HSC-1306.   - There was a simple but important algebra error in the uncertainty calculation, making the uncertainty a strong function of magnitude.  This is HSC-1313.    On the LSST side, the transfer should be quite simple; we'll have to rewrite a bit of code due to the difference in measurement frameworks, but there was very little to begin with (most of the effort in the HSC issues was in debugging).",1,DM-3821,datamanagement,recent cmodel bugfixe hsc fix critical bug cmodel code hsc introduce lsst transfer dm-2977 mininitialradius configuration parameter default small cause galaxy fit point source model lead bad star galaxy classification hsc-1306 simple important algebra error uncertainty calculation make uncertainty strong function magnitude hsc-1313 lsst transfer simple rewrite bit code difference measurement framework little begin effort hsc issue debugging,"Recent CModel bugfixes from HSC I've just fixed two rather critical bugs in the CModel code on the HSC side (they would have been introduced on the LSST side in the last transfer, DM-2977): - The {{minInitialRadius}} configuration parameter had a default that is too small, causing many galaxies to be fit with point source models, leading to bad star/galaxy classifications. This is HSC-1306. - There was a simple but important algebra error in the uncertainty calculation, making the uncertainty a strong function of magnitude. This is HSC-1313. On the LSST side, the transfer should be quite simple; we'll have to rewrite a bit of code due to the difference in measurement frameworks, but there was very little to begin with (most of the effort in the HSC issues was in debugging)."
"meas_astrom bugs exposed by new Eigen Trying a newer Eigen has exposed several issues in meas_astrom:  - tests/createWcsWithSip.py blindly uses sipWcs in the result returned by ANetBasicAstrometryTask.determineWcs2, but that attribute may be None  - ANetBasicAstrometryTask.determineWcs2 terminates iteration early if the # of matches goes down, even though the result may be improved. In the case in question the first fit iteration results in significantly better RMS error, but has one fewer matches, so the SIP fit is rejected, triggering the first bug mentioned.",6,DM-3824,datamanagement,meas_astrom bug expose new eigen try new eigen expose issue meas_astrom test createwcswithsip.py blindly use sipwcs result return anetbasicastrometrytask.determinewcs2 attribute anetbasicastrometrytask.determinewcs2 terminate iteration early match go result improve case question fit iteration result significantly well rms error few match sip fit reject trigger bug mention,"meas_astrom bugs exposed by new Eigen Trying a newer Eigen has exposed several issues in meas_astrom: - tests/createWcsWithSip.py blindly uses sipWcs in the result returned by ANetBasicAstrometryTask.determineWcs2, but that attribute may be None - ANetBasicAstrometryTask.determineWcs2 terminates iteration early if the # of matches goes down, even though the result may be improved. In the case in question the first fit iteration results in significantly better RMS error, but has one fewer matches, so the SIP fit is rejected, triggering the first bug mentioned."
"email discussion w.r.t Service separation for L1, and also some work on ITIL roles email to German about the all the L1 stuff and clarified that the L1 system were a  derive provided to the Telescopes site (important for fitting this into the proper place in the who is worrying about what hierarchy.  Also, revised SA to see fi the EPO changes discusses affected the IT roles in th model (not apparently)  lastly attitude the TOWC ",2,DM-3827,datamanagement,email discussion service separation l1 work itil role email german l1 stuff clarify l1 system derive provide telescopes site important fit proper place worry hierarchy revise sa fi epo change discuss affect role th model apparently lastly attitude towc,"email discussion w.r.t Service separation for L1, and also some work on ITIL roles email to German about the all the L1 stuff and clarified that the L1 system were a derive provided to the Telescopes site (important for fitting this into the proper place in the who is worrying about what hierarchy. Also, revised SA to see fi the EPO changes discusses affected the IT roles in th model (not apparently) lastly attitude the TOWC"
"Management for week ending sept 11 Deal with Hiring James Parsons, and interfacing with the new NCSA organizations that will support LSST at NCSA, general group management issues",2,DM-3828,datamanagement,management week end sept 11 deal hire james parsons interface new ncsa organization support lsst ncsa general group management issue,"Management for week ending sept 11 Deal with Hiring James Parsons, and interfacing with the new NCSA organizations that will support LSST at NCSA, general group management issues"
Preparation work to process raw DECam data Try to run processCcd.py with raw DECam data and see what are yet to be solved for it to run. ,7,DM-3831,datamanagement,preparation work process raw decam datum try run processccd.py raw decam datum solve run,Preparation work to process raw DECam data Try to run processCcd.py with raw DECam data and see what are yet to be solved for it to run.
Migrate LDM-230 to new docs platform Convert LDM-230 from Word to restructuredText and deploy on readthedocs.org,2,DM-3834,datamanagement,migrate ldm-230 new docs platform convert ldm-230 word restructuredtext deploy readthedocs.org,Migrate LDM-230 to new docs platform Convert LDM-230 from Word to restructuredText and deploy on readthedocs.org
Migrate LDM-135 to new design docs platform Convert LDM-135 from Word to restructuredText and deploy on readthedocs.org,5,DM-3835,datamanagement,migrate ldm-135 new design docs platform convert ldm-135 word restructuredtext deploy readthedocs.org,Migrate LDM-135 to new design docs platform Convert LDM-135 from Word to restructuredText and deploy on readthedocs.org
Migrate LDM-129 to new design docs platform Convert LDM-129 from Word to restructuredText and deploy onto readthedocs.org,2,DM-3836,datamanagement,migrate ldm-129 new design docs platform convert ldm-129 word restructuredtext deploy readthedocs.org,Migrate LDM-129 to new design docs platform Convert LDM-129 from Word to restructuredText and deploy onto readthedocs.org
LSE-72: Phase 3 in X16 Advance Phase 3 details as needed to eliminate obstacles to OCS and DM development during F16.,8,DM-3840,datamanagement,lse-72 phase x16 advance phase detail need eliminate obstacle ocs dm development f16,LSE-72: Phase 3 in X16 Advance Phase 3 details as needed to eliminate obstacles to OCS and DM development during F16.
LSE-75: Refine WCS and PSF requirements in W16 Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis.    Depends on the ability of the T&S group to engage with this subject.    Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.,8,DM-3841,datamanagement,lse-75 refine wcs psf requirement w16 clarify data format precision requirement tcs telescope site component reporting wcs psf information dm image basis depend ability t&s group engage subject current pmcs deadline phase readiness lse-75 29 sep-2015,LSE-75: Refine WCS and PSF requirements in W16 Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis. Depends on the ability of the T&S group to engage with this subject. Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.
LSE-68: ICD Details in X16 Bring ICD to phase 3 level of detail,6,DM-3842,datamanagement,lse-68 icd details x16 bring icd phase level detail,LSE-68: ICD Details in X16 Bring ICD to phase 3 level of detail
"LSE-74: ICD Details in W16 ""Bring ICD to phase 3 level of detail"" was the original specification, but actual work by the OCS group in the Winter 2016 period didn't quite reach Phase 3.  Nonetheless, a very useful revision was submitted to and recommended for approval by the CCB at the March meeting.",6,DM-3843,datamanagement,lse-74 icd details w16 bring icd phase level detail original specification actual work ocs group winter 2016 period reach phase nonetheless useful revision submit recommend approval ccb march meeting,"LSE-74: ICD Details in W16 ""Bring ICD to phase 3 level of detail"" was the original specification, but actual work by the OCS group in the Winter 2016 period didn't quite reach Phase 3. Nonetheless, a very useful revision was submitted to and recommended for approval by the CCB at the March meeting."
"Add tutorial-level documentation for ctrl_pool The new ctrl_pool package (port of hscPipeBase) has no tutorial-level documentation, making it hard to figure out how to start using the package.    Unfortunately, I think only [~price] is qualified to write it directly, though it may make sense to have someone unfamiliar write it while bugging Paul a lot, both to transfer the knowledge and target the documentation better.",4,DM-3844,datamanagement,add tutorial level documentation ctrl_pool new ctrl_pool package port hscpipebase tutorial level documentation make hard figure start package unfortunately think ~price qualified write directly sense unfamiliar write bug paul lot transfer knowledge target documentation well,"Add tutorial-level documentation for ctrl_pool The new ctrl_pool package (port of hscPipeBase) has no tutorial-level documentation, making it hard to figure out how to start using the package. Unfortunately, I think only [~price] is qualified to write it directly, though it may make sense to have someone unfamiliar write it while bugging Paul a lot, both to transfer the knowledge and target the documentation better."
"Add unit tests for ctrl_pool ctrl_pool (formerly hscPipeBase) is being ported with no unit tests - the only testing is an example script that can be run by hand to demonstrate a piece of the functionality.    Some functionality may simple not be amenable to tests (such as batch submission).  Other parts may be tricky to run via SCons because they're intrinsically parallel, and SCons naively wants to be able to run each test in a separate process.  Overcoming those problems is the reason this is challenging - there isn't really that much functionality to test.",8,DM-3845,datamanagement,add unit test ctrl_pool ctrl_pool hscpipebase port unit test testing example script run hand demonstrate piece functionality functionality simple amenable test batch submission part tricky run scons intrinsically parallel scons naively want able run test separate process overcome problem reason challenge functionality test,"Add unit tests for ctrl_pool ctrl_pool (formerly hscPipeBase) is being ported with no unit tests - the only testing is an example script that can be run by hand to demonstrate a piece of the functionality. Some functionality may simple not be amenable to tests (such as batch submission). Other parts may be tricky to run via SCons because they're intrinsically parallel, and SCons naively wants to be able to run each test in a separate process. Overcoming those problems is the reason this is challenging - there isn't really that much functionality to test."
Read over LSE-70 and LSE-209 and discuss for meeting Read over LSE-70 and LSE-209 for meeting on Friday 9/11.,2,DM-3846,datamanagement,read lse-70 lse-209 discuss meet read lse-70 lse-209 meeting friday 9/11,Read over LSE-70 and LSE-209 and discuss for meeting Read over LSE-70 and LSE-209 for meeting on Friday 9/11.
"Grid overly bug when using the grid overlay in galatic coordinate over an image that is around longitude = 0, the overlay doesn't work properly. In BOLOCAM, on fits image works but not the other one.    Reproducible:	  Steps to Reproduce:	  Go to Atlas search, and select BOLOCAM galactic plane survey (GPS).  Then enter single coordinate search on ""0 0 gal"" to find images taken around that.     See column ""FITS filename"" and search for the sharc-ii instrument images such as  images/sharc2/L000.15+0.00.fits [bad]   and   images/sharc2/L000.00+0.00.fits [good]     Then open it on irsaviewer by clicking on the icon link (first column).  On the image viewer, enable the grid overlay and click on the icon 'layer' on the toolbar.  Change the coordinate system to galactic to see the problem.     On the overlay for the associated Bolocam map there are three horizontal lines which exist only on the left hand side of the image. When the image coordinate system is set to ""Gal"", the reported GLON values range from -355.4 to -1.6 (from left to right), passing through -0.0 in the center.     Same problem can be seen for   images/v1/INNER_GALAXY/map/v1.0.2_super_gc_13pca_map50_crop.fits [bad]  ",4,DM-3847,datamanagement,grid overly bug grid overlay galatic coordinate image longitude overlay work properly bolocam fit image work reproducible step reproduce atlas search select bolocam galactic plane survey gps enter single coordinate search gal find image take column fit filename search sharc ii instrument image image sharc2 l000.15 0.00.fit bad image sharc2 l000.00 0.00.fit good open irsaviewer click icon link column image viewer enable grid overlay click icon layer toolbar change coordinate system galactic problem overlay associate bolocam map horizontal line exist left hand image image coordinate system set gal report glon value range -355.4 -1.6 leave right pass -0.0 center problem see image v1 inner_galaxy map v1.0.2_super_gc_13pca_map50_crop.fit bad,"Grid overly bug when using the grid overlay in galatic coordinate over an image that is around longitude = 0, the overlay doesn't work properly. In BOLOCAM, on fits image works but not the other one. Reproducible: Steps to Reproduce: Go to Atlas search, and select BOLOCAM galactic plane survey (GPS). Then enter single coordinate search on ""0 0 gal"" to find images taken around that. See column ""FITS filename"" and search for the sharc-ii instrument images such as images/sharc2/L000.15+0.00.fits [bad] and images/sharc2/L000.00+0.00.fits [good] Then open it on irsaviewer by clicking on the icon link (first column). On the image viewer, enable the grid overlay and click on the icon 'layer' on the toolbar. Change the coordinate system to galactic to see the problem. On the overlay for the associated Bolocam map there are three horizontal lines which exist only on the left hand side of the image. When the image coordinate system is set to ""Gal"", the reported GLON values range from -355.4 to -1.6 (from left to right), passing through -0.0 in the center. Same problem can be seen for images/v1/INNER_GALAXY/map/v1.0.2_super_gc_13pca_map50_crop.fits [bad]"
"Nebula metadata service is intermittent Upon restarting one of my nebula instances (ktl-test), I noticed a failure in the logs:  {quote}  Sep 14 18:07:29 ktl-test cloud-init: 2015-09-14 18:07:29,157 - util.py[WARNING]: Failed fetching metadata from url http://169.254.169.254/latest/meta-data  {quote}    Attempting to retrieve that URL seems to randomly vary between succeeding, which returns:  {quote}  ami-id  ami-launch-index  ami-manifest-path  [...]  {quote}    and failing, which returns:  {quote}  <html>   <head>    <title>500 Internal Server Error</title>   </head>   <body>    <h1>500 Internal Server Error</h1>    Remote metadata server experienced an internal server error.<br /><br />         </body>  </html>  {quote}  These failures may be contributing to observed sporadic {{ssh}} key injection failures.",2,DM-3850,datamanagement,"nebula metadata service intermittent restart nebula instance ktl test notice failure log quote sep 14 18:07:29 ktl test cloud init 2015 09 14 18:07:29,157 util.py[warning fail fetch metadata url http://169.254.169.254 late meta data quote attempt retrieve url randomly vary succeeding return quote ami id ami launch index ami manifest path ... quote fail return quote    500 internal server error    500 internal server error remote metadata server experience internal server error    quote failure contribute observe sporadic ssh key injection failure","Nebula metadata service is intermittent Upon restarting one of my nebula instances (ktl-test), I noticed a failure in the logs: {quote} Sep 14 18:07:29 ktl-test cloud-init: 2015-09-14 18:07:29,157 - util.py[WARNING]: Failed fetching metadata from url http://169.254.169.254/latest/meta-data {quote} Attempting to retrieve that URL seems to randomly vary between succeeding, which returns: {quote} ami-id ami-launch-index ami-manifest-path [...] {quote} and failing, which returns: {quote}   500 Internal Server Error   500 Internal Server Error Remote metadata server experienced an internal server error.   {quote} These failures may be contributing to observed sporadic {{ssh}} key injection failures."
Liaise with Long-Haul Network group on Base Site to NCSA network Coordinate with network group to establish Base Site-to-NCSA LHN.    Assignees: Paul Wefel  Duration: September 2015 - February 2016,4,DM-3851,datamanagement,liaise long haul network group base site ncsa network coordinate network group establish base site ncsa lhn assignee paul wefel duration september 2015 february 2016,Liaise with Long-Haul Network group on Base Site to NCSA network Coordinate with network group to establish Base Site-to-NCSA LHN. Assignees: Paul Wefel Duration: September 2015 - February 2016
"Web design fixes DM Design Documents on Sphinx/Read The Docs Solve fit-and-finish issues with the stock readthedocs.org Sphinx template when rendering DM design documents. Issues include:    * Sections need to be numbered and those numbers need to appear in TOC  * RTD's TOC does not properly collapse sub-topics  * Appropriate styling for document title and author list  * Wrapping the changelog table  * Adapt section references so that just the section number can be referenced, independently of the section number and title in combination  * Section labels given explicitly in the reST markup are different from the anchors that Sphinx gives to the {{<hN>}}tags; the former are simply divs inserted in the HTML.    The solutions may involve    # reconfiguring the Sphinx installation of individual documents  # forking the RTD HTML template, and/or  # developing extensions for Sphinx in {{sphinxkit}}.",2,DM-3863,datamanagement,web design fix dm design documents sphinx read docs solve fit finish issue stock readthedocs.org sphinx template render dm design document issue include section need number number need appear toc rtd toc properly collapse sub topic appropriate styling document title author list wrap changelog table adapt section reference section number reference independently section number title combination section label give explicitly rest markup different anchor sphinx give tag simply div insert html solution involve reconfigure sphinx installation individual document fork rtd html template and/or develop extension sphinx sphinxkit,"Web design fixes DM Design Documents on Sphinx/Read The Docs Solve fit-and-finish issues with the stock readthedocs.org Sphinx template when rendering DM design documents. Issues include: * Sections need to be numbered and those numbers need to appear in TOC * RTD's TOC does not properly collapse sub-topics * Appropriate styling for document title and author list * Wrapping the changelog table * Adapt section references so that just the section number can be referenced, independently of the section number and title in combination * Section labels given explicitly in the reST markup are different from the anchors that Sphinx gives to the {{}}tags; the former are simply divs inserted in the HTML. The solutions may involve # reconfiguring the Sphinx installation of individual documents # forking the RTD HTML template, and/or # developing extensions for Sphinx in {{sphinxkit}}."
"Do basic tests of CModel ellipticity measurements I have seen enough anomalies that I have had to go back a bit and do some basic tests of CModel and its ellipticity bias.  This involves running the same pipeline as before, but with controlled galaxy profiles, ellipticities, and angles.  These are zero-shear, zero-seeing tests which I probably should have run first thing.    It I understand everything I see in these tests, I will be confident that the results I am seeing for CModel with varying footprint size, varying nInitialRadii, and varying stamp size are correct.",4,DM-3866,datamanagement,basic test cmodel ellipticity measurement see anomaly bit basic test cmodel ellipticity bias involve run pipeline control galaxy profile ellipticity angle zero shear zero see test probably run thing understand test confident result see cmodel vary footprint size vary ninitialradii vary stamp size correct,"Do basic tests of CModel ellipticity measurements I have seen enough anomalies that I have had to go back a bit and do some basic tests of CModel and its ellipticity bias. This involves running the same pipeline as before, but with controlled galaxy profiles, ellipticities, and angles. These are zero-shear, zero-seeing tests which I probably should have run first thing. It I understand everything I see in these tests, I will be confident that the results I am seeing for CModel with varying footprint size, varying nInitialRadii, and varying stamp size are correct."
Gather requirements to inform a redesign of the CalibrateTask The current calibrate task is fairly brittle and hard to extend.   This task is to gather the necessary requirements for a redesigned calibrate task.,2,DM-3881,datamanagement,gather requirement inform redesign calibratetask current calibrate task fairly brittle hard extend task gather necessary requirement redesign calibrate task,Gather requirements to inform a redesign of the CalibrateTask The current calibrate task is fairly brittle and hard to extend. This task is to gather the necessary requirements for a redesigned calibrate task.
"Create initial cluster design, send internally for feedback and planning Gather feedback on initial designs for FY16 purchase plans.",2,DM-3883,datamanagement,create initial cluster design send internally feedback plan gather feedback initial design fy16 purchase plan,"Create initial cluster design, send internally for feedback and planning Gather feedback on initial designs for FY16 purchase plans."
Create data products description Addition of [https://confluence.lsstcorp.org/display/~petravick/Products+of+Image+Ingest+and+Processing] to understand more of the requirements necessary for the functional design  ,3,DM-3884,datamanagement,create data product description addition https://confluence.lsstcorp.org/display/~petravick/products+of+image+ingest+and+processing understand requirement necessary functional design,Create data products description Addition of [https://confluence.lsstcorp.org/display/~petravick/Products+of+Image+Ingest+and+Processing] to understand more of the requirements necessary for the functional design
"LSE-78: W16 revisions, harmonization with existing design Review LSE-78 for self-consistency and consistency with the current DM and overall system design.",6,DM-3885,datamanagement,lse-78 w16 revision harmonization exist design review lse-78 self consistency consistency current dm overall system design,"LSE-78: W16 revisions, harmonization with existing design Review LSE-78 for self-consistency and consistency with the current DM and overall system design."
"Revise early integration milestones, LCR-323 and beyond Revise the list of early integration milestones with OCS, TCS, CCS, and DAQ to form a coherent plan.  Coordinate with NCSA and other interested parties in DM.",6,DM-3886,datamanagement,revise early integration milestone lcr-323 revise list early integration milestone ocs tcs ccs daq form coherent plan coordinate ncsa interested party dm,"Revise early integration milestones, LCR-323 and beyond Revise the list of early integration milestones with OCS, TCS, CCS, and DAQ to form a coherent plan. Coordinate with NCSA and other interested parties in DM."
"Add missing space after if in Qserv code to conform to standard Replace ""if("" with ""if ("" to follow standard.    find core/modules/ -name ""*.cc"" |xargs grep ""if(""|wc -l  852    ",1,DM-3888,datamanagement,add missing space qserv code conform standard replace follow standard find core modules/ -name .cc |xargs grep -l 852,"Add missing space after if in Qserv code to conform to standard Replace ""if("" with ""if ("" to follow standard. find core/modules/ -name ""*.cc"" |xargs grep ""if(""|wc -l 852"
Review LCR-323 proposal for integration milestones Prepare for CCB action on LCR-323.  Ensure that DAQ integration is included (it's not in the original LCR proposal).,4,DM-3891,datamanagement,review lcr-323 proposal integration milestone prepare ccb action lcr-323 ensure daq integration include original lcr proposal,Review LCR-323 proposal for integration milestones Prepare for CCB action on LCR-323. Ensure that DAQ integration is included (it's not in the original LCR proposal).
"Review current version of LSE-78, prepare for LCR Do a comprehensive read-through of the previous released version of LSE-78.  Look for self-consistency and for consistency with the rest of the DM and overall system design.  Report issues to appropriate people.",3,DM-3892,datamanagement,review current version lse-78 prepare lcr comprehensive read previous release version lse-78 look self consistency consistency rest dm overall system design report issue appropriate people,"Review current version of LSE-78, prepare for LCR Do a comprehensive read-through of the previous released version of LSE-78. Look for self-consistency and for consistency with the rest of the DM and overall system design. Report issues to appropriate people."
Research existing DHT-based FS approaches  The previous prototype provided confidence that a DHT overlay could work for routing and placement at the scales and time constants needed for chunk distribution.  The next prototype will need actual data transport and storage management facilities layered on the node/key management provided by the DHT layer.  Explore existing works at this level to a greater depth pick from among proven approaches.,6,DM-3893,datamanagement,research exist dht base fs approach previous prototype provide confidence dht overlay work routing placement scale time constant need chunk distribution prototype need actual data transport storage management facility layer node key management provide dht layer explore exist work level great depth pick prove approach,Research existing DHT-based FS approaches The previous prototype provided confidence that a DHT overlay could work for routing and placement at the scales and time constants needed for chunk distribution. The next prototype will need actual data transport and storage management facilities layered on the node/key management provided by the DHT layer. Explore existing works at this level to a greater depth pick from among proven approaches.
"Provide values for relative astrometry KPMs in FY15 Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",2,DM-3894,datamanagement,provide value relative astrometry kpm fy15 produce numeric value require explanation available description process generate incl datum process script plotting etc,"Provide values for relative astrometry KPMs in FY15 Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc)."
"Provide values for PSF ellipticity KPMs in FY15 Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",1,DM-3895,datamanagement,provide value psf ellipticity kpm fy15 produce numeric value require explanation available description process generate incl datum process script plotting etc,"Provide values for PSF ellipticity KPMs in FY15 Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc)."
"Provide values for photometric repeatability KPMs in FY15  Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",4,DM-3896,datamanagement,provide value photometric repeatability kpm fy15 produce numeric value require explanation available description process generate incl datum process script plotting etc,"Provide values for photometric repeatability KPMs in FY15 Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc)."
"Provide value for DRP computational budget KPM in FY15 Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc).",3,DM-3897,datamanagement,provide value drp computational budget kpm fy15 produce numeric value require explanation available description process generate incl datum process script plotting etc,"Provide value for DRP computational budget KPM in FY15 Should produce the numeric values required (or an explanation of why they aren't available) together with a description of the process for generating them (incl. the data processed, scripts used for plotting, etc)."
"Fix xrootd compiler warnings with clang h5. Xrootd    {code}  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:33:  In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a        struct here but previously declared as a class [-Wmismatched-tags]  struct  XrdSsiRespInfo  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here?  class XrdSsiRespInfo;  ^~~~~  struct  {code}    {code}  core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual]      virtual int Opendir(const char *) { return XrdOssOK; }                  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function        'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1)  virtual int     Opendir(const char *, XrdOucEnv &)           {return -ENOTDIR;}                  ^  {code}    {code}  In file included from core/modules/xrdsvc/SsiSession.h:32:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of        non-void function [-Wreturn-type]                            }                            ^  {code}",1,DM-3898,datamanagement,fix xrootd compiler warning clang h5 xrootd code file include core module qdisp executive.cc:64 file include core module qdisp xrdssimocks.h:33 file include /users timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdssi xrdssirequest.hh:37 /users timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdssi xrdssirespinfo.hh:43:1 warning xrdssirespinfo define struct previously declare class -wmismatched tag struct xrdssirespinfo timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdssi xrdssisession.hh:45:1 note mean struct class xrdssirespinfo ^~~~~ struct code code core module xrdoss qservoss.h:64:17 warning lsst::qserv::xrdoss::fakeossdf::opendir hide overload virtual function -woverloade virtual virtual int opendir(const char return xrdossok timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdoss xrdoss.hh:63:17 note hide overload virtual function xrdossdf::opendir declare different number parameter vs virtual int opendir(const char xrdoucenv return -enotdir code code file include core module xrdsvc ssisession.h:32 /users timj work lsstsw stack darwinx86 xrootd u.timj dm-3584 ge22410fa7f+da39a3ee5e include xrootd xrdssi xrdssiresponder.hh:177:27 warning control reach end non void function -wreturn type code,"Fix xrootd compiler warnings with clang h5. Xrootd {code} In file included from core/modules/qdisp/Executive.cc:64: In file included from core/modules/qdisp/XrdSsiMocks.h:33: In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37: /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct XrdSsiRespInfo ^ /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here? class XrdSsiRespInfo; ^~~~~ struct {code} {code} core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual] virtual int Opendir(const char *) { return XrdOssOK; } ^ /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function 'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1) virtual int Opendir(const char *, XrdOucEnv &) {return -ENOTDIR;} ^ {code} {code} In file included from core/modules/xrdsvc/SsiSession.h:32: /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of non-void function [-Wreturn-type] } ^ {code}"
"support shared_ptr<Statistics> I would like to write some functions that return afw::math::Statistics objects and wrap them with SWIG. Unfortunately SWIG requires that any object returned by value must have a default constructor, and Statistics does not. Rather than try to add such an object I propose to make my functions return a shared_ptr to Statistics.    This ticket is a request to support that by adding the following to statistics.i:   {code}  %shared_ptr(lsst::afw::math::Statistics);  {code}  ",2,DM-3899,datamanagement,support shared_ptr like write function return afw::math::statistics object wrap swig unfortunately swig require object return value default constructor statistics try add object propose function return shared_ptr statistics ticket request support add following statistics.i code shared_ptr(lsst::afw::math::statistics code,"support shared_ptr I would like to write some functions that return afw::math::Statistics objects and wrap them with SWIG. Unfortunately SWIG requires that any object returned by value must have a default constructor, and Statistics does not. Rather than try to add such an object I propose to make my functions return a shared_ptr to Statistics. This ticket is a request to support that by adding the following to statistics.i: {code} %shared_ptr(lsst::afw::math::Statistics); {code}"
Review of [DM-2983] I was asked for a revision of [DM-2983] which is part of the Backport HSC parallelization code,4,DM-3900,datamanagement,review dm-2983 ask revision dm-2983 backport hsc parallelization code,Review of [DM-2983] I was asked for a revision of [DM-2983] which is part of the Backport HSC parallelization code
Update some tests to support nose and/or py.test When {{sconsUtils}} is migrated to use {{nose}} or {{py.test}} some test scripts will need to be modified because test discovery will be slightly different and the namespace of test execution will change.    Two things to consider:  * People would still like the option of running a test as {{python tests/testMe.py}}.  * We have to work out how to run the memory test case.  ,4,DM-3901,datamanagement,update test support nose and/or py.test sconsutils migrate use nose py.t test script need modify test discovery slightly different namespace test execution change thing consider people like option run test python test testme.py work run memory test case,Update some tests to support nose and/or py.test When {{sconsUtils}} is migrated to use {{nose}} or {{py.test}} some test scripts will need to be modified because test discovery will be slightly different and the namespace of test execution will change. Two things to consider: * People would still like the option of running a test as {{python tests/testMe.py}}. * We have to work out how to run the memory test case.
Gathering use cases for verification data sets Seeking out developer use cases of incoming data sets. Need to determine if datasets will be accessed for verification only or by developers and QA in general. Determine access methods. ,3,DM-3905,datamanagement,gather use case verification data set seek developer use case incoming data set need determine dataset access verification developer qa general determine access method,Gathering use cases for verification data sets Seeking out developer use cases of incoming data sets. Need to determine if datasets will be accessed for verification only or by developers and QA in general. Determine access methods.
"Run and document multi-node test with docker In order to validate Docker setup on CC-IN2P3 cluster, it is required to launch some test on consistent data. S15 LargeScaleTest data doesn't seems to be compliant with latest Qserv version so running multi-node test would be interesting. Nevertheless the multi-node setup doesn't seems to be documented and, hence, is difficult to reproduce.",3,DM-3910,datamanagement,run document multi node test docker order validate docker setup cc in2p3 cluster require launch test consistent datum s15 largescaletest datum compliant late qserv version run multi node test interesting multi node setup document difficult reproduce,"Run and document multi-node test with docker In order to validate Docker setup on CC-IN2P3 cluster, it is required to launch some test on consistent data. S15 LargeScaleTest data doesn't seems to be compliant with latest Qserv version so running multi-node test would be interesting. Nevertheless the multi-node setup doesn't seems to be documented and, hence, is difficult to reproduce."
"HSC backport: avoid I/O race conditions config write out This is a port of [HSC-1106|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1106]    When running tasks that write out config settings files ({{processCcd.py}}, for example), if multiple processes start simultaneously, an I/O race condition can occur in writing these files.  This is solved here by writing to temp files and then renaming them to the correct destination filename in a single operation.  Also, to avoid similar race conditions in the backup file creation (e.g. config.py~1, config.py~2, ...), a {{--no-backup-config}} option (to be used with --clobber-config) is added here to prevent the backup copies being made.  The outcome for this option is that the config that are still recorded are for the most recent run.",1,DM-3911,datamanagement,hsc backport avoid race condition config write port hsc-1106|https://hsc jira.astro.princeton.edu jira browse hsc-1106 run task write config setting file processccd.py example multiple process start simultaneously race condition occur write file solve write temp file rename correct destination filename single operation avoid similar race condition backup file creation e.g. config.py~1 config.py~2 --no backup config option --clobber config add prevent backup copy outcome option config record recent run,"HSC backport: avoid I/O race conditions config write out This is a port of [HSC-1106|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1106] When running tasks that write out config settings files ({{processCcd.py}}, for example), if multiple processes start simultaneously, an I/O race condition can occur in writing these files. This is solved here by writing to temp files and then renaming them to the correct destination filename in a single operation. Also, to avoid similar race conditions in the backup file creation (e.g. config.py~1, config.py~2, ...), a {{--no-backup-config}} option (to be used with --clobber-config) is added here to prevent the backup copies being made. The outcome for this option is that the config that are still recorded are for the most recent run."
"some ctrl_events tests execute outside of execution domain There are a couple of ctrl_events tests that attempt to execute outside of the valid domains acceptable by the tests, when they shouldn't be.  There's a check in place for tests to find this, but a couple of the tests do not have this check.",1,DM-3912,datamanagement,ctrl_event test execute outside execution domain couple ctrl_event test attempt execute outside valid domain acceptable test check place test find couple test check,"some ctrl_events tests execute outside of execution domain There are a couple of ctrl_events tests that attempt to execute outside of the valid domains acceptable by the tests, when they shouldn't be. There's a check in place for tests to find this, but a couple of the tests do not have this check."
"transfer and update orchestration documentation The self-service orchestration documentation needs to be transferred from Trac to Confluence, and updated.",2,DM-3913,datamanagement,transfer update orchestration documentation self service orchestration documentation need transfer trac confluence update,"transfer and update orchestration documentation The self-service orchestration documentation needs to be transferred from Trac to Confluence, and updated."
"Misc work for this reporting week.  Deal with comments,  coordinate with Jason Alt. Deal with comments on existing work from KT and GDF",2,DM-3916,datamanagement,misc work reporting week deal comment coordinate jason alt deal comment exist work kt gdf,"Misc work for this reporting week. Deal with comments, coordinate with Jason Alt. Deal with comments on existing work from KT and GDF"
"Begin thinking about governance aspects of DM operations Thinking, (but not delivered use case)  -- what is the flow of tickets and the division of use cases between the Science and Data Operations?  What kind of tooling needs can be assumed to exist to support service management what does that mean for the support of processes?  (specifically  looked at a summary of the state the market, and took specifics look at what ITSM process supporting tools are open source. -- as may be useful to prototype processes before committing to something..  Looked at several, ITOP seemed to be mature/documented to a level that may be useable.    ",4,DM-3917,datamanagement,begin think governance aspect dm operation thinking deliver use case flow ticket division use case science data operations kind tooling need assume exist support service management mean support process specifically look summary state market take specific look itsm process support tool open source useful prototype process commit look itop mature document level useable,"Begin thinking about governance aspects of DM operations Thinking, (but not delivered use case) -- what is the flow of tickets and the division of use cases between the Science and Data Operations? What kind of tooling needs can be assumed to exist to support service management what does that mean for the support of processes? (specifically looked at a summary of the state the market, and took specifics look at what ITSM process supporting tools are open source. -- as may be useful to prototype processes before committing to something.. Looked at several, ITOP seemed to be mature/documented to a level that may be useable."
"Conduct, document, initial follow through on Sept JCC meeting  Developed agenda items, considered JCC meeting.  Minutes are on the LSST confluence.  A major item of discussion was related to operations, since we are told this is a priority.  Befall followup on CCIN2P3 ISM practices. ",1,DM-3918,datamanagement,conduct document initial follow sept jcc meet developed agenda item consider jcc meeting minute lsst confluence major item discussion relate operation tell priority befall followup ccin2p3 ism practice,"Conduct, document, initial follow through on Sept JCC meeting Developed agenda items, considered JCC meeting. Minutes are on the LSST confluence. A major item of discussion was related to operations, since we are told this is a priority. Befall followup on CCIN2P3 ISM practices."
"General Management  Hiring,  Internal relationships within the NCSA organization. LSST meetings, general management",4,DM-3919,datamanagement,general management hiring internal relationship ncsa organization lsst meeting general management,"General Management Hiring, Internal relationships within the NCSA organization. LSST meetings, general management"
"Replace boost::regex with std::regex Boost 1.59 causes a ""keyword hidden by macro"" warn under clang in the regex package.  We should be using std::regex now anyway, so this is a good motivator to go ahead and convert.",1,DM-3920,datamanagement,replace boost::regex std::regex boost 1.59 cause keyword hide macro warn clang regex package std::regex good motivator ahead convert,"Replace boost::regex with std::regex Boost 1.59 causes a ""keyword hidden by macro"" warn under clang in the regex package. We should be using std::regex now anyway, so this is a good motivator to go ahead and convert."
Update multi-node setup documentation Workers in multi-node setup no longer require granting mysql permissions for test datasets since direct mysql connections are no longer used by the data loader.,1,DM-3922,datamanagement,update multi node setup documentation workers multi node setup long require grant mysql permission test dataset direct mysql connection long datum loader,Update multi-node setup documentation Workers in multi-node setup no longer require granting mysql permissions for test datasets since direct mysql connections are no longer used by the data loader.
Centralize Sphinx configuration for Design Documents Centralize Sphinx configuration for design documents in {{documenteer}} and provide a facility for design document authors to use YAML files to store document metadata rather than editing {{conf.py}} files.,2,DM-3924,datamanagement,centralize sphinx configuration design documents centralize sphinx configuration design document documenteer provide facility design document author use yaml file store document metadata edit conf.py file,Centralize Sphinx configuration for Design Documents Centralize Sphinx configuration for design documents in {{documenteer}} and provide a facility for design document authors to use YAML files to store document metadata rather than editing {{conf.py}} files.
Implement iostream-style formatting in log package Implement proposed in RFC-96 change to log macros. This ticket only covers defining new set of macros (LOGS() and friends) which use ostringstream for formatting messages. Migration of all clients and removal of LOGF macros will be done in separate ticket.,1,DM-3926,datamanagement,implement iostream style format log package implement propose rfc-96 change log macro ticket cover define new set macro logs friend use ostringstream format message migration client removal logf macro separate ticket,Implement iostream-style formatting in log package Implement proposed in RFC-96 change to log macros. This ticket only covers defining new set of macros (LOGS() and friends) which use ostringstream for formatting messages. Migration of all clients and removal of LOGF macros will be done in separate ticket.
"Handle queries with no database Sqlalchemy is generating some queries that are currently killing czar, the list is:    {code}  set autocommit=0  SHOW VARIABLES LIKE 'sql_mode'  SELECT DATABASE()  SELECT @@tx_isolation  show collation where `Charset` = 'utf8' and `Collation` = 'utf8_bin'  SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1  SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1  SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1  SELECT 'x' AS some_label  select @@version_comment limit 1  {code}    Czar should survive unfriendly syntax, and this will be addressed through DM-3764.    In this story we will make sure that  sqlalchemy-generated queries are properly handled (not just these particular queries, but all queries that do not involve any database and any table). We should run such queries on a local mysql instance (alternatively, perhaps redirect to one of the workers?)",4,DM-3929,datamanagement,handle query database sqlalchemy generate query currently kill czar list code set autocommit=0 variables like sql_mode select database select @@tx_isolation collation charset utf8 collation utf8_bin select cast('test plain return char(60 anon_1 select cast('test unicode return char(60 anon_1 select cast('test collate return char character set utf8 collate utf8_bin anon_1 select some_label select @@version_comment limit code czar survive unfriendly syntax address dm-3764 story sure sqlalchemy generate query properly handle particular query query involve database table run query local mysql instance alternatively redirect worker,"Handle queries with no database Sqlalchemy is generating some queries that are currently killing czar, the list is: {code} set autocommit=0 SHOW VARIABLES LIKE 'sql_mode' SELECT DATABASE() SELECT @@tx_isolation show collation where `Charset` = 'utf8' and `Collation` = 'utf8_bin' SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1 SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1 SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1 SELECT 'x' AS some_label select @@version_comment limit 1 {code} Czar should survive unfriendly syntax, and this will be addressed through DM-3764. In this story we will make sure that sqlalchemy-generated queries are properly handled (not just these particular queries, but all queries that do not involve any database and any table). We should run such queries on a local mysql instance (alternatively, perhaps redirect to one of the workers?)"
"AP, Co-add, Image cache definitions Added physical breakdown for Alert Postage Stamp Images file system, co-add images file system and image cache file system",2,DM-3930,datamanagement,ap co add image cache definition add physical breakdown alert postage stamp images file system co add image file system image cache file system,"AP, Co-add, Image cache definitions Added physical breakdown for Alert Postage Stamp Images file system, co-add images file system and image cache file system"
"sandbox-stackbuild issues A number of issues with sandbox-stackbuild, or rather its infrastructure.    1. Problem with the librarian-puppet plugin and its mismatch with the puppet forge API ([~jhoblitt] has a PR open apparently)    2. As a workaround to above, one needs to    {code}  gem install librarian-puppet  librarian-puppet install  {code}    but that runs into an issue with swap_file needing a downgrade to work with Ubuntu 14.04. Working state as of the time of this bug report for the Puppetfile is:    {code}    forge 'https://forgeapi.puppetlabs.com'    mod 'puppetlabs/stdlib'  mod 'camptocamp/augeas', '~> 1.4'  mod 'stahnma/epel', '~> 1.1'  mod 'petems/swap_file', '1.0.1'  mod 'jhoblitt/sysstat', '~> 1.1'  mod 'maestrodev/wget', '~> 1.7'    mod 'jhoblitt/lsststack', :git => 'https://github.com/lsst-sqre/puppet-lsststack.git'  {code}    3. Which brings us to the fact that the Vagrant puppet-install plugin is broken with Puppet 4, and new platforms are not supported under Puppet 3. Ergo, as is, can't bring up Ubuntu 15.05 etc.     Ticket is to get PRs merged, fork and fix them ourselves, or find alternatives.   ",4,DM-3931,datamanagement,sandbox stackbuild issue number issue sandbox stackbuild infrastructure problem librarian puppet plugin mismatch puppet forge api ~jhoblitt pr open apparently workaround need code gem install librarian puppet librarian puppet install code run issue swap_file need downgrade work ubuntu 14.04 work state time bug report puppetfile code forge https://forgeapi.puppetlabs.com mod puppetlab stdlib mod camptocamp augeas 1.4 mod stahnma epel 1.1 mod petems swap_file 1.0.1 mod jhoblitt sysstat 1.1 mod maestrodev wget 1.7 mod jhoblitt lsststack git code bring fact vagrant puppet install plugin break puppet new platform support puppet ergo bring ubuntu 15.05 etc ticket pr merge fork fix find alternative,"sandbox-stackbuild issues A number of issues with sandbox-stackbuild, or rather its infrastructure. 1. Problem with the librarian-puppet plugin and its mismatch with the puppet forge API ([~jhoblitt] has a PR open apparently) 2. As a workaround to above, one needs to {code} gem install librarian-puppet librarian-puppet install {code} but that runs into an issue with swap_file needing a downgrade to work with Ubuntu 14.04. Working state as of the time of this bug report for the Puppetfile is: {code} forge 'https://forgeapi.puppetlabs.com' mod 'puppetlabs/stdlib' mod 'camptocamp/augeas', '~> 1.4' mod 'stahnma/epel', '~> 1.1' mod 'petems/swap_file', '1.0.1' mod 'jhoblitt/sysstat', '~> 1.1' mod 'maestrodev/wget', '~> 1.7' mod 'jhoblitt/lsststack', :git => 'https://github.com/lsst-sqre/puppet-lsststack.git' {code} 3. Which brings us to the fact that the Vagrant puppet-install plugin is broken with Puppet 4, and new platforms are not supported under Puppet 3. Ergo, as is, can't bring up Ubuntu 15.05 etc. Ticket is to get PRs merged, fork and fix them ourselves, or find alternatives."
"Histogram calculation for image stretch has infinite loop  When load the big.fits file, the image never came out.  It stopped at Histogram.  There was an infinity in Histogram.   ",4,DM-3932,datamanagement,histogram calculation image stretch infinite loop load big.fit file image come stop histogram infinity histogram,"Histogram calculation for image stretch has infinite loop When load the big.fits file, the image never came out. It stopped at Histogram. There was an infinity in Histogram."
"Measurement plugin errors When doing measurements on coadds, several errors are thrown within the measurement plugins.    {code}  Error in base_GaussianFlux.measure on record 283467884979: Input shape is singular  {code}    {code}  Error in base_GaussianFlux.measure on record 283467883979:     File ""src/SdssShape.cc"", line 842, in static lsst::meas::base::FluxResult lsst::meas::base::SdssShapeAlgorithm::computeFixedMomentsFlux(const ImageT&, const lsst::afw::geom::ellipses::Quadrupole&, const Point2D&) [with ImageT = lsst::afw::image::MaskedImage<float, short unsigned int, float>; lsst::afw::geom::Point2D = lsst::afw::geom::Point<double, 2>]      Error from calcmom {0}  lsst::pex::exceptions::RuntimeError: 'Error from calcmom'  {code}    The measurements were done on tiger, and the command used was:  {code}  measureCoaddSources.py /tigress/HSC/HSC/rerun/nate/old_clip/ --output=/tigress/HSC/HSC/rerun/nate/old_clip/ -C /home/nlust/options_temp.py --id tract=0 patch=2,2 filter=HSC-I^HSC-R   {code}  The data can be found within the rerun directory specified as the input to the command. The data was created using the commands:  {code}  assembleCoadd.py  --legacyCoadd /tigress/HSC/HSC/rerun/nate/ --output=/tigress/HSC/HSC/rerun/nate/old_clip --id tract=0 patch=2,2 filter=HSC-R --selectId visit=1208 ccd=56^64^72 --selectId visit=1206 ccd=64^65^72^73^79^80 --selectId visit=1212 ccd=64^65^72^73^79^80 --selectId visit=23704 ccd=64^65^72^73 --selectId visit=23706 ccd=56^64^72 --selectId visit=23694 ccd=64^65^72^73^79^80 --selectId visit=1204 ccd=64^65^72^73^79^80 --selectId visit=1220 ccd=63^64^71^72^78^79 --selectId visit=1218 ccd=56^57^64^65^72^73 --selectId visit=23718 ccd=64^65^72^73^79^80 --selectId visit=23692 ccd=64^65^72^73^79^80 --selectId visit=1210 ccd=63^64^71^72^78^79 --selectId visit=1216 ccd=56^57^64^65^72^73 --selectId visit=1214 ccd=64^65^72^73^79^80 --selectId visit=23716 ccd=63^64^71^72^78^79 --selectId visit=1202 ccd=64^65^72^73^79^80  {code}  and   {code}  assembleCoadd.py --legacyCoadd /tigress/HSC/HSC/rerun/nate/ --output=/tigress/HSC/HSC/rerun/nate/old_clip --id tract=0 patch=2,2 filter=HSC-I --selectId visit=19658 ccd=64^65^72^73^79^80 --selectId visit=1248 ccd=56^64^72 --selectId visit=19696 ccd=65^66^73^74^80^81 --selectId visit=19684 ccd=64^65^72^73^79^80 --selectId visit=1238 ccd=64^65^72^73^79^80 --selectId visit=19710 ccd=56^64^72 --selectId visit=19680 ccd=56^64^72 --selectId visit=1230 ccd=64^65^72^73^79^80 --selectId visit=1236 ccd=63^64^71^72^78^79 --selectId visit=19694 ccd=64^65^72^73^79^80 --selectId visit=1232 ccd=64^65^72^73^79^80 --selectId visit=19698 ccd=64^65^72^73^79^80 --selectId visit=1228 ccd=64^65^72^73^79^80 --selectId visit=1246 ccd=63^64^71^72^78^79 --selectId visit=19682 ccd=63^64^71^72^78^79 --selectId visit=19708 ccd=64^65^72^73^79^80 --selectId visit=19662 ccd=64^65^72^73 --selectId visit=1240 ccd=64^65^72^73^79^80 --selectId visit=1244 ccd=56^57^64^65^72^73 --selectId visit=1242 ccd=56^57^64^65^72^73 --selectId visit=19660 ccd=64^65^72^73^79^80 --selectId visit=19712 ccd=56^57^64^65^72^73  {code}",6,DM-3935,datamanagement,"measurement plugin error measurement coadd error throw measurement plugin code error base_gaussianflux.measure record 283467884979 input shape singular code code error base_gaussianflux.measure record 283467883979 file src sdssshape.cc line 842 static lsst::meas::base::fluxresult lsst::meas::base::sdssshapealgorithm::computefixedmomentsflux(const imaget const lsst::afw::geom::ellipses::quadrupole const point2d imaget lsst::afw::image::maskedimage lsst::afw::geom::point2d lsst::afw::geom::point error calcmom lsst::pex::exceptions::runtimeerror error calcmom code measurement tiger command code measurecoaddsources.py hsc hsc rerun nate old_clip/ --output=/tigress hsc hsc rerun nate old_clip/ -c /home nlust options_temp.py --id tract=0 patch=2,2 filter hsc i^hsc code datum find rerun directory specify input command datum create command code assemblecoadd.py --legacycoadd hsc hsc rerun nate/ hsc hsc rerun nate old_clip --id tract=0 patch=2,2 filter hsc visit=1208 ccd=56 64 72 --selectid visit=1206 ccd=64 65 72 73 79 80 --selectid visit=1212 ccd=64 65 72 73 79 80 --selectid visit=23704 ccd=64 65 72 73 --selectid visit=23706 ccd=56 64 72 --selectid visit=23694 ccd=64 65 72 73 79 80 --selectid visit=1204 ccd=64 65 72 73 79 80 --selectid visit=1220 ccd=63 64 71 72 78 79 --selectid visit=1218 ccd=56 57 64 65 72 73 visit=23718 ccd=64 65 72 73 79 80 visit=23692 ccd=64 65 72 73 79 80 --selectid visit=1210 ccd=63 64 71 72 78 79 --selectid visit=1216 ccd=56 57 64 65 72 73 visit=1214 ccd=64 65 72 73 79 80 --selectid visit=23716 ccd=63 64 71 72 78 79 --selectid visit=1202 ccd=64 65 72 73 79 80 code code assemblecoadd.py --legacycoadd hsc hsc rerun nate/ hsc hsc rerun nate old_clip --id tract=0 patch=2,2 filter hsc --selectid visit=19658 ccd=64 65 72 73 79 80 --selectid visit=1248 ccd=56 64 72 --selectid visit=19696 ccd=65 66 73 74 80 81 --selectid visit=19684 ccd=64 65 72 73 79 80 --selectid visit=1238 ccd=64 65 72 73 79 80 --selectid visit=19710 ccd=56 64 72 --selectid visit=19680 ccd=56 64 72 --selectid visit=1230 ccd=64 65 72 73 79 80 --selectid visit=1236 ccd=63 64 71 72 78 79 visit=19694 ccd=64 65 72 73 79 80 --selectid visit=1232 ccd=64 65 72 73 79 80 --selectid visit=19698 ccd=64 65 72 73 79 80 --selectid visit=1228 ccd=64 65 72 73 79 80 visit=1246 ccd=63 64 71 72 78 79 --selectid visit=19682 ccd=63 64 71 72 78 79 --selectid visit=19708 ccd=64 65 72 73 79 80 --selectid visit=19662 ccd=64 65 72 73 --selectid visit=1240 ccd=64 65 72 73 79 80 --selectid visit=1244 ccd=56 57 64 65 72 73 visit=1242 ccd=56 57 64 65 72 73 --selectid visit=19660 ccd=64 65 72 73 79 80 --selectid visit=19712 ccd=56 57 64 65 72 73 code","Measurement plugin errors When doing measurements on coadds, several errors are thrown within the measurement plugins. {code} Error in base_GaussianFlux.measure on record 283467884979: Input shape is singular {code} {code} Error in base_GaussianFlux.measure on record 283467883979: File ""src/SdssShape.cc"", line 842, in static lsst::meas::base::FluxResult lsst::meas::base::SdssShapeAlgorithm::computeFixedMomentsFlux(const ImageT&, const lsst::afw::geom::ellipses::Quadrupole&, const Point2D&) [with ImageT = lsst::afw::image::MaskedImage; lsst::afw::geom::Point2D = lsst::afw::geom::Point] Error from calcmom {0} lsst::pex::exceptions::RuntimeError: 'Error from calcmom' {code} The measurements were done on tiger, and the command used was: {code} measureCoaddSources.py /tigress/HSC/HSC/rerun/nate/old_clip/ --output=/tigress/HSC/HSC/rerun/nate/old_clip/ -C /home/nlust/options_temp.py --id tract=0 patch=2,2 filter=HSC-I^HSC-R {code} The data can be found within the rerun directory specified as the input to the command. The data was created using the commands: {code} assembleCoadd.py --legacyCoadd /tigress/HSC/HSC/rerun/nate/ --output=/tigress/HSC/HSC/rerun/nate/old_clip --id tract=0 patch=2,2 filter=HSC-R --selectId visit=1208 ccd=56^64^72 --selectId visit=1206 ccd=64^65^72^73^79^80 --selectId visit=1212 ccd=64^65^72^73^79^80 --selectId visit=23704 ccd=64^65^72^73 --selectId visit=23706 ccd=56^64^72 --selectId visit=23694 ccd=64^65^72^73^79^80 --selectId visit=1204 ccd=64^65^72^73^79^80 --selectId visit=1220 ccd=63^64^71^72^78^79 --selectId visit=1218 ccd=56^57^64^65^72^73 --selectId visit=23718 ccd=64^65^72^73^79^80 --selectId visit=23692 ccd=64^65^72^73^79^80 --selectId visit=1210 ccd=63^64^71^72^78^79 --selectId visit=1216 ccd=56^57^64^65^72^73 --selectId visit=1214 ccd=64^65^72^73^79^80 --selectId visit=23716 ccd=63^64^71^72^78^79 --selectId visit=1202 ccd=64^65^72^73^79^80 {code} and {code} assembleCoadd.py --legacyCoadd /tigress/HSC/HSC/rerun/nate/ --output=/tigress/HSC/HSC/rerun/nate/old_clip --id tract=0 patch=2,2 filter=HSC-I --selectId visit=19658 ccd=64^65^72^73^79^80 --selectId visit=1248 ccd=56^64^72 --selectId visit=19696 ccd=65^66^73^74^80^81 --selectId visit=19684 ccd=64^65^72^73^79^80 --selectId visit=1238 ccd=64^65^72^73^79^80 --selectId visit=19710 ccd=56^64^72 --selectId visit=19680 ccd=56^64^72 --selectId visit=1230 ccd=64^65^72^73^79^80 --selectId visit=1236 ccd=63^64^71^72^78^79 --selectId visit=19694 ccd=64^65^72^73^79^80 --selectId visit=1232 ccd=64^65^72^73^79^80 --selectId visit=19698 ccd=64^65^72^73^79^80 --selectId visit=1228 ccd=64^65^72^73^79^80 --selectId visit=1246 ccd=63^64^71^72^78^79 --selectId visit=19682 ccd=63^64^71^72^78^79 --selectId visit=19708 ccd=64^65^72^73^79^80 --selectId visit=19662 ccd=64^65^72^73 --selectId visit=1240 ccd=64^65^72^73^79^80 --selectId visit=1244 ccd=56^57^64^65^72^73 --selectId visit=1242 ccd=56^57^64^65^72^73 --selectId visit=19660 ccd=64^65^72^73^79^80 --selectId visit=19712 ccd=56^57^64^65^72^73 {code}"
HSC backport: temporary file handling in butler The HSC fork includes additional work to improve temporary file usage in the butler:  * [HSC-1275|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1275]: Probable resource leakage by butler  * [HSC-1285|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1285]: eups.version files ignore umask  * [HSC-1292|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1292]: Prevent opening files that are already open,1,DM-3937,datamanagement,hsc backport temporary file handling butler hsc fork include additional work improve temporary file usage butler hsc-1275|https://hsc jira.astro.princeton.edu jira browse hsc-1275 probable resource leakage butler hsc-1285|https://hsc jira.astro.princeton.edu jira browse hsc-1285 eups.version file ignore umask hsc-1292|https://hsc jira.astro.princeton.edu jira browse hsc-1292 prevent open file open,HSC backport: temporary file handling in butler The HSC fork includes additional work to improve temporary file usage in the butler: * [HSC-1275|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1275]: Probable resource leakage by butler * [HSC-1285|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1285]: eups.version files ignore umask * [HSC-1292|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1292]: Prevent opening files that are already open
"move camera factory methods from obs_lsstSim to afw The methods defined in obs_lsstSim/bin/makeLsstCameraRepository.py can be easily adapted for use in generating arbitrary, non-LSST cameras.  This is useful for the sims stack, both for testing purposes, and because members of other projects have begun asking us to use our code.    This ticket will take those methods, make them fully LSST-agnostic, and place them in afw as utility functions.  The code in obs_lsstSim will refer back to these afw methods.",5,DM-3939,datamanagement,camera factory method obs_lsstsim afw method define obs_lsstsim bin makelsstcamerarepository.py easily adapt use generate arbitrary non lsst camera useful sim stack testing purpose member project begin ask use code ticket method fully lsst agnostic place afw utility function code obs_lsstsim refer afw method,"move camera factory methods from obs_lsstSim to afw The methods defined in obs_lsstSim/bin/makeLsstCameraRepository.py can be easily adapted for use in generating arbitrary, non-LSST cameras. This is useful for the sims stack, both for testing purposes, and because members of other projects have begun asking us to use our code. This ticket will take those methods, make them fully LSST-agnostic, and place them in afw as utility functions. The code in obs_lsstSim will refer back to these afw methods."
"NaiveDipoleCentroid/NaiveDipoleFlux algorithms should not require centroid slot The {{NaiveDipoleCentroid}} and {{NaiveDipoleFlux}} algorithms in {{ip_diffim}} have members which are instances of {{meas::base::SafeCentroidExtractor}}. Due to the prerequisites that imposes, it is impossible to initialize these algorithms without first defining a {{centroid}} slot.    However, there is nothing in these algorithms which actually uses the {{SafeCentroidExtractor}} or any of the information stored in the slot; this seems to be an entirely arbitrary restriction which is likely a legacy of the port to the {{meas_base}} framework. We should remove the  use of {{SafeCentroidExtractor}} to simply the code and make it easier to run the test suite (since it will no longer be necessary to run a centroider).",1,DM-3940,datamanagement,naivedipolecentroid naivedipoleflux algorithm require centroid slot naivedipolecentroid naivedipoleflux algorithm ip_diffim member instance meas::base::safecentroidextractor prerequisite impose impossible initialize algorithm define centroid slot algorithm actually use safecentroidextractor information store slot entirely arbitrary restriction likely legacy port meas_base framework remove use safecentroidextractor simply code easy run test suite long necessary run centroider,"NaiveDipoleCentroid/NaiveDipoleFlux algorithms should not require centroid slot The {{NaiveDipoleCentroid}} and {{NaiveDipoleFlux}} algorithms in {{ip_diffim}} have members which are instances of {{meas::base::SafeCentroidExtractor}}. Due to the prerequisites that imposes, it is impossible to initialize these algorithms without first defining a {{centroid}} slot. However, there is nothing in these algorithms which actually uses the {{SafeCentroidExtractor}} or any of the information stored in the slot; this seems to be an entirely arbitrary restriction which is likely a legacy of the port to the {{meas_base}} framework. We should remove the use of {{SafeCentroidExtractor}} to simply the code and make it easier to run the test suite (since it will no longer be necessary to run a centroider)."
"Split FY16 plan into audience specific documents Fy16 purchase planning needs to be split for specific audiences (NCSA planning, Aura purchase approval)",2,DM-3941,datamanagement,split fy16 plan audience specific document fy16 purchase planning need split specific audience ncsa planning aura purchase approval,"Split FY16 plan into audience specific documents Fy16 purchase planning needs to be split for specific audiences (NCSA planning, Aura purchase approval)"
"QMeta thread safety Initial QMeta implementation is not thread safe, it uses sql/mysql modules which also do not have any protection (there are some mutexes there but not used). Need an urgent fix to avoid crashes due to concurrent queries in czar.",1,DM-3943,datamanagement,qmeta thread safety initial qmeta implementation thread safe use sql mysql module protection mutexe need urgent fix avoid crash concurrent query czar,"QMeta thread safety Initial QMeta implementation is not thread safe, it uses sql/mysql modules which also do not have any protection (there are some mutexes there but not used). Need an urgent fix to avoid crashes due to concurrent queries in czar."
"orchestration slide set for DM bootcamp Create the ""Orchestration and Control"" slide set for DM Bootcamp, which is being held from Oct 5-7, 2015.  After review (and any revisions), the slide set will be uploaded to confluence, and a link to it will be put here.",5,DM-3944,datamanagement,orchestration slide set dm bootcamp create orchestration control slide set dm bootcamp hold oct 2015 review revision slide set upload confluence link,"orchestration slide set for DM bootcamp Create the ""Orchestration and Control"" slide set for DM Bootcamp, which is being held from Oct 5-7, 2015. After review (and any revisions), the slide set will be uploaded to confluence, and a link to it will be put here."
Remove dependency on mysqldb in wmgr Move remaining code that depends on mysqldb to db module,1,DM-3947,datamanagement,remove dependency mysqldb wmgr remain code depend mysqldb db module,Remove dependency on mysqldb in wmgr Move remaining code that depends on mysqldb to db module
Remove dependency on mysqldb in qserv Remove remaining dependencies on mysqldb in qserv.:  {code}  ./core/modules/tests/MySqlUdf.py  ./core/modules/wmgr/python/config.py  {code}    and use the sqlalchemy from db module instead.,2,DM-3949,datamanagement,remove dependency mysqldb qserv remove remain dependency mysqldb qserv code ./core module test mysqludf.py module wmgr python config.py code use sqlalchemy db module instead,Remove dependency on mysqldb in qserv Remove remaining dependencies on mysqldb in qserv.: {code} ./core/modules/tests/MySqlUdf.py ./core/modules/wmgr/python/config.py {code} and use the sqlalchemy from db module instead.
"Remove qserv_objectId restrictor qserv_objectId restrictor can be replaced by the IN restrictor. This story involves checking if performance is acceptable if we use IN restrictor instead of qserv_objectId restictor, and if it is, doing the switch and removing the qserv_objectId restictor code.",3,DM-3951,datamanagement,remove qserv_objectid restrictor qserv_objectid restrictor replace restrictor story involve check performance acceptable use restrictor instead qserv_objectid restictor switch remove qserv_objectid restictor code,"Remove qserv_objectId restrictor qserv_objectId restrictor can be replaced by the IN restrictor. This story involves checking if performance is acceptable if we use IN restrictor instead of qserv_objectId restictor, and if it is, doing the switch and removing the qserv_objectId restictor code."
"Cleanup lua miniParser Maybe some cleanup can also be performed in lua code. Indeed ""objectId"" hint and parseObjectId() which seems useless.    Indeed miniParser.parseIt and miniParser.setAndNeeded seems useless.    Removing this code will ease maintenance of objectId management.",1,DM-3952,datamanagement,cleanup lua miniparser maybe cleanup perform lua code objectid hint parseobjectid useless miniparser.parseit miniparser.setandneeded useless remove code ease maintenance objectid management,"Cleanup lua miniParser Maybe some cleanup can also be performed in lua code. Indeed ""objectId"" hint and parseObjectId() which seems useless. Indeed miniParser.parseIt and miniParser.setAndNeeded seems useless. Removing this code will ease maintenance of objectId management."
E/I training and interview Interviewed and attended training for E/I concerns.,1,DM-3954,datamanagement,training interview interview attend training concern,E/I training and interview Interviewed and attended training for E/I concerns.
Investigate services for backups/data replication on Nebula openstack Files generated on instances of the Nebula openstack  should be managed with some commensurate  level of data replication/backups.     We investigate services that might serve this task within the cloud context.,4,DM-3955,datamanagement,investigate service backup datum replication nebula openstack files generate instance nebula openstack manage commensurate level datum replication backup investigate service serve task cloud context,Investigate services for backups/data replication on Nebula openstack Files generated on instances of the Nebula openstack should be managed with some commensurate level of data replication/backups. We investigate services that might serve this task within the cloud context.
"Package SQLAlchemy in eups Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups.",1,DM-3956,datamanagement,package sqlalchemy eup db module expect science pipeline qserv hipchat room package eup,"Package SQLAlchemy in eups Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups."
"Enable CModel in CalibrateTask prior to PhotoCal CModel needs to run in CalibrateTask before PhotoCal in order to compute aperture corrections, but it also needs a Calib objects as input, and that isn't available until after PhotoCal is run.    On the HSC side, we dealt with this by adding preliminary PhotoCal run before CModel is run, but we could also deal with it by removing the need for a Calib as input, at least in some situations.",1,DM-3957,datamanagement,enable cmodel calibratetask prior photocal cmodel need run calibratetask photocal order compute aperture correction need calib object input available photocal run hsc deal add preliminary photocal run cmodel run deal remove need calib input situation,"Enable CModel in CalibrateTask prior to PhotoCal CModel needs to run in CalibrateTask before PhotoCal in order to compute aperture corrections, but it also needs a Calib objects as input, and that isn't available until after PhotoCal is run. On the HSC side, we dealt with this by adding preliminary PhotoCal run before CModel is run, but we could also deal with it by removing the need for a Calib as input, at least in some situations."
Revisit provenance sizing Revisit estimates of the size of provenance information.,6,DM-3959,datamanagement,revisit provenance size revisit estimate size provenance information,Revisit provenance sizing Revisit estimates of the size of provenance information.
Meetings Sep 2015 verification datasets weekly meetings and tech-talks,1,DM-3965,datamanagement,meeting sep 2015 verification dataset weekly meeting tech talk,Meetings Sep 2015 verification datasets weekly meetings and tech-talks
"Meetings, institute events, or other LOE, Sep 2015 NCSA or Astronomy Department activities.   - NCSA All-hands  - Local LSST group meetings   - DES-Illinois meetings  - Colloquia  - other seminars, info sessions, or other local meetings",5,DM-3967,datamanagement,meeting institute event loe sep 2015 ncsa astronomy department activity ncsa hand local lsst group meeting des illinois meeting colloquia seminar info session local meeting,"Meetings, institute events, or other LOE, Sep 2015 NCSA or Astronomy Department activities. - NCSA All-hands - Local LSST group meetings - DES-Illinois meetings - Colloquia - other seminars, info sessions, or other local meetings"
"detailed out alert processing/ transmission to event brokers.  Detailed out the interactions with the event broker, after consulting with John Swinbank.  Dealt with outputs of alert processing.  Dealt with corrections and comments.   Began considering annual processing",2,DM-3968,datamanagement,detail alert processing/ transmission event broker detail interaction event broker consult john swinbank deal output alert processing deal correction comment begin consider annual processing,"detailed out alert processing/ transmission to event brokers. Detailed out the interactions with the event broker, after consulting with John Swinbank. Dealt with outputs of alert processing. Dealt with corrections and comments. Began considering annual processing"
"Package sqlalchemy in eups Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups.",1,DM-3971,datamanagement,package sqlalchemy eup db module expect science pipeline qserv hipchat room package eup,"Package sqlalchemy in eups Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups."
Network / Storage reviews Initial (internal) review with networking and storage delegates,2,DM-3974,datamanagement,network storage review initial internal review networking storage delegate,Network / Storage reviews Initial (internal) review with networking and storage delegates
research items needed for wan simulator procurement locate part numbers and pricing information,1,DM-3975,datamanagement,research item need wan simulator procurement locate number price information,research items needed for wan simulator procurement locate part numbers and pricing information
"Prepare pricing estimates for networking infrastructure provide switch quantities, weight, power and budgetary costs",4,DM-3977,datamanagement,prepare pricing estimate network infrastructure provide switch quantity weight power budgetary cost,"Prepare pricing estimates for networking infrastructure provide switch quantities, weight, power and budgetary costs"
Discussion of Base site network proposal Email based discussion with Ron Lambert on his network infrastructure proposal for the Base site.,1,DM-3979,datamanagement,discussion base site network proposal email base discussion ron lambert network infrastructure proposal base site,Discussion of Base site network proposal Email based discussion with Ron Lambert on his network infrastructure proposal for the Base site.
"Post SQLAlchemy-migration tweaks Implement some minor tweaks take came in late through PR comments, mostly related to sqlalchemy related migration",1,DM-3980,datamanagement,post sqlalchemy migration tweak implement minor tweak come late pr comment relate sqlalchemy related migration,"Post SQLAlchemy-migration tweaks Implement some minor tweaks take came in late through PR comments, mostly related to sqlalchemy related migration"
Improve the performance for making the image plot Make FitsRead work better for multi-threads,1,DM-3981,datamanagement,improve performance make image plot fitsread work well multi thread,Improve the performance for making the image plot Make FitsRead work better for multi-threads
"Larger Statistics needed for CModel Studies The stampsize and nInitalRadius tests were not conclusive in the September Sprint. and the error estimates appeared to be overly large. The nGrowFootprints test was barely significant.  This is a continuation of work started on DM-1135 (DM-1135, 3375, 3376) , after a study of the sizes of our error estimates was conducted (DM-3984).    We started with a sample with 12 million galaxies (not all of which were used in DM-3375 and 3376.  They appeared to all be useful once we had new error estimates, so the studies were run against with this larger sample. ",6,DM-3983,datamanagement,larger statistics need cmodel studies stampsize ninitalradius test conclusive september sprint error estimate appear overly large ngrowfootprints test barely significant continuation work start dm-1135 dm-1135 3375 3376 study size error estimate conduct dm-3984 start sample 12 million galaxy dm-3375 3376 appear useful new error estimate study run large sample,"Larger Statistics needed for CModel Studies The stampsize and nInitalRadius tests were not conclusive in the September Sprint. and the error estimates appeared to be overly large. The nGrowFootprints test was barely significant. This is a continuation of work started on DM-1135 (DM-1135, 3375, 3376) , after a study of the sizes of our error estimates was conducted (DM-3984). We started with a sample with 12 million galaxies (not all of which were used in DM-3375 and 3376. They appeared to all be useful once we had new error estimates, so the studies were run against with this larger sample."
"Errors for shear bias fits DM-1135 mostly was inconclusive or at least not highly significant, due to the fact that the error bars were around the same size as the differences in most of the tests. This in spite of the fact that we ran 6x as many sample galaxies as great3sims.    Investigate the reason for this, and see if we can estimate the errors more accurately.  Investigate what the best way (or at least the way it is done in GREAT3) to estimate the errors on the bias parameters.     If it is not from the covariance matrix of the regression parameters, there could be some work here.",6,DM-3984,datamanagement,error shear bias fit dm-1135 inconclusive highly significant fact error bar size difference test spite fact run 6x sample galaxy great3sim investigate reason estimate error accurately investigate good way way great3 estimate error bias parameter covariance matrix regression parameter work,"Errors for shear bias fits DM-1135 mostly was inconclusive or at least not highly significant, due to the fact that the error bars were around the same size as the differences in most of the tests. This in spite of the fact that we ran 6x as many sample galaxies as great3sims. Investigate the reason for this, and see if we can estimate the errors more accurately. Investigate what the best way (or at least the way it is done in GREAT3) to estimate the errors on the bias parameters. If it is not from the covariance matrix of the regression parameters, there could be some work here."
"Run multinode test using docker on one unique host Qserv multinode test can be launched on n hosts using docker, but not on one unique host.  This ticket will allow developers to run multinode test on their workstation.",6,DM-3985,datamanagement,run multinode test docker unique host qserv multinode test launch host docker unique host ticket allow developer run multinode test workstation,"Run multinode test using docker on one unique host Qserv multinode test can be launched on n hosts using docker, but not on one unique host. This ticket will allow developers to run multinode test on their workstation."
Deploy developer code on in2p3 cluster in Docker images Qserv latest release can be deployed easily on cc-in2p3 cluster using Docker. This ticket will allow developers to prepare worker and master containers using a specific Qserv version and deploy it on cc-in2p3 cluster.,6,DM-3986,datamanagement,deploy developer code in2p3 cluster docker image qserv late release deploy easily cc in2p3 cluster docker ticket allow developer prepare worker master container specific qserv version deploy cc in2p3 cluster,Deploy developer code on in2p3 cluster in Docker images Qserv latest release can be deployed easily on cc-in2p3 cluster using Docker. This ticket will allow developers to prepare worker and master containers using a specific Qserv version and deploy it on cc-in2p3 cluster.
"remove unnecessary 'psf' arg to SourceDeblendTask.run() {{SourceDeblendTask.run}} takes both an {{Exposure}} and a {{Psf}}, even though it can get the latter from the former and always should.",1,DM-3987,datamanagement,remove unnecessary psf arg sourcedeblendtask.run sourcedeblendtask.run take exposure psf,"remove unnecessary 'psf' arg to SourceDeblendTask.run() {{SourceDeblendTask.run}} takes both an {{Exposure}} and a {{Psf}}, even though it can get the latter from the former and always should."
"Update FY2015 hardware budget plan Update the FY2015 hardware purchasing plan with new budget, equipment specifications, and general costs. ",3,DM-3988,datamanagement,update fy2015 hardware budget plan update fy2015 hardware purchasing plan new budget equipment specification general cost,"Update FY2015 hardware budget plan Update the FY2015 hardware purchasing plan with new budget, equipment specifications, and general costs."
"Week end 09/05/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 5, 2015.",5,DM-3989,datamanagement,week end 09/05/15 support lsst dev cluster openstack account week end september 2015,"Week end 09/05/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending September 5, 2015."
Provide detail specs to AURA Provided Josh Hobblitt details specs for procurement request,1,DM-3990,datamanagement,provide detail spec aura provide josh hobblitt detail spec procurement request,Provide detail specs to AURA Provided Josh Hobblitt details specs for procurement request
"Week end 09/12/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 12, 2015.",5,DM-3992,datamanagement,week end support lsst dev cluster openstack account week end september 12 2015,"Week end 09/12/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending September 12, 2015."
Display.dot origin swaps x and y Correcting for xy0 in {{dot}} currently does:  {code:hide-linenum}  r -= x0  c -= y0  {code}  which is backwards.,1,DM-3993,datamanagement,display.dot origin swap correct xy0 dot currently code hide linenum -= x0 -= y0 code backwards,Display.dot origin swaps x and y Correcting for xy0 in {{dot}} currently does: {code:hide-linenum} r -= x0 c -= y0 {code} which is backwards.
Backup Pugsley Created backup of pugsley in anticipation of new hardware and shutdown of temp Mac OS solution,1,DM-3994,datamanagement,backup pugsley create backup pugsley anticipation new hardware shutdown temp mac os solution,Backup Pugsley Created backup of pugsley in anticipation of new hardware and shutdown of temp Mac OS solution
Research using vSphere on Mac Pro Researched setting up vSphere on Mac Pro.,1,DM-3996,datamanagement,research vsphere mac pro researched set vsphere mac pro,Research using vSphere on Mac Pro Researched setting up vSphere on Mac Pro.
"Week end 09/19/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 19, 2015.",5,DM-3997,datamanagement,week end 09/19/15 support lsst dev cluster openstack account week end september 19 2015,"Week end 09/19/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending September 19, 2015."
Learn about Lenovo storage and server options Went to lunch with Lenovo to learn about systems that might be suitable for LSST deployment. Learned about server and storage options.,1,DM-3998,datamanagement,learn lenovo storage server option go lunch lenovo learn system suitable lsst deployment learn server storage option,Learn about Lenovo storage and server options Went to lunch with Lenovo to learn about systems that might be suitable for LSST deployment. Learned about server and storage options.
"Week end 09/26/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending September 26, 2015.",5,DM-3999,datamanagement,week end support lsst dev cluster openstack account week end september 26 2015,"Week end 09/26/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending September 26, 2015."
Write next-generation stack doc writing guide Write a guide in the prototype LSST Stack Docs (https://github.com/lsst-sqre/lsst_stack_docs) covering how to document the LSST Stack under the new doc infrastructure.    This exercise will implicitly involve designing how the new docs will work. Content includes:    # How to write a user guide to a package (both content wise and in terms of organizing a package's doc files)  # How to write python doc strings  # Coverage of reStructuredText and Sphinx as implemented by the LSST Stack Docs.  ,7,DM-4001,datamanagement,write generation stack doc writing guide write guide prototype lsst stack docs https://github.com/lsst-sqre/lsst_stack_docs cover document lsst stack new doc infrastructure exercise implicitly involve design new doc work content include write user guide package content wise term organize package doc file write python doc string coverage restructuredtext sphinx implement lsst stack docs,Write next-generation stack doc writing guide Write a guide in the prototype LSST Stack Docs (https://github.com/lsst-sqre/lsst_stack_docs) covering how to document the LSST Stack under the new doc infrastructure. This exercise will implicitly involve designing how the new docs will work. Content includes: # How to write a user guide to a package (both content wise and in terms of organizing a package's doc files) # How to write python doc strings # Coverage of reStructuredText and Sphinx as implemented by the LSST Stack Docs.
Replace zookeeper CSS with mysql To switch from QservAdmin to CssAccess interface in our Python tools we will need to replace zookeeper with mysql implementation because we do not have C++ KvInterface implementation for zookeeper.,2,DM-4003,datamanagement,replace zookeeper css mysql switch qservadmin cssaccess interface python tool need replace zookeeper mysql implementation c++ kvinterface implementation zookeeper,Replace zookeeper CSS with mysql To switch from QservAdmin to CssAccess interface in our Python tools we will need to replace zookeeper with mysql implementation because we do not have C++ KvInterface implementation for zookeeper.
"ctrl_execute templates still use ""root"" instead of ""config"" The templates that ctrl_execute fills in still use ""root"", instead of the new ""config"".  This causes extraneous warning messages to appear from pex_config when executing ""runOrca.py""",1,DM-4007,datamanagement,ctrl_execute template use root instead config template ctrl_execute fill use root instead new config cause extraneous warning message appear pex_config execute runorca.py,"ctrl_execute templates still use ""root"" instead of ""config"" The templates that ctrl_execute fills in still use ""root"", instead of the new ""config"". This causes extraneous warning messages to appear from pex_config when executing ""runOrca.py"""
"Allow FlagHandler to be used from Python The {{FlagHandler}} utility class makes it easier to manage the flags for a measurement algorithm, and using it also makes it possible to use the {{SafeCentroidExtractor}} and {{SafeShapeExtractor}} classes.  Unfortunately, its constructor requires arguments that can only be provided in C++.  A little extra Swig wrapper code should make it usable in Python as well.",3,DM-4009,datamanagement,allow flaghandler python flaghandler utility class make easy manage flag measurement algorithm make possible use safecentroidextractor safeshapeextractor class unfortunately constructor require argument provide c++ little extra swig wrapper code usable python,"Allow FlagHandler to be used from Python The {{FlagHandler}} utility class makes it easier to manage the flags for a measurement algorithm, and using it also makes it possible to use the {{SafeCentroidExtractor}} and {{SafeShapeExtractor}} classes. Unfortunately, its constructor requires arguments that can only be provided in C++. A little extra Swig wrapper code should make it usable in Python as well."
"Rename forced photometry CmdLineTasks to match bin scripts We use names like ""forcedPhotCcd.py"" for bin scripts but ""ProcessCcdForcedTask"" for class names; these need to be made consistent, and it's the former convention that was selected in an old (non-JIRA) RFC.",1,DM-4011,datamanagement,rename force photometry cmdlinetask match bin script use name like forcedphotccd.py bin script processccdforcedtask class name need consistent convention select old non jira rfc,"Rename forced photometry CmdLineTasks to match bin scripts We use names like ""forcedPhotCcd.py"" for bin scripts but ""ProcessCcdForcedTask"" for class names; these need to be made consistent, and it's the former convention that was selected in an old (non-JIRA) RFC."
Initial discussion EFD with Dave Mills Review LTS-210 and discuss design of EFD cluster with Dave Mills.    Are there opensource clustering solutions?,1,DM-4013,datamanagement,initial discussion efd dave mills review lts-210 discuss design efd cluster dave mills opensource clustering solution,Initial discussion EFD with Dave Mills Review LTS-210 and discuss design of EFD cluster with Dave Mills. Are there opensource clustering solutions?
Replace boost::tuple with <tuple> Replace boost::tuple with <tuple>    This ticket will be completed as part of the DM bootcamp at UW.,1,DM-4014,datamanagement,replace boost::tuple replace boost::tuple ticket complete dm bootcamp uw,Replace boost::tuple with  Replace boost::tuple with  This ticket will be completed as part of the DM bootcamp at UW.
"Document Revision Document cleanup. Added PDUs, networking estimates, power costs, vsphere licensing.",1,DM-4015,datamanagement,document revision document cleanup add pdu networking estimate power cost vsphere licensing,"Document Revision Document cleanup. Added PDUs, networking estimates, power costs, vsphere licensing."
Write developer workflow documentation Write a developer workflow guide to walk through and document best practices for developing against the LSST Stack.,6,DM-4016,datamanagement,write developer workflow documentation write developer workflow guide walk document good practice develop lsst stack,Write developer workflow documentation Write a developer workflow guide to walk through and document best practices for developing against the LSST Stack.
Fix procedure for building docker image for 2010_09 release This procedure should be straightforward but is currently failing due to gcc-4.9/boost problem (DM-4018),2,DM-4019,datamanagement,fix procedure build docker image 2010_09 release procedure straightforward currently fail gcc-4.9 boost problem dm-4018,Fix procedure for building docker image for 2010_09 release This procedure should be straightforward but is currently failing due to gcc-4.9/boost problem (DM-4018)
Replace boost::unordered_map with std::unordered_map DM boot camp tutorial.    ,2,DM-4021,datamanagement,replace boost::unordered_map std::unordered_map dm boot camp tutorial,Replace boost::unordered_map with std::unordered_map DM boot camp tutorial.
"forcedPhotCoadd.py fails on HSC data When trying to run {{forcedPhotCoadd.py}} on HSC data, I see the following error:    {code}  $ forcedPhotCoadd.py /raid/swinbank/rerun/LSST/bootcamp --id filter='HSC-I' tract=0 patch=7,7  : Loading config overrride file '/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py'  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/meas_base/11.0+2/bin/forcedPhotCoadd.py"", line 24, in <module>      ForcedPhotCoaddTask.parseAndRun()    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/cmdLineTask.py"", line 433, in parseAndRun      parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides)    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 360, in parse_args      self._applyInitialOverrides(namespace)    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 475, in _applyInitialOverrides      namespace.config.load(filePath)    File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 529, in load      self.loadFromStream(stream=code, root=root)    File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 549, in loadFromStream      exec stream in {}, local    File ""/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py"", line 10, in <module>      config.deblend.load(os.path.join(os.environ[""OBS_SUBARU_DIR""], ""config"", ""deblend.py""))  AttributeError: 'ForcedPhotCoaddConfig' object has no attribute 'deblend'  {code}    This is with the stack version 11.0+3 and {{obs_subaru}} 5.0.0.1-676-g4ae362c.",1,DM-4022,datamanagement,"forcedphotcoadd.py fail hsc datum try run forcedphotcoadd.py hsc datum follow error code forcedphotcoadd.py swinbank rerun lsst bootcamp --id filter='hsc tract=0 patch=7,7 loading config overrride file /nfs home swinbank obs_subaru config forcedphotcoadd.py import lsst.meas.extensions.photometrykron disable kron measurement traceback recent file /home lsstsw stack linux64 meas_base/11.0 bin forcedphotcoadd.py line 24 forcedphotcoaddtask.parseandrun file /home lsstsw stack linux64 pipe_base/11.0 python lsst pipe base cmdlinetask.py line 433 parseandrun parsedcmd argumentparser.parse_args(config config args args log log override cls.applyoverride file /home lsstsw stack linux64 pipe_base/11.0 python lsst pipe base argumentparser.py line 360 parse_args self._applyinitialoverrides(namespace file /home lsstsw stack linux64 pipe_base/11.0 python lsst pipe base argumentparser.py line 475 applyinitialoverride namespace.config.load(filepath file /home lsstsw stack linux64 pex_config/11.0 python lsst pex config config.py line 529 load self.loadfromstream(stream code root root file /home lsstsw stack linux64 pex_config/11.0 python lsst pex config config.py line 549 loadfromstream exec stream local file /nfs home swinbank obs_subaru config forcedphotcoadd.py line 10 config.deblend.load(os.path.join(os.environ[""obs_subaru_dir config deblend.py attributeerror forcedphotcoaddconfig object attribute deblend code stack version 11.0 obs_subaru 5.0.0.1 676 g4ae362c","forcedPhotCoadd.py fails on HSC data When trying to run {{forcedPhotCoadd.py}} on HSC data, I see the following error: {code} $ forcedPhotCoadd.py /raid/swinbank/rerun/LSST/bootcamp --id filter='HSC-I' tract=0 patch=7,7 : Loading config overrride file '/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py' Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements Traceback (most recent call last): File ""/home/lsstsw/stack/Linux64/meas_base/11.0+2/bin/forcedPhotCoadd.py"", line 24, in  ForcedPhotCoaddTask.parseAndRun() File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/cmdLineTask.py"", line 433, in parseAndRun parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides) File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 360, in parse_args self._applyInitialOverrides(namespace) File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 475, in _applyInitialOverrides namespace.config.load(filePath) File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 529, in load self.loadFromStream(stream=code, root=root) File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 549, in loadFromStream exec stream in {}, local File ""/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py"", line 10, in  config.deblend.load(os.path.join(os.environ[""OBS_SUBARU_DIR""], ""config"", ""deblend.py"")) AttributeError: 'ForcedPhotCoaddConfig' object has no attribute 'deblend' {code} This is with the stack version 11.0+3 and {{obs_subaru}} 5.0.0.1-676-g4ae362c."
Local LSST IAM Meeting Meeting notes:    * [10 minutes] Review LSST identity management statement of work    On the confluence wiki. Let me know if you have any follow-up questions/comments.    * [10 minutes] Plan initial project tasks    All: Review materials at https://confluence.lsstcorp.org/display/LAAIM. Jim to add updates from last week's meeting.    * [5 minutes] Schedule follow-on project meetings    Alex will schedule meeting with Bill Glick about LDAP at NCSA.  Terry and Daniel will attend https://community.lsst.org/t/dm-boot-camp-announcement/249 sessions of interest if available.,1,DM-4023,datamanagement,local lsst iam meeting meeting note 10 minute review lsst identity management statement work confluence wiki let know follow question comment 10 minute plan initial project task review material https://confluence.lsstcorp.org/display/laaim jim add update week meeting minute schedule follow project meeting alex schedule meeting bill glick ldap ncsa terry daniel attend https://community.lsst.org/t/dm-boot-camp-announcement/249 session interest available,Local LSST IAM Meeting Meeting notes: * [10 minutes] Review LSST identity management statement of work On the confluence wiki. Let me know if you have any follow-up questions/comments. * [10 minutes] Plan initial project tasks All: Review materials at https://confluence.lsstcorp.org/display/LAAIM. Jim to add updates from last week's meeting. * [5 minutes] Schedule follow-on project meetings Alex will schedule meeting with Bill Glick about LDAP at NCSA. Terry and Daniel will attend https://community.lsst.org/t/dm-boot-camp-announcement/249 sessions of interest if available.
"Local LSST IAM Meeting -Meeting attended by Jim Basney, Tim Fleury, Dan Thayer, and Alex Withers.  -Discussed upcoming DM bootcamp.  -Focused on SUIT diagram and determining what questions to ask at next bi-weekly meeting.  -First draft recommendations by end of October.",1,DM-4024,datamanagement,local lsst iam meeting -meete attend jim basney tim fleury dan thayer alex withers -discusse upcoming dm bootcamp -focuse suit diagram determine question ask bi weekly meeting -first draft recommendation end october,"Local LSST IAM Meeting -Meeting attended by Jim Basney, Tim Fleury, Dan Thayer, and Alex Withers. -Discussed upcoming DM bootcamp. -Focused on SUIT diagram and determining what questions to ask at next bi-weekly meeting. -First draft recommendations by end of October."
First day activities Meeting with HR at 8:30AM.  Taken around for introductions through 10AM.  Remainder of day was proceeding through HR punch list for new hires such as:  	NCSA Intranet account setup  	Gain familiarity with NCSA Intranet  	Kerberos and Enterprise ID setup  	Read and Sign-off  on Security and NCSA policy.  	Outlook mail and calendar setup  2pm DM meeting,2,DM-4025,datamanagement,day activity meet hr 8:30am take introduction 10am remainder day proceed hr punch list new hire ncsa intranet account setup gain familiarity ncsa intranet kerberos enterprise id setup read sign security ncsa policy outlook mail calendar setup pm dm meeting,First day activities Meeting with HR at 8:30AM. Taken around for introductions through 10AM. Remainder of day was proceeding through HR punch list for new hires such as: NCSA Intranet account setup Gain familiarity with NCSA Intranet Kerberos and Enterprise ID setup Read and Sign-off on Security and NCSA policy. Outlook mail and calendar setup 2pm DM meeting
Second day start activities Requisitioned used macbook and monitor from IT and set it up.  Located LSST stack site and read build and install documentation  More HR account items.  Requested confluence credentials from LSST.org  Learned about SDSS/Stripe 82,2,DM-4026,datamanagement,second day start activity requisition macbook monitor set locate lsst stack site read build install documentation hr account item request confluence credential lsst.org learned sdss stripe 82,Second day start activities Requisitioned used macbook and monitor from IT and set it up. Located LSST stack site and read build and install documentation More HR account items. Requested confluence credentials from LSST.org Learned about SDSS/Stripe 82
groundwork for file management Installed correct Xcode + misc. for stack version.  Downloaded LSST stack from GitHub repo and built/installed.  ,1,DM-4027,datamanagement,groundwork file management instal correct xcode misc stack version downloaded lsst stack github repo build instal,groundwork for file management Installed correct Xcode + misc. for stack version. Downloaded LSST stack from GitHub repo and built/installed.
"Incidental items and jobs for file mgmt. groundwork Installed and built LSST tutorials package.  Setup, fixed minor issues and ran First tutorial to check that initial stack  installation was successful.  Learned about CCD operation and how the LSST CCD is laid out.  Learned about raw CCD data from amplifiers, as well as other camera attributes  Studied stack code for these things.  Read description of Astronomy Associative relations as well as CoAdds.  Pulled PhoSim from repo. Built and Installed.  Had trouble getting PhoSim to run due to dynamic link library issues.  Instrumented PhoSim code with PDB commands.  Walked to bookstore and picked up new ID badge.",2,DM-4028,datamanagement,incidental item job file mgmt groundwork installed build lsst tutorial package setup fix minor issue run tutorial check initial stack installation successful learn ccd operation lsst ccd lay learn raw ccd datum amplifier camera attribute study stack code thing read description astronomy associative relation coadds pull phosim repo build installed trouble get phosim run dynamic link library issue instrumented phosim code pdb command walk bookstore pick new id badge,"Incidental items and jobs for file mgmt. groundwork Installed and built LSST tutorials package. Setup, fixed minor issues and ran First tutorial to check that initial stack installation was successful. Learned about CCD operation and how the LSST CCD is laid out. Learned about raw CCD data from amplifiers, as well as other camera attributes Studied stack code for these things. Read description of Astronomy Associative relations as well as CoAdds. Pulled PhoSim from repo. Built and Installed. Had trouble getting PhoSim to run due to dynamic link library issues. Instrumented PhoSim code with PDB commands. Walked to bookstore and picked up new ID badge."
"Development for Developer activities! Completed setup of confluence access.  Set up LSST HipChat.  Jumped into PhoSim a bit more and resolved errors by dropping in a few symlinks here and there;  not a solution for a proper execution environment, but a time-save just to see PhoSim work and  have a platform for tinkering with camera attar's.  PhoSim ran successfully.  Learned about FTTS format and how the file is built up. Learned about how Key/Val pairs   can proceed ANY data section throughout file by using data offset values.",2,DM-4029,datamanagement,development developer activity complete setup confluence access set lsst hipchat jump phosim bit resolve error drop symlink solution proper execution environment time save phosim work platform tinker camera attar phosim run successfully learn ftts format file build learn key val pair proceed datum section file datum offset value,"Development for Developer activities! Completed setup of confluence access. Set up LSST HipChat. Jumped into PhoSim a bit more and resolved errors by dropping in a few symlinks here and there; not a solution for a proper execution environment, but a time-save just to see PhoSim work and have a platform for tinkering with camera attar's. PhoSim ran successfully. Learned about FTTS format and how the file is built up. Learned about how Key/Val pairs can proceed ANY data section throughout file by using data offset values."
"integration of confluence data into learning curve Started studying LSST coding policies and best practices via confluence.  Colloquium.  Small meetings with other LSST team members throughout day.  Rebuilt PhoSim/LSST-Stack to take advantage of multiple cores when rendering portion of sky to a file. Built these commands into Stack code. They would have to be custom #defined by ./configure at build time depending on computer arch. which is too much to do when just gaining familiarity so stuck to MacBook  multi-core specs, where I was working.  Logged into JIRA and studied how tasks were proposed, realized, and checked as done.  Extensive talk about BBQ in group area of NCSA/LSST.  Close reading and note taking of LDM-230 and related docs.",5,DM-4030,datamanagement,integration confluence datum learn curve start study lsst code policy good practice confluence colloquium small meeting lsst team member day rebuilt phosim lsst stack advantage multiple core render portion sky file build command stack code custom define ./configure build time depend computer arch gain familiarity stuck macbook multi core spec work log jira study task propose realize check extensive talk bbq group area ncsa lsst close reading note take ldm-230 related doc,"integration of confluence data into learning curve Started studying LSST coding policies and best practices via confluence. Colloquium. Small meetings with other LSST team members throughout day. Rebuilt PhoSim/LSST-Stack to take advantage of multiple cores when rendering portion of sky to a file. Built these commands into Stack code. They would have to be custom #defined by ./configure at build time depending on computer arch. which is too much to do when just gaining familiarity so stuck to MacBook multi-core specs, where I was working. Logged into JIRA and studied how tasks were proposed, realized, and checked as done. Extensive talk about BBQ in group area of NCSA/LSST. Close reading and note taking of LDM-230 and related docs."
"Add doc directory, and fix doxygen warnings Add doc directory and fix doxygen warnings.",4,DM-4031,datamanagement,add doc directory fix doxygen warning add doc directory fix doxygen warning,"Add doc directory, and fix doxygen warnings Add doc directory and fix doxygen warnings."
"obs_sdss should use pydl.yanny instead of it's own copy thereof Inside obs_sdss there is a yanny.py that looks like it was copied from either sdss_python_module or pydl. We should just depend on pydl (https://github.com/weaverba137/pydl), so we can use whatever improvements it gets for free, and to prevent yet annother yanny reader floating around.",4,DM-4032,datamanagement,obs_sdss use pydl.yanny instead copy thereof inside obs_sdss yanny.py look like copy sdss_python_module pydl depend pydl https://github.com/weaverba137/pydl use improvement get free prevent annother yanny reader float,"obs_sdss should use pydl.yanny instead of it's own copy thereof Inside obs_sdss there is a yanny.py that looks like it was copied from either sdss_python_module or pydl. We should just depend on pydl (https://github.com/weaverba137/pydl), so we can use whatever improvements it gets for free, and to prevent yet annother yanny reader floating around."
Display DECam focal plane mosaics using showCamera Try out some new functionalities of afw.cameraGeom.utils (from DM-2437) for DECam raw data. Raw data in {{testdata_decam}} are retrieved through Butler and displayed as a focal plane mosaic.  ,2,DM-4034,datamanagement,display decam focal plane mosaic showcamera try new functionality afw.camerageom.utils dm-2437 decam raw datum raw datum testdata_decam retrieve butler display focal plane mosaic,Display DECam focal plane mosaics using showCamera Try out some new functionalities of afw.cameraGeom.utils (from DM-2437) for DECam raw data. Raw data in {{testdata_decam}} are retrieved through Butler and displayed as a focal plane mosaic.
"Replace boost::array with std::array Replace all use of boost::array with std::array in the DM stack    A quick search turned up use in 17 files spread over these packages:  ip_diffim, meas_base, meas_extensions_photometryKron and ndarray (which is presumably out of scope for this ticket)",4,DM-4035,datamanagement,replace boost::array std::array replace use boost::array std::array dm stack quick search turn use 17 file spread package ip_diffim meas_base meas_extensions_photometrykron ndarray presumably scope ticket,"Replace boost::array with std::array Replace all use of boost::array with std::array in the DM stack A quick search turned up use in 17 files spread over these packages: ip_diffim, meas_base, meas_extensions_photometryKron and ndarray (which is presumably out of scope for this ticket)"
"Change from boost::math Most boost::math contents (not including pi) are now available in standard C++. Please convert the code accordingly.    In addition to the packages listed above, boost/math is used in ""partition"" a package I don't recognize and not a component JIRA accepts.",5,DM-4036,datamanagement,change boost::math boost::math content include pi available standard c++ convert code accordingly addition package list boost math partition package recognize component jira accept,"Change from boost::math Most boost::math contents (not including pi) are now available in standard C++. Please convert the code accordingly. In addition to the packages listed above, boost/math is used in ""partition"" a package I don't recognize and not a component JIRA accepts."
"New YAML config for community_mailbot Per review comments to DM-3690, the configuration should move to YAML with the following goals    - Have a Configuration object that can be tested and passed around  - Redesign the configuration to allow for additional types of message handlers, such as twitter, hipchat/slack, etc.  - Move secret keys entirely into the configuration file  - Provide a configuration template  - Move any sort of hard-coded configuration to the expanded YAML file (e.g., Mandrill templates)",1,DM-4039,datamanagement,new yaml config community_mailbot review comment dm-3690 configuration yaml follow goal configuration object test pass redesign configuration allow additional type message handler twitter hipchat slack etc secret key entirely configuration file provide configuration template sort hard code configuration expand yaml file e.g. mandrill template,"New YAML config for community_mailbot Per review comments to DM-3690, the configuration should move to YAML with the following goals - Have a Configuration object that can be tested and passed around - Redesign the configuration to allow for additional types of message handlers, such as twitter, hipchat/slack, etc. - Move secret keys entirely into the configuration file - Provide a configuration template - Move any sort of hard-coded configuration to the expanded YAML file (e.g., Mandrill templates)"
"Refactor Scripts and Discourse interface in community_mailbot Per review comments for DM-3690, the community_mailbot can have slight code refactoring    - Refactor scripts into smaller testable units  - Refactor the Discourse feed classes around an ABC  - More testing",1,DM-4040,datamanagement,refactor scripts discourse interface community_mailbot review comment dm-3690 community_mailbot slight code refactoring refactor script small testable unit refactor discourse feed class abc testing,"Refactor Scripts and Discourse interface in community_mailbot Per review comments for DM-3690, the community_mailbot can have slight code refactoring - Refactor scripts into smaller testable units - Refactor the Discourse feed classes around an ABC - More testing"
Produce demo video for git lfs Produce a screencast tutorial of the DM git-lfs implementation.,1,DM-4042,datamanagement,produce demo video git lfs produce screencast tutorial dm git lfs implementation,Produce demo video for git lfs Produce a screencast tutorial of the DM git-lfs implementation.
"update memory management in jointcal jointcal currently uses a combination of raw pointers and a custom reference-counted smart pointer class, {{CountedRef}} (similar to {{boost::intrusive_ptr}}).  The code needs to be modified to use a combination of {{shared_ptr}} (most code), {{unique_ptr}} local-scope variables and factory functions, and {{weak_ptr}} (at least some will be necessary to avoid cycles in some of the more complex data structures).  As part of this work, we'll also have to remove a lot of inheritance from {{RefCounted}}, which is part of the {{CountedRef}} implementation.    This ticket looks like it will require a lot of work, because we'll have to be careful about every conversion to avoid cycles and memory leaks.  Nevertheless, I think it will be necessary to do this conversion before attempting any other major refactoring, as I'm worried that having a newcomer make changes to the codebase without first making the memory management less fragile could be very dangerous.",8,DM-4043,datamanagement,update memory management jointcal jointcal currently use combination raw pointer custom reference count smart pointer class countedref similar boost::intrusive_ptr code need modify use combination shared_ptr code unique_ptr local scope variable factory function weak_ptr necessary avoid cycle complex data structure work remove lot inheritance refcounted countedref implementation ticket look like require lot work careful conversion avoid cycle memory leak think necessary conversion attempt major refactoring worried have newcomer change codebase make memory management fragile dangerous,"update memory management in jointcal jointcal currently uses a combination of raw pointers and a custom reference-counted smart pointer class, {{CountedRef}} (similar to {{boost::intrusive_ptr}}). The code needs to be modified to use a combination of {{shared_ptr}} (most code), {{unique_ptr}} local-scope variables and factory functions, and {{weak_ptr}} (at least some will be necessary to avoid cycles in some of the more complex data structures). As part of this work, we'll also have to remove a lot of inheritance from {{RefCounted}}, which is part of the {{CountedRef}} implementation. This ticket looks like it will require a lot of work, because we'll have to be careful about every conversion to avoid cycles and memory leaks. Nevertheless, I think it will be necessary to do this conversion before attempting any other major refactoring, as I'm worried that having a newcomer make changes to the codebase without first making the memory management less fragile could be very dangerous."
"integrate jointcal geometry primitives (Point, Frame, FatPoint) with afw jointcal currently has three simple geometry classes that can be integrated relatively easily with existing classes in afw and meas_base:   - {{jointcal.Point}} is equivalent to {{afw.geom.Point2D}}.   - {{jointcal.Frame}} is equivalent to {{afw.geom.Box2D}}.   - {{jointcal.FatPoint}} is equivalent to {{meas.base.CentroidResult}}.  We should probably move {{CentroidResult}} to {{afw.geom}}, perhaps rename it ({{MeasuredPoint}}?), and reconsider its relationship with {{Point}}.  This will require a bit of refactoring in {{meas_base}}, but the usage in {{jointcal}} makes me think it's a sufficiently fundamental object to be included in afw.    We may find aspects of the interfaces in {{jointcal}} that we should add to {{afw.geom}}, but I think we'll mostly end up making trivial modifications to {{jointcal}} to use the {{afw}} interfaces.",8,DM-4044,datamanagement,integrate jointcal geometry primitive point frame fatpoint afw jointcal currently simple geometry class integrate relatively easily exist class afw meas_base jointcal point equivalent afw.geom point2d jointcal frame equivalent afw.geom box2d jointcal fatpoint equivalent meas.base centroidresult probably centroidresult afw.geom rename measuredpoint reconsider relationship point require bit refactoring meas_base usage jointcal make think sufficiently fundamental object include afw find aspect interface jointcal add afw.geom think end make trivial modification jointcal use afw interface,"integrate jointcal geometry primitives (Point, Frame, FatPoint) with afw jointcal currently has three simple geometry classes that can be integrated relatively easily with existing classes in afw and meas_base: - {{jointcal.Point}} is equivalent to {{afw.geom.Point2D}}. - {{jointcal.Frame}} is equivalent to {{afw.geom.Box2D}}. - {{jointcal.FatPoint}} is equivalent to {{meas.base.CentroidResult}}. We should probably move {{CentroidResult}} to {{afw.geom}}, perhaps rename it ({{MeasuredPoint}}?), and reconsider its relationship with {{Point}}. This will require a bit of refactoring in {{meas_base}}, but the usage in {{jointcal}} makes me think it's a sufficiently fundamental object to be included in afw. We may find aspects of the interfaces in {{jointcal}} that we should add to {{afw.geom}}, but I think we'll mostly end up making trivial modifications to {{jointcal}} to use the {{afw}} interfaces."
"integrate Gtransfo functionality with XYTransform {{meas_simastrom}} includes a {{Gtransfo}} class hierarchy that is similar to {{afw.geom.XYTransform}}, but with more functionality and some intentional differences, including:   - {{XYTransform}} objects are immutable; {{Gtransfo}} objects are not.   - {{Gtransfo}} objects expose their parametrization, and can compute various derivatives with respect to those parameters.  {{XYTransforms}} are essentially black-box functions, and expose no parameterization.    Unifying these classes is not entirely straightforward, and should include an RFC for the design prior to implementation.  Overall, I think {{XYTransform}}'s simpler, lower-functionality interface and immutability is worth mostly preserving somehow; I think it's a more fundamental interface than {{Gtransfo}} that can be used in more places.  But obviously we need to provide the more extensive {{Gtransfo}} interface somehow as well.    My initial thought is that we should have two parallel class hierarchies (with a concrete class for each type of transform, such as polynomial distortion, in both), and an ultimate base class shared by both hierarchies.  That ultimate base class would contain most of the current {{XYTransform}} interface but not require immutability, and one side of the tree would contain simple immutable objects while the other would contain the more extensive parameterized interface of {{Gtransfo}}.",8,DM-4045,datamanagement,integrate gtransfo functionality xytransform meas_simastrom include gtransfo class hierarchy similar afw.geom xytransform functionality intentional difference include xytransform object immutable gtransfo object gtransfo object expose parametrization compute derivative respect parameter xytransform essentially black box function expose parameterization unify class entirely straightforward include rfc design prior implementation overall think xytransform simple low functionality interface immutability worth preserve think fundamental interface gtransfo place obviously need provide extensive gtransfo interface initial thought parallel class hierarchy concrete class type transform polynomial distortion ultimate base class share hierarchy ultimate base class contain current xytransform interface require immutability tree contain simple immutable object contain extensive parameterized interface gtransfo,"integrate Gtransfo functionality with XYTransform {{meas_simastrom}} includes a {{Gtransfo}} class hierarchy that is similar to {{afw.geom.XYTransform}}, but with more functionality and some intentional differences, including: - {{XYTransform}} objects are immutable; {{Gtransfo}} objects are not. - {{Gtransfo}} objects expose their parametrization, and can compute various derivatives with respect to those parameters. {{XYTransforms}} are essentially black-box functions, and expose no parameterization. Unifying these classes is not entirely straightforward, and should include an RFC for the design prior to implementation. Overall, I think {{XYTransform}}'s simpler, lower-functionality interface and immutability is worth mostly preserving somehow; I think it's a more fundamental interface than {{Gtransfo}} that can be used in more places. But obviously we need to provide the more extensive {{Gtransfo}} interface somehow as well. My initial thought is that we should have two parallel class hierarchies (with a concrete class for each type of transform, such as polynomial distortion, in both), and an ultimate base class shared by both hierarchies. That ultimate base class would contain most of the current {{XYTransform}} interface but not require immutability, and one side of the tree would contain simple immutable objects while the other would contain the more extensive parameterized interface of {{Gtransfo}}."
Redis cache for community_mailbot Switch from a {{json}} cache to a redis cache from the community_mailbot.,1,DM-4046,datamanagement,redis cache community_mailbot switch json cache redis cache community_mailbot,Redis cache for community_mailbot Switch from a {{json}} cache to a redis cache from the community_mailbot.
"Add fake secondary index to testIndexMap.cc qproc/testIndexMap.cc is sketchy and perform a very poor validation for now. It should at least use a minimal SQL secondary index, embedded in a simple database like SQLLite, inorder to validate secondary index lookup code.",8,DM-4047,datamanagement,add fake secondary index testindexmap.cc qproc testindexmap.cc sketchy perform poor validation use minimal sql secondary index embed simple database like sqllite inorder validate secondary index lookup code,"Add fake secondary index to testIndexMap.cc qproc/testIndexMap.cc is sketchy and perform a very poor validation for now. It should at least use a minimal SQL secondary index, embedded in a simple database like SQLLite, inorder to validate secondary index lookup code."
Replace QsRestrictor::PtrVector With std::vector<QsRestrictor> and use move constructor Use of  QsRestrictor::PtrVector introduces a useless indirection. it maybe could be replace by std::vector<QsRestrictor> and use of move constructor. This would simplify code (currently a confusion exists between empty vector and nullptr) and ease maintenance.,5,DM-4048,datamanagement,replace qsrestrictor::ptrvector std::vector use constructor use qsrestrictor::ptrvector introduce useless indirection maybe replace std::vector use constructor simplify code currently confusion exist vector nullptr ease maintenance,Replace QsRestrictor::PtrVector With std::vector and use move constructor Use of QsRestrictor::PtrVector introduces a useless indirection. it maybe could be replace by std::vector and use of move constructor. This would simplify code (currently a confusion exists between empty vector and nullptr) and ease maintenance.
Meetings Oct 2015 - Verification datasets meetings  ,1,DM-4054,datamanagement,meeting oct 2015 verification dataset meeting,Meetings Oct 2015 - Verification datasets meetings
"Final additions and review Add remaining components: power estimates, vSphere annual licensing, networking, PDUs, login nodes    review: misc expense fund, decommissioned services, financial targets",3,DM-4055,datamanagement,final addition review add remain component power estimate vsphere annual licensing networking pdu login node review misc expense fund decommission service financial target,"Final additions and review Add remaining components: power estimates, vSphere annual licensing, networking, PDUs, login nodes review: misc expense fund, decommissioned services, financial targets"
Chasing down Pan Starrs Requirements Pan Starrs data release (PS1) will be used in the integration QServ environment purchased as part of FY16. Catalog and file space requirements must be understood.,1,DM-4056,datamanagement,chase pan starrs requirements pan starrs datum release ps1 integration qserv environment purchase fy16 catalog file space requirement understand,Chasing down Pan Starrs Requirements Pan Starrs data release (PS1) will be used in the integration QServ environment purchased as part of FY16. Catalog and file space requirements must be understood.
Prepare Plan for ICI Leadership Review Prepare plan and ICI-group-specific points of interest for hardware/service deployment plans. ,1,DM-4057,datamanagement,prepare plan ici leadership review prepare plan ici group specific point interest hardware service deployment plan,Prepare Plan for ICI Leadership Review Prepare plan and ICI-group-specific points of interest for hardware/service deployment plans.
Vendor Discussions Discussions with vendors on planned procurement. Details of discussions will not be described here. This is for story point tracking only.,1,DM-4058,datamanagement,vendor discussions discussions vendor plan procurement detail discussion describe story point track,Vendor Discussions Discussions with vendors on planned procurement. Details of discussions will not be described here. This is for story point tracking only.
"Begin Approval Process Approval process for FY16 procurement plan. This requires approval from Jeff K, Victor and NSF (due to the cost increment being greater than $250K).    Expected approval time frame: Dec 2015.",1,DM-4059,datamanagement,begin approval process approval process fy16 procurement plan require approval jeff victor nsf cost increment great 250 expect approval time frame dec 2015,"Begin Approval Process Approval process for FY16 procurement plan. This requires approval from Jeff K, Victor and NSF (due to the cost increment being greater than $250K). Expected approval time frame: Dec 2015."
Define policy based upon FY16 plans Re evaluate previously proposed storage policies: https://wiki.ncsa.illinois.edu/display/LSST/Storage+Policy?src=contextnavpagetreemode    Plan new policies:  https://wiki.ncsa.illinois.edu/display/LSST/Changes+to+Storage+Policy+and+Design?src=contextnavpagetreemode,1,DM-4061,datamanagement,define policy base fy16 plan evaluate previously propose storage policy https://wiki.ncsa.illinois.edu/display/lsst/storage+policy?src=contextnavpagetreemode plan new policy,Define policy based upon FY16 plans Re evaluate previously proposed storage policies: https://wiki.ncsa.illinois.edu/display/LSST/Storage+Policy?src=contextnavpagetreemode Plan new policies: https://wiki.ncsa.illinois.edu/display/LSST/Changes+to+Storage+Policy+and+Design?src=contextnavpagetreemode
"Data access rights and retention policies Added Data access center requirements, individual data access rights, data retention policies and other general cleanup.",1,DM-4062,datamanagement,datum access right retention policy add data access center requirement individual datum access right data retention policy general cleanup,"Data access rights and retention policies Added Data access center requirements, individual data access rights, data retention policies and other general cleanup."
Support new casting requirements in NumPy 1.10 The function imagesDiffer() in testUtils attempts to OR an array of unit16s (LHS) against an array of bools(RHS) {{valSkipMaskArr |= skipMaskArr}} and errors with message  {code}  TypeError: ufunc 'bitwise_or' output (typecode 'H') could not be coerced to provided output parameter (typecode '?') according to the casting rule ''same_kind''  {code}  preventing afw from building correctly. ,1,DM-4063,datamanagement,support new casting requirement numpy 1.10 function imagesdiffer testutil attempt array unit16s lhs array bools(rhs valskipmaskarr |= skipmaskarr error message code typeerror ufunc bitwise_or output typecode coerce provide output parameter typecode accord casting rule same_kind code prevent afw build correctly,Support new casting requirements in NumPy 1.10 The function imagesDiffer() in testUtils attempts to OR an array of unit16s (LHS) against an array of bools(RHS) {{valSkipMaskArr |= skipMaskArr}} and errors with message {code} TypeError: ufunc 'bitwise_or' output (typecode 'H') could not be coerced to provided output parameter (typecode '?') according to the casting rule ''same_kind'' {code} preventing afw from building correctly.
"Revisit database compression trade-offs As discussed at Qserv meeting Oct 7, it is not entire clear if it will be worth compressing data. Need to revisit baseline.",5,DM-4064,datamanagement,revisit database compression trade off discuss qserv meeting oct entire clear worth compress datum need revisit baseline,"Revisit database compression trade-offs As discussed at Qserv meeting Oct 7, it is not entire clear if it will be worth compressing data. Need to revisit baseline."
Discuss with MySQL team This story captures issues/topics that we want to bring up with mysql team.,2,DM-4065,datamanagement,discuss mysql team story capture issue topic want bring mysql team,Discuss with MySQL team This story captures issues/topics that we want to bring up with mysql team.
SuperTask structure implementation  Starting to implement the structure of the Super Task framework for process execution,7,DM-4067,datamanagement,supertask structure implementation start implement structure super task framework process execution,SuperTask structure implementation Starting to implement the structure of the Super Task framework for process execution
Bootcamp meeting I've attended LSST DM bootcamp,3,DM-4069,datamanagement,bootcamp meeting attend lsst dm bootcamp,Bootcamp meeting I've attended LSST DM bootcamp
SuperTask framework documentation and  refactorization While still prototyping I need to fill documentation on new code as well as do some clean up as well,7,DM-4070,datamanagement,supertask framework documentation refactorization prototype need fill documentation new code clean,SuperTask framework documentation and refactorization While still prototyping I need to fill documentation on new code as well as do some clean up as well
"testPsfDetermination broken due to NumPy behaviour change Old NumPy behaviour (tested on 1.6.2):  {code}  In [1]: import numpy    In [2]: a = numpy.array([])    In [3]: numpy.median(a)  /usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2374: RuntimeWarning: invalid value encountered in double_scalars    return mean(axis, dtype, out)    Out[3]: nan  {code}    New NumPy behaviour (1.10.0):  {code}  In [1]: import numpy    In [2]: a = numpy.array([])    In [3]: numpy.median(a)  [...]  IndexError: index -1 is out of bounds for axis 0 with size 0  {code}    This breaks {{testPsfDeterminer}} and {{testPsfDeterminerSubimage}}, e.g.:  {code}  ERROR: testPsfDeterminerSubimage (__main__.SpatialModelPsfTestCase)  Test the (PCA) psfDeterminer on subImages  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""./testPsfDetermination.py"", line 342, in testPsfDeterminerSubimage      trimCatalogToImage(subExp, self.catalog))    File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 377, in selectStars      widthStdAllowed=self._widthStdAllowed)    File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 195, in _kcenters      centers[i] = func(yvec[clusterId == i])    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3084, in median      overwrite_input=overwrite_input)    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 2997, in _ureduce      r = func(a, **kwargs)    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3138, in _median      n = np.isnan(part[..., -1])  IndexError: index -1 is out of bounds for axis 0 with size 0  {code}",1,DM-4071,datamanagement,testpsfdetermination break numpy behaviour change old numpy behaviour test 1.6.2 code import numpy numpy.array numpy.median(a lib64 python2.6 site package numpy core fromnumeric.py:2374 runtimewarning invalid value encounter double_scalar return mean(axis dtype out[3 nan code new numpy behaviour 1.10.0 code import numpy numpy.array numpy.median(a ... indexerror index -1 bound axis size code break testpsfdeterminer testpsfdeterminersubimage e.g. code error testpsfdeterminersubimage main__.spatialmodelpsftestcase test pca psfdeterminer subimages traceback recent file ./testpsfdetermination.py line 342 testpsfdeterminersubimage trimcatalogtoimage(subexp self.catalog file /users jds projects astronomy lsst src meas_algorithms python lsst meas algorithm objectsizestarselector.py line 377 selectstars widthstdallowed self._widthstdallowed file /users jds projects astronomy lsst src meas_algorithms python lsst meas algorithm objectsizestarselector.py line 195 kcenter centers[i func(yvec[clusterid file /opt local library frameworks python.framework versions/2.7 lib python2.7 site package numpy lib function_base.py line 3084 median overwrite_input overwrite_input file /opt local library frameworks python.framework versions/2.7 lib python2.7 site package numpy lib function_base.py line 2997 ureduce func(a kwargs file /opt local library frameworks python.framework versions/2.7 lib python2.7 site package numpy lib function_base.py line 3138 median np.isnan(part -1 indexerror index -1 bound axis size code,"testPsfDetermination broken due to NumPy behaviour change Old NumPy behaviour (tested on 1.6.2): {code} In [1]: import numpy In [2]: a = numpy.array([]) In [3]: numpy.median(a) /usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2374: RuntimeWarning: invalid value encountered in double_scalars return mean(axis, dtype, out) Out[3]: nan {code} New NumPy behaviour (1.10.0): {code} In [1]: import numpy In [2]: a = numpy.array([]) In [3]: numpy.median(a) [...] IndexError: index -1 is out of bounds for axis 0 with size 0 {code} This breaks {{testPsfDeterminer}} and {{testPsfDeterminerSubimage}}, e.g.: {code} ERROR: testPsfDeterminerSubimage (__main__.SpatialModelPsfTestCase) Test the (PCA) psfDeterminer on subImages ---------------------------------------------------------------------- Traceback (most recent call last): File ""./testPsfDetermination.py"", line 342, in testPsfDeterminerSubimage trimCatalogToImage(subExp, self.catalog)) File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 377, in selectStars widthStdAllowed=self._widthStdAllowed) File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 195, in _kcenters centers[i] = func(yvec[clusterId == i]) File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3084, in median overwrite_input=overwrite_input) File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 2997, in _ureduce r = func(a, **kwargs) File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3138, in _median n = np.isnan(part[..., -1]) IndexError: index -1 is out of bounds for axis 0 with size 0 {code}"
"Assemble eslint rules for JavaScript code quality control Review and assemble eslint rules, which enforce clean JavaScript and JSX code.    Code cleanup to avoid too many rule violations.",8,DM-4075,datamanagement,assemble eslint rule javascript code quality control review assemble eslint rule enforce clean javascript jsx code code cleanup avoid rule violation,"Assemble eslint rules for JavaScript code quality control Review and assemble eslint rules, which enforce clean JavaScript and JSX code. Code cleanup to avoid too many rule violations."
"JavaScript code cleanup - remove unused packages Remove es6-promise, react-modal, other cleanup",2,DM-4076,datamanagement,javascript code cleanup remove unused package remove es6 promise react modal cleanup,"JavaScript code cleanup - remove unused packages Remove es6-promise, react-modal, other cleanup"
"convert underscore to lodash lodash has become a superset of underscore, providing more consistent API behavior, more features, and more thorough documentation. We'd like to convert our underscore package dependencies to lodash while we have only ~20 calls to underscore functions",2,DM-4078,datamanagement,convert underscore lodash lodash superset underscore provide consistent api behavior feature thorough documentation like convert underscore package dependency lodash ~20 call underscore function,"convert underscore to lodash lodash has become a superset of underscore, providing more consistent API behavior, more features, and more thorough documentation. We'd like to convert our underscore package dependencies to lodash while we have only ~20 calls to underscore functions"
"Create a React component which manages tabs We need a React component, which manages tabs. ",6,DM-4079,datamanagement,create react component manage tab need react component manage tab,"Create a React component which manages tabs We need a React component, which manages tabs."
"Shutdown mechanism doesn't work when logging process is disabled. If the logging mechanism is turned off in ctrl_execute, the ctrl_orca Logger doesn't get launched.  The current shutdown mechanism waits for the last logging message to be transmitted before shutting down so it doesn't kill off that process.   If the logger.launch config file option is set to false, this process never get launched and ctrl_orca hangs after the shutdown waiting for the message to arrive.",8,DM-4080,datamanagement,shutdown mechanism work log process disable logging mechanism turn ctrl_execute ctrl_orca logger launch current shutdown mechanism wait log message transmit shut kill process logger.launch config file option set false process launch ctrl_orca hang shutdown wait message arrive,"Shutdown mechanism doesn't work when logging process is disabled. If the logging mechanism is turned off in ctrl_execute, the ctrl_orca Logger doesn't get launched. The current shutdown mechanism waits for the last logging message to be transmitted before shutting down so it doesn't kill off that process. If the logger.launch config file option is set to false, this process never get launched and ctrl_orca hangs after the shutdown waiting for the message to arrive."
Lead  the Firefly conversion from GWT to React/FLUX design meeting A focused week long design meeting on Firefly conversion from GWT to React/FLUX.   Agenda and notes here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41786446  Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture  ,6,DM-4081,datamanagement,lead firefly conversion gwt react flux design meeting focused week long design meeting firefly conversion gwt react flux agenda note https://confluence.lsstcorp.org/pages/viewpage.action?pageid=41786446 design document https://confluence.lsstcorp.org/display/dm/firefly+client+application+architecture,Lead the Firefly conversion from GWT to React/FLUX design meeting A focused week long design meeting on Firefly conversion from GWT to React/FLUX. Agenda and notes here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41786446 Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture
Firefly conversion from GWT to React/FLUX design meeting A focused week long design meeting on Firefly conversion from GWT to React/FLUX. Design document at  https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture  ,6,DM-4083,datamanagement,firefly conversion gwt react flux design meeting focused week long design meeting firefly conversion gwt react flux design document https://confluence.lsstcorp.org/display/dm/firefly+client+application+architecture,Firefly conversion from GWT to React/FLUX design meeting A focused week long design meeting on Firefly conversion from GWT to React/FLUX. Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture
Firefly conversion from GWT to React/FLUX design meeting A focused week long design meeting on Firefly conversion from GWT to React/FLUX.  Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture.  ,4,DM-4084,datamanagement,firefly conversion gwt react flux design meeting focused week long design meeting firefly conversion gwt react flux design document https://confluence.lsstcorp.org/display/dm/firefly+client+application+architecture,Firefly conversion from GWT to React/FLUX design meeting A focused week long design meeting on Firefly conversion from GWT to React/FLUX. Design document at https://confluence.lsstcorp.org/display/DM/Firefly+Client+Application+Architecture.
"Attend DM boot camp Attend DM boot camp to learn more about DM stack, butler, and task. ",2,DM-4085,datamanagement,attend dm boot camp attend dm boot camp learn dm stack butler task,"Attend DM boot camp Attend DM boot camp to learn more about DM stack, butler, and task."
"Attend DM boot camp Attend DM boot camp to learn more about DM stack, butler, and task.     Most of the presentations are located at URL https://community.lsst.org/t/dm-boot-camp-announcement/249. Presentations like afw, eups, tasks, and butler are necessary to participate in LSST, so everyone on LSST must understand these concepts. Look at the list of presentations covering these topics and make sure your understand them. Some of the remaining talks go into more detail or cover more specialized topics. Those talks should be scanned to see if they are of interestß to you.",3,DM-4086,datamanagement,attend dm boot camp attend dm boot camp learn dm stack butler task presentation locate url https://community.lsst.org/t/dm-boot-camp-announcement/249 presentation like afw eup task butler necessary participate lsst lsst understand concept look list presentation cover topic sure understand remain talk detail cover specialized topic talk scan interest,"Attend DM boot camp Attend DM boot camp to learn more about DM stack, butler, and task. Most of the presentations are located at URL https://community.lsst.org/t/dm-boot-camp-announcement/249. Presentations like afw, eups, tasks, and butler are necessary to participate in LSST, so everyone on LSST must understand these concepts. Look at the list of presentations covering these topics and make sure your understand them. Some of the remaining talks go into more detail or cover more specialized topics. Those talks should be scanned to see if they are of interest to you."
"Attend DM boot camp  Attend DM boot camp to learn more about DM stack, butler, and task. ",3,DM-4087,datamanagement,attend dm boot camp attend dm boot camp learn dm stack butler task,"Attend DM boot camp Attend DM boot camp to learn more about DM stack, butler, and task."
"update cat logging information The ""cat"" package has a table which the Logger in ctrl_orca uses to insert information from logging messages.  The format of the log messages has changed, and therefore the table in ""cat"" needs to be changed as well.",1,DM-4090,datamanagement,update cat log information cat package table logger ctrl_orca use insert information log message format log message change table cat need change,"update cat logging information The ""cat"" package has a table which the Logger in ctrl_orca uses to insert information from logging messages. The format of the log messages has changed, and therefore the table in ""cat"" needs to be changed as well."
"Update qserv for lastest xrootd Small API change in latest xrootd, requires a parallel change to qserv.  Paves the way for DM-2334",1,DM-4092,datamanagement,update qserv lastest xrootd small api change late xrootd require parallel change qserv pave way dm-2334,"Update qserv for lastest xrootd Small API change in latest xrootd, requires a parallel change to qserv. Paves the way for DM-2334"
"Update DECam camera geometry descriptions for raw data The overscan and prescan regions of instcal data have been trimmed, but they are included for raw data.  The amplifier information in the current camera descriptions were made for instcal data and do not include overscan and prescan regions.      While processing raw data with the current camera descriptions, the bounding boxes from the camera object seem incorrect for raw data.     Code change summary:  - Use non-zero overscan and prescan regions  - Update the pixel array layout. My schematic of pixel array layout is attached in DecamAmpInfo.png    Screenshots are post-ISR images processed with bias and flat correction using the old or updated camGeom.  ",7,DM-4093,datamanagement,update decam camera geometry description raw datum overscan prescan region instcal datum trim include raw datum amplifier information current camera description instcal datum include overscan prescan region process raw datum current camera description bound box camera object incorrect raw datum code change summary use non zero overscan prescan region update pixel array layout schematic pixel array layout attach decamampinfo.png screenshot post isr image process bias flat correction old update camgeom,"Update DECam camera geometry descriptions for raw data The overscan and prescan regions of instcal data have been trimmed, but they are included for raw data. The amplifier information in the current camera descriptions were made for instcal data and do not include overscan and prescan regions. While processing raw data with the current camera descriptions, the bounding boxes from the camera object seem incorrect for raw data. Code change summary: - Use non-zero overscan and prescan regions - Update the pixel array layout. My schematic of pixel array layout is attached in DecamAmpInfo.png Screenshots are post-ISR images processed with bias and flat correction using the old or updated camGeom."
Please port showVisitSkyMap.py from HSC The HSC documentation at http://hsca.ipmu.jp/public/scripts/showVisitSkyMap.html includes a useful script for displaying the skymap and CCDs from a set of visits. It would be convenient if a version of this script was available in the LSST stack.,1,DM-4095,datamanagement,port showvisitskymap.py hsc hsc documentation http://hsca.ipmu.jp/public/scripts/showvisitskymap.html include useful script display skymap ccd set visit convenient version script available lsst stack,Please port showVisitSkyMap.py from HSC The HSC documentation at http://hsca.ipmu.jp/public/scripts/showVisitSkyMap.html includes a useful script for displaying the skymap and CCDs from a set of visits. It would be convenient if a version of this script was available in the LSST stack.
Update Trust Level of all LSST DM Staff to Level 4 via the API It seems safe to update the Discourse trust level of all members of the LSSTDM group on community.lsst.org to Level 4 (full permissions). See https://meta.discourse.org/t/consequences-of-using-or-bypassing-trust-levels-for-company-organization-staff/34564?u=jsick    This should alleviate concerns that DM staff are being prevented from fully using the forum.    This ticket implements a small notebook to exercise the Discourse API to make this trust level migration possible.,1,DM-4098,datamanagement,update trust level lsst dm staff level api safe update discourse trust level member lsstdm group community.lsst.org level permission https://meta.discourse.org/t/consequences-of-using-or-bypassing-trust-levels-for-company-organization-staff/34564?u=jsick alleviate concern dm staff prevent fully forum ticket implement small notebook exercise discourse api trust level migration possible,Update Trust Level of all LSST DM Staff to Level 4 via the API It seems safe to update the Discourse trust level of all members of the LSSTDM group on community.lsst.org to Level 4 (full permissions). See https://meta.discourse.org/t/consequences-of-using-or-bypassing-trust-levels-for-company-organization-staff/34564?u=jsick This should alleviate concerns that DM staff are being prevented from fully using the forum. This ticket implements a small notebook to exercise the Discourse API to make this trust level migration possible.
Provide upstream improvements to sphinx-prompt Provide PRs to sphinx-prompt (or decide to own a fork of sphinx prompt in documenteer) that includes    - an actual package you can import  - better error reporting when you forget to include a class with the prompt directive,1,DM-4099,datamanagement,provide upstream improvement sphinx prompt provide pr sphinx prompt decide fork sphinx prompt documenteer include actual package import well error reporting forget include class prompt directive,Provide upstream improvements to sphinx-prompt Provide PRs to sphinx-prompt (or decide to own a fork of sphinx prompt in documenteer) that includes - an actual package you can import - better error reporting when you forget to include a class with the prompt directive
Replace use of image <<= with [:] in python code Replace all use of the afw image pixel copy operator {{<<=}} with {{\[:]}} in Python code.    See DM-4102 for the C++ version. These can be done independently.,2,DM-4100,datamanagement,replace use image :] python code replace use afw image pixel copy operator [: python code dm-4102 c++ version independently,Replace use of image <<= with [:] in python code Replace all use of the afw image pixel copy operator {{<<=}} with {{\[:]}} in Python code. See DM-4102 for the C++ version. These can be done independently.
"Remove use of <<= from C++ code in our stack Replace usage of deprecated Image operator {{<<=}} in C++ code with {{assign(rhs, bbox=Box2I(), origin=PARENT)}} as per RFC-102    Switch from [:] to assign pixels in Python code where an image view is created for the sole purpose of assigning pixels (thus turning 2-4 lines of code to one and eliminating the need to make a view).",3,DM-4102,datamanagement,remove use c++ code stack replace usage deprecate image operator c++ code assign(rhs bbox box2i origin parent switch assign pixel python code image view create sole purpose assign pixel turn line code eliminate need view,"Remove use of <<= from C++ code in our stack Replace usage of deprecated Image operator {{<<=}} in C++ code with {{assign(rhs, bbox=Box2I(), origin=PARENT)}} as per RFC-102 Switch from [:] to assign pixels in Python code where an image view is created for the sole purpose of assigning pixels (thus turning 2-4 lines of code to one and eliminating the need to make a view)."
"Update user documentation {{ORDER BY}}, {{objectId IN}} and {{objectId BETWEEN}} predicates support have been improved, this should be documented.    ",1,DM-4105,datamanagement,update user documentation order objectid objectid predicate support improve document,"Update user documentation {{ORDER BY}}, {{objectId IN}} and {{objectId BETWEEN}} predicates support have been improved, this should be documented."
"add lfs remote support to lsstsw/lsst_build Support for cloning from lfs backed repos, when indicated via repos.yaml, is needed.  ",4,DM-4113,datamanagement,add lfs remote support lsstsw lsst_build support clone lfs back repos indicate repos.yaml need,"add lfs remote support to lsstsw/lsst_build Support for cloning from lfs backed repos, when indicated via repos.yaml, is needed."
"Update cfitsio to 3.37 (adding bz2 support) Per RFC-105, we should upgrade to cfitsio 3.37.",2,DM-4115,datamanagement,update cfitsio 3.37 add bz2 support rfc-105 upgrade cfitsio 3.37,"Update cfitsio to 3.37 (adding bz2 support) Per RFC-105, we should upgrade to cfitsio 3.37."
Clean up lsst_stack_docs for preview Improve the presentation of the New docs overall:    # Add a Creative Commons license  # Remove stub documents from the presentation  # Put READMEs in all doc directories to explain what content will go in them  # Clean up and update the source installation guide to reflect 11_0,1,DM-4117,datamanagement,clean lsst_stack_docs preview improve presentation new doc overall add creative commons license remove stub document presentation readme doc directory explain content clean update source installation guide reflect 11_0,Clean up lsst_stack_docs for preview Improve the presentation of the New docs overall: # Add a Creative Commons license # Remove stub documents from the presentation # Put READMEs in all doc directories to explain what content will go in them # Clean up and update the source installation guide to reflect 11_0
"Local LSST IAM meeting October 14, 2015 (local)    - Drawing identity access management architecture on whiteboard.    - DB access via kerberos       - mariaDB does pam but does it do kerberos tickets?            - could then simply access with a ticket   - if users are exporting VMs/containers, do they need keytabs?  how do we support this?   - what does the ID linking?   - do we need replication to base site so that base site can operate independent     - Can we get 2 LSST VMs?     - NCSA cyber-infrastructure standards?  MIT Kerberos?  OpenLDAP?    - Meeting with Iain Goodenow    ",2,DM-4118,datamanagement,local lsst iam meeting october 14 2015 local draw identity access management architecture whiteboard db access kerberos mariadb pam kerberos ticket simply access ticket user export vms container need keytab support id link need replication base site base site operate independent lsst vms ncsa cyber infrastructure standard mit kerberos openldap meeting iain goodenow,"Local LSST IAM meeting October 14, 2015 (local) - Drawing identity access management architecture on whiteboard. - DB access via kerberos - mariaDB does pam but does it do kerberos tickets? - could then simply access with a ticket - if users are exporting VMs/containers, do they need keytabs? how do we support this? - what does the ID linking? - do we need replication to base site so that base site can operate independent - Can we get 2 LSST VMs? - NCSA cyber-infrastructure standards? MIT Kerberos? OpenLDAP? - Meeting with Iain Goodenow"
"Security Meeting with LSST PO October 8, 2015    - Refresh of security plan, sub-plan       - Talk about camera subsystem refusal to buy-in to security plan    - IaM work moving along       - Continue bi-weekly meetings with developers       - Most likely going with kerberos       - Iain meeting with Jim and Co on authn/z work    - Joint technical meeting in February       - when and where?  Santa Cruz, Feb 22-24th 2016    - security plan refresh:       - cover email with old document and instructions       - DM: Don P. and Jeff Kantor       - EPO       - PO: Iain       - Camera       - Incidents: all reports are collected and acknowledged",1,DM-4119,datamanagement,security meeting lsst po october 2015 refresh security plan sub plan talk camera subsystem refusal buy security plan iam work move continue bi weekly meeting developer likely go kerberos iain meeting jim co authn work joint technical meeting february santa cruz feb 22 24th 2016 security plan refresh cover email old document instruction dm don p. jeff kantor epo po iain camera incidents report collect acknowledge,"Security Meeting with LSST PO October 8, 2015 - Refresh of security plan, sub-plan - Talk about camera subsystem refusal to buy-in to security plan - IaM work moving along - Continue bi-weekly meetings with developers - Most likely going with kerberos - Iain meeting with Jim and Co on authn/z work - Joint technical meeting in February - when and where? Santa Cruz, Feb 22-24th 2016 - security plan refresh: - cover email with old document and instructions - DM: Don P. and Jeff Kantor - EPO - PO: Iain - Camera - Incidents: all reports are collected and acknowledged"
ctrl_events test failures on CentOS7 VM on Nebula I am seeing failure to build due to test  failures for ctrl_events on a CentOS7 instance on the Nebula Openstack.  Details to follow.,1,DM-4122,datamanagement,ctrl_event test failure centos7 vm nebula see failure build test failure ctrl_event centos7 instance nebula openstack detail follow,ctrl_events test failures on CentOS7 VM on Nebula I am seeing failure to build due to test failures for ctrl_events on a CentOS7 instance on the Nebula Openstack. Details to follow.
Bootcamp meeting Slides and attendance at DM Bootcamp,8,DM-4123,datamanagement,bootcamp meeting slides attendance dm bootcamp,Bootcamp meeting Slides and attendance at DM Bootcamp
Bootcamp meeting Travel to Princeton and attend DM Boot Camp ,8,DM-4124,datamanagement,bootcamp meeting travel princeton attend dm boot camp,Bootcamp meeting Travel to Princeton and attend DM Boot Camp
"pipe_tasks/examples/calibrateTask.py fails The self contained example calibrateTask.py in pipe_tasks/examples/ fails when attempting to set field ""coord"" in refCat. Exact error message -     {code}  11:04:19-vish~/lsst/pipe_tasks (u/lauren/DM-3693)$ examples/calibrateTask.py --ds9  calibrate: installInitialPsf fwhm=5.40540540548 pixels; size=15 pixels  calibrate.repair: Identified 7 cosmic rays.  calibrate.detection: Detected 4 positive sources to 5 sigma.  calibrate.detection: Resubtracting the background after object detection  calibrate.initialMeasurement: Measuring 4 sources (4 parents, 0 children)   Traceback (most recent call last):    File ""examples/calibrateTask.py"", line 150, in <module>      run(display=args.ds9)    File ""examples/calibrateTask.py"", line 119, in run      result = calibrateTask.run(exposure)    File ""/home/vish/lsst/lsstsw/stack/Linux64/pipe_base/11.0-2-g8218aaa+5/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/vish/lsst/pipe_tasks/python/lsst/pipe/tasks/calibrate.py"", line 478, in run      astromRet = self.astrometry.run(exposure, sources1)    File ""examples/calibrateTask.py"", line 90, in run      m.set(""coord"", wcs.pixelToSky(s.getCentroid()))    File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 2372, in set      self.set(self.schema.find(key).key, value)    File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 1064, in find      raise KeyError(""Field '%s' not found in Schema."" % k)  KeyError: ""Field 'coord' not found in Schema.""  {code}    Note that {{wcs.pixelToSky(s.getCentroid())}} is set to {{Fk5Coord(15.007663073114244 * afwGeom.degrees, 1.0030133772819259 * afwGeom.degrees, 2000.0)}}",1,DM-4125,datamanagement,"pipe_task example calibratetask.py fail self contain example calibratetask.py pipe_task examples/ fail attempt set field coord refcat exact error message code 11:04:19 vish~/lsst pipe_task lauren dm-3693)$ example calibratetask.py --ds9 calibrate installinitialpsf fwhm=5.40540540548 pixel pixel calibrate.repair identify cosmic ray calibrate.detection detect positive source sigma calibrate.detection resubtracte background object detection calibrate.initialmeasurement measure source parent child traceback recent file example calibratetask.py line 150 run(display args.ds9 file example calibratetask.py line 119 run result calibratetask.run(exposure file /home vish lsst lsstsw stack linux64 pipe_base/11.0 g8218aaa+5 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home vish lsst pipe_tasks python lsst pipe task calibrate.py line 478 run astromret self.astrometry.run(exposure sources1 file example calibratetask.py line 90 run m.set(""coord wcs.pixeltosky(s.getcentroid file /home vish lsst lsstsw stack linux64 afw/11.0 g97168e0 python lsst afw table tablelib.py line 2372 set self.set(self.schema.find(key).key value file /home vish lsst lsstsw stack linux64 afw/11.0 g97168e0 python lsst afw table tablelib.py line 1064 find raise keyerror(""field find schema keyerror field coord find schema code note wcs.pixeltosky(s.getcentroid set fk5coord(15.007663073114244 afwgeom.degree 1.0030133772819259 afwgeom.degree 2000.0","pipe_tasks/examples/calibrateTask.py fails The self contained example calibrateTask.py in pipe_tasks/examples/ fails when attempting to set field ""coord"" in refCat. Exact error message - {code} 11:04:19-vish~/lsst/pipe_tasks (u/lauren/DM-3693)$ examples/calibrateTask.py --ds9 calibrate: installInitialPsf fwhm=5.40540540548 pixels; size=15 pixels calibrate.repair: Identified 7 cosmic rays. calibrate.detection: Detected 4 positive sources to 5 sigma. calibrate.detection: Resubtracting the background after object detection calibrate.initialMeasurement: Measuring 4 sources (4 parents, 0 children) Traceback (most recent call last): File ""examples/calibrateTask.py"", line 150, in  run(display=args.ds9) File ""examples/calibrateTask.py"", line 119, in run result = calibrateTask.run(exposure) File ""/home/vish/lsst/lsstsw/stack/Linux64/pipe_base/11.0-2-g8218aaa+5/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/vish/lsst/pipe_tasks/python/lsst/pipe/tasks/calibrate.py"", line 478, in run astromRet = self.astrometry.run(exposure, sources1) File ""examples/calibrateTask.py"", line 90, in run m.set(""coord"", wcs.pixelToSky(s.getCentroid())) File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 2372, in set self.set(self.schema.find(key).key, value) File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 1064, in find raise KeyError(""Field '%s' not found in Schema."" % k) KeyError: ""Field 'coord' not found in Schema."" {code} Note that {{wcs.pixelToSky(s.getCentroid())}} is set to {{Fk5Coord(15.007663073114244 * afwGeom.degrees, 1.0030133772819259 * afwGeom.degrees, 2000.0)}}"
"Database connection problems in daf_ingest The DbAuth connection fallback in ingestCatalogTask passes the ""password"" keyword argument to {{MySQLdb.connect}} instead of ""passwd"", which fails. Also, the ""port"" command line argument isn't marked as an integer, causing port strings to be passed down to MySQLdb. This results in a type error. ",1,DM-4129,datamanagement,database connection problem daf_ing dbauth connection fallback ingestcatalogtask pass password keyword argument mysqldb.connect instead passwd fail port command line argument mark integer cause port string pass mysqldb result type error,"Database connection problems in daf_ingest The DbAuth connection fallback in ingestCatalogTask passes the ""password"" keyword argument to {{MySQLdb.connect}} instead of ""passwd"", which fails. Also, the ""port"" command line argument isn't marked as an integer, causing port strings to be passed down to MySQLdb. This results in a type error."
"Have a ""mark as current"" option in lsstsw Russell explained to me the advantage of having a specific eups tag associated with a given lsstsw installation. However, it would be very handy to have a way to get all installed packages automatically tagged as current as part of the installation process.    I suggest a ""-c"" option to lsstsw which will tag everything as current after the full installation is complete. This way, partially installed packages won't get marked current, and people who do full installations can not have to deal with having to say ""-t bBLAH"" every time they setup things.",4,DM-4131,datamanagement,mark current option lsstsw russell explain advantage have specific eup tag associate give lsstsw installation handy way instal package automatically tag current installation process suggest -c option lsstsw tag current installation complete way partially instal package will mark current people installation deal have bblah time setup thing,"Have a ""mark as current"" option in lsstsw Russell explained to me the advantage of having a specific eups tag associated with a given lsstsw installation. However, it would be very handy to have a way to get all installed packages automatically tagged as current as part of the installation process. I suggest a ""-c"" option to lsstsw which will tag everything as current after the full installation is complete. This way, partially installed packages won't get marked current, and people who do full installations can not have to deal with having to say ""-t bBLAH"" every time they setup things."
"Change type of LTV1/2 from int to float when writing afw images to FITS The LTV1/2 problem is originally my bug.  I used integer LTV1/2 in  {code}  afw/src/image/ExposureInfo.cc:    data.imageMetadata->set(""LTV1"", -xy0.getX());  afw/src/image/ExposureInfo.cc:    data.imageMetadata->set(""LTV2"", -xy0.getY());  {code}  whereas a more careful reading of the NOAO page [http://iraf.noao.edu/projects/ccdmosaic/imagedef/imagedef.html] introducing them includes floating point examples.    The fix is to cast the XY0 values to float.  I'm not sure if there'll be any side effects of fixing this, but if so they'll be obvious and trivial.  ",1,DM-4133,datamanagement,"change type ltv1/2 int float write afw image fits ltv1/2 problem originally bug integer ltv1/2 code afw src image exposureinfo.cc data.imagemetadata->set(""ltv1 -xy0.getx afw src image exposureinfo.cc data.imagemetadata->set(""ltv2 -xy0.gety code careful reading noao page http://iraf.noao.edu/projects/ccdmosaic/imagedef/imagedef.html introduce include float point example fix cast xy0 value float sure effect fix obvious trivial","Change type of LTV1/2 from int to float when writing afw images to FITS The LTV1/2 problem is originally my bug. I used integer LTV1/2 in {code} afw/src/image/ExposureInfo.cc: data.imageMetadata->set(""LTV1"", -xy0.getX()); afw/src/image/ExposureInfo.cc: data.imageMetadata->set(""LTV2"", -xy0.getY()); {code} whereas a more careful reading of the NOAO page [http://iraf.noao.edu/projects/ccdmosaic/imagedef/imagedef.html] introducing them includes floating point examples. The fix is to cast the XY0 values to float. I'm not sure if there'll be any side effects of fixing this, but if so they'll be obvious and trivial."
"DHT prototype: HTTP server library refactor/cleanup This experimental library turned out to be quite useful.  Next stage of prototyping will be making greater use of this, and I anticipate this library will also be used in production code.  Spend some time cleaning up and organizing this lib so it doesn't get off to a bad start, and prepare for general review/feedback from the team.",6,DM-4135,datamanagement,dht prototype http server library refactor cleanup experimental library turn useful stage prototyping make great use anticipate library production code spend time clean organize lib bad start prepare general review feedback team,"DHT prototype: HTTP server library refactor/cleanup This experimental library turned out to be quite useful. Next stage of prototyping will be making greater use of this, and I anticipate this library will also be used in production code. Spend some time cleaning up and organizing this lib so it doesn't get off to a bad start, and prepare for general review/feedback from the team."
"DHT prototype: test fixture rework The DHT test fixture, developed during the preliminary work with kademlia, needs some updates for the next stage of the prototype:  * adapt to reworked http library  * adapt DHT interface to be more generic  ",6,DM-4136,datamanagement,dht prototype test fixture rework dht test fixture develop preliminary work kademlia need update stage prototype adapt rework http library adapt dht interface generic,"DHT prototype: test fixture rework The DHT test fixture, developed during the preliminary work with kademlia, needs some updates for the next stage of the prototype: * adapt to reworked http library * adapt DHT interface to be more generic"
"Update DECam CCDs gain, read noise, and saturation values The values of DECam gain, read noise, and saturation value need to be updated.     This ticket is to update them in the Detector amplifier information, which is used in IsrTask.     Talked to Robert Gruendl. These values should take precedence over the values in the fits header. They seem stable and do not seem to vary with time.   ",1,DM-4137,datamanagement,update decam ccds gain read noise saturation value value decam gain read noise saturation value need update ticket update detector amplifier information isrtask talk robert gruendl value precedence value fit header stable vary time,"Update DECam CCDs gain, read noise, and saturation values The values of DECam gain, read noise, and saturation value need to be updated. This ticket is to update them in the Detector amplifier information, which is used in IsrTask. Talked to Robert Gruendl. These values should take precedence over the values in the fits header. They seem stable and do not seem to vary with time."
Re-implement packed keys in CSS Current implementation of JSON-packed keys in CSS has one complication - the name of the container for packed keys is the same as the key itself (plus .json suffix). This complicates handling of the subkeys because these keys need to be filtered out and these names are all different. It would be better to have easily identifiable packed key names.,4,DM-4138,datamanagement,implement pack key css current implementation json pack key css complication container pack key key plus suffix complicate handle subkey key need filter name different well easily identifiable packed key name,Re-implement packed keys in CSS Current implementation of JSON-packed keys in CSS has one complication - the name of the container for packed keys is the same as the key itself (plus .json suffix). This complicates handling of the subkeys because these keys need to be filtered out and these names are all different. It would be better to have easily identifiable packed key names.
Discover how to create Doxygen XML in build system We need XML output from Doxygen in order to import existing API documentation into Sphinx. In this story I find out how to achieve this within the stack build system.,2,DM-4142,datamanagement,discover create doxygen xml build system need xml output doxygen order import exist api documentation sphinx story find achieve stack build system,Discover how to create Doxygen XML in build system We need XML output from Doxygen in order to import existing API documentation into Sphinx. In this story I find out how to achieve this within the stack build system.
Demonstrate using Breathe for Python & C++ API reference in New Docs Demonstrate use of breathe for utilizing the existing Doxygen API documentation in the new Sphinx-based doc platform.,3,DM-4143,datamanagement,demonstrate breathe python c++ api reference new docs demonstrate use breathe utilize exist doxygen api documentation new sphinx base doc platform,Demonstrate using Breathe for Python & C++ API reference in New Docs Demonstrate use of breathe for utilizing the existing Doxygen API documentation in the new Sphinx-based doc platform.
"Reduce scons output in qserv Yesterday AndyH expressed a valid concern that qserv prints too much info which makes it hard to find errors. By default scons prints whole command line for C++ compilation and linking which are quite long (~half screen depending on your screen size). Most of the time we don't need to see that, so it would be better to replace that with shorter messages like ""Compiling Something.cxx"" and have an option to print full command with --verbose option.",1,DM-4145,datamanagement,reduce scon output qserv yesterday andyh express valid concern qserv print info make hard find error default scon print command line c++ compilation linking long ~half screen depend screen size time need well replace short message like compiling something.cxx option print command option,"Reduce scons output in qserv Yesterday AndyH expressed a valid concern that qserv prints too much info which makes it hard to find errors. By default scons prints whole command line for C++ compilation and linking which are quite long (~half screen depending on your screen size). Most of the time we don't need to see that, so it would be better to replace that with shorter messages like ""Compiling Something.cxx"" and have an option to print full command with --verbose option."
Networking requirements for design Meeting with Paul to discuss remaining network design.,1,DM-4146,datamanagement,networking requirement design meeting paul discuss remain network design,Networking requirements for design Meeting with Paul to discuss remaining network design.
Finance Contract Discussions Discussing updated contract hoops and game plan.,1,DM-4147,datamanagement,finance contract discussions discuss update contract hoop game plan,Finance Contract Discussions Discussing updated contract hoops and game plan.
Investigating procurement of individual components Component breakdown and explanation of components as part of FY16 purchases in order to plan optimal purchasing through vehicles available to NCSA.,1,DM-4150,datamanagement,investigate procurement individual component component breakdown explanation component fy16 purchase order plan optimal purchasing vehicle available ncsa,Investigating procurement of individual components Component breakdown and explanation of components as part of FY16 purchases in order to plan optimal purchasing through vehicles available to NCSA.
"Search for uses of current afw.wcs in the stack Search through the stack for all the uses of our Wcs implementation (Wcs, TanWcs, makeWcs, and any other hidden objects) and make a list of all of those uses (on Community for example). This list should note whether the usage is in C++ or python.",2,DM-4151,datamanagement,search use current afw.wcs stack search stack use wcs implementation wcs tanwcs makewcs hidden object list use community example list note usage c++ python,"Search for uses of current afw.wcs in the stack Search through the stack for all the uses of our Wcs implementation (Wcs, TanWcs, makeWcs, and any other hidden objects) and make a list of all of those uses (on Community for example). This list should note whether the usage is in C++ or python."
"Document detailing usage of Wcs in the stack The information from DM-4151 should become a brief report on the kinds of usage of Wcs in the stack. This could be posted to Community or Confluence, or it could be a brief LaTeX document attached to the afw repo.    Included in this report should be whether each current usage requires C++, or whether it could be done with e.g. vectors returned from python.",4,DM-4152,datamanagement,document detail usage wcs stack information dm-4151 brief report kind usage wcs stack post community confluence brief latex document attach afw repo include report current usage require c++ e.g. vector return python,"Document detailing usage of Wcs in the stack The information from DM-4151 should become a brief report on the kinds of usage of Wcs in the stack. This could be posted to Community or Confluence, or it could be a brief LaTeX document attached to the afw repo. Included in this report should be whether each current usage requires C++, or whether it could be done with e.g. vectors returned from python."
"RFD to collect current and future use cases of Wcs File an RFD requesting information about current and possible future use cases for a Wcs system in the stack. This should get feedback from, at minimum, the alerts pipeline, DRP, and Level III data producers. It should also get feedback from our resident wcs experts.    Whether those use cases are currently implemented in our code, or could be generalized from it, isn't important to this RFD as this information will feed into a subsequent requirements document and RFC about what needs to be written/changed.    Some use cases/buzzwords that will likely be included:   * Efficient x,y -> x',y'   * Stacking a sequence of transformations efficiently   * easy extensibility   * should color terms be included in the Wcs or dealt with elsewhere?   * pixel distortion effects (e.g. tree rings, edge roll off)   * Simultaneous Astrometry (i.e. from image stacks)",4,DM-4153,datamanagement,"rfd collect current future use case wcs file rfd request information current possible future use case wcs system stack feedback minimum alert pipeline drp level iii datum producer feedback resident wcs expert use case currently implement code generalize important rfd information feed subsequent requirement document rfc need write change use case buzzword likely include efficient x',y stack sequence transformation efficiently easy extensibility color term include wcs deal pixel distortion effect e.g. tree ring edge roll simultaneous astrometry i.e. image stack","RFD to collect current and future use cases of Wcs File an RFD requesting information about current and possible future use cases for a Wcs system in the stack. This should get feedback from, at minimum, the alerts pipeline, DRP, and Level III data producers. It should also get feedback from our resident wcs experts. Whether those use cases are currently implemented in our code, or could be generalized from it, isn't important to this RFD as this information will feed into a subsequent requirements document and RFC about what needs to be written/changed. Some use cases/buzzwords that will likely be included: * Efficient x,y -> x',y' * Stacking a sequence of transformations efficiently * easy extensibility * should color terms be included in the Wcs or dealt with elsewhere? * pixel distortion effects (e.g. tree rings, edge roll off) * Simultaneous Astrometry (i.e. from image stacks)"
"Sever side Histogram for variable bin size For any given column or columns (expression of columns such as col1+log(col2) ) of a IpacTable data, the variable bin histogram is needed.  The variable bin is based on ""Bayesian Blocks"" algorithm (http://jakevdp.github.io/blog/2012/09/12/dynamic-programming-in-python/).  The output is a new IpacTable (DataGroup) with three columns, numPoints, binMin and binMax.",6,DM-4154,datamanagement,sever histogram variable bin size give column column expression column col1+log(col2 ipactable datum variable bin histogram need variable bin base bayesian blocks algorithm http://jakevdp.github.io/blog/2012/09/12/dynamic-programming-in-python/ output new ipactable datagroup column numpoints binmin binmax,"Sever side Histogram for variable bin size For any given column or columns (expression of columns such as col1+log(col2) ) of a IpacTable data, the variable bin histogram is needed. The variable bin is based on ""Bayesian Blocks"" algorithm (http://jakevdp.github.io/blog/2012/09/12/dynamic-programming-in-python/). The output is a new IpacTable (DataGroup) with three columns, numPoints, binMin and binMax."
"Provide a recommendation for how to manage Wcs in LSST The technical report from DM-4155 and DM-4156 should have as its conclusion a recommendation for what Wcs system the LSST stack should adopt (either our own implementation, a third-party library, or some combination thereof). Beyond the conclusion section of that report, this will be provided as an RFC that includes:     * executive summary   * mock API   * links to the relevant documentation (beyond just the above technical report)   * a bullet list of the rationale for this decision    The conclusion of this RFC represents the end of this Epic.",4,DM-4157,datamanagement,provide recommendation manage wcs lsst technical report dm-4155 dm-4156 conclusion recommendation wcs system lsst stack adopt implementation party library combination thereof conclusion section report provide rfc include executive summary mock api link relevant documentation technical report bullet list rationale decision conclusion rfc represent end epic,"Provide a recommendation for how to manage Wcs in LSST The technical report from DM-4155 and DM-4156 should have as its conclusion a recommendation for what Wcs system the LSST stack should adopt (either our own implementation, a third-party library, or some combination thereof). Beyond the conclusion section of that report, this will be provided as an RFC that includes: * executive summary * mock API * links to the relevant documentation (beyond just the above technical report) * a bullet list of the rationale for this decision The conclusion of this RFC represents the end of this Epic."
"Unused variables in meas.algorithms.utils Pyflakes 1.0.0 reports:  {code}  $ pyflakes-2.7 utils.py  utils.py:232: local variable 'chi2' is assigned to but never used  utils.py:481: local variable 'numCandidates' is assigned to but never used  utils.py:482: local variable 'numBasisFuncs' is assigned to but never used  utils.py:487: local variable 'ampGood' is assigned to but never used  utils.py:492: local variable 'ampBad' is assigned to but never used  {code}  In the best case, those variables are simply unnecessary, and they should be removed to simplify the code and avoid wasting time. Alternatively, it's possible that they ought to be used elsewhere in the calculation but have been omitted accidentally. Please establish this for each one, then either remove them or fix the rest of the code.",1,DM-4160,datamanagement,unused variable meas.algorithms.util pyflakes 1.0.0 report code pyflakes-2.7 utils.py utils.py:232 local variable chi2 assign utils.py:481 local variable numcandidates assign utils.py:482 local variable numbasisfuncs assign utils.py:487 local variable ampgood assign utils.py:492 local variable ampbad assign code good case variable simply unnecessary remove simplify code avoid waste time alternatively possible ought calculation omit accidentally establish remove fix rest code,"Unused variables in meas.algorithms.utils Pyflakes 1.0.0 reports: {code} $ pyflakes-2.7 utils.py utils.py:232: local variable 'chi2' is assigned to but never used utils.py:481: local variable 'numCandidates' is assigned to but never used utils.py:482: local variable 'numBasisFuncs' is assigned to but never used utils.py:487: local variable 'ampGood' is assigned to but never used utils.py:492: local variable 'ampBad' is assigned to but never used {code} In the best case, those variables are simply unnecessary, and they should be removed to simplify the code and avoid wasting time. Alternatively, it's possible that they ought to be used elsewhere in the calculation but have been omitted accidentally. Please establish this for each one, then either remove them or fix the rest of the code."
"Please implement a warper that works with a single XYTransform At present we only warp images based on a pair of {{Wcs}}. This is needlessly restrictive. We should be able to define the transformation by function that computes {{f(x,y) -> x',y'}}, e.g. an {{XYTransform}}.    Note that reversibility, while not strictly necessary, is very desirable. Hence we might as well use {{XYTransform}}.     I suggest we have only one underlying implementation in order to avoid code duplication. This could easily be done by implementing an {{XYTransform}} that combines a pair of WCS.",6,DM-4162,datamanagement,"implement warper work single xytransform present warp image base pair wcs needlessly restrictive able define transformation function compute f(x x',y e.g. xytransform note reversibility strictly necessary desirable use xytransform suggest underlie implementation order avoid code duplication easily implement xytransform combine pair wcs","Please implement a warper that works with a single XYTransform At present we only warp images based on a pair of {{Wcs}}. This is needlessly restrictive. We should be able to define the transformation by function that computes {{f(x,y) -> x',y'}}, e.g. an {{XYTransform}}. Note that reversibility, while not strictly necessary, is very desirable. Hence we might as well use {{XYTransform}}. I suggest we have only one underlying implementation in order to avoid code duplication. This could easily be done by implementing an {{XYTransform}} that combines a pair of WCS."
"Take upstream boost 1.59 patch to squelch warnings for gcc 5.2.1 Under gcc 5.2.1, use of boost 1.59.0 produces a torrent of compiler warns from within boost headers about use of deprecated std::auto_ptr (see https://svn.boost.org/trac/boost/ticket/11622).    A patch for this is already committed upstream in boost.  It is proposed that we take this patch into the lsst t&p in interim until the next official boost release.",1,DM-4165,datamanagement,upstream boost 1.59 patch squelch warning gcc 5.2.1 gcc 5.2.1 use boost 1.59.0 produce torrent compiler warn boost header use deprecate std::auto_ptr https://svn.boost.org/trac/boost/ticket/11622 patch commit upstream boost propose patch lsst t&p interim official boost release,"Take upstream boost 1.59 patch to squelch warnings for gcc 5.2.1 Under gcc 5.2.1, use of boost 1.59.0 produces a torrent of compiler warns from within boost headers about use of deprecated std::auto_ptr (see https://svn.boost.org/trac/boost/ticket/11622). A patch for this is already committed upstream in boost. It is proposed that we take this patch into the lsst t&p in interim until the next official boost release."
"productize ""Data repository selection based on version"" finish & productize work from DM-5608",6,DM-4168,datamanagement,productize datum repository selection base version finish productize work dm-5608,"productize ""Data repository selection based on version"" finish & productize work from DM-5608"
Build AL2S vlans from Miami to NCSA To prototype the layer 2 circuit LSST will eventually have.,2,DM-4172,datamanagement,build al2s vlan miami ncsa prototype layer circuit lsst eventually,Build AL2S vlans from Miami to NCSA To prototype the layer 2 circuit LSST will eventually have.
"document proposal for Base site to NCSA data transfer With Steve and James distill the actual data movement requirements, the mechanism to broker those transfers and the network technology to ensure the real-time transfer of images",2,DM-4174,datamanagement,document proposal base site ncsa data transfer steve james distill actual data movement requirement mechanism broker transfer network technology ensure real time transfer image,"document proposal for Base site to NCSA data transfer With Steve and James distill the actual data movement requirements, the mechanism to broker those transfers and the network technology to ensure the real-time transfer of images"
update LSE-78 once current updates are applied There are several sections with inaccurate information about the North American portion of the LHN and the implementation of the networking into NCSA as well as the base site commissioning cluster architecture.,2,DM-4179,datamanagement,update lse-78 current update apply section inaccurate information north american portion lhn implementation networking ncsa base site commission cluster architecture,update LSE-78 once current updates are applied There are several sections with inaccurate information about the North American portion of the LHN and the implementation of the networking into NCSA as well as the base site commissioning cluster architecture.
"Experiment with memcached for secondary index Test whether memcached could be used to serve objectId --> chunkId mapping, in particular from the performance perspective.",6,DM-4183,datamanagement,experiment memcached secondary index test memcached serve objectid chunkid mapping particular performance perspective,"Experiment with memcached for secondary index Test whether memcached could be used to serve objectId --> chunkId mapping, in particular from the performance perspective."
"Experiment with xrootd for secondary index Test whether xrootd could be used to serve objectId --> chunkId mapping, in particular from the performance perspective.",6,DM-4184,datamanagement,experiment xrootd secondary index test xrootd serve objectid chunkid mapping particular performance perspective,"Experiment with xrootd for secondary index Test whether xrootd could be used to serve objectId --> chunkId mapping, in particular from the performance perspective."
"Experiment with c-style arrays for secondary index Use simple C-style array (chunk[objID], allocated up to maximum number of objectIDs), or a single-layer set of arrays (chunk[block][objID%blksize], where second index runs from 0 to size of each block) to store index compactly and provide minimum overhead for lookups.",5,DM-4185,datamanagement,experiment style array secondary index use simple style array chunk[objid allocate maximum number objectids single layer set array chunk[block][objid%blksize second index run size block store index compactly provide minimum overhead lookup,"Experiment with c-style arrays for secondary index Use simple C-style array (chunk[objID], allocated up to maximum number of objectIDs), or a single-layer set of arrays (chunk[block][objID%blksize], where second index runs from 0 to size of each block) to store index compactly and provide minimum overhead for lookups."
Outline of data flow Outline of data flow between Chile->NCSA and NCSA->Chile.,3,DM-4189,datamanagement,outline datum flow outline datum flow chile->ncsa ncsa->chile,Outline of data flow Outline of data flow between Chile->NCSA and NCSA->Chile.
"Other LOE -- Oct 2015 Local LSST group meetings, Ethics training, or other local meetings or tasks to comply with NCSA policies",2,DM-4192,datamanagement,loe oct 2015 local lsst group meeting ethics training local meeting task comply ncsa policy,"Other LOE -- Oct 2015 Local LSST group meetings, Ethics training, or other local meetings or tasks to comply with NCSA policies"
"Automate LSST Firefly standalone releases using Jenkins This task involves merging in feature branches, building Firefly standalone, tagging, push changes to github, generating changlog, and using github API to publish the release.  Release should be attached to the latest tag with downloading artifacts and changelog.",7,DM-4193,datamanagement,automate lsst firefly standalone release jenkins task involve merge feature branch build firefly standalone tagging push change github generate changlog github api publish release release attach late tag download artifact changelog,"Automate LSST Firefly standalone releases using Jenkins This task involves merging in feature branches, building Firefly standalone, tagging, push changes to github, generating changlog, and using github API to publish the release. Release should be attached to the latest tag with downloading artifacts and changelog."
"Python LogHandler does not pass logger name to log4cxx Not sure how or why it happened, but presently Python LogHandler for lsst.log does not pass logger name to log4cxx layer and all messages from Python logging end in root logger. ",1,DM-4194,datamanagement,python loghandler pass logg log4cxx sure happen presently python loghandler lsst.log pass logg log4cxx layer message python log end root logger,"Python LogHandler does not pass logger name to log4cxx Not sure how or why it happened, but presently Python LogHandler for lsst.log does not pass logger name to log4cxx layer and all messages from Python logging end in root logger."
"Build proof-of-concept package documentation for lsst.afw Build package documentation for {{lsst.afw}} under the new doc platform as a demonstration.    - Install a Sphinx site in lsst.afw/doc  - Implement MVP documentation pages for lsst.afw packages (table, image, etc.)  - C++ API reference from doxygen+breathe  - Python API reference from numpydoc    This ticket *will not* attempt to add new documentation content; only to show how existing content can be re-organized.",2,DM-4195,datamanagement,build proof concept package documentation lsst.afw build package documentation lsst.afw new doc platform demonstration install sphinx site lsst.afw/doc implement mvp documentation page lsst.afw package table image etc c++ api reference doxygen+breathe python api reference numpydoc ticket attempt add new documentation content exist content organize,"Build proof-of-concept package documentation for lsst.afw Build package documentation for {{lsst.afw}} under the new doc platform as a demonstration. - Install a Sphinx site in lsst.afw/doc - Implement MVP documentation pages for lsst.afw packages (table, image, etc.) - C++ API reference from doxygen+breathe - Python API reference from numpydoc This ticket *will not* attempt to add new documentation content; only to show how existing content can be re-organized."
"Improve Qserv master robustness for queries like ""select @@max_allowed_packet"" This king of query crashes Qserv:    {code:bash}  mysql --host 127.0.0.1 --port 4040 --user qsmaster -e ""select @@max_allowed_packet""  {code}",1,DM-4197,datamanagement,improve qserv master robustness query like select @@max_allowed_packet king query crash qserv code bash mysql 127.0.0.1 --port 4040 qsmaster select @@max_allowed_packet code,"Improve Qserv master robustness for queries like ""select @@max_allowed_packet"" This king of query crashes Qserv: {code:bash} mysql --host 127.0.0.1 --port 4040 --user qsmaster -e ""select @@max_allowed_packet"" {code}"
"Documentation and technical debt in meas_base/PixelFlags.cc The port from HSC of SafeClipAssembleCoaddTask has left some documentation and code quality changes to be made. Specifics include:    * In the process of porting, functionality was added which allowed users to specify additional mask planes to be converted to pixel flags. However this was fundamentally incompatible with the flag handler functionality that LSST was currently using. PixelFlags was thus modified to allow SafeClipAssemble coadd to work with the user defined masks, but that made it fundamentally different than the other plugins. In the future this plugin should be brought more in line with all the other measurement framework. This will most likely involve rewriting sections of the measurement framework, to add the ability for users to more directly set runtime behaviors of the measurement plugins (such as specifying non default mask planes to work with).  * Because PixelFlags could no longer use the the flag handler framework, sections of the SafeCentroidExtractor had to be duplicated within PixelFlags. This duplicated code is non-ideal and should be rectified. Possibly in the process of rewriting the measurement framework, the utils could be expanded to have convenience methods to access functionality when not using flag handlers  * As with all of the measurement framework, PixelFlags needs better documentation. This includes some line to line comments, but more importantly the over all functionality of the routine needs documentation. This includes: what the routine does; how it works; and why various design decisions were made",3,DM-4201,datamanagement,documentation technical debt meas_base pixelflags.cc port hsc safeclipassemblecoaddtask leave documentation code quality change specific include process porting functionality add allow user specify additional mask plane convert pixel flag fundamentally incompatible flag handler functionality lsst currently pixelflags modify allow safeclipassemble coadd work user define mask fundamentally different plugin future plugin bring line measurement framework likely involve rewrite section measurement framework add ability user directly set runtime behavior measurement plugin specify non default mask plane work pixelflags long use flag handler framework section safecentroidextractor duplicate pixelflags duplicate code non ideal rectify possibly process rewrite measurement framework util expand convenience method access functionality flag handler measurement framework pixelflags need well documentation include line line comment importantly functionality routine need documentation include routine work design decision,"Documentation and technical debt in meas_base/PixelFlags.cc The port from HSC of SafeClipAssembleCoaddTask has left some documentation and code quality changes to be made. Specifics include: * In the process of porting, functionality was added which allowed users to specify additional mask planes to be converted to pixel flags. However this was fundamentally incompatible with the flag handler functionality that LSST was currently using. PixelFlags was thus modified to allow SafeClipAssemble coadd to work with the user defined masks, but that made it fundamentally different than the other plugins. In the future this plugin should be brought more in line with all the other measurement framework. This will most likely involve rewriting sections of the measurement framework, to add the ability for users to more directly set runtime behaviors of the measurement plugins (such as specifying non default mask planes to work with). * Because PixelFlags could no longer use the the flag handler framework, sections of the SafeCentroidExtractor had to be duplicated within PixelFlags. This duplicated code is non-ideal and should be rectified. Possibly in the process of rewriting the measurement framework, the utils could be expanded to have convenience methods to access functionality when not using flag handlers * As with all of the measurement framework, PixelFlags needs better documentation. This includes some line to line comments, but more importantly the over all functionality of the routine needs documentation. This includes: what the routine does; how it works; and why various design decisions were made"
Revert temporary disabling of CModel in config override files Revert the temporary disabling of CModel that relates to a bug noted in DM-4033 that was causing too many failures to test that processCcd (etc.) would run all the way to completion (most of the other fixes/updates related to the initial disabling in the multiband tasks have now been completed in DM-2977 & DM-3821).     Relevant files:  {code}  config/processCcd.py   config/forcedPhotCcd.py   config/forcedPhotCoadd.py   config/measureCoaddSources.py  {code}  ,1,DM-4202,datamanagement,revert temporary disabling cmodel config override file revert temporary disabling cmodel relate bug note dm-4033 cause failure test processccd etc run way completion fix update relate initial disable multiband task complete dm-2977 dm-3821 relevant file code config processccd.py config forcedphotccd.py config forcedphotcoadd.py config measurecoaddsources.py code,Revert temporary disabling of CModel in config override files Revert the temporary disabling of CModel that relates to a bug noted in DM-4033 that was causing too many failures to test that processCcd (etc.) would run all the way to completion (most of the other fixes/updates related to the initial disabling in the multiband tasks have now been completed in DM-2977 & DM-3821). Relevant files: {code} config/processCcd.py config/forcedPhotCcd.py config/forcedPhotCoadd.py config/measureCoaddSources.py {code}
wmgr should delete database from inventory when dropping it When wmgr drops database it should also cleanup chunk inventory for that database.    ,2,DM-4206,datamanagement,wmgr delete database inventory drop wmgr drop database cleanup chunk inventory database,wmgr should delete database from inventory when dropping it When wmgr drops database it should also cleanup chunk inventory for that database.
"Research web authentication and authorization and gather usage stories Research SUI, DAX, Butler, and Qserv authentication and authorization requirements and schemes. Document usage stories for all layers",6,DM-4208,datamanagement,research web authentication authorization gather usage story research sui dax butler qserv authentication authorization requirement scheme document usage story layer,"Research web authentication and authorization and gather usage stories Research SUI, DAX, Butler, and Qserv authentication and authorization requirements and schemes. Document usage stories for all layers"
"Create unit tests for SafeClipAssembleCoadd Porting SafeClipAssembeCoadd from HSC to LSST left that functionality without a unit test. Currently AssembleCoadd is tested from within tests/testCoadds.py. This test does not call AssembleCoadd directly however. The actual code pertaining to assembling a coadd exists within python/lsst/pipe/tasks/mocks/mockCoadd.py. This called from testCoadds, and is used to build and coadd synthetic images from known psfs amongst other things. This should be expanded to test both AssembleCoadd and SafeClipAssembleCoadd, possibly with some sort of argument to the function call. It is important to keep testing both methods of generating a coadd.",3,DM-4209,datamanagement,create unit test safeclipassemblecoadd porting safeclipassembecoadd hsc lsst leave functionality unit test currently assemblecoadd test test testcoadds.py test assemblecoadd directly actual code pertain assemble coadd exist python lsst pipe task mocks mockcoadd.py call testcoadd build coadd synthetic image know psfs thing expand test assemblecoadd safeclipassemblecoadd possibly sort argument function important test method generate coadd,"Create unit tests for SafeClipAssembleCoadd Porting SafeClipAssembeCoadd from HSC to LSST left that functionality without a unit test. Currently AssembleCoadd is tested from within tests/testCoadds.py. This test does not call AssembleCoadd directly however. The actual code pertaining to assembling a coadd exists within python/lsst/pipe/tasks/mocks/mockCoadd.py. This called from testCoadds, and is used to build and coadd synthetic images from known psfs amongst other things. This should be expanded to test both AssembleCoadd and SafeClipAssembleCoadd, possibly with some sort of argument to the function call. It is important to keep testing both methods of generating a coadd."
"Create documentation and examples for SafeClipAssembleCoadd SafeClipAssembleCoadd in HSC did not have adequate documentation, and thus neither does LSST post port. Documentation which details the functionality and usage of this function should be created, and should be available either through the [Doxygen task documentation|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/group___l_s_s_t__task__documentation.html] or its successor . Examples of the usage and various options should also be included with the documentation.",4,DM-4210,datamanagement,create documentation example safeclipassemblecoadd safeclipassemblecoadd hsc adequate documentation lsst post port documentation detail functionality usage function create available doxygen task documentation|https://lsst web.ncsa.illinois.edu doxygen x_masterdoxydoc group___l_s_s_t__task__documentation.html successor example usage option include documentation,"Create documentation and examples for SafeClipAssembleCoadd SafeClipAssembleCoadd in HSC did not have adequate documentation, and thus neither does LSST post port. Documentation which details the functionality and usage of this function should be created, and should be available either through the [Doxygen task documentation|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/group___l_s_s_t__task__documentation.html] or its successor . Examples of the usage and various options should also be included with the documentation."
"Investigate shear bias errors from DM-1135 DM-1135 mostly was inconclusive, due to the fact that the error bars were around the same size as the differences in many of the tests.  This in spite of the fact that we ran 6x as many sample galaxies as great3sims.    Investigate the reason for this, and see if we can estimate the errors more accurately.    ",6,DM-4212,datamanagement,investigate shear bias error dm-1135 dm-1135 inconclusive fact error bar size difference test spite fact run 6x sample galaxy great3sim investigate reason estimate error accurately,"Investigate shear bias errors from DM-1135 DM-1135 mostly was inconclusive, due to the fact that the error bars were around the same size as the differences in many of the tests. This in spite of the fact that we ran 6x as many sample galaxies as great3sims. Investigate the reason for this, and see if we can estimate the errors more accurately."
"Rerun tests of DM-1135 with a larger number of galaxies As a result of the new error estimation, it became apparent that a larger number of sample galaxies were required in DM-1135.  This is an expansion of that test to a larger number of galaxies.  Since the original  great3 tests had about 2 million galaxies, we should be able to do these test reasonably well with the 6 million galaxy pool created in DM-1135.  However, the measurements need to be rerun in some cases, and the error analysis done again.",6,DM-4213,datamanagement,rerun test dm-1135 large number galaxy result new error estimation apparent large number sample galaxy require dm-1135 expansion test large number galaxy original great3 test million galaxy able test reasonably million galaxy pool create dm-1135 measurement need rerun case error analysis,"Rerun tests of DM-1135 with a larger number of galaxies As a result of the new error estimation, it became apparent that a larger number of sample galaxies were required in DM-1135. This is an expansion of that test to a larger number of galaxies. Since the original great3 tests had about 2 million galaxies, we should be able to do these test reasonably well with the 6 million galaxy pool created in DM-1135. However, the measurements need to be rerun in some cases, and the error analysis done again."
"Initial tests of ShapeletApprox This will be part of a wider set of test which I am hoping that Jim will fill in as additional stories under DM-1136    Basically, we hope to see how extensive a Shapelet Approximation must be done for a Psf in order to reach a stable result (stable meaning it would not markedly improve with increase in Shapelet order).    For this intial test, I propose to compare SingleGaussian, DoubleGaussian, Full, and 2 higher order models.  I will also add a model with inner and outer defined to see if those have a significant effect.  It will tell use how the existing models line up and which parameters matter.    This test will be done using the great3sims subfield organization, and with a single, randomly chosen Psf image from the 0.7 arcsec FWHM library (raw fwhm from PhoSim, the actual fwhm is closer to 0.8 arcsec)     This will probably be just the beginning of more extensive ShapeletApprox tests, so this story should be a sub-story of a larger test project which I am hoping Jim will define.",6,DM-4214,datamanagement,initial test shapeletapprox wide set test hope jim fill additional story dm-1136 basically hope extensive shapelet approximation psf order reach stable result stable meaning markedly improve increase shapelet order intial test propose compare singlegaussian doublegaussian high order model add model inner outer define significant effect tell use exist model line parameter matter test great3sim subfield organization single randomly choose psf image 0.7 arcsec fwhm library raw fwhm phosim actual fwhm close 0.8 arcsec probably beginning extensive shapeletapprox test story sub story large test project hope jim define,"Initial tests of ShapeletApprox This will be part of a wider set of test which I am hoping that Jim will fill in as additional stories under DM-1136 Basically, we hope to see how extensive a Shapelet Approximation must be done for a Psf in order to reach a stable result (stable meaning it would not markedly improve with increase in Shapelet order). For this intial test, I propose to compare SingleGaussian, DoubleGaussian, Full, and 2 higher order models. I will also add a model with inner and outer defined to see if those have a significant effect. It will tell use how the existing models line up and which parameters matter. This test will be done using the great3sims subfield organization, and with a single, randomly chosen Psf image from the 0.7 arcsec FWHM library (raw fwhm from PhoSim, the actual fwhm is closer to 0.8 arcsec) This will probably be just the beginning of more extensive ShapeletApprox tests, so this story should be a sub-story of a larger test project which I am hoping Jim will define."
"Butler: SafeFile and SafeFileName can overwrite good with bad in some cases A bad file can be written in Butler, in the case where 2 temp files that use SafeFile or SafeFileName to the same location are started, one closes, and then the other fails - and then closes and writes the bad file. Need to handle the exception in a way that does not write.    Also, in the case of no exception (failure), when closing B, need to compare to A and throw if different.",4,DM-4215,datamanagement,butler safefile safefilename overwrite good bad case bad file write butler case temp file use safefile safefilename location start close fail close write bad file need handle exception way write case exception failure closing need compare throw different,"Butler: SafeFile and SafeFileName can overwrite good with bad in some cases A bad file can be written in Butler, in the case where 2 temp files that use SafeFile or SafeFileName to the same location are started, one closes, and then the other fails - and then closes and writes the bad file. Need to handle the exception in a way that does not write. Also, in the case of no exception (failure), when closing B, need to compare to A and throw if different."
"Package capnproto for eups Prototype is now getting to the point where a wire-protocol package like capnproto or protobuf is needed.  capnproto is the new hotness, and we're probably going to want to migrate qserv from protobuf->capnproto at some point.    This task is to go ahead and get capnproto packaged and published for use in the replication prototype.",2,DM-4219,datamanagement,package capnproto eup prototype get point wire protocol package like capnproto protobuf need capnproto new hotness probably go want migrate qserv protobuf->capnproto point task ahead capnproto package publish use replication prototype,"Package capnproto for eups Prototype is now getting to the point where a wire-protocol package like capnproto or protobuf is needed. capnproto is the new hotness, and we're probably going to want to migrate qserv from protobuf->capnproto at some point. This task is to go ahead and get capnproto packaged and published for use in the replication prototype."
"Convert copyright/license statements to one-liners for RFC-45 Refactor how we manage copyright and license information in stack repositories    # Identify a list of packages to process  # build and test an automated system of       - adding a global COPYRIGHT to each repo. Content will be “Copyright YYYY-YYYY The LSST DM Developers”. Years will be determined by git history.     - adding a GPLv3 license file to each repo.     - changing the boilerplate in all files to say ""See the COPYRIGHT and LICENSE files in the top-level directory of this package for notices and licensing terms.” Use https://gist.github.com/ktlim/fdaea18ab3d39afdfa8e     - automatically branch, commit, merge and push    And deploy this automated system.",8,DM-4220,datamanagement,convert copyright license statement liner rfc-45 refactor manage copyright license information stack repository identify list package process build test automate system add global copyright repo content copyright yyyy yyyy lsst dm developers year determine git history add gplv3 license file repo change boilerplate file copyright license file level directory package notice licensing term use https://gist.github.com/ktlim/fdaea18ab3d39afdfa8e automatically branch commit merge push deploy automate system,"Convert copyright/license statements to one-liners for RFC-45 Refactor how we manage copyright and license information in stack repositories # Identify a list of packages to process # build and test an automated system of - adding a global COPYRIGHT to each repo. Content will be Copyright YYYY-YYYY The LSST DM Developers . Years will be determined by git history. - adding a GPLv3 license file to each repo. - changing the boilerplate in all files to say ""See the COPYRIGHT and LICENSE files in the top-level directory of this package for notices and licensing terms. Use https://gist.github.com/ktlim/fdaea18ab3d39afdfa8e - automatically branch, commit, merge and push And deploy this automated system."
IsrTask calls removeFringe in FringeTask but the method does not exist The method {{removeFringe}} of {{FringeTask}} is called in {{IsrTask}} but there is no {{removeFringe}}.      Not sure if {{removeFringe}} was meant to be a place holder,1,DM-4223,datamanagement,isrtask call removefringe fringetask method exist method removefringe fringetask call isrtask removefringe sure removefringe mean place holder,IsrTask calls removeFringe in FringeTask but the method does not exist The method {{removeFringe}} of {{FringeTask}} is called in {{IsrTask}} but there is no {{removeFringe}}. Not sure if {{removeFringe}} was meant to be a place holder
getExposureId not implemented in obs_lsstSim There is no implementation of the getExposureId method in processEimage.py.  This causes it to fail using a modern stack.,1,DM-4224,datamanagement,getexposureid implement obs_lsstsim implementation getexposureid method processeimage.py cause fail modern stack,getExposureId not implemented in obs_lsstSim There is no implementation of the getExposureId method in processEimage.py. This causes it to fail using a modern stack.
"Collect single-host performance data for secondary index Run production-scale (billions of entries) tests on different index options, collect performance statistics for allocation (CPU, memory) and for queries.",3,DM-4225,datamanagement,collect single host performance datum secondary index run production scale billion entry test different index option collect performance statistic allocation cpu memory query,"Collect single-host performance data for secondary index Run production-scale (billions of entries) tests on different index options, collect performance statistics for allocation (CPU, memory) and for queries."
"Set up multi-host tests for secondary index technologies For client-server technologies (memcached, xrootd, etc.), develop multihost test jobs to exercise production-scale indices (billions of entries, millions of queries).  Index should be allocated on single ""master"" machine, with queries generated from one or more separate machines, possibly with multiple threads/jobs per machine.",5,DM-4226,datamanagement,set multi host test secondary index technology client server technology memcache xrootd etc develop multihost test job exercise production scale index billion entry million query index allocate single master machine query generate separate machine possibly multiple thread job machine,"Set up multi-host tests for secondary index technologies For client-server technologies (memcached, xrootd, etc.), develop multihost test jobs to exercise production-scale indices (billions of entries, millions of queries). Index should be allocated on single ""master"" machine, with queries generated from one or more separate machines, possibly with multiple threads/jobs per machine."
"Experiment with bulk updates to secondary index The nightly data loader job may need to add new objectIDs, or change the chunks of existing objectIDs, _en masse_.  Develop test code (or add features to existing demonstrators) to handle bulk loading of new data to the index, or to overwrite collections of existing data.  This is significant for client-server technologies, where the bulk data may be transferred in a single transaction, with the server (possibly) handling the loop over individual elements.",7,DM-4227,datamanagement,experiment bulk update secondary index nightly data loader job need add new objectid change chunk exist objectid en masse develop test code add feature exist demonstrator handle bulk loading new datum index overwrite collection exist datum significant client server technology bulk data transfer single transaction server possibly handle loop individual element,"Experiment with bulk updates to secondary index The nightly data loader job may need to add new objectIDs, or change the chunks of existing objectIDs, _en masse_. Develop test code (or add features to existing demonstrators) to handle bulk loading of new data to the index, or to overwrite collections of existing data. This is significant for client-server technologies, where the bulk data may be transferred in a single transaction, with the server (possibly) handling the loop over individual elements."
"Collect multi-host and bulk-update performance data for secondary index Run secondary index tests across multiple hosts (server and clients), collecting performance data for production-scale indices (billions of entries, millions of queries) with many parallel queries and bulk/block updates.",5,DM-4228,datamanagement,collect multi host bulk update performance datum secondary index run secondary index test multiple host server client collect performance datum production scale index billion entry million query parallel query bulk block update,"Collect multi-host and bulk-update performance data for secondary index Run secondary index tests across multiple hosts (server and clients), collecting performance data for production-scale indices (billions of entries, millions of queries) with many parallel queries and bulk/block updates."
"Identify candidate technology for secondary index Evaluate results of production-scale performance tests, both single and multiple host.  Identify the technology most likely to meet requirements, and estimate performance capability with respect to those requirements",3,DM-4229,datamanagement,identify candidate technology secondary index evaluate result production scale performance test single multiple host identify technology likely meet requirement estimate performance capability respect requirement,"Identify candidate technology for secondary index Evaluate results of production-scale performance tests, both single and multiple host. Identify the technology most likely to meet requirements, and estimate performance capability with respect to those requirements"
"Port HSC-1355: Improved fringe subtraction [HSC-1355|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1355]: ""with this fix, we get much  better fringe subtraction"".",2,DM-4230,datamanagement,port hsc-1355 improved fringe subtraction hsc-1355|https://hsc jira.astro.princeton.edu jira browse hsc-1355 fix well fringe subtraction,"Port HSC-1355: Improved fringe subtraction [HSC-1355|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1355]: ""with this fix, we get much better fringe subtraction""."
"Variance is set after dark subtraction In the default {{IsrTask}}, the variance is currently set after dark subtraction.  This means that photon noise from the dark is not included in the variance plane, which is incorrect.  The variance should be set after bias subtraction and before dark subtraction.    [~hchiang2] also points out (DM-4191) that the {{AssembleCcdTask}} with default parameters requires amplifier images with variance planes, even though the variance cannot be set properly until after full-frame bias subtraction.  I believe that {{AssembleCcdTask}} only requires a variance plane in the amp images because it does an ""effective gain"" calculation, but I suggest that this isn't very useful (an approximation of an approximation, and you're never going to use that information anyway because it's embedded in the variance plane with better fidelity).  I therefore suggest that this effective gain calculation be stripped out and that {{AssembleCcdTask}} not require variance planes.",5,DM-4232,datamanagement,variance set dark subtraction default isrtask variance currently set dark subtraction mean photon noise dark include variance plane incorrect variance set bias subtraction dark subtraction ~hchiang2 point dm-4191 assembleccdtask default parameter require amplifier image variance plane variance set properly frame bias subtraction believe assembleccdtask require variance plane amp image effective gain calculation suggest useful approximation approximation go use information embed variance plane well fidelity suggest effective gain calculation strip assembleccdtask require variance plane,"Variance is set after dark subtraction In the default {{IsrTask}}, the variance is currently set after dark subtraction. This means that photon noise from the dark is not included in the variance plane, which is incorrect. The variance should be set after bias subtraction and before dark subtraction. [~hchiang2] also points out (DM-4191) that the {{AssembleCcdTask}} with default parameters requires amplifier images with variance planes, even though the variance cannot be set properly until after full-frame bias subtraction. I believe that {{AssembleCcdTask}} only requires a variance plane in the amp images because it does an ""effective gain"" calculation, but I suggest that this isn't very useful (an approximation of an approximation, and you're never going to use that information anyway because it's embedded in the variance plane with better fidelity). I therefore suggest that this effective gain calculation be stripped out and that {{AssembleCcdTask}} not require variance planes."
"HSC backport: Jacobian and focalplane algorithms Add algorithms to compute the *Jacobian* correction for each object (calculable from the Wcs, but sometimes convenient)  and record the *focal plane* coordinates (instead of CCD coordinates) for sources (useful for plotting).    The standalone HSC commits to be cherry-picked are:    *Jacobian*  {{meas_algorithms}}  May 3, 2013  [Jacobian: add Algorithm to compute the Jacobian.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/88d3bd3f32cf4d0138b80148e57bc275fc8c3454]  May 24, 2013  [Jacobian: fix up some cut/paste oversights.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/ecad0d2559bb9815fc5560234f4502f35f50db73]  May 28, 2013  [Jacobian: fix calculation|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/7f3db53b56279929b9e416173ed09cf00dc81406]    {{obs_subaru}}  May 3, 2013  [config: enable jacobian calculation in processCcd|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d0969911ee1a655fd82998f0b936fa90f443d2fd]  May 6, 2013  [config: set pixelScale for jacobian correction|https://github.com/HyperSuprime-Cam/obs_subaru/commit/e36bd1b4410812ca314f50c01f899d92acc0e7a5]      *focalplane*  {{meas_algorithms}}  May 24, 2013  [add algorithm to calculate position on the focal plane|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/dda3086f411d647e1a3e15451d7f093cd461873a]  May 25, 2013  [fix up building of focalplane algorithm|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/57d718bf51b255adf5789e389dfb776ecaa062d1]  Nov 21, 2014  [Adapt to removal of Point<float> from afw::table.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/95627d55cb7d64718a42027954474df5c3661a65]    {{obs_subaru}}  May 24, 2013  [config: activate focalplane algorithm|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d999a32e7e10b25cceccc94b61890486f96c0bfd]  ",4,DM-4234,datamanagement,hsc backport jacobian focalplane algorithm add algorithm compute jacobian correction object calculable wcs convenient record focal plane coordinate instead ccd coordinate source useful plotting standalone hsc commit cherry pick jacobian meas_algorithms 2013 jacobian add algorithm compute jacobian.|https://github.com hypersuprime cam meas_algorithms commit/88d3bd3f32cf4d0138b80148e57bc275fc8c3454 24 2013 jacobian fix cut paste oversights.|https://github.com hypersuprime cam meas_algorithms commit ecad0d2559bb9815fc5560234f4502f35f50db73 28 2013 jacobian fix calculation|https://github.com hypersuprime cam meas_algorithms commit/7f3db53b56279929b9e416173ed09cf00dc81406 obs_subaru 2013 config enable jacobian calculation processccd|https://github.com hypersuprime cam obs_subaru commit d0969911ee1a655fd82998f0b936fa90f443d2fd 2013 config set pixelscale jacobian correction|https://github.com hypersuprime cam obs_subaru commit e36bd1b4410812ca314f50c01f899d92acc0e7a5 focalplane meas_algorithms 24 2013 add algorithm calculate position focal plane|https://github.com hypersuprime cam meas_algorithms commit dda3086f411d647e1a3e15451d7f093cd461873a 25 2013 fix building focalplane algorithm|https://github.com hypersuprime cam meas_algorithms commit/57d718bf51b255adf5789e389dfb776ecaa062d1 nov 21 2014 adapt removal point afw::table.|https://github.com hypersuprime cam meas_algorithms commit/95627d55cb7d64718a42027954474df5c3661a65 obs_subaru 24 2013 config activate focalplane algorithm|https://github.com hypersuprime cam obs_subaru commit d999a32e7e10b25cceccc94b61890486f96c0bfd,"HSC backport: Jacobian and focalplane algorithms Add algorithms to compute the *Jacobian* correction for each object (calculable from the Wcs, but sometimes convenient) and record the *focal plane* coordinates (instead of CCD coordinates) for sources (useful for plotting). The standalone HSC commits to be cherry-picked are: *Jacobian* {{meas_algorithms}} May 3, 2013 [Jacobian: add Algorithm to compute the Jacobian.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/88d3bd3f32cf4d0138b80148e57bc275fc8c3454] May 24, 2013 [Jacobian: fix up some cut/paste oversights.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/ecad0d2559bb9815fc5560234f4502f35f50db73] May 28, 2013 [Jacobian: fix calculation|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/7f3db53b56279929b9e416173ed09cf00dc81406] {{obs_subaru}} May 3, 2013 [config: enable jacobian calculation in processCcd|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d0969911ee1a655fd82998f0b936fa90f443d2fd] May 6, 2013 [config: set pixelScale for jacobian correction|https://github.com/HyperSuprime-Cam/obs_subaru/commit/e36bd1b4410812ca314f50c01f899d92acc0e7a5] *focalplane* {{meas_algorithms}} May 24, 2013 [add algorithm to calculate position on the focal plane|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/dda3086f411d647e1a3e15451d7f093cd461873a] May 25, 2013 [fix up building of focalplane algorithm|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/57d718bf51b255adf5789e389dfb776ecaa062d1] Nov 21, 2014 [Adapt to removal of Point from afw::table.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/95627d55cb7d64718a42027954474df5c3661a65] {{obs_subaru}} May 24, 2013 [config: activate focalplane algorithm|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d999a32e7e10b25cceccc94b61890486f96c0bfd]"
"HSC backport: countInputs and per object variance functions Back port of the following two HSC tickets:    *countInputs*  [HSC-1276|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1276]  {{meas_algorithms}}  Jul 1, 2015  [add measurement algorithm to count input images in coadd|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/51db0fd2624c7f9b641c93aa3cf6366539995d50]    {{obs_subaru}}  Sep 24, 2015  [config: activate countInputs for measureCoaddSources.|https://github.com/HyperSuprime-Cam/obs_subaru/commit/13ecd1317b05b5ff9e65fba41fe27a5cffcc2fda]    *variance*  [HSC-1259|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1259]  {{meas_algorithms}}  Jul 2, 2015  [add measurement algorithm to report background variance|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/86022f4381c3cec3f7f203b831b6a306596cfa3f#diff-7ae7aea69b58dbf075350ccfd3802cfb]    {{obs_subaru}}  Oct 19, 2015  [config: activate measurement of variance for coadds|https://github.com/HyperSuprime-Cam/obs_subaru/commit/cf1e80958bb9164dacf42d2d35a94dd366c78892]  ",6,DM-4235,datamanagement,hsc backport countinput object variance function port follow hsc ticket countinput hsc-1276|https://hsc jira.astro.princeton.edu jira browse hsc-1276 meas_algorithms jul 2015 add measurement algorithm count input image coadd|https://github.com hypersuprime cam meas_algorithms commit/51db0fd2624c7f9b641c93aa3cf6366539995d50 obs_subaru sep 24 2015 config activate countinput measurecoaddsources.|https://github.com hypersuprime cam obs_subaru commit/13ecd1317b05b5ff9e65fba41fe27a5cffcc2fda variance hsc-1259|https://hsc jira.astro.princeton.edu jira browse hsc-1259 meas_algorithms jul 2015 add measurement algorithm report background variance|https://github.com hypersuprime cam meas_algorithms commit/86022f4381c3cec3f7f203b831b6a306596cfa3f#diff-7ae7aea69b58dbf075350ccfd3802cfb obs_subaru oct 19 2015 config activate measurement variance coadds|https://github.com hypersuprime cam obs_subaru commit cf1e80958bb9164dacf42d2d35a94dd366c78892,"HSC backport: countInputs and per object variance functions Back port of the following two HSC tickets: *countInputs* [HSC-1276|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1276] {{meas_algorithms}} Jul 1, 2015 [add measurement algorithm to count input images in coadd|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/51db0fd2624c7f9b641c93aa3cf6366539995d50] {{obs_subaru}} Sep 24, 2015 [config: activate countInputs for measureCoaddSources.|https://github.com/HyperSuprime-Cam/obs_subaru/commit/13ecd1317b05b5ff9e65fba41fe27a5cffcc2fda] *variance* [HSC-1259|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1259] {{meas_algorithms}} Jul 2, 2015 [add measurement algorithm to report background variance|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/86022f4381c3cec3f7f203b831b6a306596cfa3f#diff-7ae7aea69b58dbf075350ccfd3802cfb] {{obs_subaru}} Oct 19, 2015 [config: activate measurement of variance for coadds|https://github.com/HyperSuprime-Cam/obs_subaru/commit/cf1e80958bb9164dacf42d2d35a94dd366c78892]"
"Specify default output location for CmdLineTasks When neither {{\--output}} or {{\--rerun}} is specified as an argument to a {{CmdLineTask}}, any output from that task appears to be written back to the input repository. Note the use of the term ""appears"": from a preliminary inspection of the code and documentation, it's not clear if this behaviour can be overridden e.g. by environment variables.    The HSC stack behaves differently, using {{$INPUT/rerun/$USER}} as a default output location. A [brief discussion|https://community.lsst.org/t/new-argument-parser-behavior-rerun-flag-introduction-discussion/345] suggests that this is the preferred behaviour.    Please update the LSST stack to match the HSC behaviour.",2,DM-4236,datamanagement,specify default output location cmdlinetask \--output \--rerun specify argument cmdlinetask output task appear write input repository note use term appear preliminary inspection code documentation clear behaviour overridden e.g. environment variable hsc stack behave differently input rerun/$user default output location brief discussion|https://community.lsst.org new argument parser behavior rerun flag introduction discussion/345 suggest preferred behaviour update lsst stack match hsc behaviour,"Specify default output location for CmdLineTasks When neither {{\--output}} or {{\--rerun}} is specified as an argument to a {{CmdLineTask}}, any output from that task appears to be written back to the input repository. Note the use of the term ""appears"": from a preliminary inspection of the code and documentation, it's not clear if this behaviour can be overridden e.g. by environment variables. The HSC stack behaves differently, using {{$INPUT/rerun/$USER}} as a default output location. A [brief discussion|https://community.lsst.org/t/new-argument-parser-behavior-rerun-flag-introduction-discussion/345] suggests that this is the preferred behaviour. Please update the LSST stack to match the HSC behaviour."
"unable to upload images to nebula I seem to be unable to upload an image to neblua from a URL via either horizon or the nova cli client.  The request seems to queue briefly and then reports a status of {{killed}}. Eg    {code}  glance image-create --name ""centos-7.1-vagrant"" --disk-format qcow2 --container-format bare --progress --copy-from http://sqre-kvm-images.s3.amazonaws.com/centos-7.1-x86_64 --is-public False --min-disk 8 --min-ram 1024  {code}",2,DM-4237,datamanagement,unable upload image nebula unable upload image neblua url horizon nova cli client request queue briefly report status kill eg code glance image create centos-7.1 vagrant --disk format qcow2 --container format bare --copy http://sqre-kvm-images.s3.amazonaws.com/centos-7.1-x86_64 --is public false --min disk --min ram 1024 code,"unable to upload images to nebula I seem to be unable to upload an image to neblua from a URL via either horizon or the nova cli client. The request seems to queue briefly and then reports a status of {{killed}}. Eg {code} glance image-create --name ""centos-7.1-vagrant"" --disk-format qcow2 --container-format bare --progress --copy-from http://sqre-kvm-images.s3.amazonaws.com/centos-7.1-x86_64 --is-public False --min-disk 8 --min-ram 1024 {code}"
Fix integer casting error in numpy version 1.10 in obs subaru Fix type casting in obs_subaru in lates numpy in obs_subaru,1,DM-4238,datamanagement,fix integer cast error numpy version 1.10 obs subaru fix type cast obs_subaru lates numpy obs_subaru,Fix integer casting error in numpy version 1.10 in obs subaru Fix type casting in obs_subaru in lates numpy in obs_subaru
Identify Qserv areas affected by secondary index Evaluate Qserv software for the Czars and workers to identify where an interface to the secondary index will be required for efficient operation.,5,DM-4239,datamanagement,identify qserv area affect secondary index evaluate qserv software czars worker identify interface secondary index require efficient operation,Identify Qserv areas affected by secondary index Evaluate Qserv software for the Czars and workers to identify where an interface to the secondary index will be required for efficient operation.
"Implement bulk updating of secondary index in Qserv Provide a service in Qserv to support creation or modification of secondary index objectID-chunk pairs in bulk, to support data loading.",4,DM-4241,datamanagement,implement bulk updating secondary index qserv provide service qserv support creation modification secondary index objectid chunk pair bulk support datum loading,"Implement bulk updating of secondary index in Qserv Provide a service in Qserv to support creation or modification of secondary index objectID-chunk pairs in bulk, to support data loading."
"Implement secondary index query in Qserv Implement query of secondary index in Qserv, using the technology selected in DM-2119.",4,DM-4242,datamanagement,implement secondary index query qserv implement query secondary index qserv technology select dm-2119,"Implement secondary index query in Qserv Implement query of secondary index in Qserv, using the technology selected in DM-2119."
Draft of Configuration Solicitation Draft of solicitation to be used for quote and configuration from multiple vendors for FY16 purchase. ,1,DM-4243,datamanagement,draft configuration solicitation draft solicitation quote configuration multiple vendor fy16 purchase,Draft of Configuration Solicitation Draft of solicitation to be used for quote and configuration from multiple vendors for FY16 purchase.
"Image Viewer memory leak When reloading the same 500MB RAFT image into an image viewer (see the script below), it was discovered that single node Firefy server with 3G memory runs out of memory after ~15 reloads    Test case: keep reloading the html file with the following Javascript, creating an image viewer with 500MB image:    function onFireflyLoaded() {          var iv2= firefly.makeImageViewer(""plot"");          iv2.plot({               ""Title""      :""Example FITS Image'"",               ""ColorTable"" :""16"",               ""RangeValues"":firefly.serializeRangeValues(""Sigma"",-2,8,""Linear""),               ""URL""        :""http://localhost/demo/E000_RAFT_R01.fits""});  }    Follow up:    The bug was traced to java.awt.image.BufferedImage objects not being evicted from VIS_SHARED_MEM cache.    Further search showed that java.awt.image.BufferedImage (along with java.io.BufferedInputStream) is in src/firefly/java/edu/caltech/ipac/firefly/server/cache/resources/ignore_sizeof.txt, which lists the classes that have to be ignored when calculating the size of cache.    Testing on single node server (VIS_SHARED_MEM cache is not replicated), using [host:port]/fftools/admin/status page:    BEFORE (java.awt.image.BufferedImage was commented out in ignore_sizeof.txt)    After 14 reloads:  Memory    - Used                      :      3.7G    - Max                       :     3.55G    - Max Free                  :    488.0M    - Free Active               :    488.0M    - Total Active              :     3.55G     Caches:   	VIS_SHARED_MEM @327294449  	Statistics     : [  Size:15  Expired:0  Evicted:0  Hits:246  Hit-Ratio:NaN  Heap-Size:1120MB  ]  OUT OF MEMORY on next reload    AFTER THE CHANGE (Commented java.awt.image.BufferedImage in ignore_sizeof.txt)    After 36 reloads:  Memory    - Used                      :   1672.9M    - Max                       :     3.55G    - Max Free                  :   1968.0M    - Free Active               :   1468.0M    - Total Active              :      3.6G    Caches:   	VIS_SHARED_MEM @201164543  	Statistics     : [  Size:3  Expired:0  Evicted:34  Hits:659  Hit-Ratio:NaN  Heap-Size:1398MB  ]    ",2,DM-4245,datamanagement,"image viewer memory leak reload 500 mb raft image image viewer script discover single node firefy server memory run memory ~15 reload test case reload html file following javascript create image viewer 500 mb image function onfireflyloade var iv2= iv2.plot title example fit image colortable 16 rangevalues"":firefly.serializerangevalues(""sigma"",-2,8,""linear url http://localhost demo e000_raft_r01.fits follow bug trace java.awt.image bufferedimage object evict vis_shared_mem cache search show java.awt.image bufferedimage java.io bufferedinputstream src firefly java edu caltech ipac firefly server cache resource ignore_sizeof.txt list class ignore calculate size cache test single node server vis_shared_mem cache replicate host port]/fftool admin status page java.awt.image bufferedimage comment ignore_sizeof.txt 14 reload memory 3.7 max 3.55 max free 488.0 free active 488.0 total active 3.55 caches vis_shared_mem @327294449 statistics size:15 expired:0 evicted:0 hits:246 hit ratio nan heap size:1120 mb memory reload change commented java.awt.image bufferedimage ignore_sizeof.txt 36 reload memory 1672.9 max 3.55 max free 1968.0 free active 1468.0 total active 3.6 caches vis_shared_mem statistic size:3 expired:0 evicted:34 hits:659 hit ratio nan heap size:1398 mb","Image Viewer memory leak When reloading the same 500MB RAFT image into an image viewer (see the script below), it was discovered that single node Firefy server with 3G memory runs out of memory after ~15 reloads Test case: keep reloading the html file with the following Javascript, creating an image viewer with 500MB image: function onFireflyLoaded() { var iv2= firefly.makeImageViewer(""plot""); iv2.plot({ ""Title"" :""Example FITS Image'"", ""ColorTable"" :""16"", ""RangeValues"":firefly.serializeRangeValues(""Sigma"",-2,8,""Linear""), ""URL"" :""http://localhost/demo/E000_RAFT_R01.fits""}); } Follow up: The bug was traced to java.awt.image.BufferedImage objects not being evicted from VIS_SHARED_MEM cache. Further search showed that java.awt.image.BufferedImage (along with java.io.BufferedInputStream) is in src/firefly/java/edu/caltech/ipac/firefly/server/cache/resources/ignore_sizeof.txt, which lists the classes that have to be ignored when calculating the size of cache. Testing on single node server (VIS_SHARED_MEM cache is not replicated), using [host:port]/fftools/admin/status page: BEFORE (java.awt.image.BufferedImage was commented out in ignore_sizeof.txt) After 14 reloads: Memory - Used : 3.7G - Max : 3.55G - Max Free : 488.0M - Free Active : 488.0M - Total Active : 3.55G Caches: VIS_SHARED_MEM @327294449 Statistics : [ Size:15 Expired:0 Evicted:0 Hits:246 Hit-Ratio:NaN Heap-Size:1120MB ] OUT OF MEMORY on next reload AFTER THE CHANGE (Commented java.awt.image.BufferedImage in ignore_sizeof.txt) After 36 reloads: Memory - Used : 1672.9M - Max : 3.55G - Max Free : 1968.0M - Free Active : 1468.0M - Total Active : 3.6G Caches: VIS_SHARED_MEM @201164543 Statistics : [ Size:3 Expired:0 Evicted:34 Hits:659 Hit-Ratio:NaN Heap-Size:1398MB ]"
Local LSST Sec Meeting Local cyber security meeting at NCSA with DM group.,2,DM-4246,datamanagement,local lsst sec meeting local cyber security meeting ncsa dm group,Local LSST Sec Meeting Local cyber security meeting at NCSA with DM group.
"Image viewer: choosing pixel interpolation algorithm for scaled images Pixel values are defined at integer coordinate locations. This means that when an image is rendered in a scaled, rotates, or otherwise transformed coord. system, an interpolation algorithm should be used to provide a pixel value at any continuous coordinate.    Currently, Firefly is using     RenderingHints.KEY_INTERPOLATION = RenderingHints.VALUE_INTERPOLATION_NEAREST_NEIGHBOR,    which means that when an image is rendered in a transformed coord. system, the pixel value of the nearest neighboring integer coordinate sample in the image is used.     ""As the image is scaled up, it will look correspondingly blocky. As the image is scaled down, the colors for source pixels will be either used unmodified, or skipped entirely in the output representation.""    Jon Thaler's team would like to be able to choose a different interpolation algorithm, depending on the situation.      As an example see various resize algorithms in [thttp://stackoverflow.com/questions/4756268/how-to-resize-the-buffered-image-n-graphics-2d-in-java].",4,DM-4247,datamanagement,image viewer choose pixel interpolation algorithm scale image pixel value define integer coordinate location mean image render scale rotate transform coord system interpolation algorithm provide pixel value continuous coordinate currently firefly renderinghints key_interpolation renderinghints value_interpolation_nearest_neighbor mean image render transform coord system pixel value near neighboring integer coordinate sample image image scale look correspondingly blocky image scale color source pixel unmodified skip entirely output representation jon thaler team like able choose different interpolation algorithm depend situation example resize algorithm thttp://stackoverflow.com/questions/4756268/how-to-resize-the-buffered-image-n-graphics-2d-in-java,"Image viewer: choosing pixel interpolation algorithm for scaled images Pixel values are defined at integer coordinate locations. This means that when an image is rendered in a scaled, rotates, or otherwise transformed coord. system, an interpolation algorithm should be used to provide a pixel value at any continuous coordinate. Currently, Firefly is using RenderingHints.KEY_INTERPOLATION = RenderingHints.VALUE_INTERPOLATION_NEAREST_NEIGHBOR, which means that when an image is rendered in a transformed coord. system, the pixel value of the nearest neighboring integer coordinate sample in the image is used. ""As the image is scaled up, it will look correspondingly blocky. As the image is scaled down, the colors for source pixels will be either used unmodified, or skipped entirely in the output representation."" Jon Thaler's team would like to be able to choose a different interpolation algorithm, depending on the situation. As an example see various resize algorithms in [thttp://stackoverflow.com/questions/4756268/how-to-resize-the-buffered-image-n-graphics-2d-in-java]."
LSST PO Security Meeting Bi-weekly meetings with PO to discuss cyber security issues in LSST.,1,DM-4248,datamanagement,lsst po security meeting bi weekly meeting po discuss cyber security issue lsst,LSST PO Security Meeting Bi-weekly meetings with PO to discuss cyber security issues in LSST.
Please include obs_subaru in CI {{obs_subaru}} should be included in the CI system.,1,DM-4251,datamanagement,include obs_subaru ci obs_subaru include ci system,Please include obs_subaru in CI {{obs_subaru}} should be included in the CI system.
Create GitLFS Technical Note Create a SQuaRE Technical Note describing the architecture of the GitLFS service implementation.,2,DM-4252,datamanagement,create gitlfs technical note create square technical note describe architecture gitlfs service implementation,Create GitLFS Technical Note Create a SQuaRE Technical Note describing the architecture of the GitLFS service implementation.
"Implement simple reference index files We would a very simple way to make small reference catalogs for astrometry and photometry. The use case is testing and small projects, where the overhead of making a full up a.net index file (or whatever we replace that with) is excessive.",5,DM-4254,datamanagement,implement simple reference index file simple way small reference catalog astrometry photometry use case testing small project overhead make a.net index file replace excessive,"Implement simple reference index files We would a very simple way to make small reference catalogs for astrometry and photometry. The use case is testing and small projects, where the overhead of making a full up a.net index file (or whatever we replace that with) is excessive."
Sphinx support of sqr-001 technical note Support the distribution of a technical note SQR-001    - Remove oxford comma in author list (documenteer)  - Solve issue where title is repeated if the title is included in the restructured text document  - Solve issue where name of the HTML document is README.html,1,DM-4256,datamanagement,sphinx support sqr-001 technical note support distribution technical note sqr-001 remove oxford comma author list documenteer solve issue title repeat title include restructure text document solve issue html document readme.html,Sphinx support of sqr-001 technical note Support the distribution of a technical note SQR-001 - Remove oxford comma in author list (documenteer) - Solve issue where title is repeated if the title is included in the restructured text document - Solve issue where name of the HTML document is README.html
Investigate current dipole measurement examples and tests There are tests in ip_diffim/tests/dipole.py for the dipole fitting.  There is also an example in the examples directory of ip_diffim.  This story will investigate these tests and examples to see to what precision the tests go as well as how complete the tests are.    An outcome of this will be an understanding of how precise the tests need to be to show that dipole measurement is behaving as we expect.,4,DM-4258,datamanagement,investigate current dipole measurement example test test ip_diffim test dipole.py dipole fit example example directory ip_diffim story investigate test example precision test complete test outcome understanding precise test need dipole measurement behave expect,Investigate current dipole measurement examples and tests There are tests in ip_diffim/tests/dipole.py for the dipole fitting. There is also an example in the examples directory of ip_diffim. This story will investigate these tests and examples to see to what precision the tests go as well as how complete the tests are. An outcome of this will be an understanding of how precise the tests need to be to show that dipole measurement is behaving as we expect.
Create a set of tests (or update the current ones) to facilitate refactoring of dipole measurement This will create a test (not necessarily a unit test) that will simulate dipoles and measure them so that the measurement can be compared to truth values.  This may be simply refactoring the current tests.    This task should also include generating more generalizable utilities needed to create the dipoles and incorporating these and other test data into the stack so that they can be used in other studies.,5,DM-4259,datamanagement,create set test update current one facilitate refactoring dipole measurement create test necessarily unit test simulate dipole measure measurement compare truth value simply refactore current test task include generate generalizable utility need create dipole incorporate test datum stack study,Create a set of tests (or update the current ones) to facilitate refactoring of dipole measurement This will create a test (not necessarily a unit test) that will simulate dipoles and measure them so that the measurement can be compared to truth values. This may be simply refactoring the current tests. This task should also include generating more generalizable utilities needed to create the dipoles and incorporating these and other test data into the stack so that they can be used in other studies.
"Integrate qserv docs into the new doc system See Frossie 11/04/2015 email to qserv-l list:    {{5- Docs. So you guys have a sphinx site. Fab, that’ll make it super easy to drop it into the new doc system that hosts sphinx on readthedocs - you can see a small example here http://sqr-001.lsst.codes/en/master/ - the idea is to continuously deploy the docs so the release step should be very lightweight and doable via API calls. (My holy grail is to do a release with no local checkout involved).  }}",2,DM-4262,datamanagement,integrate qserv doc new doc system frossie 11/04/2015 email qserv list 5- docs guy sphinx site fab ll super easy drop new doc system host sphinx readthedoc small example http://sqr-001.lsst.codes/en/master/ idea continuously deploy doc release step lightweight doable api call holy grail release local checkout involve,"Integrate qserv docs into the new doc system See Frossie 11/04/2015 email to qserv-l list: {{5- Docs. So you guys have a sphinx site. Fab, that ll make it super easy to drop it into the new doc system that hosts sphinx on readthedocs - you can see a small example here http://sqr-001.lsst.codes/en/master/ - the idea is to continuously deploy the docs so the release step should be very lightweight and doable via API calls. (My holy grail is to do a release with no local checkout involved). }}"
SQuaRE design meeting 1 Hold an in-person design discussion with members of the SQuaRE team.  ,4,DM-4263,datamanagement,square design meeting hold person design discussion member square team,SQuaRE design meeting 1 Hold an in-person design discussion with members of the SQuaRE team.
SQuaRE supertask design meeting 2 Hold a teleconference design discussion with members of the SQuaRE team.  ,1,DM-4264,datamanagement,square supertask design meeting hold teleconference design discussion member square team,SQuaRE supertask design meeting 2 Hold a teleconference design discussion with members of the SQuaRE team.
Should only read fringe data after checking the filter The fringe subtraction is not necessarily performed if {{doFringe}} is True. It is only if the filter of the raw exposure is listed in config fringe.filters.      Fringe data should not be read unless the filter is indicated. There are likely no such filter data and it would cause runtime errors.      Seems related to changes from RFC-26 and DM-1299. ,8,DM-4266,datamanagement,read fringe datum check filter fringe subtraction necessarily perform dofringe true filter raw exposure list config fringe.filter fringe datum read filter indicate likely filter datum cause runtime error relate change rfc-26 dm-1299,Should only read fringe data after checking the filter The fringe subtraction is not necessarily performed if {{doFringe}} is True. It is only if the filter of the raw exposure is listed in config fringe.filters. Fringe data should not be read unless the filter is indicated. There are likely no such filter data and it would cause runtime errors. Seems related to changes from RFC-26 and DM-1299.
"Adapt an existing task to be usable as a SuperTask Either by wrapping, or by converting, make an existing task usable as a SuperTask subclass so that it can be run under an Activator.",8,DM-4267,datamanagement,adapt exist task usable supertask wrapping converting exist task usable supertask subclass run activator,"Adapt an existing task to be usable as a SuperTask Either by wrapping, or by converting, make an existing task usable as a SuperTask subclass so that it can be run under an Activator."
"Week end 10/03/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 3, 2015.",1,DM-4269,datamanagement,week end 10/03/15 support lsst dev cluster openstack account week end october 2015,"Week end 10/03/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending October 3, 2015."
"Week end 10/10/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 10, 2015.",4,DM-4270,datamanagement,week end 10/10/15 support lsst dev cluster openstack account week end october 10 2015,"Week end 10/10/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending October 10, 2015."
"Week end 10/17/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 17, 2015.",3,DM-4271,datamanagement,week end 10/17/15 support lsst dev cluster openstack account week end october 17 2015,"Week end 10/17/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending October 17, 2015."
"Week end 10/24/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 24, 2015.",7,DM-4272,datamanagement,week end 10/24/15 support lsst dev cluster openstack account week end october 24 2015,"Week end 10/24/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending October 24, 2015."
"Week end 10/30/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending October 30, 2015.",3,DM-4273,datamanagement,week end 10/30/15 support lsst dev cluster openstack account week end october 30 2015,"Week end 10/30/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending October 30, 2015."
"Planning for new equipment setup (week end 10/03/15) * Planning for hardware upgrades in racks  * Evaluate the setup of the new storage server installs lsst-store101, lsst-store141,lsst-store143,lsst-store144  * Set up and tested ESXi server on new Mac Pro. Worked on getting Mac OS X installed inside of ESXi on the new Mac Pro hardware. Found solution, need to find a better one if we are expecting to bring up and tear down instances on demand.",5,DM-4274,datamanagement,"plan new equipment setup week end 10/03/15 plan hardware upgrade rack evaluate setup new storage server install lsst store101 lsst store141,lsst store143,lsst store144 set test esxi server new mac pro work get mac os instal inside esxi new mac pro hardware find solution need find well expect bring tear instance demand","Planning for new equipment setup (week end 10/03/15) * Planning for hardware upgrades in racks * Evaluate the setup of the new storage server installs lsst-store101, lsst-store141,lsst-store143,lsst-store144 * Set up and tested ESXi server on new Mac Pro. Worked on getting Mac OS X installed inside of ESXi on the new Mac Pro hardware. Found solution, need to find a better one if we are expecting to bring up and tear down instances on demand."
"New equipment setup (week end 10/10/15) * Planning on how to start setting up for new equipment    * Started to coordinate with Josh Hobblitt on what has been ordered and delivered   * Received 15 Dell R730 servers (plan to set up in temporary rack before Thursday’s maintenance)  * Installed spare rack at the east end of NCSA 3003-Row A. Added three 125V 30A drops to spare rack  * Resolved problems with Puppet on lsst-stor101  * Updated OS on lsst-stor101, lsst-stor142-144  * Installed ZFS on lsst-stor101, lsst-stor142-144  * Checking config of stor142-144 and stor101 for Thursday outage  * More ESXi testing with Mac Pro",6,DM-4275,datamanagement,new equipment setup week end 10/10/15 plan start set new equipment start coordinate josh hobblitt order deliver receive 15 dell r730 server plan set temporary rack thursday maintenance instal spare rack east end ncsa 3003 row a. add 125v 30a drop spare rack resolve problem puppet lsst stor101 update os lsst stor101 lsst stor142 144 instal zfs lsst stor101 lsst stor142 144 check config stor142 144 stor101 thursday outage esxi testing mac pro,"New equipment setup (week end 10/10/15) * Planning on how to start setting up for new equipment * Started to coordinate with Josh Hobblitt on what has been ordered and delivered * Received 15 Dell R730 servers (plan to set up in temporary rack before Thursday s maintenance) * Installed spare rack at the east end of NCSA 3003-Row A. Added three 125V 30A drops to spare rack * Resolved problems with Puppet on lsst-stor101 * Updated OS on lsst-stor101, lsst-stor142-144 * Installed ZFS on lsst-stor101, lsst-stor142-144 * Checking config of stor142-144 and stor101 for Thursday outage * More ESXi testing with Mac Pro"
"New equipment setup and configuration (week end 10/24/15)   * Unbox and mount six UPS in racks, mount two new power panel PDU's for 30 Amp service, connect power cabling to UPS and to each of the supported systems. TODO: Complete the setup on each of the servers.  * Unbox and mount in the rack two R730 servers, connect network and power  * Itemized list of new Dell servers received  * Debugging networking issues on new lsst-esxi1 server - confirmed it's not an issue with switch ports or cables  * Setup vSphere virtual networking and moved 4 test VMs to new setup  * Setup vSphere Data Protection (Vmware backups via snapshots)",6,DM-4277,datamanagement,new equipment setup configuration week end 10/24/15 unbox mount ups rack mount new power panel pdu 30 amp service connect power cable ups support system todo complete setup server unbox mount rack r730 server connect network power itemized list new dell server receive debug networking issue new lsst esxi1 server confirm issue switch port cable setup vsphere virtual networking move test vms new setup setup vsphere data protection vmware backup snapshot,"New equipment setup and configuration (week end 10/24/15) * Unbox and mount six UPS in racks, mount two new power panel PDU's for 30 Amp service, connect power cabling to UPS and to each of the supported systems. TODO: Complete the setup on each of the servers. * Unbox and mount in the rack two R730 servers, connect network and power * Itemized list of new Dell servers received * Debugging networking issues on new lsst-esxi1 server - confirmed it's not an issue with switch ports or cables * Setup vSphere virtual networking and moved 4 test VMs to new setup * Setup vSphere Data Protection (Vmware backups via snapshots)"
New equipment setup and configuration (week end 10/30/15) * Moved spare rack to Row C  * Documented setup of new LSST vSphere setup (https://wiki.ncsa.illinois.edu/display/LSST/LSST+vSphere)  * Debugging networking issues on new lsst-esxi1 server - testing with new/alternate hardware  * Installed 6 new drives for historical log storage on lsst10 MySQL server,5,DM-4278,datamanagement,new equipment setup configuration week end 10/30/15 move spare rack row document setup new lsst vsphere setup https://wiki.ncsa.illinois.edu/display/lsst/lsst+vsphere debug networking issue new lsst esxi1 server testing new alternate hardware instal new drive historical log storage lsst10 mysql server,New equipment setup and configuration (week end 10/30/15) * Moved spare rack to Row C * Documented setup of new LSST vSphere setup (https://wiki.ncsa.illinois.edu/display/LSST/LSST+vSphere) * Debugging networking issues on new lsst-esxi1 server - testing with new/alternate hardware * Installed 6 new drives for historical log storage on lsst10 MySQL server
bi-weekly IaM meeting Meeting Oct 22nd  Notes on LSST confluence,1,DM-4279,datamanagement,bi weekly iam meeting meeting oct 22nd note lsst confluence,bi-weekly IaM meeting Meeting Oct 22nd Notes on LSST confluence
Bootcamp meeting IAM group attended DM bootcamp,4,DM-4280,datamanagement,bootcamp meeting iam group attend dm bootcamp,Bootcamp meeting IAM group attended DM bootcamp
Software Stack Introduction Installation of LSST stack for initial steps towards further understanding of the software components of the DM architecture. ,1,DM-4287,datamanagement,software stack introduction installation lsst stack initial step understanding software component dm architecture,Software Stack Introduction Installation of LSST stack for initial steps towards further understanding of the software components of the DM architecture.
L1 Function and Design Mtgs 2 days of design discussions and refining functional diagrams of the Alert Productions and Image Ingest system.,4,DM-4288,datamanagement,l1 function design mtgs day design discussion refine functional diagram alert productions image ingest system,L1 Function and Design Mtgs 2 days of design discussions and refining functional diagrams of the Alert Productions and Image Ingest system.
Vendor Discussions re: specification documents Specification document sent. Discussions with vendors covering document and schedule.,1,DM-4289,datamanagement,vendor discussions specification document specification document send discussion vendor cover document schedule,Vendor Discussions re: specification documents Specification document sent. Discussions with vendors covering document and schedule.
"SC15 Scheduling and Prep Vendor appt scheduling and conference workshop scheduling, logisitics, etc for SuperComputing 15 in Austin",1,DM-4290,datamanagement,sc15 scheduling prep vendor appt scheduling conference workshop scheduling logisitic etc supercomputing 15 austin,"SC15 Scheduling and Prep Vendor appt scheduling and conference workshop scheduling, logisitics, etc for SuperComputing 15 in Austin"
"Run and document multinode integration tests on Openstack+Docker Boot openstack machines using vagrant, then deploys docker images and finally launch multinodes tests.    FYI, lack of DNS on OpenStack Cloud cause problems, but a vagrant plugin seems to solve this.",8,DM-4295,datamanagement,run document multinode integration test openstack+docker boot openstack machine vagrant deploy docker image finally launch multinode test fyi lack dns openstack cloud cause problem vagrant plugin solve,"Run and document multinode integration tests on Openstack+Docker Boot openstack machines using vagrant, then deploys docker images and finally launch multinodes tests. FYI, lack of DNS on OpenStack Cloud cause problems, but a vagrant plugin seems to solve this."
Write up some introductory guides for Nebula usage We write up some introductory guides for Nebula usage to enable new users to get started.  These are located on Confluence under :    https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide,4,DM-4296,datamanagement,write introductory guide nebula usage write introductory guide nebula usage enable new user start locate confluence https://confluence.lsstcorp.org/display/ldmdg/ncsa+nebula+openstack+user+guide,Write up some introductory guides for Nebula usage We write up some introductory guides for Nebula usage to enable new users to get started. These are located on Confluence under : https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide
"want equiv of m1.xlarge flavor with smaller disk I'd like to be able to build images with vcpus & ram from the {{m1.xlarge}} flavor that can be run on a {{m1.medium}} with it's smaller disk image, this would require a new flavor with 16GiB ram/8vcpus but only 40GiB of disk.  Something along the lines of:    {{openstack flavor create --ram 16384 --disk 40 --vcpus 8 ...}}    Is that possible?",2,DM-4298,datamanagement,want equiv m1.xlarge flavor small disk like able build image vcpus ram m1.xlarge flavor run m1.medium small disk image require new flavor 16gib ram/8vcpus 40gib disk line openstack flavor create --ram 16384 --disk 40 --vcpus possible,"want equiv of m1.xlarge flavor with smaller disk I'd like to be able to build images with vcpus & ram from the {{m1.xlarge}} flavor that can be run on a {{m1.medium}} with it's smaller disk image, this would require a new flavor with 16GiB ram/8vcpus but only 40GiB of disk. Something along the lines of: {{openstack flavor create --ram 16384 --disk 40 --vcpus 8 ...}} Is that possible?"
Vagrant for Nebula OpenStack Create and document a Vagrant configuration to use [~jhoblitt] lsstsw machine images on NCSA's Nebula OpenStack cloud.,4,DM-4299,datamanagement,vagrant nebula openstack create document vagrant configuration use ~jhoblitt lsstsw machine image ncsa nebula openstack cloud,Vagrant for Nebula OpenStack Create and document a Vagrant configuration to use [~jhoblitt] lsstsw machine images on NCSA's Nebula OpenStack cloud.
"Convert banner and menu to react/flux Add flux data model to capture menu and banner information.  Convert banner and menu UI from gwt to react.  As part of this task, bring in Fetch API to simplify client/server interactions.",8,DM-4301,datamanagement,convert banner menu react flux add flux datum model capture menu banner information convert banner menu ui gwt react task bring fetch api simplify client server interaction,"Convert banner and menu to react/flux Add flux data model to capture menu and banner information. Convert banner and menu UI from gwt to react. As part of this task, bring in Fetch API to simplify client/server interactions."
"update obs_lsstSim obs_lsstSim has seen some bitrot.  In particular, the ingest task and the addition of the getExposureId methods on processImageTask have not been propagated to obs_lsstSim.  This ticket will deal with those issues.",4,DM-4302,datamanagement,update obs_lsstsim obs_lsstsim see bitrot particular ingest task addition getexposureid method processimagetask propagate obs_lsstsim ticket deal issue,"update obs_lsstSim obs_lsstSim has seen some bitrot. In particular, the ingest task and the addition of the getExposureId methods on processImageTask have not been propagated to obs_lsstSim. This ticket will deal with those issues."
re-deploy lsstsw on Jenkins Pandas was added to the bin/deploy script in lsstsw to support  sims development.  This has already been merged to master in 4b1d1a0fa.  The ticket is to ask that lsstsw be redeployed so the sims team can build branches that use pandas.,3,DM-4303,datamanagement,deploy lsstsw jenkins pandas add bin deploy script lsstsw support sim development merge master 4b1d1a0fa ticket ask lsstsw redeploy sim team build branch use panda,re-deploy lsstsw on Jenkins Pandas was added to the bin/deploy script in lsstsw to support sims development. This has already been merged to master in 4b1d1a0fa. The ticket is to ask that lsstsw be redeployed so the sims team can build branches that use pandas.
Add unit testing into gradle build for Firefly's server-side code Add a test task to Firefly's common build script.  This can be used by any sub-project to run unit test.  Added unit testing to Jenkins continuous integration job to ensure new code does not break unit testing.,1,DM-4304,datamanagement,add unit testing gradle build firefly server code add test task firefly common build script sub project run unit test add unit test jenkins continuous integration job ensure new code break unit testing,Add unit testing into gradle build for Firefly's server-side code Add a test task to Firefly's common build script. This can be used by any sub-project to run unit test. Added unit testing to Jenkins continuous integration job to ensure new code does not break unit testing.
Please add HSC tests to CI In DM-3663 we (= [~price]) provided an integration test for processing HSC data through the stack with the intention that it should be integrated with the CI system.    Having this test available and regularly run would be enormously helpful with the HSC port -- we've already run into problems which it could have helped us avoid (DM-4305).,1,DM-4307,datamanagement,add hsc test ci dm-3663 ~price provide integration test process hsc datum stack intention integrate ci system have test available regularly run enormously helpful hsc port run problem help avoid dm-4305,Please add HSC tests to CI In DM-3663 we (= [~price]) provided an integration test for processing HSC data through the stack with the intention that it should be integrated with the CI system. Having this test available and regularly run would be enormously helpful with the HSC port -- we've already run into problems which it could have helped us avoid (DM-4305).
"Try out nebula for stack developing and data processing Follow instructions on:    https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide  https://community.lsst.org/t/creating-a-nebula-instance-a-recipe/353    and create a nebula instance with the stack, update and install more packages in the stack, test by constructing a data repository and processing ISR with DECam raw images.    ",2,DM-4309,datamanagement,try nebula stack develop datum process follow instruction https://confluence.lsstcorp.org/display/ldmdg/ncsa+nebula+openstack+user+guide https://community.lsst.org/t/creating-a-nebula-instance-a-recipe/353 create nebula instance stack update install package stack test construct data repository process isr decam raw image,"Try out nebula for stack developing and data processing Follow instructions on: https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide https://community.lsst.org/t/creating-a-nebula-instance-a-recipe/353 and create a nebula instance with the stack, update and install more packages in the stack, test by constructing a data repository and processing ISR with DECam raw images."
"Missing Doxygen documentation As of [2015-11-10 02:53.26|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_02.53.27/] there were 19 ""mainpages in subpackages"" available through Doxygen.    In the next build, [2015-11-10 21:16.19|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_21.16.19/], most of them have vanished and we only provide links for {{ndarray}} and {{lsst::skymap}}.    As of filing this issue, they were still missing from the [latest build|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/].    Please bring them back!",1,DM-4310,datamanagement,missing doxygen documentation 2015 11 10 02:53.26|https://lsst web.ncsa.illinois.edu doxygen xlink_master_2015_11_10_02.53.27/ 19 mainpage subpackage available doxygen build 2015 11 10 21:16.19|https://lsst web.ncsa.illinois.edu doxygen xlink_master_2015_11_10_21.16.19/ vanish provide link ndarray lsst::skymap file issue miss late build|https://lsst web.ncsa.illinois.edu doxygen x_masterdoxydoc/ bring,"Missing Doxygen documentation As of [2015-11-10 02:53.26|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_02.53.27/] there were 19 ""mainpages in subpackages"" available through Doxygen. In the next build, [2015-11-10 21:16.19|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_21.16.19/], most of them have vanished and we only provide links for {{ndarray}} and {{lsst::skymap}}. As of filing this issue, they were still missing from the [latest build|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/]. Please bring them back!"
Oct. on-going support to Camera team in UIUC Attend UIUC weekly meeting and give support as needed. ,2,DM-4311,datamanagement,oct. go support camera team uiuc attend uiuc weekly meeting support need,Oct. on-going support to Camera team in UIUC Attend UIUC weekly meeting and give support as needed.
Nov. on-going support to Camera team in UIUC Attend UIUC weekly meeting and give support as needed. ,2,DM-4312,datamanagement,nov. go support camera team uiuc attend uiuc weekly meeting support need,Nov. on-going support to Camera team in UIUC Attend UIUC weekly meeting and give support as needed.
Configure sshd with pam_krb5 and document Configure sshd in our test IAM VM to use pam_krb5 so the user gets a Kerberos ticket when logging in as discussed in our design doc. Document the configuration steps.,4,DM-4313,datamanagement,configure sshd pam_krb5 document configure sshd test iam vm use pam_krb5 user get kerberos ticket log discuss design doc document configuration step,Configure sshd with pam_krb5 and document Configure sshd in our test IAM VM to use pam_krb5 so the user gets a Kerberos ticket when logging in as discussed in our design doc. Document the configuration steps.
Set up test IAM MariaDB instance with Kerberos Install MariaDB in our test IAM VM at NCSA and configure it for Kerberos password authentication. I understand that Kerberos ticket authentication is not yet supported by MariaDB. Document the process and include references to the ongoing work to add Kerberos ticket support.    Create example tables that demonstrate access control for different logged in users.,2,DM-4315,datamanagement,set test iam mariadb instance kerberos install mariadb test iam vm ncsa configure kerberos password authentication understand kerberos ticket authentication support mariadb document process include reference ongoing work add kerberos ticket support create example table demonstrate access control different log user,Set up test IAM MariaDB instance with Kerberos Install MariaDB in our test IAM VM at NCSA and configure it for Kerberos password authentication. I understand that Kerberos ticket authentication is not yet supported by MariaDB. Document the process and include references to the ongoing work to add Kerberos ticket support. Create example tables that demonstrate access control for different logged in users.
Configure sssd with NCSA LDAP for accounts in test IAM VM Configure [sssd|https://fedorahosted.org/sssd/] with NCSA LDAP for accounts in test IAM VM using instructions from Doug Fein.,5,DM-4316,datamanagement,configure sssd ncsa ldap account test iam vm configure sssd|https://fedorahosted.org sssd/ ncsa ldap account test iam vm instruction doug fein,Configure sssd with NCSA LDAP for accounts in test IAM VM Configure [sssd|https://fedorahosted.org/sssd/] with NCSA LDAP for accounts in test IAM VM using instructions from Doug Fein.
Configure httpd with SSL and mod_krb5 in IAM test VM Get SSL certificate and configure httpd for mod_auth_krb5 authentication in IAM test VM. Document the setup.,4,DM-4317,datamanagement,configure httpd ssl mod_krb5 iam test vm ssl certificate configure httpd mod_auth_krb5 authentication iam test vm document setup,Configure httpd with SSL and mod_krb5 in IAM test VM Get SSL certificate and configure httpd for mod_auth_krb5 authentication in IAM test VM. Document the setup.
"Replace fitsthumb in obs_subaru (port HSC-1196) {{fitsthumb}} is now obsolete; all the functionality we need is available in {{afw}}. Further, we want to drop it as a dependency to make the job of integrating {{obs_subaru}} with CI easier.",1,DM-4323,datamanagement,replace fitsthumb obs_subaru port hsc-1196 fitsthumb obsolete functionality need available afw want drop dependency job integrate obs_subaru ci easy,"Replace fitsthumb in obs_subaru (port HSC-1196) {{fitsthumb}} is now obsolete; all the functionality we need is available in {{afw}}. Further, we want to drop it as a dependency to make the job of integrating {{obs_subaru}} with CI easier."
"Bootstrap for new Sphinx/reST/RTD technical reports Create a template repository for LSST (DM/SQuaRE) technical reports that are written in reStructuredText, built with Sphinx and published with RTD",2,DM-4324,datamanagement,bootstrap new sphinx rest rtd technical report create template repository lsst dm square technical report write restructuredtext build sphinx publish rtd,"Bootstrap for new Sphinx/reST/RTD technical reports Create a template repository for LSST (DM/SQuaRE) technical reports that are written in reStructuredText, built with Sphinx and published with RTD"
"Coadd_utils tests should run and skip if afwdata is missing Currently, the {{coadd_utils}} tests are completely skipped at the scons layer if afwdata can not be located. This is bad for two reasons:  1. Are there any tests that can be run even if afwdata is missing?.  2. When we switch to a proper test harness (e.g. DM-3901) an important metric is the number of tests executed compared with the number of tests skipped.  Each test file (or even each test) should determine itself whether it should be skipped based on afwdata availability. This should not be a global switch.",1,DM-4329,datamanagement,coadd_util test run skip afwdata miss currently coadd_util test completely skip scon layer afwdata locate bad reason test run afwdata miss switch proper test harness e.g. dm-3901 important metric number test execute compare number test skip test file test determine skip base afwdata availability global switch,"Coadd_utils tests should run and skip if afwdata is missing Currently, the {{coadd_utils}} tests are completely skipped at the scons layer if afwdata can not be located. This is bad for two reasons: 1. Are there any tests that can be run even if afwdata is missing?. 2. When we switch to a proper test harness (e.g. DM-3901) an important metric is the number of tests executed compared with the number of tests skipped. Each test file (or even each test) should determine itself whether it should be skipped based on afwdata availability. This should not be a global switch."
"ActiveMQ Broker upgrade Version 5.8.0 of the ActiveMQ broker, which is not part of the regular distribution, is quite out of date.  [~tjenness] is working upgrading the ActiveMQCPP library in DM-4330 to the current version, and this would be a good time to upgrade the broker to the latest release so we try and stay in sync.",1,DM-4331,datamanagement,activemq broker upgrade version 5.8.0 activemq broker regular distribution date ~tjenness work upgrade activemqcpp library dm-4330 current version good time upgrade broker late release try stay sync,"ActiveMQ Broker upgrade Version 5.8.0 of the ActiveMQ broker, which is not part of the regular distribution, is quite out of date. [~tjenness] is working upgrading the ActiveMQCPP library in DM-4330 to the current version, and this would be a good time to upgrade the broker to the latest release so we try and stay in sync."
The astrometry task should print a warning or throw an exception when there is no reference star in the field When there is no reference star in the field the exception raised by anetAstrometry is identical to the one issued when the fit has not converged.    astrometry (matchOptimisticB) is throwing an exception with the message lsst::pex::exceptions::InvalidParameterError: 'posRefBegInd too big'.  I suggest a test to detect that there is not enough stars to fit the astrometry and to throw an exception accordingly.,1,DM-4335,datamanagement,astrometry task print warning throw exception reference star field reference star field exception raise anetastrometry identical issue fit converge astrometry matchoptimisticb throw exception message lsst::pex::exceptions::invalidparametererror posrefbegind big suggest test detect star fit astrometry throw exception accordingly,The astrometry task should print a warning or throw an exception when there is no reference star in the field When there is no reference star in the field the exception raised by anetAstrometry is identical to the one issued when the fit has not converged. astrometry (matchOptimisticB) is throwing an exception with the message lsst::pex::exceptions::InvalidParameterError: 'posRefBegInd too big'. I suggest a test to detect that there is not enough stars to fit the astrometry and to throw an exception accordingly.
Add args to s3s3bucket CLI Add {{source-bucket}} and {{dest-bucket}} arguments to {{s3s3bucket}} the command line script. This is to allow for one off duplication of buckets.    Increment version to 0.1.10.,2,DM-4343,datamanagement,add arg s3s3bucket cli add source bucket bucket argument s3s3bucket command line script allow duplication bucket increment version 0.1.10,Add args to s3s3bucket CLI Add {{source-bucket}} and {{dest-bucket}} arguments to {{s3s3bucket}} the command line script. This is to allow for one off duplication of buckets. Increment version to 0.1.10.
"tested upgraded activemqcpp package The activemqcpp package was upgraded as part of DM-4330, and I tested it to be sure the upgrade was backwards compatible with the code that exercises it in ctrl_events.   It is.",1,DM-4346,datamanagement,test upgrade activemqcpp package activemqcpp package upgrade dm-4330 test sure upgrade backwards compatible code exercise ctrl_event,"tested upgraded activemqcpp package The activemqcpp package was upgraded as part of DM-4330, and I tested it to be sure the upgrade was backwards compatible with the code that exercises it in ctrl_events. It is."
dax_imgserv 2015_10.0 build error {{2015_10.0}} has a build error under a current {{lsstsw/bin/deploy}} environment.  Current speculation is that this is related to the conda version of numpy being upgraded to {{1.10.1}}.,1,DM-4347,datamanagement,dax_imgserv 2015_10.0 build error 2015_10.0 build error current lsstsw bin deploy environment current speculation relate conda version numpy upgrade 1.10.1,dax_imgserv 2015_10.0 build error {{2015_10.0}} has a build error under a current {{lsstsw/bin/deploy}} environment. Current speculation is that this is related to the conda version of numpy being upgraded to {{1.10.1}}.
Fix publishing script async issue and add additional release notes. Async command execution causes unpredictable and unreliable results.  Switches to synchronous where possible.  Also add additional description to the release notes.,2,DM-4349,datamanagement,fix publish script async issue add additional release note async command execution cause unpredictable unreliable result switch synchronous possible add additional description release note,Fix publishing script async issue and add additional release notes. Async command execution causes unpredictable and unreliable results. Switches to synchronous where possible. Also add additional description to the release notes.
Write technote on the new technical note platform Write a technote about the platform that github.com/lsst-sqre/lsst-technote-bootstrap lets DM members publish in. Discuss current status and outline future plans.,6,DM-4351,datamanagement,write technote new technical note platform write technote platform github.com/lsst-sqre/lsst-technote-bootstrap let dm member publish discuss current status outline future plan,Write technote on the new technical note platform Write a technote about the platform that github.com/lsst-sqre/lsst-technote-bootstrap lets DM members publish in. Discuss current status and outline future plans.
"Design Mtg, Review and discussions of L1 processing Design/planning meeting for L1 system. Materials read and discourse discussions on EFD, MOPs, T&S docs and calibration production",3,DM-4352,datamanagement,design mtg review discussion l1 process design planning meeting l1 system material read discourse discussion efd mop t&s doc calibration production,"Design Mtg, Review and discussions of L1 processing Design/planning meeting for L1 system. Materials read and discourse discussions on EFD, MOPs, T&S docs and calibration production"
"Reviewing quotes, power requirements, rack layouts Review of multiple quotes from multiple vendors across full year of purchases. Derived power requirements and rack layouts for placement, networking, electrical work discussions to begin.",3,DM-4353,datamanagement,reviewing quote power requirement rack layout review multiple quote multiple vendor year purchase derive power requirement rack layout placement networking electrical work discussion begin,"Reviewing quotes, power requirements, rack layouts Review of multiple quotes from multiple vendors across full year of purchases. Derived power requirements and rack layouts for placement, networking, electrical work discussions to begin."
"SC15 Scheduling, Processor Futures Refresh Scheduling of several more meeting opportunities for next week. Also, time spent reviewing NDA materials on processor futures. ",1,DM-4354,datamanagement,sc15 scheduling processor futures refresh scheduling meeting opportunity week time spend review nda material processor future,"SC15 Scheduling, Processor Futures Refresh Scheduling of several more meeting opportunities for next week. Also, time spent reviewing NDA materials on processor futures."
"obs_subaru fails to compile after DM-3200 Due to atypical calls in {{obs_subaru}}'s {{hsc/SConscript}} to run scripts in the {{bin}} directory, {{obs_subaru}} fails to compile after the changes made in DM-3200.",1,DM-4360,datamanagement,obs_subaru fail compile dm-3200 atypical call obs_subaru hsc sconscript run script bin directory obs_subaru fail compile change dm-3200,"obs_subaru fails to compile after DM-3200 Due to atypical calls in {{obs_subaru}}'s {{hsc/SConscript}} to run scripts in the {{bin}} directory, {{obs_subaru}} fails to compile after the changes made in DM-3200."
"SuperTask phase 1 implementation This story represents the implementation of the first part of the SuperTask framework design,",8,DM-4362,datamanagement,supertask phase implementation story represent implementation supertask framework design,"SuperTask phase 1 implementation This story represents the implementation of the first part of the SuperTask framework design,"
Implement configuration for activator parsing Need to add configuration to the CmdLineActivator in new Workflow tasks,2,DM-4363,datamanagement,implement configuration activator parse need add configuration cmdlineactivator new workflow task,Implement configuration for activator parsing Need to add configuration to the CmdLineActivator in new Workflow tasks
First implementation demo First stage demo of Super Task and WorkFlowTask Framework,4,DM-4364,datamanagement,implementation demo stage demo super task workflowtask framework,First implementation demo First stage demo of Super Task and WorkFlowTask Framework
"Improve overscan correction for DECam raw data Currently, the default overscan correction from IsrTask is used for processing DECam raw data. Overscan subtraction is done one amplifier at a time.     However, a bias jump occurs due to the simultaneous readout of the smaller ancillary CCDs on DECam, some images show discontinuity in the y direction across one amplifier, as in the example screen shot.     This ticket is to improve overscan correction for DECam data so to mitigate this discontinuity in the ISR processing.    Arrangement of CCDs on DECam: http://www.ctio.noao.edu/noao/sites/default/files/DECam/DECamPixelOrientation.png      h3. More details:  There are 6 backplanes in the readout system, shown by the colors in DECamPixelOrientation.png. In raw data files, the CCD's backplane is noted in the header keyword ""FPA"".  Examination of some images suggests that science CCDs on orange and yellow backplanes show bias jump at 2098 pixels from the y readout. That is the y size of the focus CCDs.     h3. Actions:  For CCDs on the affected backplanes, divide the array into two pieces at the jump location, and do overscan correction on the upper and lower pieces separately.    ",5,DM-4366,datamanagement,improve overscan correction decam raw datum currently default overscan correction isrtask process decam raw datum overscan subtraction amplifier time bias jump occur simultaneous readout small ancillary ccd decam image discontinuity direction amplifier example screen shot ticket improve overscan correction decam datum mitigate discontinuity isr processing arrangement ccds decam http://www.ctio.noao.edu/noao/sites/default/files/decam/decampixelorientation.png h3 detail backplane readout system show color decampixelorientation.png raw data file ccd backplane note header keyword fpa examination image suggest science ccd orange yellow backplane bias jump 2098 pixel readout size focus ccds h3 action ccd affect backplane divide array piece jump location overscan correction upper low piece separately,"Improve overscan correction for DECam raw data Currently, the default overscan correction from IsrTask is used for processing DECam raw data. Overscan subtraction is done one amplifier at a time. However, a bias jump occurs due to the simultaneous readout of the smaller ancillary CCDs on DECam, some images show discontinuity in the y direction across one amplifier, as in the example screen shot. This ticket is to improve overscan correction for DECam data so to mitigate this discontinuity in the ISR processing. Arrangement of CCDs on DECam: http://www.ctio.noao.edu/noao/sites/default/files/DECam/DECamPixelOrientation.png h3. More details: There are 6 backplanes in the readout system, shown by the colors in DECamPixelOrientation.png. In raw data files, the CCD's backplane is noted in the header keyword ""FPA"". Examination of some images suggests that science CCDs on orange and yellow backplanes show bias jump at 2098 pixels from the y readout. That is the y size of the focus CCDs. h3. Actions: For CCDs on the affected backplanes, divide the array into two pieces at the jump location, and do overscan correction on the upper and lower pieces separately."
"Fix bug and add unit tests for PsfShapeletApprox  We discovered during this Sprint that this plugin was giving us faulty values for all the models except for SingleGaussian.  I will fix that bug on this issue.    Obviously, a better unit test would have caught this.  I am adding a DoubleGaussian unit test, plus a test that the default models provide different results.  Also a timing test for all the models, as we do not really have enough information about the performance of the shapelet approximation.  ",3,DM-4367,datamanagement,fix bug add unit test psfshapeletapprox discover sprint plugin give faulty value model singlegaussian fix bug issue obviously well unit test catch add doublegaussian unit test plus test default model provide different result timing test model information performance shapelet approximation,"Fix bug and add unit tests for PsfShapeletApprox We discovered during this Sprint that this plugin was giving us faulty values for all the models except for SingleGaussian. I will fix that bug on this issue. Obviously, a better unit test would have caught this. I am adding a DoubleGaussian unit test, plus a test that the default models provide different results. Also a timing test for all the models, as we do not really have enough information about the performance of the shapelet approximation."
Duration for various ShapeletPsfApprox Models This is just a report of the amount of time it takes to run ShapeletPsfApprox and CModel over 10000 galaxies from GalSim,2,DM-4368,datamanagement,duration shapeletpsfapprox model report time take run shapeletpsfapprox cmodel 10000 galaxy galsim,Duration for various ShapeletPsfApprox Models This is just a report of the amount of time it takes to run ShapeletPsfApprox and CModel over 10000 galaxies from GalSim
"Migrate lsst/ci_hsc repo to git-lfs.lsst.codes The github lfs backed repo https://github.com/lsst/ci_hsc needs to be migrated to git-lfs.lsst.codes.  A sanity check for any other ""live"" lfs repos under the lsst github org might also be a good idea.",4,DM-4369,datamanagement,migrate lsst ci_hsc repo git-lfs.lsst.code github lfs back repo https://github.com/lsst/ci_hsc need migrate git-lfs.lsst.codes sanity check live lfs repos lsst github org good idea,"Migrate lsst/ci_hsc repo to git-lfs.lsst.codes The github lfs backed repo https://github.com/lsst/ci_hsc needs to be migrated to git-lfs.lsst.codes. A sanity check for any other ""live"" lfs repos under the lsst github org might also be a good idea."
"Migrate testdata for DECam from disk to git-lfs There is a package full with test data for the obs_decam.  It is an eups package currently, but not a git repository.  I would like to migrate that into our hosted git-lfs so the obs_decam package can be built by Jenkins.  [~jmatt] I'm hoping you would be willing to handle this for me.  If not I can find somebody else.  Thanks!",1,DM-4370,datamanagement,migrate testdata decam disk git lfs package test datum obs_decam eup package currently git repository like migrate host git lfs obs_decam package build jenkins ~jmatt hope willing handle find somebody thank,"Migrate testdata for DECam from disk to git-lfs There is a package full with test data for the obs_decam. It is an eups package currently, but not a git repository. I would like to migrate that into our hosted git-lfs so the obs_decam package can be built by Jenkins. [~jmatt] I'm hoping you would be willing to handle this for me. If not I can find somebody else. Thanks!"
"HSC backport: Add tract conveniences This is a port of [HSC-715: Add tract conveniences|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-715].  Here is the original description:    {panel}  In regular HSC survey processing, we'll run with a ""rings"" skymap to cover the entire survey area. meas_mosaic does not currently efficiently or conveniently iterate over tracts. For example:  {code}  mosaic.py /tigress/HSC/HSC --rerun price/cosmos --id field=SSP_UDEEP_COSMOS filter=HSC-R  {code}  Note the lack of a tract in the --id specifier — we want to iterate over all tracts. This is not currently possible. Instead, if we do not know the tract of interest (which the user should not be required to know), we have to iterate over all the tracts (e.g., tract=0..12345), but the user should not be required to know the number of tracts, and this is slow (and possibly memory-hungry: currently consuming 11GB on tiger3 just for 12 exposures).  We need an efficient mechanism to iterate over all tracts by not specifying any tract on the command-line.  {panel}    As this functionality was added specifically for {{meas_mosaic}}, it was going to be ported as part of DM-2674.  Due to a recent desire to use this functionality, this ticket will be ported here.",1,DM-4373,datamanagement,hsc backport add tract convenience port hsc-715 add tract conveniences|https://hsc jira.astro.princeton.edu jira browse hsc-715 original description panel regular hsc survey processing run ring skymap cover entire survey area meas_mosaic currently efficiently conveniently iterate tract example code mosaic.py hsc hsc price cosmos --id field ssp_udeep_cosmo filter hsc code note lack tract --id specifi want iterate tract currently possible instead know tract interest user require know iterate tract e.g. tract=0 .. 12345 user require know number tract slow possibly memory hungry currently consume 11 gb tiger3 12 exposure need efficient mechanism iterate tract specify tract command line panel functionality add specifically meas_mosaic go port dm-2674 recent desire use functionality ticket port,"HSC backport: Add tract conveniences This is a port of [HSC-715: Add tract conveniences|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-715]. Here is the original description: {panel} In regular HSC survey processing, we'll run with a ""rings"" skymap to cover the entire survey area. meas_mosaic does not currently efficiently or conveniently iterate over tracts. For example: {code} mosaic.py /tigress/HSC/HSC --rerun price/cosmos --id field=SSP_UDEEP_COSMOS filter=HSC-R {code} Note the lack of a tract in the --id specifier we want to iterate over all tracts. This is not currently possible. Instead, if we do not know the tract of interest (which the user should not be required to know), we have to iterate over all the tracts (e.g., tract=0..12345), but the user should not be required to know the number of tracts, and this is slow (and possibly memory-hungry: currently consuming 11GB on tiger3 just for 12 exposures). We need an efficient mechanism to iterate over all tracts by not specifying any tract on the command-line. {panel} As this functionality was added specifically for {{meas_mosaic}}, it was going to be ported as part of DM-2674. Due to a recent desire to use this functionality, this ticket will be ported here."
"A slimmer testdata_decam Before this ticket, the files in {{testdata_decam}} are as they are downloaded from the archive.  Some are MEF, and the total is a bit big (1.2G).    I made a trimmed down version of {{testdata_decam}}, 109M and available here:  [https://github.com/hchiang2/testdata_decam.git]  I trimmed it down by only saving the primary HDU and one data HDU.  However the unit tests in {{getRaw.py}} become less meaningful and I am not sure if we really want to do this, because some complexities of DECam files are about the MEF. {{getRaw.py}} tests Butler retrieval of multiple dataset types, in particular tests if the correct HDU is retrieved.     Nonetheless, {{getRaw.py}} can pass  (with branch u/hfc/DM-4375 of obs_decam)    Note: the old testdata_decam still live on lsst-dev:/lsst8/testdata_decam/",2,DM-4375,datamanagement,slimmer testdata_decam ticket file testdata_decam download archive mef total bit big 1.2 trim version testdata_decam 109 available https://github.com/hchiang2/testdata_decam.git trim save primary hdu datum hdu unit test getraw.py meaningful sure want complexity decam file mef getraw.py test butler retrieval multiple dataset type particular test correct hdu retrieve nonetheless getraw.py pass branch hfc dm-4375 obs_decam note old testdata_decam live lsst dev:/lsst8 testdata_decam/,"A slimmer testdata_decam Before this ticket, the files in {{testdata_decam}} are as they are downloaded from the archive. Some are MEF, and the total is a bit big (1.2G). I made a trimmed down version of {{testdata_decam}}, 109M and available here: [https://github.com/hchiang2/testdata_decam.git] I trimmed it down by only saving the primary HDU and one data HDU. However the unit tests in {{getRaw.py}} become less meaningful and I am not sure if we really want to do this, because some complexities of DECam files are about the MEF. {{getRaw.py}} tests Butler retrieval of multiple dataset types, in particular tests if the correct HDU is retrieved. Nonetheless, {{getRaw.py}} can pass (with branch u/hfc/DM-4375 of obs_decam) Note: the old testdata_decam still live on lsst-dev:/lsst8/testdata_decam/"
Learn and setup nebula as a development machine Personally establish Nebula as a stack development platform.,2,DM-4376,datamanagement,learn setup nebula development machine personally establish nebula stack development platform,Learn and setup nebula as a development machine Personally establish Nebula as a stack development platform.
"Review VAO/IVOA protocols for use in LSST IAM Following the good principals of re-using prior work, review VAO/IVOA protocols for use in LSST IAM, in particular the [IVOA Credential Delegation Protocol|http://www.ivoa.net/documents/latest/CredentialDelegation.html] and include a summary in our LSST IAM Design Doc.",1,DM-4377,datamanagement,review vao ivoa protocol use lsst iam follow good principal prior work review vao ivoa protocol use lsst iam particular ivoa credential delegation protocol|http://www.ivoa.net document late credentialdelegation.html include summary lsst iam design doc,"Review VAO/IVOA protocols for use in LSST IAM Following the good principals of re-using prior work, review VAO/IVOA protocols for use in LSST IAM, in particular the [IVOA Credential Delegation Protocol|http://www.ivoa.net/documents/latest/CredentialDelegation.html] and include a summary in our LSST IAM Design Doc."
"""SHUTOFF"" nebula instances consume core/ramIt  quota It appears that halted/shutoff instances have no effect on resource quota usage.  Eg:    {code:java}  $ openstack server list  +--------------------------------------+-----------------------+-------------------+----------------------------------------+  | ID                                   | Name                  | Status            | Networks                               |  +--------------------------------------+-----------------------+-------------------+----------------------------------------+  ...  | 1956c6d0-8aec-4f42-a781-8a68fd10179d | el7-jhoblitt          | SHUTOFF           | LSST-net=172.16.1.171, 141.142.208.150 |  ...  $ nova absolute-limits  +--------------------+--------+--------+  | Name               | Used   | Max    |  +--------------------+--------+--------+  | Cores              | 141    | 150    |  | FloatingIps        | 0      | 10     |  | ImageMeta          | -      | 128    |  | Instances          | 44     | 100    |  | Keypairs           | -      | 100    |  | Personality        | -      | 5      |  | Personality Size   | -      | 10240  |  | RAM                | 342016 | 400000 |  | SecurityGroupRules | -      | 20     |  | SecurityGroups     | 1      | 10     |  | Server Meta        | -      | 128    |  | ServerGroupMembers | -      | 10     |  | ServerGroups       | 0      | 10     |  +--------------------+--------+--------+  $ openstack server delete 1956c6d0-8aec-4f42-a781-8a68fd10179d  $ nova absolute-limits  +--------------------+--------+--------+  | Name               | Used   | Max    |  +--------------------+--------+--------+  | Cores              | 133    | 150    |  | FloatingIps        | 0      | 10     |  | ImageMeta          | -      | 128    |  | Instances          | 43     | 100    |  | Keypairs           | -      | 100    |  | Personality        | -      | 5      |  | Personality Size   | -      | 10240  |  | RAM                | 325632 | 400000 |  | SecurityGroupRules | -      | 20     |  | SecurityGroups     | 1      | 10     |  | Server Meta        | -      | 128    |  | ServerGroupMembers | -      | 10     |  | ServerGroups       | 0      | 10     |  +--------------------+--------+--------+    {code}  ",2,DM-4381,datamanagement,shutoff nebula instance consume core ramit quota appear halt shutoff instance effect resource quota usage eg code java openstack server list id status network 1956c6d0 8aec-4f42 a781 8a68fd10179d el7 jhoblitt shutoff lsst net=172.16.1.171 141.142.208.150 nova absolute limit --------------------+--------+--------+ max --------------------+--------+--------+ core 141 150 floatingips 10 imagemeta 128 instance 44 100 keypairs 100 personality personality size 10240 ram 342016 400000 securitygrouprule 20 securitygroups 10 server meta 128 servergroupmember 10 servergroups 10 --------------------+--------+--------+ openstack server delete 1956c6d0 8aec-4f42 a781 8a68fd10179d nova absolute limit --------------------+--------+--------+ max --------------------+--------+--------+ core 133 150 floatingips 10 imagemeta 128 instance 43 100 keypairs 100 personality personality size 10240 ram 325632 400000 securitygrouprule 20 securitygroups 10 server meta 128 servergroupmember 10 servergroups 10 --------------------+--------+--------+ code,"""SHUTOFF"" nebula instances consume core/ramIt quota It appears that halted/shutoff instances have no effect on resource quota usage. Eg: {code:java} $ openstack server list +--------------------------------------+-----------------------+-------------------+----------------------------------------+ | ID | Name | Status | Networks | +--------------------------------------+-----------------------+-------------------+----------------------------------------+ ... | 1956c6d0-8aec-4f42-a781-8a68fd10179d | el7-jhoblitt | SHUTOFF | LSST-net=172.16.1.171, 141.142.208.150 | ... $ nova absolute-limits +--------------------+--------+--------+ | Name | Used | Max | +--------------------+--------+--------+ | Cores | 141 | 150 | | FloatingIps | 0 | 10 | | ImageMeta | - | 128 | | Instances | 44 | 100 | | Keypairs | - | 100 | | Personality | - | 5 | | Personality Size | - | 10240 | | RAM | 342016 | 400000 | | SecurityGroupRules | - | 20 | | SecurityGroups | 1 | 10 | | Server Meta | - | 128 | | ServerGroupMembers | - | 10 | | ServerGroups | 0 | 10 | +--------------------+--------+--------+ $ openstack server delete 1956c6d0-8aec-4f42-a781-8a68fd10179d $ nova absolute-limits +--------------------+--------+--------+ | Name | Used | Max | +--------------------+--------+--------+ | Cores | 133 | 150 | | FloatingIps | 0 | 10 | | ImageMeta | - | 128 | | Instances | 43 | 100 | | Keypairs | - | 100 | | Personality | - | 5 | | Personality Size | - | 10240 | | RAM | 325632 | 400000 | | SecurityGroupRules | - | 20 | | SecurityGroups | 1 | 10 | | Server Meta | - | 128 | | ServerGroupMembers | - | 10 | | ServerGroups | 0 | 10 | +--------------------+--------+--------+ {code}"
"Port registryInfo.py from obs_subaru into Butler in butler (probably butlerUtils), users would like the ability to dump info from a repository's sqlite registry to text (console output). This is already implemented in {{obs_subaru/.../registryInfo.py}}, and basically just needs to be ported to butler in a sensible place. There are a few cases that assume certain columns are present and we need either to make the script more generic, or [~rhl] suggests that maybe we need to standardize the registries.",8,DM-4382,datamanagement,port registryinfo.py obs_subaru butler butler probably butlerutil user like ability dump info repository sqlite registry text console output implement obs_subaru/ ... /registryinfo.py basically need port butler sensible place case assume certain column present need script generic ~rhl suggest maybe need standardize registry,"Port registryInfo.py from obs_subaru into Butler in butler (probably butlerUtils), users would like the ability to dump info from a repository's sqlite registry to text (console output). This is already implemented in {{obs_subaru/.../registryInfo.py}}, and basically just needs to be ported to butler in a sensible place. There are a few cases that assume certain columns are present and we need either to make the script more generic, or [~rhl] suggests that maybe we need to standardize the registries."
"Avoid restarting czar when empty chunk list changes Currently czar caches empty chunk list after it reads the list from file. This complicates things when we need to update the list, integration test for example has to restart czar process after it loads new data to make sure that czar updates its cached list. Would be nice to have simpler mechanism to resetting cached list in czar without restarting it completely. It could be done via special query (abusing FLUSH for example) or via sending signal (problematic if czar runs remotely).    This can be potentially useful even after we replace empty chunk list file with some other mechanism as I expect that cache will stay around even for that.",2,DM-4383,datamanagement,avoid restart czar chunk list change currently czar cache chunk list read list file complicate thing need update list integration test example restart czar process load new datum sure czar update cache list nice simple mechanism reset cache list czar restart completely special query abuse flush example send signal problematic czar run remotely potentially useful replace chunk list file mechanism expect cache stay,"Avoid restarting czar when empty chunk list changes Currently czar caches empty chunk list after it reads the list from file. This complicates things when we need to update the list, integration test for example has to restart czar process after it loads new data to make sure that czar updates its cached list. Would be nice to have simpler mechanism to resetting cached list in czar without restarting it completely. It could be done via special query (abusing FLUSH for example) or via sending signal (problematic if czar runs remotely). This can be potentially useful even after we replace empty chunk list file with some other mechanism as I expect that cache will stay around even for that."
"Clean up ProcessCcdDecam ProcessCcdDecam needs some cleanup:  * {{run}} method simply delegates to the base class  * {{propagateCalibFlags}} is a no-op (deliberately in {{cab69086}}, need to explore if the original problem still exists)  * The config overrides (in config/processCcdDecam.py):  ** Uses the catalog star selector, which isn't wise given the current heterogeneity of reference catalogs.  ** Sets the background {{undersampleStype}} to {{REDUCE_INTERP_ORDER}}, which is the default.",1,DM-4386,datamanagement,clean processccddecam processccddecam need cleanup run method simply delegate base class propagatecalibflag op deliberately cab69086 need explore original problem exist config override config processccddecam.py use catalog star selector wise give current heterogeneity reference catalog set background undersamplestype reduce_interp_order default,"Clean up ProcessCcdDecam ProcessCcdDecam needs some cleanup: * {{run}} method simply delegates to the base class * {{propagateCalibFlags}} is a no-op (deliberately in {{cab69086}}, need to explore if the original problem still exists) * The config overrides (in config/processCcdDecam.py): ** Uses the catalog star selector, which isn't wise given the current heterogeneity of reference catalogs. ** Sets the background {{undersampleStype}} to {{REDUCE_INTERP_ORDER}}, which is the default."
"Skymap fails tests on testFindTractPatchList When skymap is built and healpy is loaded, {{testFindTractPatchList}} fails with:    {quote}======================================================================  FAIL: Test findTractPatchList  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 245, in testFindTractPatchList      self.assertClosestTractPatchList(skyMap, [tractInfo.getCtrCoord()], tractId)    File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 284, in assertClosestTractPatchList      tractPatchList = skyMap.findClosestTractPatchList(coordList)    File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/baseSkyMap.py"", line 146, in findClosestTractPatchList      tractInfo = self.findTract(coord)    File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/healpixSkyMap.py"", line 97, in findTract      index = healpy.ang2pix(self._nside, theta, phi, nest=self.config.nest)    File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 367, in ang2pix      check_theta_valid(theta)    File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 110, in check_theta_valid      assert (np.asarray(theta) >= 0).all() & (np.asarray(theta) <= np.pi + 1e-5).all(), ""Theta is defined between 0 and pi""  AssertionError: Theta is defined between 0 and pi{quote}    This was missed during regular CI testing since healpy is not normally setup. ",1,DM-4387,datamanagement,skymap fail test testfindtractpatchlist skymap build healpy load testfindtractpatchlist fail quote}====================================================================== fail test findtractpatchlist traceback recent file /users ctslater lsstsw build skymap test skymaptestcase.py line 245 testfindtractpatchlist self.assertclosesttractpatchlist(skymap tractinfo.getctrcoord tractid file /users ctslater lsstsw build skymap test skymaptestcase.py line 284 assertclosesttractpatchlist tractpatchlist skymap.findclosesttractpatchlist(coordlist file /users ctslater lsstsw build skymap python lsst skymap baseskymap.py line 146 findclosesttractpatchlist tractinfo self.findtract(coord file /users ctslater lsstsw build skymap python lsst skymap healpixskymap.py line 97 findtract index healpy.ang2pix(self._nside theta phi nest self.config.n file /users ctslater lsstsw stack darwinx86 healpy/1.8.1 12 lib python healpy-1.8.1 py2.7 macosx-10.5 x86_64.egg healpy pixelfunc.py line 367 ang2pix check_theta_valid(theta file /users ctslater lsstsw stack darwinx86 healpy/1.8.1 12 lib python healpy-1.8.1 py2.7 macosx-10.5 x86_64.egg healpy pixelfunc.py line 110 check_theta_valid assert np.asarray(theta 0).all np.asarray(theta np.pi 1e-5).all theta define pi assertionerror theta define pi{quote miss regular ci testing healpy normally setup,"Skymap fails tests on testFindTractPatchList When skymap is built and healpy is loaded, {{testFindTractPatchList}} fails with: {quote}====================================================================== FAIL: Test findTractPatchList ---------------------------------------------------------------------- Traceback (most recent call last): File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 245, in testFindTractPatchList self.assertClosestTractPatchList(skyMap, [tractInfo.getCtrCoord()], tractId) File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 284, in assertClosestTractPatchList tractPatchList = skyMap.findClosestTractPatchList(coordList) File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/baseSkyMap.py"", line 146, in findClosestTractPatchList tractInfo = self.findTract(coord) File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/healpixSkyMap.py"", line 97, in findTract index = healpy.ang2pix(self._nside, theta, phi, nest=self.config.nest) File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 367, in ang2pix check_theta_valid(theta) File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 110, in check_theta_valid assert (np.asarray(theta) >= 0).all() & (np.asarray(theta) <= np.pi + 1e-5).all(), ""Theta is defined between 0 and pi"" AssertionError: Theta is defined between 0 and pi{quote} This was missed during regular CI testing since healpy is not normally setup."
"Update testCoadds.py to accommodate changes in DM-2915 As of DM-2915, the config setting:  {code}self.measurement.plugins['base_PixelFlags'].masksFpAnywhere = ['CLIPPED']{code}  is set as a default for {{MeasureMergedCoaddSourcesTask}}.  However, this *CLIPPED* mask plane only exists if a given coadd was created using the newly implemented {{SafeClipAssembleCoaddTask}}.  If a coadd was built using {{AssembleCoaddTask}}, the *CLIPPED* mask plane is not present, so the above default must be overridden to exclude it when using {{MeasureMergedCoaddSourcesTask}}.  This is the case for the mock coadd that is assembled in the unittest code in {{testCoadds.py}}, so the config needs to be set for the test to run properly.    Note that the associated tests for {{SafeClipAssembleCoaddTask}} will be added as part of DM-4209.",1,DM-4391,datamanagement,update testcoadds.py accommodate change dm-2915 dm-2915 config set code}self.measurement.plugins['base_pixelflags'].masksfpanywhere clipped']{code set default measuremergedcoaddsourcestask clipped mask plane exist give coadd create newly implement safeclipassemblecoaddtask coadd build assemblecoaddtask clipped mask plane present default overridden exclude measuremergedcoaddsourcestask case mock coadd assemble unittest code testcoadds.py config need set test run properly note associate test safeclipassemblecoaddtask add dm-4209,"Update testCoadds.py to accommodate changes in DM-2915 As of DM-2915, the config setting: {code}self.measurement.plugins['base_PixelFlags'].masksFpAnywhere = ['CLIPPED']{code} is set as a default for {{MeasureMergedCoaddSourcesTask}}. However, this *CLIPPED* mask plane only exists if a given coadd was created using the newly implemented {{SafeClipAssembleCoaddTask}}. If a coadd was built using {{AssembleCoaddTask}}, the *CLIPPED* mask plane is not present, so the above default must be overridden to exclude it when using {{MeasureMergedCoaddSourcesTask}}. This is the case for the mock coadd that is assembled in the unittest code in {{testCoadds.py}}, so the config needs to be set for the test to run properly. Note that the associated tests for {{SafeClipAssembleCoaddTask}} will be added as part of DM-4209."
"Get analysis script working for HSC/LSST stack comparisons A script for performing pipeline output QA is under development for HSC.  The script provides many useful tools for plotting and analyzing pipeline outputs on single visits and coadds.  This is of general use for LSST and, in particular, will be adapted/expanded to include tools for the direct comparisons of identical data sets processed with both the HSC and LSST pipelines (i.e. DM-2984).  Appropriate adaptations for this script to run with the LSST stack will be made here (with the understanding that development is still ongoing on HSC and further adaptations will be accommodated as necessary/desired).    See [HSC-1320|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1320] and [HSC-1359|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1359] for details and examples of the output of this script.",6,DM-4393,datamanagement,analysis script work hsc lsst stack comparison script perform pipeline output qa development hsc script provide useful tool plot analyze pipeline output single visit coadd general use lsst particular adapt expand include tool direct comparison identical data set process hsc lsst pipeline i.e. dm-2984 appropriate adaptation script run lsst stack understanding development ongoing hsc adaptation accommodate necessary desire hsc-1320|https://hsc jira.astro.princeton.edu jira browse hsc-1320 hsc-1359|https://hsc jira.astro.princeton.edu jira browse hsc-1359 detail example output script,"Get analysis script working for HSC/LSST stack comparisons A script for performing pipeline output QA is under development for HSC. The script provides many useful tools for plotting and analyzing pipeline outputs on single visits and coadds. This is of general use for LSST and, in particular, will be adapted/expanded to include tools for the direct comparisons of identical data sets processed with both the HSC and LSST pipelines (i.e. DM-2984). Appropriate adaptations for this script to run with the LSST stack will be made here (with the understanding that development is still ongoing on HSC and further adaptations will be accommodated as necessary/desired). See [HSC-1320|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1320] and [HSC-1359|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1359] for details and examples of the output of this script."
"Update cmsd configuration for multi-node tests A particular cmsd configuration parameter prefixes a hardcoded path for QueryResource, which needs to be removed. This seems to appear only during multi-node tests.",1,DM-4395,datamanagement,update cmsd configuration multi node test particular cmsd configuration parameter prefix hardcode path queryresource need remove appear multi node test,"Update cmsd configuration for multi-node tests A particular cmsd configuration parameter prefixes a hardcoded path for QueryResource, which needs to be removed. This seems to appear only during multi-node tests."
"ctrl_execute test fails to find test binary There's a test in ctrl_execute that exercises the bin/dagIdInfo.py test program.   Since the rewrite_shebang rewrites happen after the tests are executed, the test that looks for the bin/dagIdInfo.py binary fails, since it's not there before the tests execute.",1,DM-4396,datamanagement,ctrl_execute test fail find test binary test ctrl_execute exercise bin dagidinfo.py test program rewrite_shebang rewrite happen test execute test look bin dagidinfo.py binary fail test execute,"ctrl_execute test fails to find test binary There's a test in ctrl_execute that exercises the bin/dagIdInfo.py test program. Since the rewrite_shebang rewrites happen after the tests are executed, the test that looks for the bin/dagIdInfo.py binary fails, since it's not there before the tests execute."
"Scale CommandLineTask multiprocessing timeout with workload The default timeout value for aborting a multiprocessing run in CommandLineTask is too short. Currently if no time length is supplied by the user, the default value gets set to 9999s. However if a processing task is quite large it is possible for the processing pool to take much longer to arrive at the result. Currently if the processing pool does not complete it's run within that time limit, python multiprocessing will throw a timeout error. The timeout value should be scaled such that the supplied value is assumed to be the timeout length for one processing task, and should be scaled by the number of tasks divided by the number of cpus available. The command line task documentation should be updated to reflect this change.",2,DM-4397,datamanagement,scale commandlinetask multiprocesse timeout workload default timeout value abort multiprocesse run commandlinetask short currently time length supply user default value get set 9999 processing task large possible processing pool long arrive result currently processing pool complete run time limit python multiprocessing throw timeout error timeout value scale supply value assume timeout length processing task scale number task divide number cpus available command line task documentation update reflect change,"Scale CommandLineTask multiprocessing timeout with workload The default timeout value for aborting a multiprocessing run in CommandLineTask is too short. Currently if no time length is supplied by the user, the default value gets set to 9999s. However if a processing task is quite large it is possible for the processing pool to take much longer to arrive at the result. Currently if the processing pool does not complete it's run within that time limit, python multiprocessing will throw a timeout error. The timeout value should be scaled such that the supplied value is assumed to be the timeout length for one processing task, and should be scaled by the number of tasks divided by the number of cpus available. The command line task documentation should be updated to reflect this change."
"Fix regexp for gcc48 DM-2622 inttoduced some regexes which raise exceptions when built with gcc48 (e.g. on centos7). gcc48 support for regexes is generally broken, so it's better to replace that with boost regexes.",1,DM-4398,datamanagement,fix regexp gcc48 dm-2622 inttoduce regexe raise exception build gcc48 e.g. centos7 gcc48 support regexe generally break well replace boost regexe,"Fix regexp for gcc48 DM-2622 inttoduced some regexes which raise exceptions when built with gcc48 (e.g. on centos7). gcc48 support for regexes is generally broken, so it's better to replace that with boost regexes."
"ctrl_execute test fails under El Capitan The test/testDagIdInfo.py because it runs a script from bin.src, rather than bin.   This test needs to be rewritten.",1,DM-4399,datamanagement,ctrl_execute test fail el capitan test testdagidinfo.py run script bin.src bin test need rewrite,"ctrl_execute test fails under El Capitan The test/testDagIdInfo.py because it runs a script from bin.src, rather than bin. This test needs to be rewritten."
"SuperTask demo on other older tasks The exampleCmdLine task worked fine, need to show demo for other tasks from pipe_tasks",4,DM-4400,datamanagement,supertask demo old task examplecmdline task work fine need demo task pipe_task,"SuperTask demo on other older tasks The exampleCmdLine task worked fine, need to show demo for other tasks from pipe_tasks"
"Experiment with light-weight SQL databases for secondary index Evaluate the use of light-weight SQL, such as InnoDB, TokuDB (now Kyoto Cabinet), or RocksDB to create and manage the secondary index.",8,DM-4402,datamanagement,experiment light weight sql database secondary index evaluate use light weight sql innodb tokudb kyoto cabinet rocksdb create manage secondary index,"Experiment with light-weight SQL databases for secondary index Evaluate the use of light-weight SQL, such as InnoDB, TokuDB (now Kyoto Cabinet), or RocksDB to create and manage the secondary index."
Review of [DM-2983] part 2 Second part and final of reviewing DM-2983,2,DM-4406,datamanagement,review dm-2983 second final review dm-2983,Review of [DM-2983] part 2 Second part and final of reviewing DM-2983
"Debug Qserv on ccqserv125..ccqserv149 It seems that some chunkQuery doesn't return on long queries like ""Select count(*) From Object""  The query stall and print in czar log:  {code}  2015-11-24T15:07:55.324Z [0x7f06b37fe700] INFO  root (core/modules/qdisp/Executive.cc:395) - Still 9 in flight.  {code}  If we look in the logs with next commands:  {code:bash}  cat  /qserv/run/var/log/qserv-czar.log | egrep  ""Executive::add\(job\(id="" | grep LSST | cut -d'=' -f2 | cut -d' ' -f1 | sort > JOBID_ADD.txt  cat  /qserv/run/var/log/qserv-czar.log | egrep  ""markCompleted""  | cut -d'(' -f 3 | cut -d',' -f1 | sort > JOBID_DONE.txt  {code}  And then  {code}  qserv@ccqserv125:/qserv$ diff JOBID_ADD.txt JOBID_DONE.txt   1826d1825  < 2639  2217d2215  < 2991  2904d2901  < 3609  3088d3084  < 3775  3152d3147  < 3832  3433d3427  < 4085  4227d4220  < 480  5478d5470  < 5926  6937d6928  < 7239  {code}  9 jobs aren't completed on czar.  If we look the the chunk_id of one of this job:  {code}  /opt/shmux/bin/shmux -c ""locate 5299"" ccqserv{125..149}.in2p3.fr  ...  ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.MYD  ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.MYI  ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.frm  ...  {code}  Data is on worker node and onccqserv148 xrootd log:  {code}  [2015-11-24T15:07:42.990Z] [0x7ffbc8244700] INFO  root (core/modules/xrdsvc/SsiSession.cc:125) - Enqueued TaskMsg for Resource(/chk/LSST/5299) in 0.000465  {code}  But a thread seems to be locked:  {code}  #gdb on ccqserv148 xrootd process:  (gdb) thread 7  [Switching to thread 7 (Thread 0x7ffbad7fa700 (LWP 192))]  #0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185  185     ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S: No such file or directory.  (gdb) bt  #0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185  #1  0x00007ffbd4c45c7c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6  #2  0x00007ffbccbe8db9 in std::condition_variable::wait<lsst::qserv::wsched::BlendScheduler::getCmd(bool)::<lambda()> >(std::unique_lock<std::mutex> &, lsst::qserv::wsched::BlendScheduler::<lambda()>) (      this=0xfba620, __lock=..., __p=...) at /usr/include/c++/4.9/condition_variable:98  #3  0x00007ffbccbe88f9 in lsst::qserv::wsched::BlendScheduler::getCmd (this=0xfba5c0, wait=true) at core/modules/wsched/BlendScheduler.cc:156  #4  0x00007ffbccba3b07 in lsst::qserv::util::EventThread::handleCmds (this=0xfbe890) at core/modules/util/EventThread.cc:45  #5  0x00007ffbccbab137 in std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()>::operator()<, void>(lsst::qserv::util::EventThread*) const (this=0xfbe900, __object=0xfbe890)      at /usr/include/c++/4.9/functional:569  #6  0x00007ffbccbaafff in std::_Bind_simple<std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()> (lsst::qserv::util::EventThread*)>::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0xfbe8f8)      at /usr/include/c++/4.9/functional:1700  #7  0x00007ffbccbaae45 in std::_Bind_simple<std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()> (lsst::qserv::util::EventThread*)>::operator()() (this=0xfbe8f8) at /usr/include/c++/4.9/functional:1688  #8  0x00007ffbccbaacfa in std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<void (lsst::qserv::util::EventThread::*)()> (lsst::qserv::util::EventThread*)> >::_M_run() (this=0xfbe8e0)      at /usr/include/c++/4.9/thread:115  #9  0x00007ffbd4c49970 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6  #10 0x00007ffbd50ae0a4 in start_thread (arg=0x7ffbad7fa700) at pthread_create.c:309  #11 0x00007ffbd43b904d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111  {code}    Something seems to prevent return of chunk query result...          ",5,DM-4407,datamanagement,debug qserv ccqserv125 ccqserv149 chunkquery return long query like select count object query stall print czar log code 2015 11 24t15:07:55.324z 0x7f06b37fe700 info root core module qdisp executive.cc:395 flight code look log command code bash cat run var log qserv czar.log egrep executive::add\(job\(id= grep lsst cut -f2 cut -f1 sort jobid_add.txt cat run var log qserv czar.log egrep markcomplete cut -d -f cut -f1 sort jobid_done.txt code code qserv@ccqserv125:/qserv$ diff jobid_add.txt jobid_done.txt 1826d1825 2639 2217d2215 2991 2904d2901 3609 3088d3084 3775 3152d3147 3832 3433d3427 4085 4227d4220 480 5478d5470 5926 6937d6928 7239 code job complete czar look chunk_id job code shmux bin shmux locate 5299 ccqserv{125 .. 149}.in2p3.fr ccqserv148.in2p3.fr /qserv data mysql lsst object_5299.myd ccqserv148.in2p3.fr /qserv data mysql lsst object_5299.myi ccqserv148.in2p3.fr /qserv data mysql lsst object_5299.frm code data worker node onccqserv148 xrootd log code 2015 11 24t15:07:42.990z 0x7ffbc8244700 info root core module xrdsvc ssisession.cc:125 enqueued taskmsg resource(/chk lsst/5299 0.000465 code thread lock code gdb ccqserv148 xrootd process gdb thread switch thread thread 0x7ffbad7fa700 lwp 192 pthread_cond_wait@@glibc_2.3.2 /nptl sysdeps unix sysv linux x86_64 pthread_cond_wait s:185 185 /nptl sysdeps unix sysv linux x86_64 pthread_cond_wait file directory gdb bt pthread_cond_wait@@glibc_2.3.2 /nptl sysdeps unix sysv linux x86_64 pthread_cond_wait s:185 0x00007ffbd4c45c7c std::condition_variable::wait(std::unique_lock /usr lib x86_64 linux gnu libstdc++.so.6 0x00007ffbccbe8db9 std::condition_variable::wait std::unique_lock lsst::qserv::wsched::blendscheduler this=0xfba620 lock= p= /usr include c++/4.9 condition_variable:98 0x00007ffbccbe88f9 lsst::qserv::wsched::blendscheduler::getcmd this=0xfba5c0 wait true core module wsched blendscheduler.cc:156 0x00007ffbccba3b07 lsst::qserv::util::eventthread::handlecmds this=0xfbe890 core module util eventthread.cc:45 0x00007ffbccbab137 std::_mem_fn::operator void>(lsst::qserv::util::eventthread const this=0xfbe900 object=0xfbe890 /usr include c++/4.9 functional:569 0x00007ffbccbaafff std::_bind_simple lsst::qserv::util::eventthread*)>::_m_invoke<0ul>(std::_index_tuple<0ul this=0xfbe8f8 /usr include c++/4.9 functional:1700 0x00007ffbccbaae45 std::_bind_simple lsst::qserv::util::eventthread*)>::operator this=0xfbe8f8 /usr include c++/4.9 functional:1688 0x00007ffbccbaacfa std::thread::_impl lsst::qserv::util::eventthread this=0xfbe8e0 /usr include c++/4.9 thread:115 0x00007ffbd4c49970 /usr lib x86_64 linux gnu libstdc++.so.6 10 0x00007ffbd50ae0a4 start_thread arg=0x7ffbad7fa700 pthread_create.c:309 11 0x00007ffbd43b904d clone .. /sysdeps unix sysv linux x86_64 clone s:111 code prevent return chunk query result,"Debug Qserv on ccqserv125..ccqserv149 It seems that some chunkQuery doesn't return on long queries like ""Select count(*) From Object"" The query stall and print in czar log: {code} 2015-11-24T15:07:55.324Z [0x7f06b37fe700] INFO root (core/modules/qdisp/Executive.cc:395) - Still 9 in flight. {code} If we look in the logs with next commands: {code:bash} cat /qserv/run/var/log/qserv-czar.log | egrep ""Executive::add\(job\(id="" | grep LSST | cut -d'=' -f2 | cut -d' ' -f1 | sort > JOBID_ADD.txt cat /qserv/run/var/log/qserv-czar.log | egrep ""markCompleted"" | cut -d'(' -f 3 | cut -d',' -f1 | sort > JOBID_DONE.txt {code} And then {code} qserv@ccqserv125:/qserv$ diff JOBID_ADD.txt JOBID_DONE.txt 1826d1825 < 2639 2217d2215 < 2991 2904d2901 < 3609 3088d3084 < 3775 3152d3147 < 3832 3433d3427 < 4085 4227d4220 < 480 5478d5470 < 5926 6937d6928 < 7239 {code} 9 jobs aren't completed on czar. If we look the the chunk_id of one of this job: {code} /opt/shmux/bin/shmux -c ""locate 5299"" ccqserv{125..149}.in2p3.fr ... ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.MYD ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.MYI ccqserv148.in2p3.fr: /qserv/data/mysql/LSST/Object_5299.frm ... {code} Data is on worker node and onccqserv148 xrootd log: {code} [2015-11-24T15:07:42.990Z] [0x7ffbc8244700] INFO root (core/modules/xrdsvc/SsiSession.cc:125) - Enqueued TaskMsg for Resource(/chk/LSST/5299) in 0.000465 {code} But a thread seems to be locked: {code} #gdb on ccqserv148 xrootd process: (gdb) thread 7 [Switching to thread 7 (Thread 0x7ffbad7fa700 (LWP 192))] #0 pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185 185 ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S: No such file or directory. (gdb) bt #0 pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185 #1 0x00007ffbd4c45c7c in std::condition_variable::wait(std::unique_lock&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6 #2 0x00007ffbccbe8db9 in std::condition_variable::wait >(std::unique_lock &, lsst::qserv::wsched::BlendScheduler::) ( this=0xfba620, __lock=..., __p=...) at /usr/include/c++/4.9/condition_variable:98 #3 0x00007ffbccbe88f9 in lsst::qserv::wsched::BlendScheduler::getCmd (this=0xfba5c0, wait=true) at core/modules/wsched/BlendScheduler.cc:156 #4 0x00007ffbccba3b07 in lsst::qserv::util::EventThread::handleCmds (this=0xfbe890) at core/modules/util/EventThread.cc:45 #5 0x00007ffbccbab137 in std::_Mem_fn::operator()<, void>(lsst::qserv::util::EventThread*) const (this=0xfbe900, __object=0xfbe890) at /usr/include/c++/4.9/functional:569 #6 0x00007ffbccbaafff in std::_Bind_simple (lsst::qserv::util::EventThread*)>::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0xfbe8f8) at /usr/include/c++/4.9/functional:1700 #7 0x00007ffbccbaae45 in std::_Bind_simple (lsst::qserv::util::EventThread*)>::operator()() (this=0xfbe8f8) at /usr/include/c++/4.9/functional:1688 #8 0x00007ffbccbaacfa in std::thread::_Impl (lsst::qserv::util::EventThread*)> >::_M_run() (this=0xfbe8e0) at /usr/include/c++/4.9/thread:115 #9 0x00007ffbd4c49970 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6 #10 0x00007ffbd50ae0a4 in start_thread (arg=0x7ffbad7fa700) at pthread_create.c:309 #11 0x00007ffbd43b904d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111 {code} Something seems to prevent return of chunk query result..."
"HSC backport: fix memory leak in afw:geom:polygon This is a backport of a bug fix that got included as part of [HSC-1311|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1311].  It is not related to that issue in particular, so is being ported here as an isolated bug fix.    {panel}  Original commit message:  pprice@tiger-sumire:/tigress/pprice/hsc-1311/afw (tickets/HSC-1311=) $ git sub  commit 55ad42d37fd1346f8ebc11e4077366dff4eaa87b  Author: Paul Price <price@astro.princeton.edu>  Date:   Wed Oct 21 10:59:56 2015 -0400         imageLib: import polygonLib to prevent memory leak             When doing ""exposure.getInfo().getValidPolygon()"", was getting:             swig/python detected a memory leak of type 'boost::shared_ptr< lsst::afw::geom::polygon::Polygon > *', no destructor found.             This was due to the polygonLib not being imported in imageLib.      Using polygonLib in imageLib then requires adding polygon.h to all      the swig interface files that use imageLib.i.      examples/testSpatialCellLib.i              | 1 +   python/lsst/afw/cameraGeom/cameraGeomLib.i | 1 +   python/lsst/afw/detection/detectionLib.i   | 1 +   python/lsst/afw/display/displayLib.i       | 1 +   python/lsst/afw/geom/polygon/Polygon.i     | 1 +   python/lsst/afw/image/imageLib.i           | 2 ++   python/lsst/afw/math/detail/detailLib.i    | 1 +   python/lsst/afw/math/mathLib.i             | 1 +   8 files changed, 9 insertions(+)  {panel}",1,DM-4408,datamanagement,hsc backport fix memory leak afw geom polygon backport bug fix get include hsc-1311|https://hsc jira.astro.princeton.edu jira browse hsc-1311 relate issue particular port isolated bug fix panel original commit message pprice@tiger sumire:/tigress pprice hsc-1311 afw ticket hsc-1311= git sub commit 55ad42d37fd1346f8ebc11e4077366dff4eaa87b author paul price date oct 21 10:59:56 2015 -0400 imagelib import polygonlib prevent memory leak exposure.getinfo().getvalidpolygon get swig python detect memory leak type boost::shared_ptr lsst::afw::geom::polygon::polygon destructor find polygonlib import imagelib polygonlib imagelib require add polygon.h swig interface file use imagelib.i example testspatialcelllib.i python lsst afw camerageom camerageomlib.i python lsst afw detection detectionlib.i python lsst afw display displaylib.i python lsst afw geom polygon polygon.i python lsst afw image imagelib.i python lsst afw math detail detaillib.i python lsst afw math mathlib.i file change insertions(+ panel,"HSC backport: fix memory leak in afw:geom:polygon This is a backport of a bug fix that got included as part of [HSC-1311|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1311]. It is not related to that issue in particular, so is being ported here as an isolated bug fix. {panel} Original commit message: pprice@tiger-sumire:/tigress/pprice/hsc-1311/afw (tickets/HSC-1311=) $ git sub commit 55ad42d37fd1346f8ebc11e4077366dff4eaa87b Author: Paul Price  Date: Wed Oct 21 10:59:56 2015 -0400 imageLib: import polygonLib to prevent memory leak When doing ""exposure.getInfo().getValidPolygon()"", was getting: swig/python detected a memory leak of type 'boost::shared_ptr< lsst::afw::geom::polygon::Polygon > *', no destructor found. This was due to the polygonLib not being imported in imageLib. Using polygonLib in imageLib then requires adding polygon.h to all the swig interface files that use imageLib.i. examples/testSpatialCellLib.i | 1 + python/lsst/afw/cameraGeom/cameraGeomLib.i | 1 + python/lsst/afw/detection/detectionLib.i | 1 + python/lsst/afw/display/displayLib.i | 1 + python/lsst/afw/geom/polygon/Polygon.i | 1 + python/lsst/afw/image/imageLib.i | 2 ++ python/lsst/afw/math/detail/detailLib.i | 1 + python/lsst/afw/math/mathLib.i | 1 + 8 files changed, 9 insertions(+) {panel}"
"Port detection task footprint growth changes from HSC In hsc the default behavior for the detection task is to updated footprints with a footprint which has been grown by the psf. This behavior needs to be ported to LSST, as some source records have footprints which are too small. When making this change, the new default needs to be overridden for the calibrateTask, as it needs the original size.    The port includes 8e9fb159a3227f848e0db1ecacf7819599f1c03b from meas_algorithms and 8bf0f4a44c924259d9eefbd109aadec7d839e0f2 from pipe_tasks",5,DM-4410,datamanagement,port detection task footprint growth change hsc hsc default behavior detection task update footprint footprint grow psf behavior need port lsst source record footprint small make change new default need overridden calibratetask need original size port include 8e9fb159a3227f848e0db1ecacf7819599f1c03b meas_algorithm 8bf0f4a44c924259d9eefbd109aadec7d839e0f2 pipe_task,"Port detection task footprint growth changes from HSC In hsc the default behavior for the detection task is to updated footprints with a footprint which has been grown by the psf. This behavior needs to be ported to LSST, as some source records have footprints which are too small. When making this change, the new default needs to be overridden for the calibrateTask, as it needs the original size. The port includes 8e9fb159a3227f848e0db1ecacf7819599f1c03b from meas_algorithms and 8bf0f4a44c924259d9eefbd109aadec7d839e0f2 from pipe_tasks"
Add git-lfs to packer-newinstall git-lfs is not available in our deliverables. Artifacts (binaries) such as VM images and docker data containers should provide a stable version of git-lfs.,2,DM-4414,datamanagement,add git lfs packer newinstall git lfs available deliverable artifact binary vm image docker datum container provide stable version git lfs,Add git-lfs to packer-newinstall git-lfs is not available in our deliverables. Artifacts (binaries) such as VM images and docker data containers should provide a stable version of git-lfs.
"Meetings - Nov 2015 Verification dataset meetings, Illinois DES meeting, single-frame processing discussions, supertask meeting, OpenStack User meeting",2,DM-4415,datamanagement,meeting nov 2015 verification dataset meeting illinois des meeting single frame processing discussion supertask meeting openstack user meeting,"Meetings - Nov 2015 Verification dataset meetings, Illinois DES meeting, single-frame processing discussions, supertask meeting, OpenStack User meeting"
"Other LOE - Nov 2015 Local LSST group meetings, NCSA postdoc meetings, NCSA Physics & Astronomy theme meeting, or other local meetings, events, or tasks to comply with NCSA policies",5,DM-4416,datamanagement,loe nov 2015 local lsst group meeting ncsa postdoc meeting ncsa physics astronomy theme meeting local meeting event task comply ncsa policy,"Other LOE - Nov 2015 Local LSST group meetings, NCSA postdoc meetings, NCSA Physics & Astronomy theme meeting, or other local meetings, events, or tasks to comply with NCSA policies"
Crash course on using git-lfs Learn to install and use git-lfs; help testing with migrating {{testdata_decam}} to git-lfs; verify tests pass with the new repository (DM-4370).,1,DM-4417,datamanagement,crash course git lfs learn install use git lfs help test migrating testdata_decam git lfs verify test pass new repository dm-4370,Crash course on using git-lfs Learn to install and use git-lfs; help testing with migrating {{testdata_decam}} to git-lfs; verify tests pass with the new repository (DM-4370).
"Learn about the task design in ISR processing Learn the concept behind the previous API changes (RFC-26) in the tasks of ISR processing, and data storage/retrieval involved. ",3,DM-4418,datamanagement,learn task design isr processing learn concept previous api change rfc-26 task isr processing datum storage retrieval involve,"Learn about the task design in ISR processing Learn the concept behind the previous API changes (RFC-26) in the tasks of ISR processing, and data storage/retrieval involved."
Explore basic middleware and orchestration tools Use {{runOrca}} to launch jobs through {{lsst-dev}} and do some single frame processing with it. Also learn a little about process execution. ,3,DM-4419,datamanagement,explore basic middleware orchestration tool use runorca launch job lsst dev single frame processing learn little process execution,Explore basic middleware and orchestration tools Use {{runOrca}} to launch jobs through {{lsst-dev}} and do some single frame processing with it. Also learn a little about process execution.
Avoid bash usage in batch submission {{ctrl_pool}} currently creates bash submission scripts with an explicit {{/bin/bash}} bang line.  [~rhl] [argues| https://github.com/lsst/ctrl_pool/commit/047f0de5a682ad9e9a6f65ccc7cc296e0a0d4ee7#commitcomment-14573759] on the review of DM-2983 that we should using posix shell constructions instead.,1,DM-4420,datamanagement,avoid bash usage batch submission ctrl_pool currently create bash submission script explicit /bin bash bang line ~rhl argues| https://github.com/lsst/ctrl_pool/commit/047f0de5a682ad9e9a6f65ccc7cc296e0a0d4ee7#commitcomment-14573759 review dm-2983 posix shell construction instead,Avoid bash usage in batch submission {{ctrl_pool}} currently creates bash submission scripts with an explicit {{/bin/bash}} bang line. [~rhl] [argues| https://github.com/lsst/ctrl_pool/commit/047f0de5a682ad9e9a6f65ccc7cc296e0a0d4ee7#commitcomment-14573759] on the review of DM-2983 that we should using posix shell constructions instead.
"faulty assumption about order dependency in ctrl_event unit tests A recent change to daf_base uncovered a couple of faulty tests in ctrl_events that incorrectly assumes the order in which assumed the order in which data in a PropertySet would be received.   We can't assume which order these values will be put into the property set, and therefore into the list retrieved from the Event object.",1,DM-4421,datamanagement,faulty assumption order dependency ctrl_event unit test recent change daf_base uncover couple faulty test ctrl_event incorrectly assume order assume order datum propertyset receive assume order value property set list retrieve event object,"faulty assumption about order dependency in ctrl_event unit tests A recent change to daf_base uncovered a couple of faulty tests in ctrl_events that incorrectly assumes the order in which assumed the order in which data in a PropertySet would be received. We can't assume which order these values will be put into the property set, and therefore into the list retrieved from the Event object."
"Remove Task.display() As of DM-927 (included in release 9.2, end of S14), {{lsst.pipe.base.task.Task.display()}} was marked as deprecated. It should now be removed.",3,DM-4428,datamanagement,remove task.display dm-927 include release 9.2 end s14 lsst.pipe.base.task.task.display mark deprecate remove,"Remove Task.display() As of DM-927 (included in release 9.2, end of S14), {{lsst.pipe.base.task.Task.display()}} was marked as deprecated. It should now be removed."
"Improve docker storage backend on RedHat-like distributions Solve startup log message RedHat-like distro: ""level=warning msg=""Usage of loopback devices is strongly discouraged for production""?    This is due to use of DeviceMapper (default package option on RedHAt-like distros) without a dedicated hard-disk, use of ""overlay"" backed storage seems better.  ",4,DM-4437,datamanagement,"improve docker storage backend redhat like distribution solve startup log message redhat like distro level warn msg=""usage loopback device strongly discourage production use devicemapper default package option redhat like distro dedicated hard disk use overlay back storage well","Improve docker storage backend on RedHat-like distributions Solve startup log message RedHat-like distro: ""level=warning msg=""Usage of loopback devices is strongly discouraged for production""? This is due to use of DeviceMapper (default package option on RedHAt-like distros) without a dedicated hard-disk, use of ""overlay"" backed storage seems better."
"Replace sed with stronger template engine in docker scripts Dockerfile are generated using templates and sed, this should be strengthened.",2,DM-4438,datamanagement,replace se strong template engine docker script dockerfile generate template se strengthen,"Replace sed with stronger template engine in docker scripts Dockerfile are generated using templates and sed, this should be strengthened."
"Remove useless xrootd client parameters This extract of etc/sysconfig/docker:    {code:bash}  # used by qserv-czar  export QSW_RESULTDIR=${QSERV_RUN_DIR}/qserv-run/tmp  # Disabling buffering in python in order to enable ""real-time"" logging.  export PYTHONUNBUFFERED=1    export XRD_LOGLEVEL=Debug  export XRDSSIDEBUG=1    # Increase timeouts, without that, long-running queries fail.  # The proper fix is coming, see DM-3433  export XRD_REQUESTTIMEOUT=64000  export XRD_STREAMTIMEOUT=64000  export XRD_DATASERVERTTL=64000  export XRD_TIMEOUTRESOLUTION=64000  {code}    Has to be moved to:    {code:bash}   # used by qserv-czar  export QSW_RESULTDIR=${QSERV_RUN_DIR}/qserv-run/tmp  export XRD_LOGLEVEL=Debug  export XRDSSIDEBUG=1  # Disabling buffering in python in order to enable ""real-time"" logging.  export PYTHONUNBUFFERED=1  {code}    And then tested in multi-node, and on in2p3 cluster.",1,DM-4439,datamanagement,remove useless xrootd client parameter extract etc sysconfig docker code bash qserv czar export qsw_resultdir=${qserv_run_dir}/qserv run tmp disable buffering python order enable real time log export pythonunbuffered=1 export xrd_loglevel debug export xrdssidebug=1 increase timeout long run query fail proper fix come dm-3433 export xrd_requesttimeout=64000 export xrd_streamtimeout=64000 export xrd_dataserverttl=64000 export xrd_timeoutresolution=64000 code move code bash qserv czar export qsw_resultdir=${qserv_run_dir}/qserv run tmp export xrd_loglevel debug export xrdssidebug=1 disable buffering python order enable real time log export pythonunbuffered=1 code test multi node in2p3 cluster,"Remove useless xrootd client parameters This extract of etc/sysconfig/docker: {code:bash} # used by qserv-czar export QSW_RESULTDIR=${QSERV_RUN_DIR}/qserv-run/tmp # Disabling buffering in python in order to enable ""real-time"" logging. export PYTHONUNBUFFERED=1 export XRD_LOGLEVEL=Debug export XRDSSIDEBUG=1 # Increase timeouts, without that, long-running queries fail. # The proper fix is coming, see DM-3433 export XRD_REQUESTTIMEOUT=64000 export XRD_STREAMTIMEOUT=64000 export XRD_DATASERVERTTL=64000 export XRD_TIMEOUTRESOLUTION=64000 {code} Has to be moved to: {code:bash} # used by qserv-czar export QSW_RESULTDIR=${QSERV_RUN_DIR}/qserv-run/tmp export XRD_LOGLEVEL=Debug export XRDSSIDEBUG=1 # Disabling buffering in python in order to enable ""real-time"" logging. export PYTHONUNBUFFERED=1 {code} And then tested in multi-node, and on in2p3 cluster."
Remove QSW_RESULTPATH and XROOTD_RUN_DIR if useless These parameters may be useless (see DM-4395). If yes they can be removed to simplify configuration procedure.,2,DM-4440,datamanagement,remove qsw_resultpath xrootd_run_dir useless parameter useless dm-4395 yes remove simplify configuration procedure,Remove QSW_RESULTPATH and XROOTD_RUN_DIR if useless These parameters may be useless (see DM-4395). If yes they can be removed to simplify configuration procedure.
IAM process for granting data access rights Document a process for granting of data access rights to LSST users according to the Data Access White Paper ([Doc-53733|http://ls.st/Document-5373]).    On the wiki: https://confluence.lsstcorp.org/display/LAAIM/Granting+Data+Access+Rights,1,DM-4442,datamanagement,iam process grant datum access right document process grant datum access right lsst user accord data access white paper doc-53733|http://ls.st document-5373 wiki https://confluence.lsstcorp.org/display/laaim/granting+data+access+right,IAM process for granting data access rights Document a process for granting of data access rights to LSST users according to the Data Access White Paper ([Doc-53733|http://ls.st/Document-5373]). On the wiki: https://confluence.lsstcorp.org/display/LAAIM/Granting+Data+Access+Rights
"Please document the --rerun option DM-3371 adds the {{--rerun}} option to command line tasks. The help for this option reads:  {quote}  rerun name: sets OUTPUT to ROOT/rerun/OUTPUT; optionally sets ROOT to ROOT/rerun/INPUT  {quote}  While essentially correct, that's not particularly helpful in understanding what's actually going on here. A motivation and description of this functionality is available in RFC-95: please ensure that, or some variation of it, is included in the stack documentation.",1,DM-4443,datamanagement,document --rerun option dm-3371 add --rerun option command line task help option read quote rerun set output root rerun output optionally set root root rerun input quote essentially correct particularly helpful understand actually go motivation description functionality available rfc-95 ensure variation include stack documentation,"Please document the --rerun option DM-3371 adds the {{--rerun}} option to command line tasks. The help for this option reads: {quote} rerun name: sets OUTPUT to ROOT/rerun/OUTPUT; optionally sets ROOT to ROOT/rerun/INPUT {quote} While essentially correct, that's not particularly helpful in understanding what's actually going on here. A motivation and description of this functionality is available in RFC-95: please ensure that, or some variation of it, is included in the stack documentation."
ISR and calibration of a tiny set of DECam raw data Learn more about the beginning steps of single frame processing by processing a small subset of DECam Stripe 82 raw data (~10 visits) and performing instrument signature removal with features currently implemented.,6,DM-4444,datamanagement,isr calibration tiny set decam raw datum learn begin step single frame processing process small subset decam stripe 82 raw datum ~10 visit perform instrument signature removal feature currently implement,ISR and calibration of a tiny set of DECam raw data Learn more about the beginning steps of single frame processing by processing a small subset of DECam Stripe 82 raw data (~10 visits) and performing instrument signature removal with features currently implemented.
IaM work in November Work done in support of LSST's IaM efforts.,1,DM-4445,datamanagement,iam work november work support lsst iam effort,IaM work in November Work done in support of LSST's IaM efforts.
"Management for November Centered around DMLT meeting, design process and hiring, in addition to general steering of activities at NCSA ",6,DM-4446,datamanagement,management november center dmlt meeting design process hiring addition general steering activity ncsa,"Management for November Centered around DMLT meeting, design process and hiring, in addition to general steering of activities at NCSA"
"November TOWG/opeartions design  work Towg attendance/ note + participating in Beth's group.       Detailed  WBS for DM,  Condensed  WBS  to show high level,  Effort estimates, point out problematic thinking in the estimates. ",8,DM-4447,datamanagement,november towg opeartion design work towg attendance/ note participate beth group detailed wbs dm condense wbs high level effort estimate point problematic thinking estimate,"November TOWG/opeartions design work Towg attendance/ note + participating in Beth's group. Detailed WBS for DM, Condensed WBS to show high level, Effort estimates, point out problematic thinking in the estimates."
"JCC Two day JCC activity at NCSA, including extended JCC meeting with HEP centers likely to   host people exploiting LSST Data.      writeup of extended meeting is here: https://confluence.lsstcorp.org/display/JCC/Extended+JCC+meeting+--+2015-11-23   in the JCC section.     organize, coordinate, and write up meeting notes. ",5,DM-4448,datamanagement,jcc day jcc activity ncsa include extend jcc meeting hep center likely host people exploit lsst data writeup extended meeting https://confluence.lsstcorp.org/display/jcc/extended+jcc+meeting+--+2015-11-23 jcc section organize coordinate write meeting note,"JCC Two day JCC activity at NCSA, including extended JCC meeting with HEP centers likely to host people exploiting LSST Data. writeup of extended meeting is here: https://confluence.lsstcorp.org/display/JCC/Extended+JCC+meeting+--+2015-11-23 in the JCC section. organize, coordinate, and write up meeting notes."
"HTML5 Sphinx theme for technotes Build a Sphinx theme for the Technote platform. Treat this work as exploratory, proof of concept work for customizing the HTML, CSS and JS of the software docs.    Objectives are    1. Create a Sphinx theme repo  2. Show how gulp can be used to help develop web assets for the theme  3. Establish a pattern for table contents columns that scroll independently of the main article  4. Show how we can implement a HTML5 rst builder in documenteer to fix permalink issues and build true HTML5 output.",2,DM-4452,datamanagement,html5 sphinx theme technote build sphinx theme technote platform treat work exploratory proof concept work customize html css js software doc objective create sphinx theme repo gulp help develop web asset theme establish pattern table content column scroll independently main article implement html5 rst builder documenteer fix permalink issue build true html5 output,"HTML5 Sphinx theme for technotes Build a Sphinx theme for the Technote platform. Treat this work as exploratory, proof of concept work for customizing the HTML, CSS and JS of the software docs. Objectives are 1. Create a Sphinx theme repo 2. Show how gulp can be used to help develop web assets for the theme 3. Establish a pattern for table contents columns that scroll independently of the main article 4. Show how we can implement a HTML5 rst builder in documenteer to fix permalink issues and build true HTML5 output."
"Finish documentation and comments on SuperTask  Need to finish documentation, implementation and comments on the code",4,DM-4453,datamanagement,finish documentation comment supertask need finish documentation implementation comment code,"Finish documentation and comments on SuperTask Need to finish documentation, implementation and comments on the code"
"Fix multiple patch catalog sorting for forcedPhotCcd.py {{forcedPhotCcd.py}} is currently broken due to the requirement of the {{lsst.afw.table.getChildren()}} function that the *SourceCatalog* is sorted by the parent key (i.e. {{lsst.afw.table.SourceTable.getParentKey()}}).  This occurs naturally in the case of *SourceCatalogs* produced by the detection and deblending tasks, but it may not be true when concatenating multiple such catalogs.  This is indeed the case for {{forcedPhotCcd.py}} as a given CCD can be overlapped by multiple patches, thus requiring a concatenation of the reference catalogs of all overlapping patches.    There two places in the running of {{forcedPhotCcd.py}} where calls to {{getChildren()}} can cause a failure: one in the {{subset()}} function in {{references.py}}, and the other in the {{run}} function of *SingleFrameMeasurementTask* in {{sfm.py}}.",2,DM-4454,datamanagement,fix multiple patch catalog sort forcedphotccd.py forcedphotccd.py currently break requirement lsst.afw.table.getchildren function sourcecatalog sort parent key i.e. lsst.afw.table sourcetable.getparentkey occur naturally case sourcecatalogs produce detection deblending task true concatenate multiple catalog case forcedphotccd.py give ccd overlap multiple patch require concatenation reference catalog overlap patch place running forcedphotccd.py call getchildren cause failure subset function references.py run function singleframemeasurementtask sfm.py,"Fix multiple patch catalog sorting for forcedPhotCcd.py {{forcedPhotCcd.py}} is currently broken due to the requirement of the {{lsst.afw.table.getChildren()}} function that the *SourceCatalog* is sorted by the parent key (i.e. {{lsst.afw.table.SourceTable.getParentKey()}}). This occurs naturally in the case of *SourceCatalogs* produced by the detection and deblending tasks, but it may not be true when concatenating multiple such catalogs. This is indeed the case for {{forcedPhotCcd.py}} as a given CCD can be overlapped by multiple patches, thus requiring a concatenation of the reference catalogs of all overlapping patches. There two places in the running of {{forcedPhotCcd.py}} where calls to {{getChildren()}} can cause a failure: one in the {{subset()}} function in {{references.py}}, and the other in the {{run}} function of *SingleFrameMeasurementTask* in {{sfm.py}}."
Understand how the proposed interfaces fit with Qserv code Understand how the interfaces proposed by [~abh] in DM-3755 fit with the existing qserv code.,3,DM-4455,datamanagement,understand propose interface fit qserv code understand interface propose ~abh dm-3755 fit exist qserv code,Understand how the proposed interfaces fit with Qserv code Understand how the interfaces proposed by [~abh] in DM-3755 fit with the existing qserv code.
Investigate MemSQL Take a look at the MemSQL distributed database.,8,DM-4457,datamanagement,investigate memsql look memsql distribute database,Investigate MemSQL Take a look at the MemSQL distributed database.
"Week end 11/07/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 7, 2015.",2,DM-4458,datamanagement,week end 11/07/15 support lsst dev cluster openstack account week end november 2015,"Week end 11/07/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending November 7, 2015."
"Week end 11/14/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 14, 2015.",4,DM-4459,datamanagement,week end 11/14/15 support lsst dev cluster openstack account week end november 14 2015,"Week end 11/14/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending November 14, 2015."
"Week end 11/21/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 21, 2015.",2,DM-4460,datamanagement,week end 11/21/15 support lsst dev cluster openstack account week end november 21 2015,"Week end 11/21/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending November 21, 2015."
New equipment setup and configuration (week end 11/07/15) * Three Dell R730's:  ** Mount in A row racks  ** Complete Bios updates  * Moved ~25 VMs over to new lsst-vsphere infrastructure  * Setup lsst-condor\[1-6\] VMs  * Setup lsst-esxmac1 with networking  * Fixed networking issues on new lsst-esx1 server (an undocumented host was squatting on the IP address),5,DM-4462,datamanagement,new equipment setup configuration week end 11/07/15 dell r730 mount row rack complete bios update move ~25 vms new lsst vsphere infrastructure setup lsst condor\[1 6\ vms setup lsst esxmac1 network fix networking issue new lsst esx1 server undocumented host squat ip address,New equipment setup and configuration (week end 11/07/15) * Three Dell R730's: ** Mount in A row racks ** Complete Bios updates * Moved ~25 VMs over to new lsst-vsphere infrastructure * Setup lsst-condor\[1-6\] VMs * Setup lsst-esxmac1 with networking * Fixed networking issues on new lsst-esx1 server (an undocumented host was squatting on the IP address)
New equipment setup and configuration (week end 11/14/15) * Received Dell iDrac license upgrades for new Dell R730 servers  * Received and configured VMware vSphere licenses from CDW-G & AURA  * Converted three physical systems to VM's:  ** lsst-nagios  *** Problems with software mirror raids.  **** VMware converter does not see software raided drives.  **** Split the raid 1 into discrete drives.  **** Chose sda to modify - failed as sad was faulty.  *** Built new Centos 6.6 VM  **** Used CrashPlan to rebuild.  ** lsst7 - converted after learning how to convert system to a fixed IP subnet  ** lsst8 - converted  *** Debugged lsst8 system migration to vmware.  Partition table was invalid and was preventing move  * Finished moving all VMs to new lsst-vsphere infrastructure,6,DM-4463,datamanagement,new equipment setup configuration week end 11/14/15 received dell idrac license upgrade new dell r730 server received configure vmware vsphere license cdw aura convert physical system vm lsst nagio problem software mirror raid vmware converter software raid drive split raid discrete drive choose sda modify fail sad faulty build new centos 6.6 vm crashplan rebuild lsst7 convert learn convert system fix ip subnet lsst8 convert debugged lsst8 system migration vmware partition table invalid prevent finish move vms new lsst vsphere infrastructure,New equipment setup and configuration (week end 11/14/15) * Received Dell iDrac license upgrades for new Dell R730 servers * Received and configured VMware vSphere licenses from CDW-G & AURA * Converted three physical systems to VM's: ** lsst-nagios *** Problems with software mirror raids. **** VMware converter does not see software raided drives. **** Split the raid 1 into discrete drives. **** Chose sda to modify - failed as sad was faulty. *** Built new Centos 6.6 VM **** Used CrashPlan to rebuild. ** lsst7 - converted after learning how to convert system to a fixed IP subnet ** lsst8 - converted *** Debugged lsst8 system migration to vmware. Partition table was invalid and was preventing move * Finished moving all VMs to new lsst-vsphere infrastructure
"New equipment setup, configuration, and regular monthly maintenance (week end 11/21/15) * Virtualized physical system lsst-xfer  * Worked with bmather or dell Dirac licensing issue  * Cleaned up old and new hosts in NCSA DNS, Nagios monitoring, and Qualys vulnerability scanner  * Completed connections for six UPS",2,DM-4464,datamanagement,new equipment setup configuration regular monthly maintenance week end 11/21/15 virtualize physical system lsst xfer work bmather dell dirac licensing issue clean old new host ncsa dns nagios monitoring qualys vulnerability scanner complete connection ups,"New equipment setup, configuration, and regular monthly maintenance (week end 11/21/15) * Virtualized physical system lsst-xfer * Worked with bmather or dell Dirac licensing issue * Cleaned up old and new hosts in NCSA DNS, Nagios monitoring, and Qualys vulnerability scanner * Completed connections for six UPS"
New equipment setup and configuration (week end 11/28/15) * Obtained Dell iDRAC Enterprise licenses & upgraded 4 of the 13 servers  * Installed base VM templates for OS X 10.8-10.11,1,DM-4465,datamanagement,new equipment setup configuration week end 11/28/15 obtain dell idrac enterprise license upgrade 13 server instal base vm template os 10.8 10.11,New equipment setup and configuration (week end 11/28/15) * Obtained Dell iDRAC Enterprise licenses & upgraded 4 of the 13 servers * Installed base VM templates for OS X 10.8-10.11
Decommissioning old equipment (week end 11/14/15) * Shutdown 17 (all) old ESXi servers  * Shutdown 3 old condor servers,1,DM-4466,datamanagement,decommission old equipment week end 11/14/15 shutdown 17 old esxi server shutdown old condor server,Decommissioning old equipment (week end 11/14/15) * Shutdown 17 (all) old ESXi servers * Shutdown 3 old condor servers
"Decommissioning old equipment (week end 11/21/15) * Shutdown last 3 old condor servers  * Shutdown lsst-netem, lsst-ps, & lsst-ps-base servers  * Surplussed equipment:  ** NCSA servers ( 5 Dell 1950's, 2 Dell 2950) repurposed from A22 to C17  ** Moved blade chassis to C20  * Move lsst-test systems in A23",2,DM-4467,datamanagement,decommission old equipment week end 11/21/15 shutdown old condor server shutdown lsst netem lsst ps lsst ps base server surplusse equipment ncsa server dell 1950 dell 2950 repurpose a22 c17 moved blade chassis c20 lsst test system a23,"Decommissioning old equipment (week end 11/21/15) * Shutdown last 3 old condor servers * Shutdown lsst-netem, lsst-ps, & lsst-ps-base servers * Surplussed equipment: ** NCSA servers ( 5 Dell 1950's, 2 Dell 2950) repurposed from A22 to C17 ** Moved blade chassis to C20 * Move lsst-test systems in A23"
Write DM Git LFS Guide Refactor words from DM-4412 into a top-level Doc page for using Git LFS. This will leave DM-4412 more as a Collaboration Workflow document.,1,DM-4469,datamanagement,write dm git lfs guide refactor word dm-4412 level doc page git lfs leave dm-4412 collaboration workflow document,Write DM Git LFS Guide Refactor words from DM-4412 into a top-level Doc page for using Git LFS. This will leave DM-4412 more as a Collaboration Workflow document.
"Consulting in November Review of design documents, correspondence with the design team regarding Data Center details and floor space, and conferences via web links.",4,DM-4471,datamanagement,consult november review design document correspondence design team data center detail floor space conference web link,"Consulting in November Review of design documents, correspondence with the design team regarding Data Center details and floor space, and conferences via web links."
Improvements to logging in xrootd Improve logging in xrootd to make it more compatible with qserv logging.,6,DM-4473,datamanagement,improvement log xrootd improve log xrootd compatible qserv logging,Improvements to logging in xrootd Improve logging in xrootd to make it more compatible with qserv logging.
"FITS Visualizer porting: Show FITS Header Task involves several steps:    * Server side: VisServerCommands.Header needs to change to check to the JSON_DEEP parameter. In this case the return from  VisServerOps.getFitsHeaderInfo should be converted into a format that the new javascript tables should understand (see loi how to get this format). Look at VisServerCommands.AreaStat for an example.  * Client side: a call to the server: need to add getFitsHeaderInfo into PlotServiceJson.js. For reference, look at other calls in  PlotServiceJson.js and the java version of the getFitsHeaderInfo PlotServiceJson.java  * Client side: when header toolbar button pushed then make the call to the server.  * Client side: when server call promise is resolved then show a dialog with the table data. Remember 3 color images should have a tab per color.",6,DM-4494,datamanagement,fit visualizer porting fit header task involve step server visservercommand header need change check json_deep parameter case return visserverops.getfitsheaderinfo convert format new javascript table understand loi format look visservercommand areastat example client server need add getfitsheaderinfo plotservicejson.js reference look call plotservicejson.js java version getfitsheaderinfo plotservicejson.java client header toolbar button push server client server promise resolve dialog table datum remember color image tab color,"FITS Visualizer porting: Show FITS Header Task involves several steps: * Server side: VisServerCommands.Header needs to change to check to the JSON_DEEP parameter. In this case the return from VisServerOps.getFitsHeaderInfo should be converted into a format that the new javascript tables should understand (see loi how to get this format). Look at VisServerCommands.AreaStat for an example. * Client side: a call to the server: need to add getFitsHeaderInfo into PlotServiceJson.js. For reference, look at other calls in PlotServiceJson.js and the java version of the getFitsHeaderInfo PlotServiceJson.java * Client side: when header toolbar button pushed then make the call to the server. * Client side: when server call promise is resolved then show a dialog with the table data. Remember 3 color images should have a tab per color."
"FITS Visualizer porting: selecting points of catalog from image view, showing selected points able to draw a rectangle on the image, and select the catalog entries overlaid on the image",5,DM-4503,datamanagement,fit visualizer porting select point catalog image view show select point able draw rectangle image select catalog entry overlay image,"FITS Visualizer porting: selecting points of catalog from image view, showing selected points able to draw a rectangle on the image, and select the catalog entries overlaid on the image"
"FITS Visualizer porting: Image Select Panel/Dialog Converting the image select dialog/panel is a very big job and should be to be broken up into several tickets: Each ticket should reference this ticket as the base.    Panel includes the following:  * issa, 2mass, wise, dss, sdss tabs  * file upload tab, upload widget might have to be written  * url tab  * blank image tab  * target info reusable widget  * 3 color support - any panel should show for 3 times, for read, green, and blue in 3 color mode  * must be able to appear in a panel or dialog  * must add or modify a plot  * Allow to create version with most or less than the standard tabs. example - see existing wise 3 color or finder chart 3 color  * A plot might need to be tied to specific type of image select dialog, we need a way to tie a plotId to and non-standard image select panel.",1,DM-4504,datamanagement,fit visualizer porting image select panel dialog convert image select dialog panel big job break ticket ticket reference ticket base panel include following issa 2mass wise dss sdss tab file upload tab upload widget write url tab blank image tab target info reusable widget color support panel time read green blue color mode able appear panel dialog add modify plot allow create version standard tab example exist wise color finder chart color plot need tie specific type image select dialog need way tie plotid non standard image select panel,"FITS Visualizer porting: Image Select Panel/Dialog Converting the image select dialog/panel is a very big job and should be to be broken up into several tickets: Each ticket should reference this ticket as the base. Panel includes the following: * issa, 2mass, wise, dss, sdss tabs * file upload tab, upload widget might have to be written * url tab * blank image tab * target info reusable widget * 3 color support - any panel should show for 3 times, for read, green, and blue in 3 color mode * must be able to appear in a panel or dialog * must add or modify a plot * Allow to create version with most or less than the standard tabs. example - see existing wise 3 color or finder chart 3 color * A plot might need to be tied to specific type of image select dialog, we need a way to tie a plotId to and non-standard image select panel."
Experimentation and testing of new SuperTask infrastructure WBS deliberately left unset as this is tracking LOE work.,6,DM-4508,datamanagement,experimentation testing new supertask infrastructure wbs deliberately leave unset track loe work,Experimentation and testing of new SuperTask infrastructure WBS deliberately left unset as this is tracking LOE work.
Migrate prototype SuperTask code to upstream repository The prototype SuperTask code is now ready to be moved from the experimental repo to the upstream {{pipe_base}} repo. This requires the commits to be squashed and tidied.,6,DM-4509,datamanagement,migrate prototype supertask code upstream repository prototype supertask code ready move experimental repo upstream pipe_base repo require commit squashed tidy,Migrate prototype SuperTask code to upstream repository The prototype SuperTask code is now ready to be moved from the experimental repo to the upstream {{pipe_base}} repo. This requires the commits to be squashed and tidied.
makeDocs uses old style python {{makeDocs}} is written in python 2.4 style. This ticket is for updating it to python 2.7.,1,DM-4510,datamanagement,makedocs use old style python makedocs write python 2.4 style ticket update python 2.7,makeDocs uses old style python {{makeDocs}} is written in python 2.4 style. This ticket is for updating it to python 2.7.
"Improve reStructuredText documentation Enhance docs by covering    - Images as links  - Table spans  - Abbreviations  - :file: semantics, etc.",2,DM-4511,datamanagement,improve restructuredtext documentation enhance doc cover images link table span abbreviations file semantic etc,"Improve reStructuredText documentation Enhance docs by covering - Images as links - Table spans - Abbreviations - :file: semantics, etc."
Quoting of paths in doxygen configuration files breaks makeDocs In DM-3200 I modified {{sconsUtils}} such that all the paths used in {{doxygen.conf}} files were quoted so that spaces could be used. This change broke documentation building (DM-4310) because {{makeDocs}} did not expect double quotes to be relevant. This ticket is to fix {{makeDocs}} and to re-enable quoting of paths in config files.,4,DM-4513,datamanagement,quoting path doxygen configuration file break dm-3200 modify sconsutils path doxygen.conf file quote space change break documentation building dm-4310 makedocs expect double quote relevant ticket fix makedocs enable quoting path config file,Quoting of paths in doxygen configuration files breaks makeDocs In DM-3200 I modified {{sconsUtils}} such that all the paths used in {{doxygen.conf}} files were quoted so that spaces could be used. This change broke documentation building (DM-4310) because {{makeDocs}} did not expect double quotes to be relevant. This ticket is to fix {{makeDocs}} and to re-enable quoting of paths in config files.
"Assess DECam ISR up to currently implemented Not all known ISR corrections are applied or implemented to DECam data yet. For example, no cross-talk, edge-bleed, non-linearity, sky pattern removal, satellite trail masking, brighter-fatter, or illumination correction.    But we have most of the basic ISR already. With what we already have, identify issues that would severely affect the quality of post-ISR processing.",3,DM-4514,datamanagement,assess decam isr currently implement know isr correction apply implement decam datum example cross talk edge bleed non linearity sky pattern removal satellite trail masking bright fat illumination correction basic isr identify issue severely affect quality post isr processing,"Assess DECam ISR up to currently implemented Not all known ISR corrections are applied or implemented to DECam data yet. For example, no cross-talk, edge-bleed, non-linearity, sky pattern removal, satellite trail masking, brighter-fatter, or illumination correction. But we have most of the basic ISR already. With what we already have, identify issues that would severely affect the quality of post-ISR processing."
"Flag out the glowing edges of DECam CCDs Pixels near the edges of the DECam CCDs are bigger/brighter and correcting them is not trivial. One way to move forward is to mask them out.      DESDM and CP mask 15 pixels on each edge.  The cut was later raised to 25 pixels, with the inner 10 pixels flagged as SUSPECT.  ",5,DM-4515,datamanagement,flag glow edge decam ccds pixels near edge decam ccds big bright correct trivial way forward mask desdm cp mask 15 pixel edge cut later raise 25 pixel inner 10 pixel flagged suspect,"Flag out the glowing edges of DECam CCDs Pixels near the edges of the DECam CCDs are bigger/brighter and correcting them is not trivial. One way to move forward is to mask them out. DESDM and CP mask 15 pixels on each edge. The cut was later raised to 25 pixels, with the inner 10 pixels flagged as SUSPECT."
Fix startup.py inside Docker container qserv tag should be replace with qserv_latest,1,DM-4523,datamanagement,fix startup.py inside docker container qserv tag replace qserv_latest,Fix startup.py inside Docker container qserv tag should be replace with qserv_latest
"Ops Planning and TOWG attendance - November TOWG meetings, review service catalog as input to LOPT, review draft of operations WBS with Don and Athol",2,DM-4525,datamanagement,ops planning towg attendance november towg meeting review service catalog input lopt review draft operation wbs don athol,"Ops Planning and TOWG attendance - November TOWG meetings, review service catalog as input to LOPT, review draft of operations WBS with Don and Athol"
"Facility Coordination Meeting and JCC meeting @ NCSA Day 1 : Facility Coordination Day with Argonne, CC-IN2P3, NERSC, and NCSA  Day 2: Extended JCC meeting    Notes posted on Confluence: https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=JCC&title=Extended+JCC+meeting+--+2015-11-23 ",4,DM-4527,datamanagement,facility coordination meeting jcc meeting ncsa day facility coordination day argonne cc in2p3 nersc ncsa day extend jcc meeting note post confluence https://confluence.lsstcorp.org/pages/viewpage.action?spacekey=jcc&title=extended+jcc+meeting+--+2015-11-23,"Facility Coordination Meeting and JCC meeting @ NCSA Day 1 : Facility Coordination Day with Argonne, CC-IN2P3, NERSC, and NCSA Day 2: Extended JCC meeting Notes posted on Confluence: https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=JCC&title=Extended+JCC+meeting+--+2015-11-23"
"Compilation errors from CLang (Apple LLVM 7.0) in XCode 7 on MacOSX Compiling on MacOSX Yosemite with XCode 7, a number of files fail compilation.  ----  {{core/modules/util/EventThread.h,cc}} fails because {{uint}} is used as a data type.  This is non-standard (though some compilers support it), and should be replaced with {{unsigned int}}.  ----  {{core/modules/wbase/SendChannel.h,cc}} fails because {{#include <functional>}} is missing.  ----  {{core/modules/wsched/ChunkState.cc}} fails because {{#include <iostream>}} is missing.  ----  {{build/qmeta/qmetaLib_wrap.cc}} (generated by SWIG) fails with many errors because the {{typedef unsigned long int uint64_t}} included in {{qmetaLib.i}} conflicts with MacOSX's typedef of it as {{unsigned long long}}.",1,DM-4529,datamanagement,compilation error clang apple llvm 7.0 xcode macosx compiling macosx yosemite xcode number file fail compilation core module util eventthread.h cc fail uint data type non standard compiler support replace unsigned int core module wbase sendchannel.h cc fail include miss core module wsche chunkstate.cc fail include miss build qmeta qmetalib_wrap.cc generate swig fail error typedef unsigned long int uint64_t include qmetalib.i conflict macosx typedef unsigned long long,"Compilation errors from CLang (Apple LLVM 7.0) in XCode 7 on MacOSX Compiling on MacOSX Yosemite with XCode 7, a number of files fail compilation. ---- {{core/modules/util/EventThread.h,cc}} fails because {{uint}} is used as a data type. This is non-standard (though some compilers support it), and should be replaced with {{unsigned int}}. ---- {{core/modules/wbase/SendChannel.h,cc}} fails because {{#include }} is missing. ---- {{core/modules/wsched/ChunkState.cc}} fails because {{#include }} is missing. ---- {{build/qmeta/qmetaLib_wrap.cc}} (generated by SWIG) fails with many errors because the {{typedef unsigned long int uint64_t}} included in {{qmetaLib.i}} conflicts with MacOSX's typedef of it as {{unsigned long long}}."
shellcheck linting of lsstsw bash scripts This issue is to recover a branch from DM-4113 that was not merged due to issues with installing shellcheck under travis.,1,DM-4534,datamanagement,shellcheck linte lsstsw bash script issue recover branch dm-4113 merge issue instal shellcheck travis,shellcheck linting of lsstsw bash scripts This issue is to recover a branch from DM-4113 that was not merged due to issues with installing shellcheck under travis.
Create release notes docs template Create a template for release notes/other release-type documentation in the new docs.,2,DM-4536,datamanagement,create release note docs template create template release note release type documentation new doc,Create release notes docs template Create a template for release notes/other release-type documentation in the new docs.
"Research simulation tools We need to do simulations of DCR and other effects when determining appropriate mitigation techniques.  This will require settling on a simulation tool for doing this.    The obvious choices are: phosim, galsim, and a roll your own solution.  Look into which of these is the most reasonable choice and make a recommendation.",4,DM-4538,datamanagement,research simulation tool need simulation dcr effect determine appropriate mitigation technique require settle simulation tool obvious choice phosim galsim roll solution look reasonable choice recommendation,"Research simulation tools We need to do simulations of DCR and other effects when determining appropriate mitigation techniques. This will require settling on a simulation tool for doing this. The obvious choices are: phosim, galsim, and a roll your own solution. Look into which of these is the most reasonable choice and make a recommendation."
Roll Qserv into SQuaRE release - part I Improvements to codekit to support release process. ,4,DM-4540,datamanagement,roll qserv square release improvement codekit support release process,Roll Qserv into SQuaRE release - part I Improvements to codekit to support release process.
Revisit short and long term plans for butler Revisit short and long term requirements and needs and capture it through stories.,5,DM-4544,datamanagement,revisit short long term plan butler revisit short long term requirement need capture story,Revisit short and long term plans for butler Revisit short and long term requirements and needs and capture it through stories.
"Review of storage quotes Reviewed 4 products from 3 vendors with storage group. Prices and features comparison along with discussions on whether to integrate into condo or not. Also, discussed security considerations.    Preliminary vendor/product chosen. Follow up questions sent to vendor. Will review with storage before we are able to purchase.",2,DM-4545,datamanagement,review storage quote review product vendor storage group price feature comparison discussion integrate condo discuss security consideration preliminary vendor product choose follow question send vendor review storage able purchase,"Review of storage quotes Reviewed 4 products from 3 vendors with storage group. Prices and features comparison along with discussions on whether to integrate into condo or not. Also, discussed security considerations. Preliminary vendor/product chosen. Follow up questions sent to vendor. Will review with storage before we are able to purchase."
"Update to Sizing Model Removed 'memory effectiveness' factor as it was already included in compute efficiency  Updated capacity / tape to follow LTO roadmap  Updated bandwidth / tape drive to follow LTO roadmap  Changed ""tape!Number of new tapes purchased"" to round up to an even number for the archive site  Changed ""tape!Number of new tapes needed"" to round up for both sites  Changed ""tape!Number of tape drives needed based on bandwidth"" to round up for both sites  Fixed 'tape!tape bandwidth' colums to take into account mixed tape drive types  Updated 'tape!HPSS' to take into account retiring mover nodes    Also began work with Spectra Logic to further improve tape predictions.",2,DM-4546,datamanagement,update sizing model removed memory effectiveness factor include compute efficiency updated capacity tape follow lto roadmap updated bandwidth tape drive follow lto roadmap change tape!number new tape purchase round number archive site change tape!number new tape need round site change tape!number tape drive need base bandwidth round site fix tape!tape bandwidth colum account mixed tape drive type update tape!hpss account retire mover node begin work spectra logic improve tape prediction,"Update to Sizing Model Removed 'memory effectiveness' factor as it was already included in compute efficiency Updated capacity / tape to follow LTO roadmap Updated bandwidth / tape drive to follow LTO roadmap Changed ""tape!Number of new tapes purchased"" to round up to an even number for the archive site Changed ""tape!Number of new tapes needed"" to round up for both sites Changed ""tape!Number of tape drives needed based on bandwidth"" to round up for both sites Fixed 'tape!tape bandwidth' colums to take into account mixed tape drive types Updated 'tape!HPSS' to take into account retiring mover nodes Also began work with Spectra Logic to further improve tape predictions."
"Power requirements and LSST footprint at NCSA Finalized power requirements with the UofI engineer. I plan to distribute verification compute across multiple racks in order to reduce per-rack power requirements and reduce per-rack network port counts. This allows us to drop from 3x 60A for the verification cluster to 3x 30A which is the same for the other racks. This will result in some cost savings and simpler planning for future use of the racks.    Also, provided a plan for LSST's footprint in NPCF until 2032. We now have space reserved from the south side of Roger to the north side of Blue Waters. This space should be very visible from the room camera I believe. ",2,DM-4547,datamanagement,power requirement lsst footprint ncsa finalize power requirement uofi engineer plan distribute verification compute multiple rack order reduce rack power requirement reduce rack network port count allow drop 3x 60a verification cluster 3x 30a rack result cost saving simple planning future use rack provide plan lsst footprint npcf 2032 space reserve south roger north blue waters space visible room camera believe,"Power requirements and LSST footprint at NCSA Finalized power requirements with the UofI engineer. I plan to distribute verification compute across multiple racks in order to reduce per-rack power requirements and reduce per-rack network port counts. This allows us to drop from 3x 60A for the verification cluster to 3x 30A which is the same for the other racks. This will result in some cost savings and simpler planning for future use of the racks. Also, provided a plan for LSST's footprint in NPCF until 2032. We now have space reserved from the south side of Roger to the north side of Blue Waters. This space should be very visible from the room camera I believe."
"Fix docker workflow Some issues where discovered while trying to package DM-2699 in Docker (for IN2P3 cluster deployment), they're fixed here.    - apt-get update times out: why?  - git clone then pull is too weak (if building the first clone fails, pull never occurs) => step merged  - eupspkg -er build creates lib/python in /qserv/stack/.../qserv/... and next install can't remove it for unknow reason => build and install merged.",2,DM-4556,datamanagement,fix docker workflow issue discover try package dm-2699 docker in2p3 cluster deployment fix apt update time git clone pull weak build clone fail pull occur step merge eupspkg build create lib python stack/ /qserv/ install remove unknow reason build install merge,"Fix docker workflow Some issues where discovered while trying to package DM-2699 in Docker (for IN2P3 cluster deployment), they're fixed here. - apt-get update times out: why? - git clone then pull is too weak (if building the first clone fails, pull never occurs) => step merged - eupspkg -er build creates lib/python in /qserv/stack/.../qserv/... and next install can't remove it for unknow reason => build and install merged."
Bug fix and improvement for DECam processing - Bug fix in DecamMapper policy of fringe dataset  - Improve readme documentation about ingesting and processing raw data  - Bug fix on translating Community Pipeline's Bad Pixel Mask (BPM) --- Previously in DM-4191 I looked up the wrong table for the BPM bit definition.  - Flag the potentially problematic edge pixels as SUSPECT (DM-4515)  - Add data products for coaddition processing,6,DM-4559,datamanagement,bug fix improvement decam processing bug fix decammapper policy fringe dataset improve readme documentation ingest process raw datum bug fix translate community pipeline bad pixel mask bpm previously dm-4191 look wrong table bpm bit definition flag potentially problematic edge pixel suspect dm-4515 add datum product coaddition processing,Bug fix and improvement for DECam processing - Bug fix in DecamMapper policy of fringe dataset - Improve readme documentation about ingesting and processing raw data - Bug fix on translating Community Pipeline's Bad Pixel Mask (BPM) --- Previously in DM-4191 I looked up the wrong table for the BPM bit definition. - Flag the potentially problematic edge pixels as SUSPECT (DM-4515) - Add data products for coaddition processing
Table (JS): selection feature. This task is composed of:  - converting java class SelectionInfo.  - reducing data into its table model state  - rendering SelectionInfo onto the TablePanel  - creating action and action creator,6,DM-4565,datamanagement,table js selection feature task compose convert java class selectioninfo reduce datum table model state render selectioninfo tablepanel create action action creator,Table (JS): selection feature. This task is composed of: - converting java class SelectionInfo. - reducing data into its table model state - rendering SelectionInfo onto the TablePanel - creating action and action creator
"Upgrade to the latest react-highcharts library We need to upgrade from the early version of react-highcharts to the latest one, compatible with React 0.14.3. Just switching to the new library does not work, need to resolve issues.",6,DM-4566,datamanagement,upgrade late react highchart library need upgrade early version react highchart late compatible react 0.14.3 switch new library work need resolve issue,"Upgrade to the latest react-highcharts library We need to upgrade from the early version of react-highcharts to the latest one, compatible with React 0.14.3. Just switching to the new library does not work, need to resolve issues."
Table (JS): sorting This task is composed of:  - introduce sorting feature into TablePanel  - creating action and action creator  - reducing data into its table model state,4,DM-4568,datamanagement,table js sort task compose introduce sort feature tablepanel create action action creator reduce datum table model state,Table (JS): sorting This task is composed of: - introduce sorting feature into TablePanel - creating action and action creator - reducing data into its table model state
"Table (JS): filtering This task is composed of:  - adding filter toolbar into TablePanel  - filter validation syntax  - creating action, action creator, and reducing data into its table model state  - -generating meta info for enumerated columns-  not sure if we wanted this.    Also, added actOn feature to FieldInput.",6,DM-4570,datamanagement,table js filter task compose add filter toolbar tablepanel filter validation syntax create action action creator reduce datum table model state -generate meta info enumerate columns- sure want add acton feature fieldinput,"Table (JS): filtering This task is composed of: - adding filter toolbar into TablePanel - filter validation syntax - creating action, action creator, and reducing data into its table model state - -generating meta info for enumerated columns- not sure if we wanted this. Also, added actOn feature to FieldInput."
"Suggestion Box widget We need to find or implement a suggestion box widget in JS. Currently, it is used to suggest table column names in XY plot and in some forms.",6,DM-4571,datamanagement,suggestion box widget need find implement suggestion box widget js currently suggest table column name xy plot form,"Suggestion Box widget We need to find or implement a suggestion box widget in JS. Currently, it is used to suggest table column names in XY plot and in some forms."
"Table (JS): table options This task is composed of:  - adding table options panel to TablePanel.  - providing features:    - show/hide units in header    - show/hide columns, reset to defaults, etc    - page size",5,DM-4572,datamanagement,table js table option task compose add table option panel tablepanel provide feature hide unit header hide column reset default etc page size,"Table (JS): table options This task is composed of: - adding table options panel to TablePanel. - providing features: - show/hide units in header - show/hide columns, reset to defaults, etc - page size"
JS expression parsing library Since we are allowing column expressions we need a way to validate them on client side.,6,DM-4573,datamanagement,js expression parse library allow column expression need way validate client,JS expression parsing library Since we are allowing column expressions we need a way to validate them on client side.
Table (JS): text view This task is composed of:  - adding text view option to TablePanel,2,DM-4574,datamanagement,table js text view task compose add text view option tablepanel,Table (JS): text view This task is composed of: - adding text view option to TablePanel
"XY Plot view of the table (JS): define state tree Define state tree, actions, and reducers for XY Plot view of the table.",4,DM-4575,datamanagement,xy plot view table js define state tree define state tree action reducer xy plot view table,"XY Plot view of the table (JS): define state tree Define state tree, actions, and reducers for XY Plot view of the table."
XY Scatter Plot (JS)  Implement basic scatter plot widget using react-highcharts library,8,DM-4576,datamanagement,xy scatter plot js implement basic scatter plot widget react highchart library,XY Scatter Plot (JS) Implement basic scatter plot widget using react-highcharts library
"XY Plot view of a table (JS) - Toolbar Toolbar, which toggles plot options, selection and filter buttons    Extra:   - handling zoom from the toolbar rather than using built-in zoom  - ability to switch between histogram and scatter plot view",8,DM-4580,datamanagement,xy plot view table js toolbar toolbar toggle plot option selection filter button extra handle zoom toolbar build zoom ability switch histogram scatter plot view,"XY Plot view of a table (JS) - Toolbar Toolbar, which toggles plot options, selection and filter buttons Extra: - handling zoom from the toolbar rather than using built-in zoom - ability to switch between histogram and scatter plot view"
"XY Plot View of a table (JS) - selection support Show/change selected/highlighted points. Ideally, this should be done without redrawing the whole plot. ",8,DM-4582,datamanagement,xy plot view table js selection support change select highlighted point ideally redrawe plot,"XY Plot View of a table (JS) - selection support Show/change selected/highlighted points. Ideally, this should be done without redrawing the whole plot."
"SUIT: search returning images in a directory - Create a sample search processor, which returns images in a given directory.  - It should be using an external python task  - Update search form configuration to use this search processor to return image metadata",2,DM-4583,datamanagement,suit search return image directory create sample search processor return image give directory external python task update search form configuration use search processor return image metadata,"SUIT: search returning images in a directory - Create a sample search processor, which returns images in a given directory. - It should be using an external python task - Update search form configuration to use this search processor to return image metadata"
XY plot view of a table (JS) - density plot zoom support Density plot zooming requires server call.,6,DM-4584,datamanagement,xy plot view table js density plot zoom support density plot zooming require server,XY plot view of a table (JS) - density plot zoom support Density plot zooming requires server call.
XY Plot view of a table (JS) - density plot selection support (?) density plot - how do we support selection?    (In current version we turn off selection support when the plot is density plot),6,DM-4585,datamanagement,xy plot view table js density plot selection support density plot support selection current version turn selection support plot density plot,XY Plot view of a table (JS) - density plot selection support (?) density plot - how do we support selection? (In current version we turn off selection support when the plot is density plot)
"GWT Conversion: Login This task is composed of:  - adding user info into banner    - includes user name and links for login, logout, and profile.  - convert server-side code to return json  - use messaging to handle current user state.    - depends on DM-4578	Integrate websocket messaging into flux",4,DM-4586,datamanagement,gwt conversion login task compose add user info banner include user link login logout profile convert server code return json use messaging handle current user state depend dm-4578 integrate websocket message flux,"GWT Conversion: Login This task is composed of: - adding user info into banner - includes user name and links for login, logout, and profile. - convert server-side code to return json - use messaging to handle current user state. - depends on DM-4578 Integrate websocket messaging into flux"
"GWT Conversion: basic layout for results This task is composed of:  - creating a results container that handle the layout of its components  - define and load layout info into application state  - creating action, action creator, and reducing functions  - components include:    - vis toolbar    - last searched description    - layout options: tri-view.  side-by-side, single and popout can be added at a later time.    - tables, image plots, xy plots.  - depends on DM-4590: GWT Conversion: advance resizable layout panel",4,DM-4589,datamanagement,gwt conversion basic layout result task compose create result container handle layout component define load layout info application state create action action creator reduce function component include vis toolbar search description layout option tri view single popout add later time table image plot xy plot depend dm-4590 gwt conversion advance resizable layout panel,"GWT Conversion: basic layout for results This task is composed of: - creating a results container that handle the layout of its components - define and load layout info into application state - creating action, action creator, and reducing functions - components include: - vis toolbar - last searched description - layout options: tri-view. side-by-side, single and popout can be added at a later time. - tables, image plots, xy plots. - depends on DM-4590: GWT Conversion: advance resizable layout panel"
GWT Conversion: advance resizable layout panel Create an advance React component for layout.  Features should include:  - a set of predefined layouts  - resize strategies  - generic for reuse,6,DM-4590,datamanagement,gwt conversion advance resizable layout panel create advance react component layout feature include set predefine layout resize strategy generic reuse,GWT Conversion: advance resizable layout panel Create an advance React component for layout. Features should include: - a set of predefined layouts - resize strategies - generic for reuse
"GWT conversion: System notifications This task is composed of:  - adding notification panel to the application  - convert server-side code to use messaging for notifications  - use messaging on client-side to handle notifications  - creating action, action creator, and reducing functions  - depends on DM-4578	Integrate websocket messaging into flux  ",3,DM-4591,datamanagement,gwt conversion system notification task compose add notification panel application convert server code use message notification use messaging client handle notification create action action creator reduce function depend dm-4578 integrate websocket message flux,"GWT conversion: System notifications This task is composed of: - adding notification panel to the application - convert server-side code to use messaging for notifications - use messaging on client-side to handle notifications - creating action, action creator, and reducing functions - depends on DM-4578 Integrate websocket messaging into flux"
GWT Conversion: History and routing First pass at the implementation of history and routing.  Define a framework in which the application can be:  - bookmarked  - record state in history  - retore application from a url  ,4,DM-4595,datamanagement,gwt conversion history route pass implementation history routing define framework application bookmarke record state history retore application url,GWT Conversion: History and routing First pass at the implementation of history and routing. Define a framework in which the application can be: - bookmarked - record state in history - retore application from a url
"Remove deprecated versions of warpExposure and warpImage afw.math supports two templated variants of warpExposure and warpImage, one that takes a warping control and the other which does not. The latter have been deprecated for a long time and are no longer used. I think it is time to remove them.",1,DM-4596,datamanagement,remove deprecate version warpexposure warpimage afw.math support template variant warpexposure warpimage take warping control deprecate long time long think time remove,"Remove deprecated versions of warpExposure and warpImage afw.math supports two templated variants of warpExposure and warpImage, one that takes a warping control and the other which does not. The latter have been deprecated for a long time and are no longer used. I think it is time to remove them."
"Build docs.lsst.io Doc Index Page Create an HTML landing page for all DM documentation/documents    - Software Documentation  - Developer Guides  - Requirement and Design Documentation  - Technotes  - Papers  - Presentations    The page will be implemented as a static site. The page build will be template driven, with content scraped from the metadata.yaml resources of technotes (among other sources).    Since this is the first SQuaRE web project, this ticket will also involve effort in establishing a CSS+HTML pattern library and Gulp-based development workflows. Long term, this investment will be returned with new dm.lsst.org, technote and Sphinx documentation web designs.",2,DM-4601,datamanagement,build docs.lsst.io doc index page create html landing page dm documentation document software documentation developer guides requirement design documentation technotes papers presentations page implement static site page build template drive content scrape metadata.yaml resource technote source square web project ticket involve effort establish css+html pattern library gulp base development workflow long term investment return new dm.lsst.org technote sphinx documentation web design,"Build docs.lsst.io Doc Index Page Create an HTML landing page for all DM documentation/documents - Software Documentation - Developer Guides - Requirement and Design Documentation - Technotes - Papers - Presentations The page will be implemented as a static site. The page build will be template driven, with content scraped from the metadata.yaml resources of technotes (among other sources). Since this is the first SQuaRE web project, this ticket will also involve effort in establishing a CSS+HTML pattern library and Gulp-based development workflows. Long term, this investment will be returned with new dm.lsst.org, technote and Sphinx documentation web designs."
sconsUtils tests should depend on shebang target Some tests rely on code in the {{bin}} directory. Whilst these tests have been modified to use {{bin.src}} the general feeling is that the test code should be able to rely upon the {{shebang}} target having been executed before they are run.,1,DM-4603,datamanagement,sconsutil test depend shebang target test rely code bin directory whilst test modify use bin.src general feeling test code able rely shebang target having execute run,sconsUtils tests should depend on shebang target Some tests rely on code in the {{bin}} directory. Whilst these tests have been modified to use {{bin.src}} the general feeling is that the test code should be able to rely upon the {{shebang}} target having been executed before they are run.
make codekit repos.yaml aware Up to now codekit assumed the repo name is the same as the eups package name. FIx it by using repos.yaml. ,1,DM-4604,datamanagement,codekit repos.yaml aware codekit assume repo eup package fix repos.yaml,make codekit repos.yaml aware Up to now codekit assumed the repo name is the same as the eups package name. FIx it by using repos.yaml.
Partition package should use the standard package layout The partition package does not build on OS X El Capitan because the package is not laid out in the standard manner and whilst {{sconsUtils}} is used most of the default behaviors are over-ridden. This means that fixes implemented for DM-3200 do not migrate over to {{partition}}. I think the best approach would be to reorganize the package so that it does build in the normal way.,1,DM-4609,datamanagement,partition package use standard package layout partition package build os el capitan package lay standard manner whilst sconsutils default behavior ride mean fix implement dm-3200 migrate partition think good approach reorganize package build normal way,Partition package should use the standard package layout The partition package does not build on OS X El Capitan because the package is not laid out in the standard manner and whilst {{sconsUtils}} is used most of the default behaviors are over-ridden. This means that fixes implemented for DM-3200 do not migrate over to {{partition}}. I think the best approach would be to reorganize the package so that it does build in the normal way.
"Research Kerberos and LDAP replication options IAM components, including the Kerberos KDC, need to be replicated between NCSA and Chile machine rooms. This may impact whether LSST can use NCSA's production Kerberos instance (if it supports selective replication) or needs a separate Kerberos instance that can be replicated outside NCSA. This task is to research and document the options, in consultation with NCSA Kerberos admins, and propose a Kerberos replication approach.",1,DM-4610,datamanagement,research kerberos ldap replication option iam component include kerberos kdc need replicate ncsa chile machine room impact lsst use ncsa production kerberos instance support selective replication need separate kerberos instance replicate outside ncsa task research document option consultation ncsa kerberos admin propose kerberos replication approach,"Research Kerberos and LDAP replication options IAM components, including the Kerberos KDC, need to be replicated between NCSA and Chile machine rooms. This may impact whether LSST can use NCSA's production Kerberos instance (if it supports selective replication) or needs a separate Kerberos instance that can be replicated outside NCSA. This task is to research and document the options, in consultation with NCSA Kerberos admins, and propose a Kerberos replication approach."
"Deploy heirarchical queuing to test image precedence Once we have a working workflow ready to move data between the ""base site"" and ""archive site"" (both at NCSA), deploy a base site exit router with stacked queuing to test prioritization of image traffic over various network conditions.",8,DM-4615,datamanagement,deploy heirarchical queue test image precedence work workflow ready datum base site archive site ncsa deploy base site exit router stacked queue test prioritization image traffic network condition,"Deploy heirarchical queuing to test image precedence Once we have a working workflow ready to move data between the ""base site"" and ""archive site"" (both at NCSA), deploy a base site exit router with stacked queuing to test prioritization of image traffic over various network conditions."
"Send all chunk-queries to primary copy of the chunk We are planning to distribute chunks / replicas across worker nodes such that each node will have a mix of primary copies for some chunks, and backup copies for some chunks. While doing shared scan, we are going to always rely on the primary chunks (e.g., all queries that need a given chunk should be sent to the same machine so that we read that chunks only once on one node). This story involves tweaking xrootd to ensure we don't send chunk-queries to nodes hosting non-primary copies.",5,DM-4617,datamanagement,send chunk query primary copy chunk plan distribute chunk replicas worker node node mix primary copy chunk backup copy chunk share scan go rely primary chunk e.g. query need give chunk send machine read chunk node story involve tweak xrootd ensure send chunk query node host non primary copy,"Send all chunk-queries to primary copy of the chunk We are planning to distribute chunks / replicas across worker nodes such that each node will have a mix of primary copies for some chunks, and backup copies for some chunks. While doing shared scan, we are going to always rely on the primary chunks (e.g., all queries that need a given chunk should be sent to the same machine so that we read that chunks only once on one node). This story involves tweaking xrootd to ensure we don't send chunk-queries to nodes hosting non-primary copies."
"Design for butler support of multiple repositories Work on design for Gregory/SUI's request:    We need to understand how put()/writing works when multiple repositories are made visible through a single Butler.  For get()/reading a single search order makes sense.  For put() it may be desirable to support alternative destinations (local disk, user workspace, Level 3 DB) or even multiple destinations for a single put().",6,DM-4625,datamanagement,design butler support multiple repository work design gregory sui request need understand put()/writing work multiple repository visible single butler get()/reade single search order make sense desirable support alternative destination local disk user workspace level db multiple destination single,"Design for butler support of multiple repositories Work on design for Gregory/SUI's request: We need to understand how put()/writing works when multiple repositories are made visible through a single Butler. For get()/reading a single search order makes sense. For put() it may be desirable to support alternative destinations (local disk, user workspace, Level 3 DB) or even multiple destinations for a single put()."
"Explore coadd processing with DECam data with default config Starting with raw DECam data, perform single frame processing and then try image coaddition with a few visits of images. ",6,DM-4628,datamanagement,explore coadd process decam datum default config start raw decam datum perform single frame processing try image coaddition visit image,"Explore coadd processing with DECam data with default config Starting with raw DECam data, perform single frame processing and then try image coaddition with a few visits of images."
"Create IDL pipeline workflow for DRP processing - processCcdDecam For the verification datasets work we need the ability to run a dataset all the way through DRP processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataIDs, creates individual log files, and creates helpful QA plots/metrics/webpages.  Nidever will use his PHOTRED IDL workflow and rewrite it for the stack.  The first step is processCcdDecam.",5,DM-4631,datamanagement,create idl pipeline workflow drp processing processccddecam verification dataset work need ability run dataset way drp processing take advantage core orchestration keep track successful fail dataid create individual log file create helpful qa plot metric webpage nidever use photred idl workflow rewrite stack step processccddecam,"Create IDL pipeline workflow for DRP processing - processCcdDecam For the verification datasets work we need the ability to run a dataset all the way through DRP processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataIDs, creates individual log files, and creates helpful QA plots/metrics/webpages. Nidever will use his PHOTRED IDL workflow and rewrite it for the stack. The first step is processCcdDecam."
"Create IDL pipeline workflow for DRP processing - makeCoaddTempExp For the verification datasets work we need the ability to run a dataset all the way through DRP processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataIDs, creates individual log files, and creates helpful QA plots/metrics/webpages. Nidever will use his PHOTRED IDL workflow and rewrite it for the stack.  processCcdDecam is working.  The next step is makeCoaddTempExp.  ",4,DM-4632,datamanagement,create idl pipeline workflow drp processing makecoaddtempexp verification dataset work need ability run dataset way drp processing take advantage core orchestration keep track successful fail dataid create individual log file create helpful qa plot metric webpage nidever use photred idl workflow rewrite stack processccddecam work step makecoaddtempexp,"Create IDL pipeline workflow for DRP processing - makeCoaddTempExp For the verification datasets work we need the ability to run a dataset all the way through DRP processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataIDs, creates individual log files, and creates helpful QA plots/metrics/webpages. Nidever will use his PHOTRED IDL workflow and rewrite it for the stack. processCcdDecam is working. The next step is makeCoaddTempExp."
"lsstsw should symlink afwdata or allow an envvar To reduce disk usage, it is very handy to be able to make build/afwdata and stack/afwdata/BLAH be symlinks. build/ is easy: just make the symlink and then never have touch it unless you rm your whole stack. stack/afwdata/BLAH is harder: each time you rebuild something that depends on afwdata, it will install a new copy of afwdata, which you'll have to manually remove and declare your symlink with a new tag.    A couple of ways to make this more automatic:     * lsstsw checks whether build/afwdata is a symlink, and if so just makes the new stack ""install"" a symlink to the same directory.   * Check for some environment variable (e.g. AFWDATA_BASE_DIR or something) and if that exists, just make a symlink to it, or make a dummy eups table that points to that directory and don't put anything in stack at all.",5,DM-4637,datamanagement,lsstsw symlink afwdata allow envvar reduce disk usage handy able build afwdata stack afwdata blah symlink build/ easy symlink touch rm stack stack afwdata blah hard time rebuild depend afwdata install new copy afwdata manually remove declare symlink new tag couple way automatic lsstsw check build afwdata symlink make new stack install symlink directory check environment variable e.g. afwdata_base_dir exist symlink dummy eup table point directory stack,"lsstsw should symlink afwdata or allow an envvar To reduce disk usage, it is very handy to be able to make build/afwdata and stack/afwdata/BLAH be symlinks. build/ is easy: just make the symlink and then never have touch it unless you rm your whole stack. stack/afwdata/BLAH is harder: each time you rebuild something that depends on afwdata, it will install a new copy of afwdata, which you'll have to manually remove and declare your symlink with a new tag. A couple of ways to make this more automatic: * lsstsw checks whether build/afwdata is a symlink, and if so just makes the new stack ""install"" a symlink to the same directory. * Check for some environment variable (e.g. AFWDATA_BASE_DIR or something) and if that exists, just make a symlink to it, or make a dummy eups table that points to that directory and don't put anything in stack at all."
"modernize afw code and reduce doxygen errors I would like to make some simple modernizations afw code and reduce doxygen warnings as much as practical. The modernizations I had in mind were:  - Move doc strings from .cc files to .h files and standardize their format  - Use {{namespace lsst { namespace afw { ...}} in .cc files to make the code easier to read  - Eliminate all {{<Class>::Ptr}} and {{<Class>::ConstPtr}} typedefs (replacing with {{PTR(<Class>)}} and {{CONST_PTR(<Class>)}}).  - Make sure .py files import the appropriate packages from future and (where practical) pass the flake8 linter    Regarding doxygen warnings: I think moving the documentation to .h files will help in many cases. Some warnings may be impractical to fix, such as complaining about not documenting ""cls"" for python class methods.  ",6,DM-4639,datamanagement,modernize afw code reduce doxygen error like simple modernization afw code reduce doxygen warning practical modernization mind doc string .cc file .h file standardize format use namespace lsst namespace afw .cc file code easy read eliminate ptr constptr typedef replace ptr const_ptr sure file import appropriate package future practical pass flake8 linter doxygen warning think move documentation .h file help case warning impractical fix complain document cls python class method,"modernize afw code and reduce doxygen errors I would like to make some simple modernizations afw code and reduce doxygen warnings as much as practical. The modernizations I had in mind were: - Move doc strings from .cc files to .h files and standardize their format - Use {{namespace lsst { namespace afw { ...}} in .cc files to make the code easier to read - Eliminate all {{::Ptr}} and {{::ConstPtr}} typedefs (replacing with {{PTR()}} and {{CONST_PTR()}}). - Make sure .py files import the appropriate packages from future and (where practical) pass the flake8 linter Regarding doxygen warnings: I think moving the documentation to .h files will help in many cases. Some warnings may be impractical to fix, such as complaining about not documenting ""cls"" for python class methods."
"Migrate scisql and mysqlproxy to mariadb MySQLproxy and SciSQL relies on MySQL, they should now move to MariaDB",4,DM-4642,datamanagement,migrate scisql mysqlproxy mariadb mysqlproxy scisql rely mysql mariadb,"Migrate scisql and mysqlproxy to mariadb MySQLproxy and SciSQL relies on MySQL, they should now move to MariaDB"
Add utility function to handle client-side download requests. Create utility function to handle client-side download requests.  It needs to be done in a way that does not mess with history and current page state.,1,DM-4643,datamanagement,add utility function handle client download request create utility function handle client download request need way mess history current page state,Add utility function to handle client-side download requests. Create utility function to handle client-side download requests. It needs to be done in a way that does not mess with history and current page state.
"Add workflow code to lsst-dm github Since we have split the code for Super Task, all the workflow code should  go in a different repository",2,DM-4644,datamanagement,add workflow code lsst dm github split code super task workflow code different repository,"Add workflow code to lsst-dm github Since we have split the code for Super Task, all the workflow code should go in a different repository"
"investigate replicating EUPS published packages (This ticket is for work that has already been done, per internal discussion in SQRE, but accidentally without an open ticket)    https://github.com/lsst-sqre/lsyncd-eupspkg  https://github.com/lsst-sqre/sandbox-pkg",4,DM-4647,datamanagement,investigate replicate eups publish package ticket work internal discussion sqre accidentally open ticket https://github.com/lsst-sqre/lsyncd-eupspkg https://github.com/lsst-sqre/sandbox-pkg,"investigate replicating EUPS published packages (This ticket is for work that has already been done, per internal discussion in SQRE, but accidentally without an open ticket) https://github.com/lsst-sqre/lsyncd-eupspkg https://github.com/lsst-sqre/sandbox-pkg"
"Support sqlalchemy use with qserv When one tries to connect to qserv using sqlalchemy there is an exception generated currently:  {noformat}  $ python -c 'import sqlalchemy; sqlalchemy.create_engine(""mysql+mysqldb://qsmaster@127.0.0.1:4040/test"").connect()'  /u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py:298: SAWarning: Exception attempting to detect unicode returns: InterfaceError(""(_mysql_exceptions.InterfaceError) (-1, 'error totally whack')"",)    ""detect unicode returns: %r"" % de)  Traceback (most recent call last):    File ""<string>"", line 1, in <module>    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2018, in connect      return self._connection_cls(self, **kwargs)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 72, in __init__      if connection is not None else engine.raw_connection()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2104, in raw_connection      self.pool.unique_connection, _connection)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2078, in _wrap_pool_connect      e, dialect, self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1405, in _handle_dbapi_exception_noconnection      exc_info    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 199, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2074, in _wrap_pool_connect      return fn()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 318, in unique_connection      return _ConnectionFairy._checkout(self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 713, in _checkout      fairy = _ConnectionRecord.checkout(pool)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 480, in checkout      rec = pool._do_get()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1060, in _do_get      self._dec_overflow()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 60, in __exit__      compat.reraise(exc_type, exc_value, exc_tb)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1057, in _do_get      return self._create_connection()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 323, in _create_connection      return _ConnectionRecord(self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 454, in __init__      exec_once(self.connection, self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 246, in exec_once      self(*args, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 256, in __call__      fn(*args, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 1312, in go      return once_fn(*arg, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/strategies.py"", line 165, in first_connect      dialect.initialize(c)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/dialects/mysql/base.py"", line 2626, in initialize      default.DefaultDialect.initialize(self, connection)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 256, in initialize      self._check_unicode_description(connection):    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 343, in _check_unicode_description      ]).compile(dialect=self)    File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute      self.errorhandler(self, exc, value)    File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler      raise errorclass, errorvalue  sqlalchemy.exc.InterfaceError: (_mysql_exceptions.InterfaceError) (-1, 'error totally whack')  {noformat}    The reason for that is that sqlalchemy generate few SELECT queries to figure out unicode support by the engine, and those selects are passed to qserv which cannot parse them. Here is the list of SELECTs which appears in proxy log:  {code:sql}  SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1  SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1  SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1  SELECT 'x' AS some_label  {code}",3,DM-4648,datamanagement,"support sqlalchemy use qserv try connect qserv sqlalchemy exception generate currently noformat python import sqlalchemy sqlalchemy.create_engine(""mysql+mysqldb://qsmaster@127.0.0.1:4040 test"").connect /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine default.py:298 sawarne exception attempt detect unicode return interfaceerror(""(_mysql_exception interfaceerror -1 error totally whack detect unicode return de traceback recent file line file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine base.py line 2018 connect return self._connection_cls(self kwargs file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine base.py line 72 init connection engine.raw_connection file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine base.py line 2104 raw_connection self.pool.unique_connection connection file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine base.py line 2078 wrap_pool_connect dialect self file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine base.py line 1405 handle_dbapi_exception_noconnection exc_info file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy util compat.py line 199 raise_from_cause reraise(type(exception exception tb exc_tb file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine base.py line 2074 wrap_pool_connect return fn file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy pool.py line 318 unique_connection return connectionfairy._checkout(self file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy pool.py line 713 checkout fairy connectionrecord.checkout(pool file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy pool.py line 480 checkout rec pool._do_get file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy pool.py line 1060 do_get self._dec_overflow file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy util langhelpers.py line 60 exit compat.reraise(exc_type exc_value exc_tb file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy pool.py line 1057 do_get return self._create_connection file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy pool.py line 323 create_connection return connectionrecord(self file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy pool.py line 454 init exec_once(self.connection self file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy event attr.py line 246 exec_once self(*args kw file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy event attr.py line 256 fn(*args kw file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy util langhelpers.py line 1312 return once_fn(*arg kw file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine strategies.py line 165 first_connect dialect.initialize(c file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy dialect mysql base.py line 2626 initialize default defaultdialect.initialize(self connection file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine default.py line 256 initialize self._check_unicode_description(connection file /u2 salnikov stack linux64 sqlalchemy/2015_10.0 lib python sqlalchemy-1.0.8 py2.7 linux x86_64.egg sqlalchemy engine default.py line 343 check_unicode_description self file /u2 salnikov stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb cursors.py line 174 execute self.errorhandler(self exc value file /u2 salnikov stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb connections.py line 36 defaulterrorhandler raise errorclass errorvalue sqlalchemy.exc interfaceerror mysql_exception interfaceerror -1 error totally whack noformat reason sqlalchemy generate select query figure unicode support engine select pass qserv parse list selects appear proxy log code sql select cast('test plain return char(60 anon_1 select cast('test unicode return char(60 anon_1 select cast('test collate return char character set utf8 collate utf8_bin anon_1 select some_label code","Support sqlalchemy use with qserv When one tries to connect to qserv using sqlalchemy there is an exception generated currently: {noformat} $ python -c 'import sqlalchemy; sqlalchemy.create_engine(""mysql+mysqldb://qsmaster@127.0.0.1:4040/test"").connect()' /u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py:298: SAWarning: Exception attempting to detect unicode returns: InterfaceError(""(_mysql_exceptions.InterfaceError) (-1, 'error totally whack')"",) ""detect unicode returns: %r"" % de) Traceback (most recent call last): File """", line 1, in  File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2018, in connect return self._connection_cls(self, **kwargs) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 72, in __init__ if connection is not None else engine.raw_connection() File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2104, in raw_connection self.pool.unique_connection, _connection) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2078, in _wrap_pool_connect e, dialect, self) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1405, in _handle_dbapi_exception_noconnection exc_info File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 199, in raise_from_cause reraise(type(exception), exception, tb=exc_tb) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2074, in _wrap_pool_connect return fn() File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 318, in unique_connection return _ConnectionFairy._checkout(self) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 713, in _checkout fairy = _ConnectionRecord.checkout(pool) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 480, in checkout rec = pool._do_get() File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1060, in _do_get self._dec_overflow() File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 60, in __exit__ compat.reraise(exc_type, exc_value, exc_tb) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1057, in _do_get return self._create_connection() File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 323, in _create_connection return _ConnectionRecord(self) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 454, in __init__ exec_once(self.connection, self) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 246, in exec_once self(*args, **kw) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 256, in __call__ fn(*args, **kw) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 1312, in go return once_fn(*arg, **kw) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/strategies.py"", line 165, in first_connect dialect.initialize(c) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/dialects/mysql/base.py"", line 2626, in initialize default.DefaultDialect.initialize(self, connection) File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 256, in initialize self._check_unicode_description(connection): File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 343, in _check_unicode_description ]).compile(dialect=self) File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute self.errorhandler(self, exc, value) File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler raise errorclass, errorvalue sqlalchemy.exc.InterfaceError: (_mysql_exceptions.InterfaceError) (-1, 'error totally whack') {noformat} The reason for that is that sqlalchemy generate few SELECT queries to figure out unicode support by the engine, and those selects are passed to qserv which cannot parse them. Here is the list of SELECTs which appears in proxy log: {code:sql} SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1 SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1 SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1 SELECT 'x' AS some_label {code}"
"Create and rename the sims_dustmaps repository to sims_maps Create and rename the sims_dustmaps repository to sims_maps.    This is my plan after talking to [~jhoblitt]:    Add sims_maps to {{lsst_build/repos.yml}}. Change related dependencies and create ticket branches, run CI to confirm the changes.",2,DM-4649,datamanagement,create rename sims_dustmap repository sims_map create rename sims_dustmap repository sims_map plan talk ~jhoblitt add sims_map lsst_build repos.yml change relate dependency create ticket branch run ci confirm change,"Create and rename the sims_dustmaps repository to sims_maps Create and rename the sims_dustmaps repository to sims_maps. This is my plan after talking to [~jhoblitt]: Add sims_maps to {{lsst_build/repos.yml}}. Change related dependencies and create ticket branches, run CI to confirm the changes."
CI debugging diagnosing build failures and refreshing build slaves,1,DM-4652,datamanagement,ci debugging diagnose build failure refreshing build slave,CI debugging diagnosing build failures and refreshing build slaves
Port code style guidelines to new DM Developer Guide Verbatim port of DM Coding style guidelines to Sphinx doc platform from Confluence.    - https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy  - https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666 and contents    I’m unclear whether these pages should be included:    - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++ ‘using’)  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190 (how to use C++ templates)  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++11/14; which should seem to belong in the code style guide)    Any temptation to amend and update the style guideline content will be avoided.,5,DM-4656,datamanagement,port code style guideline new dm developer guide verbatim port dm code style guideline sphinx doc platform confluence https://confluence.lsstcorp.org/display/ldmdg/dm+coding+style+policy https://confluence.lsstcorp.org/display/ldmdg/python+coding+standard https://confluence.lsstcorp.org/pages/viewpage.action?pageid=16908666 content unclear page include https://confluence.lsstcorp.org/pages/viewpage.action?pageid=20283399 c++ https://confluence.lsstcorp.org/pages/viewpage.action?pageid=20284190 use c++ template https://confluence.lsstcorp.org/pages/viewpage.action?pageid=20283399 c++11/14 belong code style guide temptation amend update style guideline content avoid,Port code style guidelines to new DM Developer Guide Verbatim port of DM Coding style guidelines to Sphinx doc platform from Confluence. - https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy - https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666 and contents I m unclear whether these pages should be included: - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++ using ) - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190 (how to use C++ templates) - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++11/14; which should seem to belong in the code style guide) Any temptation to amend and update the style guideline content will be avoided.
Port RFC/RFD/Decision Making Page to new docs Port to new Sphinx docs: https://confluence.lsstcorp.org/display/LDMDG/Discussion+and+Decision+Making+Process?src=contextnavpagetreemode,3,DM-4657,datamanagement,port rfc rfd decision making page new doc port new sphinx doc https://confluence.lsstcorp.org/display/ldmdg/discussion+and+decision+making+process?src=contextnavpagetreemode,Port RFC/RFD/Decision Making Page to new docs Port to new Sphinx docs: https://confluence.lsstcorp.org/display/LDMDG/Discussion+and+Decision+Making+Process?src=contextnavpagetreemode
Read and extend SuperTask technical note Read the current version of DMTN-002 and comment.    Write new sections describing the overall architecture and the expected role of SuperTask in the system.,3,DM-4665,datamanagement,read extend supertask technical note read current version dmtn-002 comment write new section describe overall architecture expected role supertask system,Read and extend SuperTask technical note Read the current version of DMTN-002 and comment. Write new sections describing the overall architecture and the expected role of SuperTask in the system.
"Review existing CmdLineTask instances' inputs and outputs Review most or all existing DM CmdLineTask subclasses to understand their external inputs and outputs.  This will inform the design of the successor to the interim SuperTask.execute( DataRef ) interface.    The issue is that the single-DataRef interface supports only 1:1 input:output relationships, or N:1 relationships where a list of inputs is derivable from an output dataid.  This is believed to be insufficiently general.",6,DM-4666,datamanagement,review exist cmdlinetask instance input output review exist dm cmdlinetask subclasse understand external input output inform design successor interim supertask.execute dataref interface issue single dataref interface support 1:1 input output relationship n:1 relationship list input derivable output dataid believe insufficiently general,"Review existing CmdLineTask instances' inputs and outputs Review most or all existing DM CmdLineTask subclasses to understand their external inputs and outputs. This will inform the design of the successor to the interim SuperTask.execute( DataRef ) interface. The issue is that the single-DataRef interface supports only 1:1 input:output relationships, or N:1 relationships where a list of inputs is derivable from an output dataid. This is believed to be insufficiently general."
"Improve sphgeom documentation Per RFC-117, the sphgeom package needs decent overview documentation, linked from the top-level README.md. The doxygen output should also be reviewed.",2,DM-4667,datamanagement,improve sphgeom documentation rfc-117 sphgeom package need decent overview documentation link level readme.md doxygen output review,"Improve sphgeom documentation Per RFC-117, the sphgeom package needs decent overview documentation, linked from the top-level README.md. The doxygen output should also be reviewed."
"audit obs_subaru defaults and move them to lower-level code The obs_subaru config overrides contain many useful settings that aren't actually specific to HSC or Suprimecam.  These should be moved down to the low-level defaults in the config classes themselves, so new obs_ packages don't have to copy these configurations explicitly.",2,DM-4668,datamanagement,audit obs_subaru default low level code obs_subaru config overrides contain useful setting actually specific hsc suprimecam move low level default config class new obs package copy configuration explicitly,"audit obs_subaru defaults and move them to lower-level code The obs_subaru config overrides contain many useful settings that aren't actually specific to HSC or Suprimecam. These should be moved down to the low-level defaults in the config classes themselves, so new obs_ packages don't have to copy these configurations explicitly."
"configure WebDAV with Kerberos/LDAP on lsst-auth1 Configure WebDAV for Kerberos authentication and LDAP authorization. Create example subdirectories where LDAP groups determine access (using .htaccess files):  * lsst: anyone in lsst group can read/write to this directory  * ncsa: anyone in all_ncsa_employe can write, anyone in lsst can read",6,DM-4671,datamanagement,configure webdav kerberos ldap lsst auth1 configure webdav kerberos authentication ldap authorization create example subdirectory ldap group determine access file lsst lsst group read write directory ncsa all_ncsa_employe write lsst read,"configure WebDAV with Kerberos/LDAP on lsst-auth1 Configure WebDAV for Kerberos authentication and LDAP authorization. Create example subdirectories where LDAP groups determine access (using .htaccess files): * lsst: anyone in lsst group can read/write to this directory * ncsa: anyone in all_ncsa_employe can write, anyone in lsst can read"
IAM process for managing L3 data access Document a process for managing access to L3 Data Products.    On the wiki: https://confluence.lsstcorp.org/display/LAAIM/Managing+L3+Data+Access,1,DM-4672,datamanagement,iam process manage l3 datum access document process manage access l3 data products wiki https://confluence.lsstcorp.org/display/laaim/managing+l3+data+access,IAM process for managing L3 data access Document a process for managing access to L3 Data Products. On the wiki: https://confluence.lsstcorp.org/display/LAAIM/Managing+L3+Data+Access
Design Interfaces for Memory Management for Shared Scans Part of the shared scans involve memory management - a system that will be used by Qserv that will manage memory allocation / pin chunks in memory. This story involves designing the API between Qserv and the memory management system.  ,8,DM-4677,datamanagement,design interfaces memory management shared scans share scan involve memory management system qserv manage memory allocation pin chunk memory story involve design api qserv memory management system,Design Interfaces for Memory Management for Shared Scans Part of the shared scans involve memory management - a system that will be used by Qserv that will manage memory allocation / pin chunks in memory. This story involves designing the API between Qserv and the memory management system.
"qserv/cfg has to be removed by ""scons -c"" qserv-meta.cong was still pointing on MySQL instead of MariaDB, even after running ""scons -c"". This error-prone behaviour should be fixed.",2,DM-4678,datamanagement,qserv cfg remove scon qserv-meta.cong point mysql instead mariadb run scon error prone behaviour fix,"qserv/cfg has to be removed by ""scons -c"" qserv-meta.cong was still pointing on MySQL instead of MariaDB, even after running ""scons -c"". This error-prone behaviour should be fixed."
"Changed the implementation of HistogramProcessorTest due to the minor change about the algorithm in the HistogramProcessor In Histogram, when the data points fall on the bin edges,  the following rules are used:  #  For each bin, it contains the data points fall inside the bin and the data point fall on the left edge.  For example, if binSize=2, the bin[0] is in the range of [0,2].  The data value 0 is in bin[0] .  #  For each bin, the data point falls on the right edge is not included in the number point count. For example if binSize=2, the bin[0] is having the range of [0,2].  The data value 0 is in bin[0] but the data value 2 is not in the bin[0].  # For the last bin, the data points fall inside the bin or fall on the left or right bin are counted as the number of bin points.    The last rule is newly introduced.      ",2,DM-4688,datamanagement,"change implementation histogramprocessort minor change algorithm histogramprocessor histogram datum point fall bin edge follow rule bin contain datum point fall inside bin datum point fall left edge example binsize=2 bin[0 range 0,2 data value bin[0 bin datum point fall right edge include number point count example binsize=2 bin[0 have range 0,2 data value bin[0 data value bin[0 bin datum point fall inside bin fall left right bin count number bin point rule newly introduce","Changed the implementation of HistogramProcessorTest due to the minor change about the algorithm in the HistogramProcessor In Histogram, when the data points fall on the bin edges, the following rules are used: # For each bin, it contains the data points fall inside the bin and the data point fall on the left edge. For example, if binSize=2, the bin[0] is in the range of [0,2]. The data value 0 is in bin[0] . # For each bin, the data point falls on the right edge is not included in the number point count. For example if binSize=2, the bin[0] is having the range of [0,2]. The data value 0 is in bin[0] but the data value 2 is not in the bin[0]. # For the last bin, the data points fall inside the bin or fall on the left or right bin are counted as the number of bin points. The last rule is newly introduced."
"Promote IsrTask to command line task. As pointed out by [~nidever] in DM-4635, it would be quite useful to have the IsrTask callable as a command line task without having to do all the other steps in ProcessCoaddTask.",4,DM-4701,datamanagement,promote isrtask command line task point ~nidever dm-4635 useful isrtask callable command line task have step processcoaddtask,"Promote IsrTask to command line task. As pointed out by [~nidever] in DM-4635, it would be quite useful to have the IsrTask callable as a command line task without having to do all the other steps in ProcessCoaddTask."
"Promote CharacterizationTask to command line task In refactoring the processCcd.py script, we'd like to make each component callable by command line as well.  This is to promote the image characterization task to a command line task.  A requirement will be that this task be able to run on data without IsrTask having been run (command line tasks should be insulated as much as possible from knowing about previous processing).",4,DM-4702,datamanagement,promote characterizationtask command line task refactore processccd.py script like component callable command line promote image characterization task command line task requirement task able run datum isrtask having run command line task insulate possible know previous processing,"Promote CharacterizationTask to command line task In refactoring the processCcd.py script, we'd like to make each component callable by command line as well. This is to promote the image characterization task to a command line task. A requirement will be that this task be able to run on data without IsrTask having been run (command line tasks should be insulated as much as possible from knowing about previous processing)."
"Promote CalibrateTask to command line task The task that takes care of measurement and calibration on characterized images will be promoted to a command line task.  As with the other command line tasks, it should be possible to run the calibration and measurement command line task on data without necessarily running IsrTask or CharacterizeTask.    Of course, this means the task will have to get a PSF from somewhere, see [clo|https://community.lsst.org/t/requirements-for-overhauled-calibration-task/370] for suggestions.",4,DM-4703,datamanagement,promote calibratetask command line task task take care measurement calibration characterize image promote command line task command line task possible run calibration measurement command line task datum necessarily run isrtask characterizetask course mean task psf clo|https://community.lsst.org requirement overhaul calibration task/370 suggestion,"Promote CalibrateTask to command line task The task that takes care of measurement and calibration on characterized images will be promoted to a command line task. As with the other command line tasks, it should be possible to run the calibration and measurement command line task on data without necessarily running IsrTask or CharacterizeTask. Of course, this means the task will have to get a PSF from somewhere, see [clo|https://community.lsst.org/t/requirements-for-overhauled-calibration-task/370] for suggestions."
"Qserv integration tests fail on CentOS7 with gcc 4.8.5 The version of gcc that ships with CentOS7, {{gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)}}, appears to miscompile the qserv worker source in a way that makes it impossible to actually run queries. Installing {{devtoolset-3-toolchain}} and {{devtoolset-3-perftools}} via {{yum}} to get gcc 4.9 resolves the issue.",2,DM-4704,datamanagement,qserv integration test fail centos7 gcc 4.8.5 version gcc ship centos7 gcc gcc 4.8.5 20150623 red hat 4.8.5 appear miscompile qserv worker source way make impossible actually run query instal devtoolset-3 toolchain devtoolset-3 perftool yum gcc 4.9 resolve issue,"Qserv integration tests fail on CentOS7 with gcc 4.8.5 The version of gcc that ships with CentOS7, {{gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)}}, appears to miscompile the qserv worker source in a way that makes it impossible to actually run queries. Installing {{devtoolset-3-toolchain}} and {{devtoolset-3-perftools}} via {{yum}} to get gcc 4.9 resolves the issue."
"qdisp/testQDisp fails with mariadb Fabrice fried to build qserv with mariadb and it caused failure in one of the unit test: qdisp/testQDisp with the message:  {noformat}  pure virtual method called  terminate called without an active exception  {noformat}    Runnig it with GDB it' obvious that there is a problem with resource lifetime management in qdisp/testQDisp.cc. The problem is that XrdSsiSessionMock is destroyed sooner than other objects that use it.     One way to resolve this problem is to instantiate XrdSsiSessionMock earlier than other objects that use it (to reverse the order of destructors), possibly make it a global instance.    Big mystery here is how mariadb could trigger this interesting behavior and why did not we see this earlier.",1,DM-4705,datamanagement,qdisp testqdisp fail mariadb fabrice fry build qserv mariadb cause failure unit test qdisp message noformat pure virtual method call terminate call active exception noformat runnig gdb obvious problem resource lifetime management qdisp testqdisp.cc problem xrdssisessionmock destroy soon object use way resolve problem instantiate xrdssisessionmock early object use reverse order destructor possibly global instance big mystery mariadb trigger interesting behavior early,"qdisp/testQDisp fails with mariadb Fabrice fried to build qserv with mariadb and it caused failure in one of the unit test: qdisp/testQDisp with the message: {noformat} pure virtual method called terminate called without an active exception {noformat} Runnig it with GDB it' obvious that there is a problem with resource lifetime management in qdisp/testQDisp.cc. The problem is that XrdSsiSessionMock is destroyed sooner than other objects that use it. One way to resolve this problem is to instantiate XrdSsiSessionMock earlier than other objects that use it (to reverse the order of destructors), possibly make it a global instance. Big mystery here is how mariadb could trigger this interesting behavior and why did not we see this earlier."
"Rerun and create a repository for CFHT astrometry test. Understand, re-rerun, and recreate clean version of [~boutigny] 's CFHT astrometry test for the astrometry RMS for two sample CFHT observations.  This test is on the NCSA machines in  /lsst8/boutigny/valid_cfht    Create a repository for this test with an eye toward it becoming integrated in a validation suite for the stack.        ",1,DM-4706,datamanagement,rerun create repository cfht astrometry test understand rerun recreate clean version ~boutigny cfht astrometry test astrometry rms sample cfht observation test ncsa machine /lsst8 boutigny valid_cfht create repository test eye integrate validation suite stack,"Rerun and create a repository for CFHT astrometry test. Understand, re-rerun, and recreate clean version of [~boutigny] 's CFHT astrometry test for the astrometry RMS for two sample CFHT observations. This test is on the NCSA machines in /lsst8/boutigny/valid_cfht Create a repository for this test with an eye toward it becoming integrated in a validation suite for the stack."
Adapt CFHT astrometry test for DECam COSMOS field validation Adapt the CFHT astrometry validation test to the DECam reprocessing effort.      Will focus on the repeat observations of the COSMOS field.  Goal is to just do a simple two-observation comparison.  Doing a full test of all of the observations will be a later story.  ,1,DM-4707,datamanagement,adapt cfht astrometry test decam cosmos field validation adapt cfht astrometry validation test decam reprocess effort focus repeat observation cosmos field goal simple observation comparison test observation later story,Adapt CFHT astrometry test for DECam COSMOS field validation Adapt the CFHT astrometry validation test to the DECam reprocessing effort. Will focus on the repeat observations of the COSMOS field. Goal is to just do a simple two-observation comparison. Doing a full test of all of the observations will be a later story.
Integrate astrometry test into SDSS demo lsst_dm_stack_demo Incorporate the astrometry test as an optional component in lsst_dm_stack_demo.    This is chosen because lsst_dm_stack_demo currently serves as the very loose stack validation and understanding how to do astrometric repeatibility testing in this demo will help explore how it would make sense to put in a fuller CFHT validation test of the DM stack.,1,DM-4708,datamanagement,integrate astrometry test sdss demo lsst_dm_stack_demo incorporate astrometry test optional component lsst_dm_stack_demo choose lsst_dm_stack_demo currently serve loose stack validation understand astrometric repeatibility testing demo help explore sense full cfht validation test dm stack,Integrate astrometry test into SDSS demo lsst_dm_stack_demo Incorporate the astrometry test as an optional component in lsst_dm_stack_demo. This is chosen because lsst_dm_stack_demo currently serves as the very loose stack validation and understanding how to do astrometric repeatibility testing in this demo will help explore how it would make sense to put in a fuller CFHT validation test of the DM stack.
"Prototype a validation module of the stack using CFHT data. Create a prototype standalone validation test of the astrometric performance of the stack on suitable CFHT data.  Module is called `validate_drp`     http://github.com/lsst/validate_drp_cfht    Decide how those data should be provided (testdata_cfht being one obvious possibility), and determine if obs_cfht tests and the tests for this validate_drp module should use the same test datasets.    This is prototyping for DM-2518.",1,DM-4709,datamanagement,prototype validation module stack cfht datum create prototype standalone validation test astrometric performance stack suitable cfht datum module call validate_drp http://github.com/lsst/validate_drp_cfht decide datum provide testdata_cfht obvious possibility determine obs_cfht test test validate_drp module use test dataset prototype dm-2518,"Prototype a validation module of the stack using CFHT data. Create a prototype standalone validation test of the astrometric performance of the stack on suitable CFHT data. Module is called `validate_drp` http://github.com/lsst/validate_drp_cfht Decide how those data should be provided (testdata_cfht being one obvious possibility), and determine if obs_cfht tests and the tests for this validate_drp module should use the same test datasets. This is prototyping for DM-2518."
host identification info needs to be part of log message The EventAppender needs to add host identification (host/process/id) information to the log message it transmits.   This was inadvertently left out.,3,DM-4710,datamanagement,host identification info need log message eventappender need add host identification host process id information log message transmit inadvertently leave,host identification info needs to be part of log message The EventAppender needs to add host identification (host/process/id) information to the log message it transmits. This was inadvertently left out.
Edit testdata_cfht to pass obs_cfht unit tests This ticket covers the first half of the issues in DM-2917.     {{testdata_cfht}} was left unedited while some past changes in {{obs_cfht}} {{MegacamMapper}} required coordinated changes.  The goal of this ticket is to simply pass the unit tests currently in {{obs_cfht}}.   ,1,DM-4711,datamanagement,edit testdata_cfht pass obs_cfht unit test ticket cover half issue dm-2917 testdata_cfht leave unedited past change obs_cfht megacammapper require coordinated change goal ticket simply pass unit test currently obs_cfht,Edit testdata_cfht to pass obs_cfht unit tests This ticket covers the first half of the issues in DM-2917. {{testdata_cfht}} was left unedited while some past changes in {{obs_cfht}} {{MegacamMapper}} required coordinated changes. The goal of this ticket is to simply pass the unit tests currently in {{obs_cfht}}.
"Bad OpenBlas setting in miniconda/numpy causes very poor performance for running multiple processes I have been running many processes of processCcdDecam.py on my new linux machine in Tucson (bambam).  To my surprise, running 40 processes at once gets very poor performance (~70 sec per process) compared to running a single process (~16 sec). I expected some performance hit because of larger overheads but not a factor of 4!    I ran it both on a spinning HDD and PCIe SSD but they both had the same problem.  I also tried running it on multiple visits versus multiple chips for a single visit (all accessing the same MEF FITS file) but this made no difference.  I tested it with various numbers of processes and found that the time per processes increases linearly with the number of processes running.      [~jmatt] has been helping me track this down.  We used some performance tools (htop, iotop, and perf) to figure out what was going on.  It was clear that the issue was not a RAM or I/O problem.  By watching htop while the 40 processes were running it became clear that once some of the processes hit ""deblending"" everything slowed down considerably and all cores were maxed out and showing lots of kernel traffic.  I also ran processCcdDecam.py with deblending turned off and the performance was much more reasonable (~24 sec. per process).    After more digging (with perftop), we found that there was a lot of swapping going on during the deblending step by ""openblas"".  This is a package that numpy uses for speeding up certain computationally intensive tasks using multithreading (e.g. linear algebra).  By default each openblas instance takes advantage of ALL cores on a machine.  So all 40 processes were trying to use all available cores and most of the time was spent swapping between all of these threads.    OpenBlas can be configured to use a more reasonable number of cores/threads, but the version that LSSTSW uses is installed by miniconda via a dependency of numpy and, as far as we could tell, it's not possible to configured NUM_THREADS for OpenBlas with miniconda.    We ended up compiling our own version of OpenBlas with NUM_THREADS = 6 (the maximum threads that OpenBlas uses) and the performance was great, 24 sec.    I'm not sure what the solution is for this but we probably don't want to go with miniconda for the default LSSTSW installation (uses currently done by bin/deploy).    JMatt might have comments to add.   ",4,DM-4714,datamanagement,bad openblas set miniconda numpy cause poor performance run multiple process run process processccddecam.py new linux machine tucson bambam surprise run 40 process get poor performance sec process compare run single process ~16 sec expect performance hit large overhead factor run spin hdd pcie ssd problem try run multiple visit versus multiple chip single visit access mef fit file difference test number process find time process increase linearly number process run ~jmatt help track performance tool htop iotop perf figure go clear issue ram problem watch htop 40 process run clear process hit deblending slow considerably core maxe show lot kernel traffic run deblending turn performance reasonable ~24 sec process digging perftop find lot swap go deblending step openblas package numpy use speed certain computationally intensive task multithreade e.g. linear algebra default openblas instance take advantage core machine 40 process try use available core time spend swap thread openblas configure use reasonable number core thread version lsstsw use instal miniconda dependency numpy far tell possible configure num_threads openblas miniconda end compile version openblas num_threads maximum thread openblas use performance great 24 sec sure solution probably want miniconda default lsstsw installation use currently bin deploy jmatt comment add,"Bad OpenBlas setting in miniconda/numpy causes very poor performance for running multiple processes I have been running many processes of processCcdDecam.py on my new linux machine in Tucson (bambam). To my surprise, running 40 processes at once gets very poor performance (~70 sec per process) compared to running a single process (~16 sec). I expected some performance hit because of larger overheads but not a factor of 4! I ran it both on a spinning HDD and PCIe SSD but they both had the same problem. I also tried running it on multiple visits versus multiple chips for a single visit (all accessing the same MEF FITS file) but this made no difference. I tested it with various numbers of processes and found that the time per processes increases linearly with the number of processes running. [~jmatt] has been helping me track this down. We used some performance tools (htop, iotop, and perf) to figure out what was going on. It was clear that the issue was not a RAM or I/O problem. By watching htop while the 40 processes were running it became clear that once some of the processes hit ""deblending"" everything slowed down considerably and all cores were maxed out and showing lots of kernel traffic. I also ran processCcdDecam.py with deblending turned off and the performance was much more reasonable (~24 sec. per process). After more digging (with perftop), we found that there was a lot of swapping going on during the deblending step by ""openblas"". This is a package that numpy uses for speeding up certain computationally intensive tasks using multithreading (e.g. linear algebra). By default each openblas instance takes advantage of ALL cores on a machine. So all 40 processes were trying to use all available cores and most of the time was spent swapping between all of these threads. OpenBlas can be configured to use a more reasonable number of cores/threads, but the version that LSSTSW uses is installed by miniconda via a dependency of numpy and, as far as we could tell, it's not possible to configured NUM_THREADS for OpenBlas with miniconda. We ended up compiling our own version of OpenBlas with NUM_THREADS = 6 (the maximum threads that OpenBlas uses) and the performance was great, 24 sec. I'm not sure what the solution is for this but we probably don't want to go with miniconda for the default LSSTSW installation (uses currently done by bin/deploy). JMatt might have comments to add."
"Track down reason for slow performance when running many jobs of processCcdDEcam on bambam During the processing of the COSMOS data for the verification dataset work I ran many jobs of processCcdDecam.py on the new linux server, bambam.  The performance was very slow, 4x longer than running a single job at a time.  Figure out what is going on.",2,DM-4716,datamanagement,track reason slow performance run job processccddecam bambam processing cosmos datum verification dataset work run job processccddecam.py new linux server bambam performance slow 4x long run single job time figure go,"Track down reason for slow performance when running many jobs of processCcdDEcam on bambam During the processing of the COSMOS data for the verification dataset work I ran many jobs of processCcdDecam.py on the new linux server, bambam. The performance was very slow, 4x longer than running a single job at a time. Figure out what is going on."
"Meetings Dec 2015 Verification dataset meetings, RFD meetings, DES Chicagoland meeting and preparation",6,DM-4717,datamanagement,meeting dec 2015 verification dataset meeting rfd meeting des chicagoland meeting preparation,"Meetings Dec 2015 Verification dataset meetings, RFD meetings, DES Chicagoland meeting and preparation"
"Other LOE -- Dec 2015 weekly LSST local grouop meetings, NCSA meetings (All-hands, software, etc), code review, other local meetings, postdoc meetings and tasks",6,DM-4718,datamanagement,loe dec 2015 weekly lsst local grouop meeting ncsa meeting hand software etc code review local meeting postdoc meeting task,"Other LOE -- Dec 2015 weekly LSST local grouop meetings, NCSA meetings (All-hands, software, etc), code review, other local meetings, postdoc meetings and tasks"
Vendor input on sizing predictions Discussions about tape-pricing and disk-pricing predictions from Spectra and DDN respectively in order to improve our forecasting. This information needs to be incorporated into LDM-144.,2,DM-4719,datamanagement,vendor input size prediction discussion tape pricing disk pricing prediction spectra ddn respectively order improve forecasting information need incorporate ldm-144,Vendor input on sizing predictions Discussions about tape-pricing and disk-pricing predictions from Spectra and DDN respectively in order to improve our forecasting. This information needs to be incorporated into LDM-144.
"More preparation for FY16 hardware More pricing iterations with several companies and incorporating that information into our final decision. More Q&A with storage companies re: comparable features. Compiled all storage option quotes into a spread sheet which now forms as a good comparison and helps LDM-144  forecasting.     Awaiting quotes for racks and PDUs. Now that rack size is known for the Chilean DC, this will serve us well for LDM-144 costs.     Power issues for FY16 hardware are settled and we are ready to schedule installation as soon as the hardware purchase contract is complete.    Tagging issues for FY15 hardware complete. Looking into pre FY15 tagging. Requested that LSST/Aura perform an inventory request to complete the circle and prove the process.    Working with Spectra / NetSource to create a sustainable tape condo that can serve LSST through 2030.",4,DM-4720,datamanagement,preparation fy16 hardware pricing iteration company incorporate information final decision q&a storage company comparable feature compile storage option quote spread sheet form good comparison help forecasting await quote rack pdu rack size know chilean dc serve ldm-144 cost power issue fy16 hardware settle ready schedule installation soon hardware purchase contract complete tagging issue fy15 hardware complete look pre fy15 tagging request lsst aura perform inventory request complete circle prove process work spectra netsource create sustainable tape condo serve lsst 2030,"More preparation for FY16 hardware More pricing iterations with several companies and incorporating that information into our final decision. More Q&A with storage companies re: comparable features. Compiled all storage option quotes into a spread sheet which now forms as a good comparison and helps LDM-144 forecasting. Awaiting quotes for racks and PDUs. Now that rack size is known for the Chilean DC, this will serve us well for LDM-144 costs. Power issues for FY16 hardware are settled and we are ready to schedule installation as soon as the hardware purchase contract is complete. Tagging issues for FY15 hardware complete. Looking into pre FY15 tagging. Requested that LSST/Aura perform an inventory request to complete the circle and prove the process. Working with Spectra / NetSource to create a sustainable tape condo that can serve LSST through 2030."
"Plan DM’s communication / documentation / information architecture strategy Plan and write a technote outlining communication and documentation platforms from a DM perspective. The technote will specify    - how each platform is used  - what developments need to be done  - address integrations with LSST-wide communications projects  - address information architecture (generally, the ease of discovering the right information)",2,DM-4721,datamanagement,plan dm communication documentation information architecture strategy plan write technote outline communication documentation platform dm perspective technote specify platform development need address integration lsst wide communication project address information architecture generally ease discover right information,"Plan DM s communication / documentation / information architecture strategy Plan and write a technote outlining communication and documentation platforms from a DM perspective. The technote will specify - how each platform is used - what developments need to be done - address integrations with LSST-wide communications projects - address information architecture (generally, the ease of discovering the right information)"
"File tickets for list of stack deficiencies and suggested upgrades K-T suggested that I take my list of ""stack deficiencies and suggested improvements"" [https://confluence.lsstcorp.org/display/SQRE/Stack+Deficiencies+and+Suggested+Upgrades] on confluence and (with Tim J.'s help) create tickets for each item (as much as possible) so that the work could be scheduled.  ",3,DM-4722,datamanagement,file ticket list stack deficiency suggest upgrade suggest list stack deficiency suggest improvement https://confluence.lsstcorp.org/display/sqre/stack+deficiencies+and+suggested+upgrades confluence tim j. help create ticket item possible work schedule,"File tickets for list of stack deficiencies and suggested upgrades K-T suggested that I take my list of ""stack deficiencies and suggested improvements"" [https://confluence.lsstcorp.org/display/SQRE/Stack+Deficiencies+and+Suggested+Upgrades] on confluence and (with Tim J.'s help) create tickets for each item (as much as possible) so that the work could be scheduled."
"Continue learning about middleware Learn more about orchestration, task execution, and logging.  ",3,DM-4723,datamanagement,continue learn middleware learn orchestration task execution log,"Continue learning about middleware Learn more about orchestration, task execution, and logging."
Implement zenodio.harvest Harvest metadata about records in a Zenodo Community collection using the {{oai_datacite3}} format. See https://zenodo.org/dev    Part of the [zenodio|https://github.com/lsst-sqre/zenodio] Python package. This tool will be used by our technote and the documentation indexing platforms.,3,DM-4724,datamanagement,implement zenodio.harvest harvest metadata record zenodo community collection oai_datacite3 format https://zenodo.org/dev zenodio|https://github.com lsst sqre zenodio python package tool technote documentation indexing platform,Implement zenodio.harvest Harvest metadata about records in a Zenodo Community collection using the {{oai_datacite3}} format. See https://zenodo.org/dev Part of the [zenodio|https://github.com/lsst-sqre/zenodio] Python package. This tool will be used by our technote and the documentation indexing platforms.
Doxygen package fails to build with flex 2.6 To wit:    {code}  $ flex --version  flex 2.6.0    $ bash newinstall.sh    LSST Software Stack Builder  [...stuff...]  eups distrib: Failed to build doxygen-1.8.5.eupspkg: Command:  	source /Users/jds/Projects/Astronomy/LSST/stack/eups/bin/setups.sh; export EUPS_PATH=/Users/jds/Projects/Astronomy/LSST/stack; (/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.sh) >> /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log 2>&1 4>/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.msg  exited with code 252    $ grep error /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log  commentscan.l:1064:55: error: use of undeclared identifier 'yy_current_buffer'  commentscan.l:1126:58: error: use of undeclared identifier 'yy_current_buffer'  {code}    Builds fine using {{flex 2.5.35 Apple(flex-31)}}.,1,DM-4728,datamanagement,doxygen package fail build flex 2.6 wit code flex flex 2.6.0 bash newinstall.sh lsst software stack builder stuff eup distrib fail build doxygen-1.8.5.eupspkg command source /users jds projects astronomy lsst stack eup bin setups.sh export eups_path=/users jds projects astronomy lsst stack /users jds projects astronomy lsst stack eupsbuilddir darwinx86 doxygen-1.8.5 build.sh /users jds projects astronomy lsst stack eupsbuilddir darwinx86 doxygen-1.8.5 build.log 2>&1 4>/users jds projects astronomy lsst stack eupsbuilddir darwinx86 doxygen-1.8.5 build.msg exit code 252 grep error /users jds projects astronomy lsst stack eupsbuilddir darwinx86 doxygen-1.8.5 build.log commentscan.l:1064:55 error use undeclared identifier yy_current_buffer commentscan.l:1126:58 error use undeclared identifier yy_current_buffer code build fine flex 2.5.35 apple(flex-31,Doxygen package fails to build with flex 2.6 To wit: {code} $ flex --version flex 2.6.0 $ bash newinstall.sh LSST Software Stack Builder [...stuff...] eups distrib: Failed to build doxygen-1.8.5.eupspkg: Command: source /Users/jds/Projects/Astronomy/LSST/stack/eups/bin/setups.sh; export EUPS_PATH=/Users/jds/Projects/Astronomy/LSST/stack; (/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.sh) >> /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log 2>&1 4>/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.msg exited with code 252 $ grep error /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log commentscan.l:1064:55: error: use of undeclared identifier 'yy_current_buffer' commentscan.l:1126:58: error: use of undeclared identifier 'yy_current_buffer' {code} Builds fine using {{flex 2.5.35 Apple(flex-31)}}.
"HSC backport: Add functions to generate 'unpacked matches' in a Catalog The qa analysis script under development (see DM-4393) calls to HSC {{hscPipeBase}}'s [matches.py|https://github.com/HyperSuprime-Cam/hscPipeBase/blob/master/python/hsc/pipe/base/matches.py] which adds functions to generate ""unpacked matches"" in a Catalog (and vice versa).  It will be ported into {{lsst.afw.table}}.    The port includes following HSC commits:  *Add functions to generate 'unpacked matches' in a Catalog.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/210fcdc6e1d19219e2d9365adeefd9289b2e1186    *Adding check to prevent more obscure error.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/344a96de741cd5aafb5e368f7fa59fa248305af5    *Some little error handling helps.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/61cc053b873d42802581adff8cbbdb52a348879e  (from branch: {{stage-ncsa-3}})    *matches: add ArrayI to list of field types that require a size*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/d4ccd11d8afbcdd9cf0b35eba948cca4b5d09ba5  (from branch {{tickets/HSC-1228}})    Please also include a unittest.",3,DM-4729,datamanagement,hsc backport add function generate unpacked match catalog qa analysis script development dm-4393 call hsc hscpipebase matches.py|https://github.com hypersuprime cam hscpipebase blob master python hsc pipe base matches.py add function generate unpacked match catalog vice versa port lsst.afw.table port include follow hsc commit add function generate unpacked match catalog https://github.com/hypersuprime-cam/hscpipebase/commit/210fcdc6e1d19219e2d9365adeefd9289b2e1186 add check prevent obscure error https://github.com/hypersuprime-cam/hscpipebase/commit/344a96de741cd5aafb5e368f7fa59fa248305af5 little error handling help https://github.com/hypersuprime-cam/hscpipebase/commit/61cc053b873d42802581adff8cbbdb52a348879e branch stage ncsa-3 match add arrayi list field type require size https://github.com/hypersuprime-cam/hscpipebase/commit/d4ccd11d8afbcdd9cf0b35eba948cca4b5d09ba5 branch ticket hsc-1228 include unitt,"HSC backport: Add functions to generate 'unpacked matches' in a Catalog The qa analysis script under development (see DM-4393) calls to HSC {{hscPipeBase}}'s [matches.py|https://github.com/HyperSuprime-Cam/hscPipeBase/blob/master/python/hsc/pipe/base/matches.py] which adds functions to generate ""unpacked matches"" in a Catalog (and vice versa). It will be ported into {{lsst.afw.table}}. The port includes following HSC commits: *Add functions to generate 'unpacked matches' in a Catalog.* https://github.com/HyperSuprime-Cam/hscPipeBase/commit/210fcdc6e1d19219e2d9365adeefd9289b2e1186 *Adding check to prevent more obscure error.* https://github.com/HyperSuprime-Cam/hscPipeBase/commit/344a96de741cd5aafb5e368f7fa59fa248305af5 *Some little error handling helps.* https://github.com/HyperSuprime-Cam/hscPipeBase/commit/61cc053b873d42802581adff8cbbdb52a348879e (from branch: {{stage-ncsa-3}}) *matches: add ArrayI to list of field types that require a size* https://github.com/HyperSuprime-Cam/hscPipeBase/commit/d4ccd11d8afbcdd9cf0b35eba948cca4b5d09ba5 (from branch {{tickets/HSC-1228}}) Please also include a unittest."
Add labels to qa analysis plots for better interpretation The plots output by the qa analysis script (see DM-4393) currently do not display any information regarding the selection/rejection criteria used in making the figures and computing the basic statistics.  This includes magnitude and clipping thresholds.  This information should be added to each plot such that the figures can be interpreted properly.,2,DM-4731,datamanagement,add label qa analysis plot well interpretation plot output qa analysis script dm-4393 currently display information selection rejection criterion make figure compute basic statistic include magnitude clip threshold information add plot figure interpret properly,Add labels to qa analysis plots for better interpretation The plots output by the qa analysis script (see DM-4393) currently do not display any information regarding the selection/rejection criteria used in making the figures and computing the basic statistics. This includes magnitude and clipping thresholds. This information should be added to each plot such that the figures can be interpreted properly.
"lsst-build should support enabling Git LFS in an already-cloned repository A repository which does not use Git LFS is created and described in {{repos.yaml}}. It runs through CI, and is cloned onto a Jenkins build slave. Subsequently, the repository configuration in {{repos.yaml}} is updated to enable LFS. The build system should notice this change and update the cloned repository on disk appropriately. Currently, it doesn't.",1,DM-4733,datamanagement,lsst build support enable git lfs clone repository repository use git lfs create describe repos.yaml run ci clone jenkins build slave subsequently repository configuration repos.yaml update enable lfs build system notice change update clone repository disk appropriately currently,"lsst-build should support enabling Git LFS in an already-cloned repository A repository which does not use Git LFS is created and described in {{repos.yaml}}. It runs through CI, and is cloned onto a Jenkins build slave. Subsequently, the repository configuration in {{repos.yaml}} is updated to enable LFS. The build system should notice this change and update the cloned repository on disk appropriately. Currently, it doesn't."
"afw fails to build on a machine with many cores The afw package does not build reliably (if at all) on a linux box at UW (""magneto"", which has 32 cores and 128 Gb of RAM). The failure is that some unit tests fail with the following error:  {code}      OpenBLAS: pthread_creat error in blas_thread_init function. Error code:11  {code}    For the record, /usr/include/bits/local_lim.h contains this:  {code}  /* The number of threads per process.  */  #define _POSIX_THREAD_THREADS_MAX	64  /* We have no predefined limit on the number of threads.  */  #undef PTHREAD_THREADS_MAX  {code}    It appears that the build system is trying to use too many threads when building afw, which presumably means it is trying to use too many cores. According to [~mjuric] the package responsible for this is {{eupspkg}}, and it tries to use all available cores.    A workaround suggested by [~mjuric] is to set environment variable {{EUPSPKG_NJOBS}} to the max number of cores wanted. However, I suggest we fix our build system so that setting this variable is unnecessary. I suggest we hard-code an upper limit for now, though fancier logic is certainly possible.    A related request is to document the environment variables that control our build system. I searched for {{NJOBS}} on confluence and found nothing.",1,DM-4734,datamanagement,afw fail build machine core afw package build reliably linux box uw magneto 32 core 128 gb ram failure unit test fail follow error code openblas pthread_creat error blas_thread_init function error code record /usr include bit local_lim.h contain code number thread process define posix_thread_threads_max 64 predefine limit number thread undef pthread_threads_max code appear build system try use thread build afw presumably mean try use core accord ~mjuric package responsible eupspkg try use available core workaround suggest ~mjuric set environment variable eupspkg_njobs max number core want suggest fix build system set variable unnecessary suggest hard code upper limit fancy logic certainly possible related request document environment variable control build system search njobs confluence find,"afw fails to build on a machine with many cores The afw package does not build reliably (if at all) on a linux box at UW (""magneto"", which has 32 cores and 128 Gb of RAM). The failure is that some unit tests fail with the following error: {code} OpenBLAS: pthread_creat error in blas_thread_init function. Error code:11 {code} For the record, /usr/include/bits/local_lim.h contains this: {code} /* The number of threads per process. */ #define _POSIX_THREAD_THREADS_MAX 64 /* We have no predefined limit on the number of threads. */ #undef PTHREAD_THREADS_MAX {code} It appears that the build system is trying to use too many threads when building afw, which presumably means it is trying to use too many cores. According to [~mjuric] the package responsible for this is {{eupspkg}}, and it tries to use all available cores. A workaround suggested by [~mjuric] is to set environment variable {{EUPSPKG_NJOBS}} to the max number of cores wanted. However, I suggest we fix our build system so that setting this variable is unnecessary. I suggest we hard-code an upper limit for now, though fancier logic is certainly possible. A related request is to document the environment variables that control our build system. I searched for {{NJOBS}} on confluence and found nothing."
Remove dead code from configuration procedure  - remove scratch db?  - cleanup tmp/sql/*.sql filesi  - remove xrootd configuration script if useless?   - cleanup configuration script style (i.e. tmp/*.sh)  ,3,DM-4735,datamanagement,remove dead code configuration procedure remove scratch db cleanup tmp sql/*.sql filesi remove xrootd configuration script useless cleanup configuration script style i.e. tmp/*.sh,Remove dead code from configuration procedure - remove scratch db? - cleanup tmp/sql/*.sql filesi - remove xrootd configuration script if useless? - cleanup configuration script style (i.e. tmp/*.sh)
Study if mysqlproxy can be compatible with mariaDB client mysqlproxy is not compliant with mariaDB client: see https://mariadb.com/kb/en/mariadb/mariadb-vs-mysql-compatibility/#incompatibilities-between-mariadb-and-mysql-proxy    Nevertheless the trivial fix proposed (remove progress-report options) doesn't seems to work...  {code:bash}    mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch qservTest_case01_qserv    ERROR 1043 (08S01): Bad handshake  {code},4,DM-4736,datamanagement,study mysqlproxy compatible mariadb client mysqlproxy compliant mariadb client https://mariadb.com/kb/en/mariadb/mariadb-vs-mysql-compatibility/#incompatibilities-between-mariadb-and-mysql-proxy trivial fix propose remove progress report option work code bash mysql --port=4040 qsmaster qservtest_case01_qserv error 1043 08s01 bad handshake code,Study if mysqlproxy can be compatible with mariaDB client mysqlproxy is not compliant with mariaDB client: see https://mariadb.com/kb/en/mariadb/mariadb-vs-mysql-compatibility/#incompatibilities-between-mariadb-and-mysql-proxy Nevertheless the trivial fix proposed (remove progress-report options) doesn't seems to work... {code:bash} mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch qservTest_case01_qserv ERROR 1043 (08S01): Bad handshake {code}
Improve 'unit' tests using database - Add mock database for it to work during unit tests or run it apart from unit tests?  - Fix testLocalInfile (read configuration file)  - Try all core/modules/sql/testSql*           ,5,DM-4737,datamanagement,improve unit test database add mock database work unit test run apart unit test fix testlocalinfile read configuration file try core module sql testsql,Improve 'unit' tests using database - Add mock database for it to work during unit tests or run it apart from unit tests? - Fix testLocalInfile (read configuration file) - Try all core/modules/sql/testSql*
" Improve  LOAD LOCAL INFILE management on czar side Option ""mysql_options( m, MYSQL_OPT_LOCAL_INFILE, 0 );"" is added to all C++ sql client instance due to common sql interface, but is only required on master (for merging results) is it possible:   - to remove LOCAL keyword (on czar virtfile is on the same machine that mariadb server)  - or to set it in Qserv czar/master configuration  - or to set it in master MariaDB instance only?",4,DM-4738,datamanagement,improve load local infile management czar option mysql_option mysql_opt_local_infile add c++ sql client instance common sql interface require master merge result possible remove local keyword czar virtfile machine mariadb server set qserv czar master configuration set master mariadb instance,"Improve LOAD LOCAL INFILE management on czar side Option ""mysql_options( m, MYSQL_OPT_LOCAL_INFILE, 0 );"" is added to all C++ sql client instance due to common sql interface, but is only required on master (for merging results) is it possible: - to remove LOCAL keyword (on czar virtfile is on the same machine that mariadb server) - or to set it in Qserv czar/master configuration - or to set it in master MariaDB instance only?"
"Update kernel on IN2P3 cluster The ""Kernel Panic"" issue is non-blocking right-now for John due to machine automated reboot, but we have to solve it to target a stable production system.    With Yvan, we're converging on next update for the cluster:    - on my side I update Qserv metadata on ccqserv100 w.r.t. new Qserv metadata format, and then I test Docker+Qserv on ccqserv100->ccqserv124,  - then CC-IN2P3 team launches a upgrade of the kernel to kernel-ml ( ""mainline stable"" branch of The Linux Kernel Archives) on ccqserv100->ccqserv124, this could be done in January,  - I control Qserv behaviour is still the same than before,  - then Qserv developpers can use this cluster to see if ""Kernel Panic' issue is solved.    If it work we'll update to kernel-ml on ccqserv125->ccqserv149, if it doesn't, cc-in2p3 and Qserv developpers will have to find an other solution.    Regards,    Fabrice",5,DM-4739,datamanagement,update kernel in2p3 cluster kernel panic issue non blocking right john machine automate reboot solve target stable production system yvan converge update cluster update qserv metadata ccqserv100 w.r.t new qserv metadata format test docker+qserv ccqserv100->ccqserv124 cc in2p3 team launch upgrade kernel kernel ml mainline stable branch linux kernel archives ccqserv100->ccqserv124 january control qserv behaviour qserv developper use cluster kernel panic issue solve work update kernel ml ccqserv125->ccqserv149 cc in2p3 qserv developper find solution regard fabrice,"Update kernel on IN2P3 cluster The ""Kernel Panic"" issue is non-blocking right-now for John due to machine automated reboot, but we have to solve it to target a stable production system. With Yvan, we're converging on next update for the cluster: - on my side I update Qserv metadata on ccqserv100 w.r.t. new Qserv metadata format, and then I test Docker+Qserv on ccqserv100->ccqserv124, - then CC-IN2P3 team launches a upgrade of the kernel to kernel-ml ( ""mainline stable"" branch of The Linux Kernel Archives) on ccqserv100->ccqserv124, this could be done in January, - I control Qserv behaviour is still the same than before, - then Qserv developpers can use this cluster to see if ""Kernel Panic' issue is solved. If it work we'll update to kernel-ml on ccqserv125->ccqserv149, if it doesn't, cc-in2p3 and Qserv developpers will have to find an other solution. Regards, Fabrice"
"Audit and document obs_subaru scripts {{obs_subaru}} has a {{bin.src}} directory containing a variety of miscellaneous scripts. Some of these may be actively useful; others could be useful, but require modernizing to work with the latest version of the LSST codebase; others are obsolete or duplicate functionality available elsewhere. Throughout, documentation is lacking.    Please audit this directory: remove the scripts which are useless and ensure the others are working and properly documented.",5,DM-4740,datamanagement,audit document obs_subaru script obs_subaru bin.src directory contain variety miscellaneous script actively useful useful require modernize work late version lsst codebase obsolete duplicate functionality available documentation lack audit directory remove script useless ensure work properly document,"Audit and document obs_subaru scripts {{obs_subaru}} has a {{bin.src}} directory containing a variety of miscellaneous scripts. Some of these may be actively useful; others could be useful, but require modernizing to work with the latest version of the LSST codebase; others are obsolete or duplicate functionality available elsewhere. Throughout, documentation is lacking. Please audit this directory: remove the scripts which are useless and ensure the others are working and properly documented."
Cyber security infrastructure document This document details anticipated security infrastructure and roles needed for the operations of LSST at the base and summit observatory site.  ,1,DM-4741,datamanagement,cyber security infrastructure document document detail anticipate security infrastructure role need operation lsst base summit observatory site,Cyber security infrastructure document This document details anticipated security infrastructure and roles needed for the operations of LSST at the base and summit observatory site.
Bi-weekly LSST IaM meetings for December Bi-weekly IaM meeting between NCSA and LSST for the month of December 2015.  Local coordinating meetings also included.,1,DM-4742,datamanagement,bi weekly lsst iam meeting december bi weekly iam meeting ncsa lsst month december 2015 local coordinate meeting include,Bi-weekly LSST IaM meetings for December Bi-weekly IaM meeting between NCSA and LSST for the month of December 2015. Local coordinating meetings also included.
"Make deblender more robust against weird PSF dimensions [~boutigny] reports two problems with PSF dimension calculations in the deblender that result in fatal errors, because earlier checks for bad dimensions intended to cause more graceful failures are incomplete.    The first appears to happen when the PSF dimensions are highly non-square, and the image width is smaller than 1.5x FWHM while the image height is more than 1.5x FWHM (or the opposite).  {code:hide-linenum}  measureCoaddSources FATAL: Failed on dataId={'filter': 'i', 'tract': 1, 'patch': '8,2'}:     File ""src/image/Image.cc"", line 92, in static typename lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw:  :image::ImageBase<PixelT>::_makeSubView(const lsst::afw::geom::Extent2I&, const lsst::afw::geom::Extent2I&, cons  t typename lsst::afw::image::detail::types_traits<PixelT, false>::view_t&) [with PixelT = double]      Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15 {0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15'    Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 321, in __call__      result = task.run(dataRef, **kwargs)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/pipe_tasks/python/lsst/pipe/tasks/multiBand.py"", line 552, in run      self.deblend.run(exposure, sources, exposure.getPsf())    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 231, in run      self.deblend(exposure, sources, psf)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 308, in deblend      clipStrayFluxFraction=self.config.clipStrayFluxFraction,    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 354, in deblend      psf, pk, sigma1, patchEdges)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 1073, in _handle_flux_at_edge      psfim = psfim.Factory(psfim, Sbox, afwImage.PARENT, True)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/afw/2015_10.0-8-g4057726/python/lsst/afw/image/imageLib.py"", line 4630, in Factory      return ImageD(*args)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/afw/2015_10.0-8-g4057726/python/lsst/afw/image/imageLib.py"", line 4472, in __init__      this = _imageLib.new_ImageD(*args)  LengthError:     File ""src/image/Image.cc"", line 92, in static typename lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const lsst::afw::geom::Extent2I&, const lsst::afw::geom::Extent2I&, const typename lsst::afw::image::detail::types_traits<PixelT, false>::view_t&) [with PixelT = double]      Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15 {0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15'  {code}    The second problem may occur when the overlap region between a PSF image and the data image it corresponds to is only 1 pixel in either dimension.  In any case, there's a gap in the graceful-failure logic that could let such a problem through, which would result in received error message:  {code:hide-linenum}  measureCoaddSources FATAL: Failed on dataId={'filter': 'i', 'tract': 1, 'patch': '8,1'}:   Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.  py"", line 321, in __call__      result = task.run(dataRef, **kwargs)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/pipe_tasks/python/lsst/pipe/tasks/multiBand.py"", line 552, i  n run      self.deblend.run(exposure, sources, exposure.getPsf())    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", l  ine 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/de  blend.py"", line 231, in run      self.deblend(exposure, sources, psf)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", l  ine 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/de  blend.py"", line 308, in deblend      clipStrayFluxFraction=self.config.clipStrayFluxFraction,    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 312, in deblend      tinyFootprintSize=tinyFootprintSize)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 575, in _fitPsfs      **kwargs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 752, in _fitPsf      sx1, sx2, sx3, sx4 = _overlap(xlo, xhi, px0+1, px1-1)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba  seline.py"", line 721, in _overlap      (xlo <= xhi)  and (xmin <= xmax))  AssertionError  {code}",2,DM-4743,datamanagement,"deblender robust weird psf dimension ~boutigny report problem psf dimension calculation deblender result fatal error early check bad dimension intend cause graceful failure incomplete appear happen psf dimension highly non square image width small 1.5x fwhm image height 1.5x fwhm opposite code hide linenum measurecoaddsource fatal fail dataid={'filter tract patch 8,2 file src image image.cc line 92 static typename lsst::afw::image::imagebase::_view_t lsst::afw image::imagebase::_makesubview(const lsst::afw::geom::extent2i const lsst::afw::geom::extent2i con typename lsst::afw::image::detail::types_traits::view_t pixelt double box2i(point2i(-2,2),extent2i(11,11 fit image 7x15 lsst::pex::exceptions::lengtherror box2i(point2i(-2,2),extent2i(11,11 fit image 7x15 traceback recent file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base cmdlinetask.py line 321 result task.run(dataref kwargs file /sps lsst dev lsstprod cluster my_package pipe_tasks python lsst pipe task multiband.py line 552 run self.deblend.run(exposure source exposure.getpsf file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender deblend.py line 231 run self.deblend(exposure source psf file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender deblend.py line 308 deblend clipstrayfluxfraction self.config.clipstrayfluxfraction file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender baseline.py line 354 deblend psf pk sigma1 patchedge file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender baseline.py line 1073 handle_flux_at_edge psfim psfim factory(psfim sbox afwimage parent true file /sps lsst library lsstsw stack linux64 afw/2015_10.0 g4057726 python lsst afw image imagelib.py line 4630 factory return imaged(*args file /sps lsst library lsstsw stack linux64 afw/2015_10.0 g4057726 python lsst afw image imagelib.py line 4472 init imagelib.new_imaged(*args lengtherror file src image image.cc line 92 static typename lsst::afw::image::imagebase::_view_t lsst::afw::image::imagebase::_makesubview(const lsst::afw::geom::extent2i const lsst::afw::geom::extent2i const typename lsst::afw::image::detail::types_traits::view_t pixelt double box2i(point2i(-2,2),extent2i(11,11 fit image 7x15 lsst::pex::exceptions::lengtherror box2i(point2i(-2,2),extent2i(11,11 fit image 7x15 code second problem occur overlap region psf image data image correspond pixel dimension case gap graceful failure logic let problem result receive error message code hide linenum measurecoaddsource fatal fail dataid={'filter tract patch 8,1 traceback recent file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base cmdlinetask py line 321 result task.run(dataref kwargs file /sps lsst dev lsstprod cluster my_package pipe_tasks python lsst pipe task multiband.py line 552 run self.deblend.run(exposure source exposure.getpsf file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base timer.py ine 118 wrapper res func(self args keyargs file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender de blend.py line 231 run self.deblend(exposure source psf file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base timer.py ine 118 wrapper res func(self args keyargs file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender de blend.py line 308 deblend clipstrayfluxfraction self.config.clipstrayfluxfraction file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender ba seline.py line 312 deblend tinyfootprintsize tinyfootprintsize file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender ba seline.py line 575 kwargs file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender ba seline.py line 752 fitpsf sx1 sx2 sx3 sx4 overlap(xlo xhi px0 px1 file /sps lsst library lsstsw stack linux64 meas_deblender/2015_10.0 ga5a97e7 python lsst meas deblender ba seline.py line 721 overlap xlo xhi xmin xmax assertionerror code","Make deblender more robust against weird PSF dimensions [~boutigny] reports two problems with PSF dimension calculations in the deblender that result in fatal errors, because earlier checks for bad dimensions intended to cause more graceful failures are incomplete. The first appears to happen when the PSF dimensions are highly non-square, and the image width is smaller than 1.5x FWHM while the image height is more than 1.5x FWHM (or the opposite). {code:hide-linenum} measureCoaddSources FATAL: Failed on dataId={'filter': 'i', 'tract': 1, 'patch': '8,2'}: File ""src/image/Image.cc"", line 92, in static typename lsst::afw::image::ImageBase::_view_t lsst::afw: :image::ImageBase::_makeSubView(const lsst::afw::geom::Extent2I&, const lsst::afw::geom::Extent2I&, cons t typename lsst::afw::image::detail::types_traits::view_t&) [with PixelT = double] Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15 {0} lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15' Traceback (most recent call last): File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 321, in __call__ result = task.run(dataRef, **kwargs) File ""/sps/lsst/dev/lsstprod/clusters/my_packages/pipe_tasks/python/lsst/pipe/tasks/multiBand.py"", line 552, in run self.deblend.run(exposure, sources, exposure.getPsf()) File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 231, in run self.deblend(exposure, sources, psf) File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/deblend.py"", line 308, in deblend clipStrayFluxFraction=self.config.clipStrayFluxFraction, File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 354, in deblend psf, pk, sigma1, patchEdges) File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/baseline.py"", line 1073, in _handle_flux_at_edge psfim = psfim.Factory(psfim, Sbox, afwImage.PARENT, True) File ""/sps/lsst/Library/lsstsw/stack/Linux64/afw/2015_10.0-8-g4057726/python/lsst/afw/image/imageLib.py"", line 4630, in Factory return ImageD(*args) File ""/sps/lsst/Library/lsstsw/stack/Linux64/afw/2015_10.0-8-g4057726/python/lsst/afw/image/imageLib.py"", line 4472, in __init__ this = _imageLib.new_ImageD(*args) LengthError: File ""src/image/Image.cc"", line 92, in static typename lsst::afw::image::ImageBase::_view_t lsst::afw::image::ImageBase::_makeSubView(const lsst::afw::geom::Extent2I&, const lsst::afw::geom::Extent2I&, const typename lsst::afw::image::detail::types_traits::view_t&) [with PixelT = double] Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15 {0} lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-2,2),Extent2I(11,11)) doesn't fit in image 7x15' {code} The second problem may occur when the overlap region between a PSF image and the data image it corresponds to is only 1 pixel in either dimension. In any case, there's a gap in the graceful-failure logic that could let such a problem through, which would result in received error message: {code:hide-linenum} measureCoaddSources FATAL: Failed on dataId={'filter': 'i', 'tract': 1, 'patch': '8,1'}: Traceback (most recent call last): File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask. py"", line 321, in __call__ result = task.run(dataRef, **kwargs) File ""/sps/lsst/dev/lsstprod/clusters/my_packages/pipe_tasks/python/lsst/pipe/tasks/multiBand.py"", line 552, i n run self.deblend.run(exposure, sources, exposure.getPsf()) File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", l ine 118, in wrapper res = func(self, *args, **keyArgs) File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/de blend.py"", line 231, in run self.deblend(exposure, sources, psf) File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/timer.py"", l ine 118, in wrapper res = func(self, *args, **keyArgs) File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/de blend.py"", line 308, in deblend clipStrayFluxFraction=self.config.clipStrayFluxFraction, File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba seline.py"", line 312, in deblend tinyFootprintSize=tinyFootprintSize) File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba seline.py"", line 575, in _fitPsfs **kwargs) File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba seline.py"", line 752, in _fitPsf sx1, sx2, sx3, sx4 = _overlap(xlo, xhi, px0+1, px1-1) File ""/sps/lsst/Library/lsstsw/stack/Linux64/meas_deblender/2015_10.0-3-ga5a97e7/python/lsst/meas/deblender/ba seline.py"", line 721, in _overlap (xlo <= xhi) and (xmin <= xmax)) AssertionError {code}"
Demonstrate web authentication using CILogon and Globus Configure mod_auth_oidc on lsst-auth1 with CILogon and Globus.,4,DM-4744,datamanagement,demonstrate web authentication cilogon globus configure mod_auth_oidc lsst auth1 cilogon globus,Demonstrate web authentication using CILogon and Globus Configure mod_auth_oidc on lsst-auth1 with CILogon and Globus.
"Prepare  activity diagrams and backing conops  for LI provisioning and ARP, including satellite computing centers. Prepared two longer con accompanied by (hand drawn activity diagrams).      One conops /activity  diagram describe the work at the archive center to provide and support the L1 services used by telescope operations.    the second activity diagram and conops respdes to Chuck Clavers' request to have materials that explain the relationship of the stiletto computing center at CCIN2P3 to Archive Center at NCSA.    Both are DRAFT; and coops seem to be systems engineering documents, and where to deliver a blessed version and who is responsible for this is unclear to me. ",5,DM-4746,datamanagement,prepare activity diagram back conop li provisioning arp include satellite computing center prepared long con accompany hand draw activity diagram conop /activity diagram describe work archive center provide support l1 service telescope operation second activity diagram conop respde chuck clavers request material explain relationship stiletto compute center ccin2p3 archive center ncsa draft coop system engineering document deliver bless version responsible unclear,"Prepare activity diagrams and backing conops for LI provisioning and ARP, including satellite computing centers. Prepared two longer con accompanied by (hand drawn activity diagrams). One conops /activity diagram describe the work at the archive center to provide and support the L1 services used by telescope operations. the second activity diagram and conops respdes to Chuck Clavers' request to have materials that explain the relationship of the stiletto computing center at CCIN2P3 to Archive Center at NCSA. Both are DRAFT; and coops seem to be systems engineering documents, and where to deliver a blessed version and who is responsible for this is unclear to me."
Beth Willman visit self explanatory,2,DM-4747,datamanagement,beth willman visit self explanatory,Beth Willman visit self explanatory
"Review gartner materials relating ITIL, Devops and related topic Read Gartner materials related to ITIL, devops  IT organization in preparation for more detailed thinking about Data Operations.    One major category of thought is ""Mode 1 and Mode 2"" type organizations. Mode ! is the current typical controlled environment the strength is when something precious needs to be managed.  For LSST this might be the data release production, which is baselined to be 9 months on a unique resource that the project procured.  Mode 2 is ""doves"" which is best used for nimble, fall fast software.  An example e testing algorithms.",3,DM-4748,datamanagement,review gartner material relate itil devops related topic read gartner material relate itil devop organization preparation detailed thinking data operations major category thought mode mode type organization mode current typical control environment strength precious need manage lsst data release production baseline month unique resource project procure mode dove well nimble fall fast software example testing algorithm,"Review gartner materials relating ITIL, Devops and related topic Read Gartner materials related to ITIL, devops IT organization in preparation for more detailed thinking about Data Operations. One major category of thought is ""Mode 1 and Mode 2"" type organizations. Mode ! is the current typical controlled environment the strength is when something precious needs to be managed. For LSST this might be the data release production, which is baselined to be 9 months on a unique resource that the project procured. Mode 2 is ""doves"" which is best used for nimble, fall fast software. An example e testing algorithms."
"Connecting table with histogram viewer Create a demo, which takes a URL and shows a table with a histogram viewer connected to it.",6,DM-4751,datamanagement,connect table histogram viewer create demo take url show table histogram viewer connect,"Connecting table with histogram viewer Create a demo, which takes a URL and shows a table with a histogram viewer connected to it."
"Cleanup location of anonymous namespaces we place anonymous namespace in two ways: (a), INSIDE lsst::qserv::<module> namespace, or (b) BEFORE. This story involves cleaning it up - move them to before lsst::qserv::<module>",1,DM-4753,datamanagement,cleanup location anonymous namespace place anonymous namespace way inside lsst::qserv namespace story involve clean lsst::qserv,"Cleanup location of anonymous namespaces we place anonymous namespace in two ways: (a), INSIDE lsst::qserv:: namespace, or (b) BEFORE. This story involves cleaning it up - move them to before lsst::qserv::"
"Add mysql connection to QueryContext  We need access to database schema for various reasons (analyzing queries, checking authorization, for queries like ""show create table"" and others).",3,DM-4754,datamanagement,add mysql connection querycontext need access database schema reason analyze query check authorization query like create table,"Add mysql connection to QueryContext We need access to database schema for various reasons (analyzing queries, checking authorization, for queries like ""show create table"" and others)."
"Support human-friendly Thread ID in logging messages Per discussion 1/6/2016, it'd be nice to have a function that generates user-friendly threadId  on Linux to simplify debugging.",4,DM-4756,datamanagement,support human friendly thread id log message discussion 1/6/2016 nice function generate user friendly threadid linux simplify debugging,"Support human-friendly Thread ID in logging messages Per discussion 1/6/2016, it'd be nice to have a function that generates user-friendly threadId on Linux to simplify debugging."
"Port Data  set info converter achitechture defines various image data types, how to get them, groupings, artifacts.   I am not quite happy with how we did in in GWT so the design needs to be improved.  Must be less complex.",8,DM-4759,datamanagement,port data set info converter achitechture define image datum type grouping artifact happy gwt design need improve complex,"Port Data set info converter achitechture defines various image data types, how to get them, groupings, artifacts. I am not quite happy with how we did in in GWT so the design needs to be improved. Must be less complex."
"Ops Planning - December - LOPT and TOWG meetings  - Beth Willman 2-day visit; discussions about operations, proposal timeline and deliverables    - Prepared FTE estimates for IT roles  	- Reviewed ITIL roles and clarified work descriptions  	- Met with NCSA ICI leads to get input on FTE estimates for various roles  - Timing diagrams  	- Worked on first draft of cycle diagram showing 24 hours of operations at NCSA",7,DM-4761,datamanagement,ops planning december lopt towg meeting beth willman day visit discussion operation proposal timeline deliverable prepared fte estimate role review itil role clarify work description met ncsa ici lead input fte estimate role time diagram work draft cycle diagram show 24 hour operation ncsa,"Ops Planning - December - LOPT and TOWG meetings - Beth Willman 2-day visit; discussions about operations, proposal timeline and deliverables - Prepared FTE estimates for IT roles - Reviewed ITIL roles and clarified work descriptions - Met with NCSA ICI leads to get input on FTE estimates for various roles - Timing diagrams - Worked on first draft of cycle diagram showing 24 hours of operations at NCSA"
CONOPS-V1 Evaluated first cut of CONOPS document. Formulated queries for clarifying specific questions regarding ldm-230 - 2,2,DM-4764,datamanagement,conops v1 evaluate cut conops document formulated query clarify specific question ldm-230,CONOPS-V1 Evaluated first cut of CONOPS document. Formulated queries for clarifying specific questions regarding ldm-230 - 2
Port W16 CModel improvements from HSC Three significant changes were made to CModel in [HSC-1339|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1339]. They were described by [~jbosch] in a [post to {{hsc_software}}|http://jeeves.astro.princeton.edu/pipermail/hsc_software/all/4568.html]. They include:    * Changing the method by which the initial approximation is determined;  * Changing the determination of the pixel region to use in fitting;  * A new prior on ellipticity and radius.    Please port these changes to LSST.    Also include the results of fixing the bug described in [HSC-1384|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1384].,4,DM-4768,datamanagement,port w16 cmodel improvement hsc significant change cmodel hsc-1339|https://hsc jira.astro.princeton.edu jira browse hsc-1339 describe ~jbosch post hsc_software}}|http://jeeves.astro.princeton.edu pipermail hsc_software all/4568.html include change method initial approximation determine change determination pixel region use fitting new prior ellipticity radius port change lsst include result fix bug describe hsc-1384|https://hsc jira.astro.princeton.edu jira browse hsc-1384,Port W16 CModel improvements from HSC Three significant changes were made to CModel in [HSC-1339|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1339]. They were described by [~jbosch] in a [post to {{hsc_software}}|http://jeeves.astro.princeton.edu/pipermail/hsc_software/all/4568.html]. They include: * Changing the method by which the initial approximation is determined; * Changing the determination of the pixel region to use in fitting; * A new prior on ellipticity and radius. Please port these changes to LSST. Also include the results of fixing the bug described in [HSC-1384|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1384].
"Week end 12/12/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending December 12, 2015.",2,DM-4771,datamanagement,week end 12/12/15 support lsst dev cluster openstack account week end december 12 2015,"Week end 12/12/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending December 12, 2015."
"Week end 12/19/15 Support for lsst-dev cluster, OpenStack, and accounts  for week ending December 19, 2015.",2,DM-4772,datamanagement,week end 12/19/15 support lsst dev cluster openstack account week end december 19 2015,"Week end 12/19/15 Support for lsst-dev cluster, OpenStack, and accounts for week ending December 19, 2015."
"New equipment setup and configuration (week end 12/05/15) * Setup IPMI on lsst-esxi3, lsst-esxi5, lsst-esxi6  ** Created ipmitools binary - Ubuntu 12.04 has the correct libraries for ESXi6  ** Installed on lsst-esxi3, lsst-esxi5, lsst-esxi6  ** Configuration worked well on lsst-esxi5, lsst-esxi6.  ** lsst-esxi3 is actually lsst-esxi4.  System will not respond to ipmitool commands - suggest waiting for scheduled outage and manually setting up ipmi.  * Debugged networking issues with new equipment  * Cleanup of crashplan archives for new lsst system  * Mac VMs  ** Working on figuring out Mac VM requirements and process with Josh Hobblitt  ** Attempted to install and configure puppet on Mac VMs - running into configuration issues  ",3,DM-4773,datamanagement,new equipment setup configuration week end 12/05/15 setup ipmi lsst esxi3 lsst esxi5 lsst esxi6 create ipmitool binary ubuntu 12.04 correct library esxi6 instal lsst esxi3 lsst esxi5 lsst esxi6 configuration work lsst esxi5 lsst esxi6 lsst esxi3 actually lsst esxi4 system respond ipmitool command suggest wait schedule outage manually set ipmi debug networking issue new equipment cleanup crashplan archive new lsst system mac vms work figure mac vm requirement process josh hobblitt attempt install configure puppet mac vms run configuration issue,"New equipment setup and configuration (week end 12/05/15) * Setup IPMI on lsst-esxi3, lsst-esxi5, lsst-esxi6 ** Created ipmitools binary - Ubuntu 12.04 has the correct libraries for ESXi6 ** Installed on lsst-esxi3, lsst-esxi5, lsst-esxi6 ** Configuration worked well on lsst-esxi5, lsst-esxi6. ** lsst-esxi3 is actually lsst-esxi4. System will not respond to ipmitool commands - suggest waiting for scheduled outage and manually setting up ipmi. * Debugged networking issues with new equipment * Cleanup of crashplan archives for new lsst system * Mac VMs ** Working on figuring out Mac VM requirements and process with Josh Hobblitt ** Attempted to install and configure puppet on Mac VMs - running into configuration issues"
"New equipment setup and configuration (week end 12/12/15) * Set up IPMI on lsst-test1, lsst-test2,…lsst-test9  * More research on using Puppet on Mac VMs – little progress  * Cleanup of NFS space in ITS  ",2,DM-4774,datamanagement,new equipment setup configuration week end 12/12/15 set ipmi lsst test1 lsst test2 lsst test9 research puppet mac vms little progress cleanup nfs space,"New equipment setup and configuration (week end 12/12/15) * Set up IPMI on lsst-test1, lsst-test2, lsst-test9 * More research on using Puppet on Mac VMs little progress * Cleanup of NFS space in ITS"
New equipment setup and configuration (week end 12/19/15) * Research on using puppet on Mac VMs  ** Considering using Vagrant to manage VirtualBox or Fusion Mac VMs  ** Tried to setup LDAP auth for Mac user auth  * Cleanup of NFS space in ITS,2,DM-4775,datamanagement,new equipment setup configuration week end 12/19/15 research puppet mac vms consider vagrant manage virtualbox fusion mac vms try setup ldap auth mac user auth cleanup nfs space,New equipment setup and configuration (week end 12/19/15) * Research on using puppet on Mac VMs ** Considering using Vagrant to manage VirtualBox or Fusion Mac VMs ** Tried to setup LDAP auth for Mac user auth * Cleanup of NFS space in ITS
"Updates to the Sizing Model Updated processor projections based upon Haswell and Skylake expectations. Added Shipping rates, Chilean and US power and cooling rates, updated memory pricing projections. Working with Spectra Logic on updating and improving tape predictions, library space and power requirements, upgrade options and mapping of bandwidth and capacity requirements to hardware (need to figure in fudge factors for latency of mounting tapes, latency of seeks times, maybe space for tape migrations, replace replacement tapes with updating pricing that includes tape replacement). Inclusion of that into the document will be in the next story.",3,DM-4777,datamanagement,update sizing model updated processor projection base haswell skylake expectation add shipping rate chilean power cool rate update memory pricing projection work spectra logic update improve tape prediction library space power requirement upgrade option mapping bandwidth capacity requirement hardware need figure fudge factor latency mount tape latency seek time maybe space tape migration replace replacement tape update pricing include tape replacement inclusion document story,"Updates to the Sizing Model Updated processor projections based upon Haswell and Skylake expectations. Added Shipping rates, Chilean and US power and cooling rates, updated memory pricing projections. Working with Spectra Logic on updating and improving tape predictions, library space and power requirements, upgrade options and mapping of bandwidth and capacity requirements to hardware (need to figure in fudge factors for latency of mounting tapes, latency of seeks times, maybe space for tape migrations, replace replacement tapes with updating pricing that includes tape replacement). Inclusion of that into the document will be in the next story."
"Contractual work, justifications, inventory for LSST hardware Reviewing hardware purchase contracts, reviewing internal hardware budget justifications (and attending related meetings), working with purchasing on 'vendor specific' purchasing options, incorporating updated vendor-quoted pricing into expected hardware expenditures. ",2,DM-4778,datamanagement,contractual work justification inventory lsst hardware reviewing hardware purchase contract review internal hardware budget justification attend related meeting work purchase vendor specific purchasing option incorporate update vendor quote pricing expect hardware expenditure,"Contractual work, justifications, inventory for LSST hardware Reviewing hardware purchase contracts, reviewing internal hardware budget justifications (and attending related meetings), working with purchasing on 'vendor specific' purchasing options, incorporating updated vendor-quoted pricing into expected hardware expenditures."
"meas_extensions_shapeHSM seems to be broken I have installed the meas_extensions_shapeHSM package together with galsim and tmv (I documented it at : https://github.com/DarkEnergyScienceCollaboration/ReprocessingTaskForce/wiki/Installing-the-LSST-DM-stack-and-the-related-packages#installing-meas_extensions_shapehsm) and tried to run it on CFHT cluster data.     My config file is the following:    {code:python}  import lsst.meas.extensions.shapeHSM  config.measurement.plugins.names |= [""ext_shapeHSM_HsmShapeRegauss"", ""ext_shapeHSM_HsmMoments"",                                      ""ext_shapeHSM_HsmPsfMoments""]  config.measurement.plugins['ext_shapeHSM_HsmShapeRegauss'].deblendNChild=''  config.measurement.slots.shape = ""ext_shapeHSM_HsmMoments""  {code}    When I run measCoaddSources.py, I get the following error :    {code}  Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/bin/measureCoaddSources.py"", line 3, in <module>      MeasureMergedCoaddSourcesTask.parseAndRun()    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 444, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run      if self.precall(parsedCmd):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall      task = self.makeTask(parsedCmd=parsedCmd)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 363, in makeTask      return self.TaskClass(config=self.config, log=self.log, butler=butler)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/python/lsst/pipe/tasks/multiBand.py"", line 530, in __init__      self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/task.py"", line 255, in makeSubtask      subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pex_config/2015_10.0-1-gc006da1/python/lsst/pex/config/configurableField.py"", line 77, in apply      return self.target(*args, config=self.value, **kw)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/sfm.py"", line 247, in __init__      self.initializePlugins(schema=self.schema)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins      self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 15, in __init__      self.cpp = self.factory(config, name, schema, metadata)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 223, in factory      return AlgClass(config.makeControl(), name, schema)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_extensions_shapeHSM/python/lsst/meas/extensions/shapeHSM/hsmLib.py"", line 964, in __init__      def __init__(self, *args, **kwargs): raise AttributeError(""No constructor defined - class is abstract"")  AttributeError: No constructor defined - class is abstract  {code}",1,DM-4780,datamanagement,"meas_extensions_shapehsm break instal meas_extensions_shapehsm package galsim tmv document https://github.com/darkenergysciencecollaboration/reprocessingtaskforce/wiki/installing-the-lsst-dm-stack-and-the-related-packages#installing-meas_extensions_shapehsm try run cfht cluster datum config file follow code python import lsst.meas.extensions.shapehsm config.measurement.plugins.name |= ext_shapehsm_hsmshaperegauss ext_shapehsm_hsmmoment ext_shapehsm_hsmpsfmoment config.measurement.plugins['ext_shapehsm_hsmshaperegauss'].deblendnchild= config.measurement.slots.shape ext_shapehsm_hsmmoment code run meascoaddsources.py follow error code traceback recent file /sps lsst library lsstsw stack linux64 pipe_tasks/2015_10.0 10 g1170fd0 bin measurecoaddsources.py line measuremergedcoaddsourcestask.parseandrun file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base cmdlinetask.py line 444 parseandrun resultlist taskrunner.run(parsedcmd file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base cmdlinetask.py line 192 run self.precall(parsedcmd file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base cmdlinetask.py line 279 precall task self.maketask(parsedcmd parsedcmd file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base cmdlinetask.py line 363 maketask return self taskclass(config self.config log self.log butler butler file /sps lsst library lsstsw stack linux64 pipe_tasks/2015_10.0 10 g1170fd0 python lsst pipe task multiband.py line 530 init self.makesubtask(""measurement schema self.schema algmetadata self.algmetadata file /sps lsst library lsstsw stack linux64 pipe_base/2015_10.0 g24e103a python lsst pipe base task.py line 255 makesubtask subtask configurablefield.apply(name parenttask self keyargs file /sps lsst library lsstsw stack linux64 pex_config/2015_10.0 gc006da1 python lsst pex config configurablefield.py line 77 apply return self.target(*args config self.value kw file /sps lsst dev lsstprod cluster my_packages meas_base python lsst meas base sfm.py line 247 init self.initializeplugins(schema self.schema file /sps lsst dev lsstprod cluster my_packages meas_base python lsst meas base basemeasurement.py line 298 initializeplugin self.plugins[name pluginclass(config metadata self.algmetadata kwds file /sps lsst dev lsstprod cluster my_packages meas_base python lsst meas base wrappers.py line 15 init self.cpp self.factory(config schema metadata file /sps lsst dev lsstprod cluster my_packages meas_base python lsst meas base wrappers.py line 223 factory return algclass(config.makecontrol schema file /sps lsst dev lsstprod cluster my_packages meas_extensions_shapehsm python lsst mea extension shapehsm hsmlib.py line 964 init def init__(self args kwargs raise attributeerror(""no constructor define class abstract attributeerror constructor define class abstract code","meas_extensions_shapeHSM seems to be broken I have installed the meas_extensions_shapeHSM package together with galsim and tmv (I documented it at : https://github.com/DarkEnergyScienceCollaboration/ReprocessingTaskForce/wiki/Installing-the-LSST-DM-stack-and-the-related-packages#installing-meas_extensions_shapehsm) and tried to run it on CFHT cluster data. My config file is the following: {code:python} import lsst.meas.extensions.shapeHSM config.measurement.plugins.names |= [""ext_shapeHSM_HsmShapeRegauss"", ""ext_shapeHSM_HsmMoments"", ""ext_shapeHSM_HsmPsfMoments""] config.measurement.plugins['ext_shapeHSM_HsmShapeRegauss'].deblendNChild='' config.measurement.slots.shape = ""ext_shapeHSM_HsmMoments"" {code} When I run measCoaddSources.py, I get the following error : {code} Traceback (most recent call last): File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/bin/measureCoaddSources.py"", line 3, in  MeasureMergedCoaddSourcesTask.parseAndRun() File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 444, in parseAndRun resultList = taskRunner.run(parsedCmd) File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run if self.precall(parsedCmd): File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall task = self.makeTask(parsedCmd=parsedCmd) File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 363, in makeTask return self.TaskClass(config=self.config, log=self.log, butler=butler) File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/python/lsst/pipe/tasks/multiBand.py"", line 530, in __init__ self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata) File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/task.py"", line 255, in makeSubtask subtask = configurableField.apply(name=name, parentTask=self, **keyArgs) File ""/sps/lsst/Library/lsstsw/stack/Linux64/pex_config/2015_10.0-1-gc006da1/python/lsst/pex/config/configurableField.py"", line 77, in apply return self.target(*args, config=self.value, **kw) File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/sfm.py"", line 247, in __init__ self.initializePlugins(schema=self.schema) File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds) File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 15, in __init__ self.cpp = self.factory(config, name, schema, metadata) File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 223, in factory return AlgClass(config.makeControl(), name, schema) File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_extensions_shapeHSM/python/lsst/meas/extensions/shapeHSM/hsmLib.py"", line 964, in __init__ def __init__(self, *args, **kwargs): raise AttributeError(""No constructor defined - class is abstract"") AttributeError: No constructor defined - class is abstract {code}"
MariaDB does not work together with mysql-proxy We have switched to MAriaDB but there is one issue that complicates things - mysql client from mariadb fails to connect to mysql-proxy with an error:  {noformat}  ERROR 1043 (08S01): Bad handshake  {noformat}  so Fabrice had to find a workaround for our setup to use client from mysqlclient package instead. This workaround is not perfect and it complicates other things. Would be nice to make things work transparently for mariadb.    ,2,DM-4781,datamanagement,mariadb work mysql proxy switch mariadb issue complicate thing mysql client mariadb fail connect mysql proxy error noformat error 1043 08s01 bad handshake noformat fabrice find workaround setup use client mysqlclient package instead workaround perfect complicate thing nice thing work transparently mariadb,MariaDB does not work together with mysql-proxy We have switched to MAriaDB but there is one issue that complicates things - mysql client from mariadb fails to connect to mysql-proxy with an error: {noformat} ERROR 1043 (08S01): Bad handshake {noformat} so Fabrice had to find a workaround for our setup to use client from mysqlclient package instead. This workaround is not perfect and it complicates other things. Would be nice to make things work transparently for mariadb.
JIRA project for the publication board The LSST Publication Board requests a JIRA project for managing its workload.       ,2,DM-4782,datamanagement,jira project publication board lsst publication board request jira project manage workload,JIRA project for the publication board The LSST Publication Board requests a JIRA project for managing its workload.
"Rename temporarily mariadb client MariaDB client isn't compliant with mysqlproxy and eups doesn't allow to override it with regular mysql client, so it will be rename, so that MYSQLCLIENT_DIR reference in qserv table file can be removed (indeed, it brokes qserv_distrib setup, but not qserv setup, ...)",1,DM-4783,datamanagement,rename temporarily mariadb client mariadb client compliant mysqlproxy eup allow override regular mysql client rename mysqlclient_dir reference qserv table file remove broke qserv_distrib setup qserv setup,"Rename temporarily mariadb client MariaDB client isn't compliant with mysqlproxy and eups doesn't allow to override it with regular mysql client, so it will be rename, so that MYSQLCLIENT_DIR reference in qserv table file can be removed (indeed, it brokes qserv_distrib setup, but not qserv setup, ...)"
Update provenance in baseline schema Current provenance schema in baseline (cat/sql) is very old and no longer reflect latest thinking. This story involves bringing cat/sql up to data and replacing existing prv_* tables with tables we came up with in the epic.,2,DM-4785,datamanagement,update provenance baseline schema current provenance schema baseline cat sql old long reflect late thinking story involve bring cat sql datum replace exist prv table table come epic,Update provenance in baseline schema Current provenance schema in baseline (cat/sql) is very old and no longer reflect latest thinking. This story involves bringing cat/sql up to data and replacing existing prv_* tables with tables we came up with in the epic.
"FITS Visualizer porting: Mouse Readout: part 2: flux value Call the server when mouse pauses, include the flux value in the readout. The should also include support for 3 color.",4,DM-4788,datamanagement,fit visualizer porting mouse readout flux value server mouse pause include flux value readout include support color,"FITS Visualizer porting: Mouse Readout: part 2: flux value Call the server when mouse pauses, include the flux value in the readout. The should also include support for 3 color."
FITS Visualizer porting: Mouse Readout: part 3: Lock by click & 3 color support add toggle button that make the mouse readout lock to last position click on.  It will not longer update on move but by click  Include: 3 Color Support,8,DM-4789,datamanagement,fit visualizer porting mouse readout lock click color support add toggle button mouse readout lock position click long update click include color support,FITS Visualizer porting: Mouse Readout: part 3: Lock by click & 3 color support add toggle button that make the mouse readout lock to last position click on. It will not longer update on move but by click Include: 3 Color Support
"Refactor prototype docs into “Developer Guide” and Science Pipelines doc projects Refactor [lsst_stack_docs|https://github.com/lsst-sqre/lsst_stack_docs] into two doc projects    - LSST DM Developer Guide that will be published to {{developer.lsst.io}}, and  - LSST Science Pipelines that will be published to {{pipelines.lsst.io}}",3,DM-4793,datamanagement,refactor prototype doc developer guide science pipelines doc project refactor lsst_stack_docs|https://github.com lsst sqre lsst_stack_docs doc project lsst dm developer guide publish developer.lsst.io lsst science pipelines publish pipelines.lsst.io,"Refactor prototype docs into Developer Guide and Science Pipelines doc projects Refactor [lsst_stack_docs|https://github.com/lsst-sqre/lsst_stack_docs] into two doc projects - LSST DM Developer Guide that will be published to {{developer.lsst.io}}, and - LSST Science Pipelines that will be published to {{pipelines.lsst.io}}"
Write Zoom Options Popup Write the simple zoom options popup that is show when the user clicks zoom too fast or the zoom level exceeds  the maximum size.      activate this popup from visualize/ui/ZoomButton.jsx,2,DM-4794,datamanagement,write zoom options popup write simple zoom option popup user click zoom fast zoom level exceed maximum size activate popup visualize ui zoombutton.jsx,Write Zoom Options Popup Write the simple zoom options popup that is show when the user clicks zoom too fast or the zoom level exceeds the maximum size. activate this popup from visualize/ui/ZoomButton.jsx
"DetectCoaddSourcesTask.scaleVariance gets wrong result DetectCoaddSourcesTask.scaleVariance is used to adjust the variance plane in the coadd to match the observed variance in the image plane (necessary after warping because we've lost variance into covariance). The current implementation produces the wrong scaling in cases where the image has strongly variable variance (e.g., 10 inputs contributed to half the image, but only 1 input contributed to the other half) because it calculates the variance of the image and the mean of the variance separately so that clipping can affect different pixels.    Getting this scaling very wrong can make us dig into the dirt when detecting objects, with drastic implications for the resultant catalog.    This is a port of [HSC-1357|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1357] and [HSC-1383|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1383].",1,DM-4798,datamanagement,detectcoaddsourcestask.scalevariance get wrong result detectcoaddsourcestask.scalevariance adjust variance plane coadd match observed variance image plane necessary warp lose variance covariance current implementation produce wrong scaling case image strongly variable variance e.g. 10 input contribute half image input contribute half calculate variance image mean variance separately clipping affect different pixel get scaling wrong dig dirt detect object drastic implication resultant catalog port hsc-1357|https://hsc jira.astro.princeton.edu jira browse hsc-1357 hsc-1383|https://hsc jira.astro.princeton.edu jira browse hsc-1383,"DetectCoaddSourcesTask.scaleVariance gets wrong result DetectCoaddSourcesTask.scaleVariance is used to adjust the variance plane in the coadd to match the observed variance in the image plane (necessary after warping because we've lost variance into covariance). The current implementation produces the wrong scaling in cases where the image has strongly variable variance (e.g., 10 inputs contributed to half the image, but only 1 input contributed to the other half) because it calculates the variance of the image and the mean of the variance separately so that clipping can affect different pixels. Getting this scaling very wrong can make us dig into the dirt when detecting objects, with drastic implications for the resultant catalog. This is a port of [HSC-1357|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1357] and [HSC-1383|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1383]."
"Update the ground truth values in the lsst_dm_demo to reflect new defaults in deblending In DM-4410 default configuration options were changed such that footprints are now grown in the detection task, and the deblender is run by default. This breaks the lsst_dm_demo, as now the results of processing are slightly different. The short term solution as part of DM-4410 was to run the demo with the defaults overridden to be what they were prior to DM-4410. In the long term the values used in the compare script should be updated to reflect what would be generated with running processCcd with the stack defaults. ",1,DM-4801,datamanagement,update ground truth value lsst_dm_demo reflect new default deblende dm-4410 default configuration option change footprint grow detection task deblender run default break lsst_dm_demo result processing slightly different short term solution dm-4410 run demo default overridden prior dm-4410 long term value compare script update reflect generate run processccd stack default,"Update the ground truth values in the lsst_dm_demo to reflect new defaults in deblending In DM-4410 default configuration options were changed such that footprints are now grown in the detection task, and the deblender is run by default. This breaks the lsst_dm_demo, as now the results of processing are slightly different. The short term solution as part of DM-4410 was to run the demo with the defaults overridden to be what they were prior to DM-4410. In the long term the values used in the compare script should be updated to reflect what would be generated with running processCcd with the stack defaults."
"Some wcs keywords need to be removed from the metadata of raw DECam data Header keys such as PVi_j left in the raw metadata confuse the making of wcs in later processing steps.   For example, when {{calexp}} is read in {{makeDiscreteSkyMap.py}}, {{makeCoaddTempExp.py}}, and so on, this message appears:  {code:java}  makeWcs: Interpreting RA---TAN-SIP/DEC--TAN-SIP + PVi_j as TPV  makeWcs WARNING: Stripping PVi_j keys from projection RA---TPV-SIP/DEC--TPV-SIP  {code}  These {{calexp}} are created by running {{processCcd.py}} on raw data, and are mis-interpreted as TPV.  ",7,DM-4805,datamanagement,wcs keyword need remove metadata raw decam datum header key pvi_j left raw metadata confuse making wcs later process step example calexp read makediscreteskymap.py makecoaddtempexp.py message appear code java makewcs interpreting ra tan sip dec tan sip pvi_j tpv makewcs warning strip pvi_j key projection ra tpv sip dec tpv sip code calexp create run processccd.py raw datum mis interpret tpv,"Some wcs keywords need to be removed from the metadata of raw DECam data Header keys such as PVi_j left in the raw metadata confuse the making of wcs in later processing steps. For example, when {{calexp}} is read in {{makeDiscreteSkyMap.py}}, {{makeCoaddTempExp.py}}, and so on, this message appears: {code:java} makeWcs: Interpreting RA---TAN-SIP/DEC--TAN-SIP + PVi_j as TPV makeWcs WARNING: Stripping PVi_j keys from projection RA---TPV-SIP/DEC--TPV-SIP {code} These {{calexp}} are created by running {{processCcd.py}} on raw data, and are mis-interpreted as TPV."
"Test stack with mariadbclient Now that we switched Qserv to mariadb, it'd be good to switch the rest of the stack. This story involves trying out if things still work if we switch mysqlclient to mariadbclient.",2,DM-4806,datamanagement,test stack mariadbclient switch qserv mariadb good switch rest stack story involve try thing work switch mysqlclient mariadbclient,"Test stack with mariadbclient Now that we switched Qserv to mariadb, it'd be good to switch the rest of the stack. This story involves trying out if things still work if we switch mysqlclient to mariadbclient."
Add Shared Scan Table Information to CSS  Some information should be added to CSS to indicate if a table should be locked in memory for shared scans and the effect the table is likely to have on the time it takes to complete a query.,4,DM-4807,datamanagement,add shared scan table information css information add css indicate table lock memory share scan effect table likely time take complete query,Add Shared Scan Table Information to CSS Some information should be added to CSS to indicate if a table should be locked in memory for shared scans and the effect the table is likely to have on the time it takes to complete a query.
"Package mariadbclient There are some very low level modules that depend on mysqlclient (for example daf_persistence). It'd be too harsh to make them depend on mariadb, so we should package mariadb client.",1,DM-4808,datamanagement,package mariadbclient low level module depend mysqlclient example daf_persistence harsh depend mariadb package mariadb client,"Package mariadbclient There are some very low level modules that depend on mysqlclient (for example daf_persistence). It'd be too harsh to make them depend on mariadb, so we should package mariadb client."
Create validation_data set for DECam validation test Create a `validation_data_decam` to provide a few images for DECam validation tests.    Use the COSMOS field data as currently available on NCSA being processed by [~nidever].    Select just a few images for now.,2,DM-4814,datamanagement,create validation_data set decam validation test create validation_data_decam provide image decam validation test use cosmos field datum currently available ncsa process ~nidever select image,Create validation_data set for DECam validation test Create a `validation_data_decam` to provide a few images for DECam validation tests. Use the COSMOS field data as currently available on NCSA being processed by [~nidever]. Select just a few images for now.
Planning for Software Documentation Deployment Service Write initial draft of [SQR-006|http://sqr-006.lsst.io] that specifies how the documentation deployment service will work.,4,DM-4815,datamanagement,plan software documentation deployment service write initial draft sqr-006|http://sqr-006.lsst.io specify documentation deployment service work,Planning for Software Documentation Deployment Service Write initial draft of [SQR-006|http://sqr-006.lsst.io] that specifies how the documentation deployment service will work.
Read and understand `ci_hsc` and plan relationship with `validate_drp` Read through and run the `ci_hsc` tests and plan for how this module and efforts should relate to `validate_drp`.    a. Add capabilities to `validate_drp` to run the tests in `ci_hsc`.  (/)  b. Compare frameworks. (/)  c. Plan for how such validation and continuous integration data sets should be constructed. (/)  ,2,DM-4817,datamanagement,read understand ci_hsc plan relationship validate_drp read run ci_hsc test plan module effort relate validate_drp a. add capability validate_drp run test ci_hsc compare framework c. plan validation continuous integration datum set construct,Read and understand `ci_hsc` and plan relationship with `validate_drp` Read through and run the `ci_hsc` tests and plan for how this module and efforts should relate to `validate_drp`. a. Add capabilities to `validate_drp` to run the tests in `ci_hsc`. (/) b. Compare frameworks. (/) c. Plan for how such validation and continuous integration data sets should be constructed. (/)
Improvement of raw data handling in DecamMapper Two minor improvements with better coding practice:  - Be more specific copying FITS header keywords. Avoid potential problems if unwelcome keywords appear in the header in the future. Suggested in the discussions in DM-4133.   - Reuse {{isr.getDefectListFromMask}} for converting defects. A more efficient method that uses the FootprintSet constructor with a Mask and a threshold has just been adopted in DM-4800.     Processing is not changed effectively.  ,1,DM-4820,datamanagement,improvement raw datum handling decammapper minor improvement well code practice specific copy fit header keyword avoid potential problem unwelcome keyword appear header future suggest discussion dm-4133 reuse isr.getdefectlistfrommask convert defect efficient method use footprintset constructor mask threshold adopt dm-4800 processing change effectively,Improvement of raw data handling in DecamMapper Two minor improvements with better coding practice: - Be more specific copying FITS header keywords. Avoid potential problems if unwelcome keywords appear in the header in the future. Suggested in the discussions in DM-4133. - Reuse {{isr.getDefectListFromMask}} for converting defects. A more efficient method that uses the FootprintSet constructor with a Mask and a threshold has just been adopted in DM-4800. Processing is not changed effectively.
HSC backport: Remove interpolated background before detection to reduce junk sources This is a port of [HSC-1353|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1353] and [HSC-1360|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1360].    Descriptions from HSC:   {panel:title=HSC-1353}  We typically get a large number of junk detections around bright objects due to noise fluctuations in the elevated background. We can try to reduce the number of junk detections by adding an additional local background subtraction before object detection. We can then add this back in after detection of footprints and peaks.  {panel}  {panel:title=HSC-1360}  I forgot to set the useApprox=True for the background subtraction that runs before footprint and peak detection. This will then use the Chebyshev instead of the spline.  {panel},1,DM-4821,datamanagement,hsc backport remove interpolate background detection reduce junk source port hsc-1353|https://hsc jira.astro.princeton.edu jira browse hsc-1353 hsc-1360|https://hsc jira.astro.princeton.edu jira browse hsc-1360 description hsc panel title hsc-1353 typically large number junk detection bright object noise fluctuation elevated background try reduce number junk detection add additional local background subtraction object detection add detection footprint peak panel panel title hsc-1360 forgot set useapprox true background subtraction run footprint peak detection use chebyshev instead spline panel,HSC backport: Remove interpolated background before detection to reduce junk sources This is a port of [HSC-1353|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1353] and [HSC-1360|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1360]. Descriptions from HSC: {panel:title=HSC-1353} We typically get a large number of junk detections around bright objects due to noise fluctuations in the elevated background. We can try to reduce the number of junk detections by adding an additional local background subtraction before object detection. We can then add this back in after detection of footprints and peaks. {panel} {panel:title=HSC-1360} I forgot to set the useApprox=True for the background subtraction that runs before footprint and peak detection. This will then use the Chebyshev instead of the spline. {panel}
Add Dropdowns to Vis toolbar Add the dropdown to the vis tool bar,2,DM-4823,datamanagement,add dropdowns vis toolbar add dropdown vis tool bar,Add Dropdowns to Vis toolbar Add the dropdown to the vis tool bar
Clean up div and css layout on FitsDownloadDialog FitsDownload dialogs html and css is not quite right. Needs some clean up.,1,DM-4824,datamanagement,clean div css layout fitsdownloaddialog fitsdownload dialog html css right need clean,Clean up div and css layout on FitsDownloadDialog FitsDownload dialogs html and css is not quite right. Needs some clean up.
"makeDiscreteSkymap has a default dataset of 'raw' The default dataset type for command line tasks is raw.  In the case MakeDiscreteSkyMapTask is asking the butler for calexp images.  This shouldn't be a problem, but in my case I have calexp images, but no raw images.  This causes the task to think there is no data to work on, so it exits.",1,DM-4825,datamanagement,makediscreteskymap default dataset raw default dataset type command line task raw case makediscreteskymaptask ask butler calexp image problem case calexp image raw image cause task think datum work exit,"makeDiscreteSkymap has a default dataset of 'raw' The default dataset type for command line tasks is raw. In the case MakeDiscreteSkyMapTask is asking the butler for calexp images. This shouldn't be a problem, but in my case I have calexp images, but no raw images. This causes the task to think there is no data to work on, so it exits."
Adapt `validate_drp` to standard python and bin subdir sturcture Move Python files into python/lsst namespace convention.  Decide on where {{validateCfht.py}} and {{validateDecam.py}} executables should live  Add package requirements to {{ups/validate_drp.table}},1,DM-4827,datamanagement,adapt validate_drp standard python bin subdir sturcture python file python lsst namespace convention decide validatecfht.py validatedecam.py executable live add package requirement ups validate_drp.table,Adapt `validate_drp` to standard python and bin subdir sturcture Move Python files into python/lsst namespace convention. Decide on where {{validateCfht.py}} and {{validateDecam.py}} executables should live Add package requirements to {{ups/validate_drp.table}}
"Add error handling to PsfFitter in meas::modelfit The {{ShapeletPsfApprox}} Task uses a class called {{PsfFitter}} which is not a {{SimpleAlgorithm}} and does not support error handling.  Add error handling to this class, and modify the Task definition in {{psf.py}} to call an {{errorHandler fail()}} function when the algorithm's {{optimizer.run()}} call fails.    Also, add unit tests",6,DM-4830,datamanagement,add error handling psffitter meas::modelfit shapeletpsfapprox task use class call psffitter simplealgorithm support error handling add error handling class modify task definition psf.py errorhandler fail function algorithm optimizer.run fail add unit test,"Add error handling to PsfFitter in meas::modelfit The {{ShapeletPsfApprox}} Task uses a class called {{PsfFitter}} which is not a {{SimpleAlgorithm}} and does not support error handling. Add error handling to this class, and modify the Task definition in {{psf.py}} to call an {{errorHandler fail()}} function when the algorithm's {{optimizer.run()}} call fails. Also, add unit tests"
"Add bright object masks to pipeline outputs Given per-patch inputs providing   {code}  id, B, V, R, ra, dec, radius    {code}  for each star to be masked, use this information to set:  * A bit in the mask plane for each affected pixel  * A flag in the source catalogues for each object that has a centroid lying within this mask area    This is a port of [HSC-1342|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1342] and [HSC-1381|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1381].",3,DM-4831,datamanagement,add bright object mask pipeline output give patch input provide code ra dec radius code star mask use information set bit mask plane affect pixel flag source catalogue object centroid lie mask area port hsc-1342|https://hsc jira.astro.princeton.edu jira browse hsc-1342 hsc-1381|https://hsc jira.astro.princeton.edu jira browse hsc-1381,"Add bright object masks to pipeline outputs Given per-patch inputs providing {code} id, B, V, R, ra, dec, radius {code} for each star to be masked, use this information to set: * A bit in the mask plane for each affected pixel * A flag in the source catalogues for each object that has a centroid lying within this mask area This is a port of [HSC-1342|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1342] and [HSC-1381|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1381]."
Update configuration for Suprime-Cam The {{obs_subaru}} configuration for Suprime-Cam needs updating to match recent changes in the stack.    Port of [HSC-1372|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1372].,1,DM-4833,datamanagement,update configuration suprime cam obs_subaru configuration suprime cam need update match recent change stack port hsc-1372|https://hsc jira.astro.princeton.edu jira browse hsc-1372,Update configuration for Suprime-Cam The {{obs_subaru}} configuration for Suprime-Cam needs updating to match recent changes in the stack. Port of [HSC-1372|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1372].
Preliminaries for LSST vs HSC pipeline comparison through coadd processing This is the equivalent of DM-3942 but through coadd processing.    Relevant HSC tickets include:    * [HSC-1371|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1371],1,DM-4834,datamanagement,preliminary lsst vs hsc pipeline comparison coadd process equivalent dm-3942 coadd processing relevant hsc ticket include hsc-1371|https://hsc jira.astro.princeton.edu jira browse hsc-1371,Preliminaries for LSST vs HSC pipeline comparison through coadd processing This is the equivalent of DM-3942 but through coadd processing. Relevant HSC tickets include: * [HSC-1371|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1371]
"Allow slurm to request total CPUs rather than nodes*processors. On some systems, we are asked to request a total number of tasks, rather than specify a combination of nodes and processors per node.    It also makes sense to use the SMP option this way.    This is a port of [HSC-1369|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1369].",2,DM-4835,datamanagement,allow slurm request total cpu nodes*processor system ask request total number task specify combination node processor node make sense use smp option way port hsc-1369|https://hsc jira.astro.princeton.edu jira browse hsc-1369,"Allow slurm to request total CPUs rather than nodes*processors. On some systems, we are asked to request a total number of tasks, rather than specify a combination of nodes and processors per node. It also makes sense to use the SMP option this way. This is a port of [HSC-1369|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1369]."
"Fix logic for applying aperture corrections With the current flow, the aperture corrections are being applied only after all the measurement plugins have run through, independent of their execution order.  This results in plugins whose measurements rely on aperture corrected fluxes (i.e. with execution order > APCORR_ORDER) being applied prior to the aperture correction, leading to erroneous results.  The only plugin currently affected by this is {{base_ClassificationExtendedness}}.    This ticket involves applying a temporary fix to ensure proper application and order of aperture corrections.  However, the problem highlights the fact that the current logic of how and when aperture corrections are applied should be reworked (on another ticket) to be less error-prone.",6,DM-4836,datamanagement,fix logic apply aperture correction current flow aperture correction apply measurement plugin run independent execution order result plugin measurement rely aperture correct flux i.e. execution order apply prior aperture correction lead erroneous result plugin currently affect base_classificationextendedness ticket involve apply temporary fix ensure proper application order aperture correction problem highlight fact current logic aperture correction apply rework ticket error prone,"Fix logic for applying aperture corrections With the current flow, the aperture corrections are being applied only after all the measurement plugins have run through, independent of their execution order. This results in plugins whose measurements rely on aperture corrected fluxes (i.e. with execution order > APCORR_ORDER) being applied prior to the aperture correction, leading to erroneous results. The only plugin currently affected by this is {{base_ClassificationExtendedness}}. This ticket involves applying a temporary fix to ensure proper application and order of aperture corrections. However, the problem highlights the fact that the current logic of how and when aperture corrections are applied should be reworked (on another ticket) to be less error-prone."
"Implement brighter-fatter correction Please port the prototype Brighter-Fatter correction work by Will Coulton from HSC.    This covers [HSC-1189|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1189], [HSC-1332| https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1368], [HSC-1368|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1368]. Note also the stand alone commits [783b124|https://github.com/HyperSuprime-Cam/obs_subaru/commit/783b124b6813f5745ce1e444f61fb0114d055907] and (if this work is performed after DM-3373) [9fc5e78| https://github.com/HyperSuprime-Cam/obs_subaru/commit/9fc5e78247e7173e095255dba34e994f73a6bd1d].",4,DM-4837,datamanagement,implement bright fatter correction port prototype bright fatter correction work coulton hsc cover hsc-1189|https://hsc jira.astro.princeton.edu jira browse hsc-1189 hsc-1332| https://hsc-jira.astro.princeton.edu/jira/browse/hsc-1368 hsc-1368|https://hsc jira.astro.princeton.edu jira browse hsc-1368 note stand commit 783b124|https://github.com hypersuprime cam obs_subaru commit/783b124b6813f5745ce1e444f61fb0114d055907 work perform dm-3373 9fc5e78| https://github.com/hypersuprime-cam/obs_subaru/commit/9fc5e78247e7173e095255dba34e994f73a6bd1d,"Implement brighter-fatter correction Please port the prototype Brighter-Fatter correction work by Will Coulton from HSC. This covers [HSC-1189|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1189], [HSC-1332| https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1368], [HSC-1368|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1368]. Note also the stand alone commits [783b124|https://github.com/HyperSuprime-Cam/obs_subaru/commit/783b124b6813f5745ce1e444f61fb0114d055907] and (if this work is performed after DM-3373) [9fc5e78| https://github.com/HyperSuprime-Cam/obs_subaru/commit/9fc5e78247e7173e095255dba34e994f73a6bd1d]."
"High-level overview of DRP processing Create high-level overview of Data Release Production, probably as an annotated flowchart, for use in sizing model work and as a graphical table of contents for more detailed descriptions.",6,DM-4839,datamanagement,high level overview drp processing create high level overview data release production probably annotated flowchart use size model work graphical table content detailed description,"High-level overview of DRP processing Create high-level overview of Data Release Production, probably as an annotated flowchart, for use in sizing model work and as a graphical table of contents for more detailed descriptions."
"Add sky objects Please add ""sources"" corresponding to empty sky (ie, at positions where nothing else has been detected) and include them in multiband processing.    This is a port of [HSC-1336|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1336] and [HSC-1358|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1358]. ",4,DM-4840,datamanagement,add sky object add source correspond sky ie position detect include multiband processing port hsc-1336|https://hsc jira.astro.princeton.edu jira browse hsc-1336 hsc-1358|https://hsc jira.astro.princeton.edu jira browse hsc-1358,"Add sky objects Please add ""sources"" corresponding to empty sky (ie, at positions where nothing else has been detected) and include them in multiband processing. This is a port of [HSC-1336|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1336] and [HSC-1358|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1358]."
Use high S/N band as reference for multiband forced photometry We are currently choosing the priority band as the reference band for forced photometry as long as it has a peak in the priority band regardless of the S/N.  Please change this to pick the highest S/N band as the reference band when the priority band S/N is sufficiently low.    This is a port of [HSC-1349|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1349].,1,DM-4841,datamanagement,use high band reference multiband force photometry currently choose priority band reference band force photometry long peak priority band regardless n. change pick high band reference band priority band sufficiently low port hsc-1349|https://hsc jira.astro.princeton.edu jira browse hsc-1349,Use high S/N band as reference for multiband forced photometry We are currently choosing the priority band as the reference band for forced photometry as long as it has a peak in the priority band regardless of the S/N. Please change this to pick the highest S/N band as the reference band when the priority band S/N is sufficiently low. This is a port of [HSC-1349|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1349].
Don't write HeavyFootprints in forced photometry There's no need to persist {{HeavyFootprint}}s while performing forced photometry since retrieving them is as simple as loading the _meas catalog.    This is a port of [HSC-1345|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1345].,1,DM-4842,datamanagement,write heavyfootprints force photometry need persist heavyfootprint}}s perform force photometry retrieve simple load mea catalog port hsc-1345|https://hsc jira.astro.princeton.edu jira browse hsc-1345,Don't write HeavyFootprints in forced photometry There's no need to persist {{HeavyFootprint}}s while performing forced photometry since retrieving them is as simple as loading the _meas catalog. This is a port of [HSC-1345|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1345].
Add new blendedness metric [HSC-1316|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1316] shifts the calculation of blendedness from {{meas_deblender}} to {{meas_algorithms}} and defines a new blendedness metric in the process. Please port it.,3,DM-4847,datamanagement,add new blendedness metric hsc-1316|https://hsc jira.astro.princeton.edu jira browse hsc-1316 shift calculation blendedness meas_deblender meas_algorithms define new blendedness metric process port,Add new blendedness metric [HSC-1316|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1316] shifts the calculation of blendedness from {{meas_deblender}} to {{meas_algorithms}} and defines a new blendedness metric in the process. Please port it.
Measure photometric repeatability and correctness of reported errors 1. Calculate and plot photometric variability across series of N images.  Compare to reported photometric errors.  Designed for N > 5.  2. Calculate and plot Delta flux / sigma_flux for multiple observations of stars in field.  This is related to 1. but is focused on N=2 to N=5.  3. Fit uncertainty distribution vs. magnitude to identify any floor in the photometric uncertainty and to check performance vs. photon counts.,5,DM-4848,datamanagement,measure photometric repeatability correctness report error calculate plot photometric variability series image compare report photometric error design calculate plot delta flux sigma_flux multiple observation star field relate focus n=2 n=5 fit uncertainty distribution vs. magnitude identify floor photometric uncertainty check performance vs. photon count,Measure photometric repeatability and correctness of reported errors 1. Calculate and plot photometric variability across series of N images. Compare to reported photometric errors. Designed for N > 5. 2. Calculate and plot Delta flux / sigma_flux for multiple observations of stars in field. This is related to 1. but is focused on N=2 to N=5. 3. Fit uncertainty distribution vs. magnitude to identify any floor in the photometric uncertainty and to check performance vs. photon counts.
"LDM-151 - comments from Jacek I am reading your https://github.com/lsst/LDM-151/blob/draft/DM_Applications_Design.tex, and I have some minor comments suggestions. I am going to add comments to this story to capture it. Feel free to apply to ignore :)",1,DM-4849,datamanagement,ldm-151 comment jacek read https://github.com/lsst/ldm-151/blob/draft/dm_applications_design.tex minor comment suggestion go add comment story capture feel free apply ignore,"LDM-151 - comments from Jacek I am reading your https://github.com/lsst/LDM-151/blob/draft/DM_Applications_Design.tex, and I have some minor comments suggestions. I am going to add comments to this story to capture it. Feel free to apply to ignore :)"
Factor out duplicate setIsPrimaryFlag from MeasureMergedCoaddSourcesTask and ProcessCoaddTask {{MeasureMergedCoaddSourcesTask.setIsPrimaryFlag()}} and {{ProcessCoaddTask.setIsPrimaryFlag()}} are effectively the same code. Please split this out into a separate task which both of the above can call.    This is a (partial) port of [HSC-1112|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1112] and should include fixes from [HSC-1297|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1297].,2,DM-4850,datamanagement,factor duplicate setisprimaryflag measuremergedcoaddsourcestask processcoaddtask measuremergedcoaddsourcestask.setisprimaryflag processcoaddtask.setisprimaryflag effectively code split separate task partial port hsc-1112|https://hsc jira.astro.princeton.edu jira browse hsc-1112 include fix hsc-1297|https://hsc jira.astro.princeton.edu jira browse hsc-1297,Factor out duplicate setIsPrimaryFlag from MeasureMergedCoaddSourcesTask and ProcessCoaddTask {{MeasureMergedCoaddSourcesTask.setIsPrimaryFlag()}} and {{ProcessCoaddTask.setIsPrimaryFlag()}} are effectively the same code. Please split this out into a separate task which both of the above can call. This is a (partial) port of [HSC-1112|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1112] and should include fixes from [HSC-1297|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1297].
XY Plot action and reducers Write action and reducers for XY Plot,6,DM-4851,datamanagement,xy plot action reducer write action reducer xy plot,XY Plot action and reducers Write action and reducers for XY Plot
"Implement zenodio.metadata to mediate Zenodo's API with local YAML metadata [Zenodio|http://zenodio.lsst.io] is a Python package we’re building to interact with Zenodo. For our various doc/technote/publishing projects we want to use YAML files (embedded in a Git repository, for example) to maintain deposition metadata so that the upload process itself can be automated.    The {{zenodio.metadata}} sub package provides a Python representation of Zenodo metadata (but not File or Zenodo deposition metadata).    See DM-4725 for the upload API work, which consumes the metadata objects.",2,DM-4852,datamanagement,implement zenodio.metadata mediate zenodo api local yaml metadata zenodio|http://zenodio.lsst.io python package build interact zenodo doc technote publishing project want use yaml file embed git repository example maintain deposition metadata upload process automate zenodio.metadata sub package provide python representation zenodo metadata file zenodo deposition metadata dm-4725 upload api work consume metadata object,"Implement zenodio.metadata to mediate Zenodo's API with local YAML metadata [Zenodio|http://zenodio.lsst.io] is a Python package we re building to interact with Zenodo. For our various doc/technote/publishing projects we want to use YAML files (embedded in a Git repository, for example) to maintain deposition metadata so that the upload process itself can be automated. The {{zenodio.metadata}} sub package provides a Python representation of Zenodo metadata (but not File or Zenodo deposition metadata). See DM-4725 for the upload API work, which consumes the metadata objects."
"Add __setitem__ for columns in afw.table It's confusing to have to use an extra {{[:]}} to set a column in afw.table, and we can make that unnecessary if we override {{\_\_setitem\_\_}} as well as {{\_\_getitem\_\_}}.",2,DM-4856,datamanagement,add setitem column afw.table confusing use extra :] set column afw.table unnecessary override \_\_setitem\_\ \_\_getitem\_\,"Add __setitem__ for columns in afw.table It's confusing to have to use an extra {{[:]}} to set a column in afw.table, and we can make that unnecessary if we override {{\_\_setitem\_\_}} as well as {{\_\_getitem\_\_}}."
Replace killproc and pidofproc with kill and pidof Running at NCSA on OpenStack revealed that our qserv-stop.sh and qserv-status.sh fail because of missing killproc and pidofproc. It looks like (see eg http://stackoverflow.com/questions/3013866/killproc-and-pidofproc-on-linux) these are not very portable and it is better to use kill and pidof.,1,DM-4857,datamanagement,replace killproc pidofproc kill pidof run ncsa openstack reveal qserv-stop.sh qserv-status.sh fail miss killproc pidofproc look like eg http://stackoverflow.com/questions/3013866/killproc-and-pidofproc-on-linux portable well use kill pidof,Replace killproc and pidofproc with kill and pidof Running at NCSA on OpenStack revealed that our qserv-stop.sh and qserv-status.sh fail because of missing killproc and pidofproc. It looks like (see eg http://stackoverflow.com/questions/3013866/killproc-and-pidofproc-on-linux) these are not very portable and it is better to use kill and pidof.
"imagesDiffer doesn't handle overflow for unsigned integers I'm seeing a test failure in afw's testTestMethods.py, apparently due to my numpy (1.8.2) treating images that differ by -1 as differing by 65535 in both {{numpy.allclose}} and array subtraction (which doesn't promote to an unsigned type).    Does this still cause problems in more recent versions of {{numpy}}?  If not, I imagine it's up to me to find a workaround for older versions if I want it fixed?    (assigning to [~rowen] for now, just because I know he originally wrote this test and I hope he might know more)",1,DM-4858,datamanagement,imagesdiffer handle overflow unsigned integer see test failure afw testtestmethods.py apparently numpy 1.8.2 treat image differ -1 differ 65535 numpy.allclose array subtraction promote unsigned type cause problem recent version numpy imagine find workaround old version want fix assign ~rowen know originally write test hope know,"imagesDiffer doesn't handle overflow for unsigned integers I'm seeing a test failure in afw's testTestMethods.py, apparently due to my numpy (1.8.2) treating images that differ by -1 as differing by 65535 in both {{numpy.allclose}} and array subtraction (which doesn't promote to an unsigned type). Does this still cause problems in more recent versions of {{numpy}}? If not, I imagine it's up to me to find a workaround for older versions if I want it fixed? (assigning to [~rowen] for now, just because I know he originally wrote this test and I hope he might know more)"
"Please provide ""getting started"" documentation on writing meas_base algorithms {{meas_base}} provides a framework for writing measurement algorithms in a uniform way. However, documentation on exactly how this should be done is fragmentary:    * There's some basic documentation in [Doxygen|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/meas_base.html] which provides a useful introduction, but doesn't discuss common idioms and helpers such as {{FlagHandler}}, {{SafeCentroidExtractor}} and transformations.  * The [design notes on Confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284390] are not intended as documentation and aren't kept up-to-date as new features are added, but can still be a useful reference.  * [~jbosch] gave a [nice introduction|https://github.com/lsst-dm/Oct15_bootcamp/blob/measurement/measurement/measurement.pdf] at the October 2015 Bootcamp, but a set of slides is no substitute for proper documentation, and again there's no expectation that these will be kept up-to-date.    Please provide a centralized, maintained guide to writing {{meas_base}} plugins.",4,DM-4861,datamanagement,provide getting start documentation write meas_base algorithm meas_base provide framework write measurement algorithm uniform way documentation exactly fragmentary basic documentation doxygen|https://lsst web.ncsa.illinois.edu doxygen x_masterdoxydoc meas_base.html provide useful introduction discuss common idiom helper flaghandler safecentroidextractor transformation design note confluence|https://confluence.lsstcorp.org pages viewpage.action?pageid=20284390 intend documentation keep date new feature add useful reference ~jbosch give nice introduction|https://github.com lsst dm oct15_bootcamp blob measurement measurement measurement.pdf october 2015 bootcamp set slide substitute proper documentation expectation keep date provide centralize maintain guide writing meas_base plugin,"Please provide ""getting started"" documentation on writing meas_base algorithms {{meas_base}} provides a framework for writing measurement algorithms in a uniform way. However, documentation on exactly how this should be done is fragmentary: * There's some basic documentation in [Doxygen|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/meas_base.html] which provides a useful introduction, but doesn't discuss common idioms and helpers such as {{FlagHandler}}, {{SafeCentroidExtractor}} and transformations. * The [design notes on Confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284390] are not intended as documentation and aren't kept up-to-date as new features are added, but can still be a useful reference. * [~jbosch] gave a [nice introduction|https://github.com/lsst-dm/Oct15_bootcamp/blob/measurement/measurement/measurement.pdf] at the October 2015 Bootcamp, but a set of slides is no substitute for proper documentation, and again there's no expectation that these will be kept up-to-date. Please provide a centralized, maintained guide to writing {{meas_base}} plugins."
"Add point selection click and highlight a point.  Is on when mouse readout ""Lock by Click"" is on. However, can me turned on externally by adding toolbar context menu options.",2,DM-4862,datamanagement,add point selection click highlight point mouse readout lock click turn externally add toolbar context menu option,"Add point selection click and highlight a point. Is on when mouse readout ""Lock by Click"" is on. However, can me turned on externally by adding toolbar context menu options."
Port HSC background matching routines HSC has its own implementation of background matching: see {{background.py}} in {{hscPipe}}. Please port it to the LSST stack.,4,DM-4865,datamanagement,port hsc background matching routine hsc implementation background matching background.py hscpipe port lsst stack,Port HSC background matching routines HSC has its own implementation of background matching: see {{background.py}} in {{hscPipe}}. Please port it to the LSST stack.
"Filter mask planes propagated to coadds Some mask planes -- {{CROSSTALK}}, {{NOT_DEBLENDED}} -- do not need to be propagated to coadds. Add an option to remove them.    This is a port of work performed on [HSC-1174|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1174] and [HSC-1294|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1294].",2,DM-4866,datamanagement,filter mask plane propagate coadd mask plane crosstalk not_deblended need propagate coadd add option remove port work perform hsc-1174|https://hsc jira.astro.princeton.edu jira browse hsc-1174 hsc-1294|https://hsc jira.astro.princeton.edu jira browse hsc-1294,"Filter mask planes propagated to coadds Some mask planes -- {{CROSSTALK}}, {{NOT_DEBLENDED}} -- do not need to be propagated to coadds. Add an option to remove them. This is a port of work performed on [HSC-1174|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1174] and [HSC-1294|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1294]."
"scisql build scripts are buggy  The scisql build script logic for MySQL/MariaDB version checking is broken on all platforms. There are also assumptions about shared library naming that do not hold on OS/X, which means that the deployment scripts are likely broken on all platforms other than Linux.",2,DM-4867,datamanagement,scisql build script buggy scisql build script logic mysql mariadb version checking break platform assumption share library naming hold os mean deployment script likely break platform linux,"scisql build scripts are buggy The scisql build script logic for MySQL/MariaDB version checking is broken on all platforms. There are also assumptions about shared library naming that do not hold on OS/X, which means that the deployment scripts are likely broken on all platforms other than Linux."
Installing a reference catalog to use in bulge survey processing Install Astrometry.net Index Files for 2MASS all sky catalog,5,DM-4869,datamanagement,instal reference catalog use bulge survey process install astrometry.net index files 2mass sky catalog,Installing a reference catalog to use in bulge survey processing Install Astrometry.net Index Files for 2MASS all sky catalog
"Test the matchOptimisticB astrometric matcher The matchOptimisticB matcher fails on many visits of the bulge verification dataset.  This prompted a deeper investigation of the performance of the matcher.  Angelo and David developed a test script and discovered that the matcher works well with offsets of the two source catalogs of up to 80 arcsec, but fails beyond that.  This should be robust enough for nearly all datasets that the LSST stack will be used on.",3,DM-4873,datamanagement,test matchoptimisticb astrometric matcher matchoptimisticb matcher fail visit bulge verification dataset prompt deep investigation performance matcher angelo david develop test script discover matcher work offset source catalog 80 arcsec fail robust nearly dataset lsst stack,"Test the matchOptimisticB astrometric matcher The matchOptimisticB matcher fails on many visits of the bulge verification dataset. This prompted a deeper investigation of the performance of the matcher. Angelo and David developed a test script and discovered that the matcher works well with offsets of the two source catalogs of up to 80 arcsec, but fails beyond that. This should be robust enough for nearly all datasets that the LSST stack will be used on."
"base has no readme The base package does not have a readme file, so it's unclear what it's for. The package name is also somewhat unfortunate, being so generic, but at least with a readme it would be clearer how important it is (if it is, in fact, important).",1,DM-4875,datamanagement,base readme base package readme file unclear package somewhat unfortunate generic readme clear important fact important,"base has no readme The base package does not have a readme file, so it's unclear what it's for. The package name is also somewhat unfortunate, being so generic, but at least with a readme it would be clearer how important it is (if it is, in fact, important)."
Compile list of DM simulation needs for Andy Connolly Compile list of DM simulation needs over the next ~6 months to give to Andy Connolly (simulations lead).,3,DM-4876,datamanagement,compile list dm simulation need andy connolly compile list dm simulation need ~6 month andy connolly simulation lead,Compile list of DM simulation needs for Andy Connolly Compile list of DM simulation needs over the next ~6 months to give to Andy Connolly (simulations lead).
"Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources. Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources.    - Use the butler to iterate over the data ids, read the src catalog and count the number of sources per ccd.    - Use afw.display.ds9 to display the image and overlay the sources",5,DM-4877,datamanagement,diagnostic plot show number process ccd failure visit function density source diagnostic plot show number process ccd failure visit function density source use butler iterate datum id read src catalog count number source ccd use afw.display.ds9 display image overlay source,"Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources. Diagnostic plot showing the number of process ccd failures in each visit as function of the density of sources. - Use the butler to iterate over the data ids, read the src catalog and count the number of sources per ccd. - Use afw.display.ds9 to display the image and overlay the sources"
"Propagate flags from individual visit measurements to coadd measurements It is useful to be able to identify suitable PSF stars from a coadd catalogue. However, the PSF is not determined on the coadd, but from all the inputs. Add a mechanism for propagating flags from the input catalogues to the coadd catalogue indicating stars that were used for measuring the PSF.    Make the inclusion fraction threshold configurable so we can tweak it (so we only get stars that were consistently used for the PSF model; the threshold might be set it to 0 for ""or"", 1 for ""all"" and something in between for ""some"").    Make the task sufficiently general that it can be used for propagating arbitrary flags.    This is a port of work carried out on [HSC-1052|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1052] and (part of) [HSC-1293|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1293].",2,DM-4878,datamanagement,propagate flag individual visit measurement coadd measurement useful able identify suitable psf star coadd catalogue psf determined coadd input add mechanism propagate flag input catalogue coadd catalogue indicate star measure psf inclusion fraction threshold configurable tweak star consistently psf model threshold set task sufficiently general propagate arbitrary flag port work carry hsc-1052|https://hsc jira.astro.princeton.edu jira browse hsc-1052 hsc-1293|https://hsc jira.astro.princeton.edu jira browse hsc-1293,"Propagate flags from individual visit measurements to coadd measurements It is useful to be able to identify suitable PSF stars from a coadd catalogue. However, the PSF is not determined on the coadd, but from all the inputs. Add a mechanism for propagating flags from the input catalogues to the coadd catalogue indicating stars that were used for measuring the PSF. Make the inclusion fraction threshold configurable so we can tweak it (so we only get stars that were consistently used for the PSF model; the threshold might be set it to 0 for ""or"", 1 for ""all"" and something in between for ""some""). Make the task sufficiently general that it can be used for propagating arbitrary flags. This is a port of work carried out on [HSC-1052|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1052] and (part of) [HSC-1293|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1293]."
"Make coadd input catalogs contiguous It's convenient if we can assume that coadd input catalogs are contiguous -- it simplifies the implementation of {{PropagateVisitFlagsTask}} (DM-4878), for example. Make it so.    This is port of work carried out on [HSC-1293|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1293].",1,DM-4879,datamanagement,coadd input catalog contiguous convenient assume coadd input catalog contiguous simplify implementation propagatevisitflagstask dm-4878 example port work carry hsc-1293|https://hsc jira.astro.princeton.edu jira browse hsc-1293,"Make coadd input catalogs contiguous It's convenient if we can assume that coadd input catalogs are contiguous -- it simplifies the implementation of {{PropagateVisitFlagsTask}} (DM-4878), for example. Make it so. This is port of work carried out on [HSC-1293|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1293]."
"base_Variance plugin generates errors in lsst_dm_stack_demo Since DM-4235 was merged, we see a bunch of messages along the lines of:  {code}  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797076: The center is outside the Footprint of the source record  {code}  in the output from {{lsst_dm_stack_demo}}. (See e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/7482/console#console-section-3]). It's not fatal, but the warnings are disconcerting and could be indicative of a deeper problem.",2,DM-4882,datamanagement,base_variance plugin generate error lsst_dm_stack_demo dm-4235 merge bunch message line code processccd.measurement warning error base_variance.measure record 427969358631797076 center outside footprint source record code output lsst_dm_stack_demo e.g. here|https://ci.lsst.codes job stack os matrix label centos-6/7482 console#console section-3 fatal warning disconcert indicative deep problem,"base_Variance plugin generates errors in lsst_dm_stack_demo Since DM-4235 was merged, we see a bunch of messages along the lines of: {code} processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797076: The center is outside the Footprint of the source record {code} in the output from {{lsst_dm_stack_demo}}. (See e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/7482/console#console-section-3]). It's not fatal, but the warnings are disconcerting and could be indicative of a deeper problem."
"Refactor measurement afterburners into a new plugin system Some of the operations we currently run as part of measurement (or would like to) share some features that make them a bit different from most plugin algorithms:   - They must be run after at least some other high-level plugins, and may be run after all of them.   - They do not require access to pixel data, as they derive their outputs entirely from other plugins' catalog outputs.   - They may require an aggregation stage of some sort to be run on the regular plugin output before they can be run.    Some examples include:   - Star/Galaxy classification (with training done after measurement and before classification).   - Applying aperture corrections (estimating the correction must be done first).   - BFD's P, Q, R statistics (requires a prior estimated from deep data).    We should move these algorithms to a new plugin system that's run by a new subtask, allowing these plugins to be run entirely separately from {{SingleFrameMeasurementTask}}.  This will simplify some of the currently contorted logic required to make S/G classification happen after aperture correction, while making room for hierarchical inference algorithms like BFD and Bayesian S/G classification in the future.    (We will not be able to support BFD immediately, as this will also require changes to our parallelization approach, but this will be a step in the right direction).    This work should *probably* be delayed until after the HSC merge and [~rowen]'s rewrite of {{ProcessCcdTask}} are complete, but it's conceivable that this refactoring could solve emergent problems there and be worth doing earlier as a result.",8,DM-4887,datamanagement,refactor measurement afterburner new plugin system operation currently run measurement like share feature bit different plugin algorithm run high level plugin run require access pixel datum derive output entirely plugin catalog output require aggregation stage sort run regular plugin output run example include star galaxy classification training measurement classification apply aperture correction estimate correction bfd statistic require prior estimate deep datum algorithm new plugin system run new subtask allow plugin run entirely separately singleframemeasurementtask simplify currently contort logic require classification happen aperture correction make room hierarchical inference algorithm like bfd bayesian classification future able support bfd immediately require change parallelization approach step right direction work probably delay hsc merge ~rowen rewrite processccdtask complete conceivable refactoring solve emergent problem worth early result,"Refactor measurement afterburners into a new plugin system Some of the operations we currently run as part of measurement (or would like to) share some features that make them a bit different from most plugin algorithms: - They must be run after at least some other high-level plugins, and may be run after all of them. - They do not require access to pixel data, as they derive their outputs entirely from other plugins' catalog outputs. - They may require an aggregation stage of some sort to be run on the regular plugin output before they can be run. Some examples include: - Star/Galaxy classification (with training done after measurement and before classification). - Applying aperture corrections (estimating the correction must be done first). - BFD's P, Q, R statistics (requires a prior estimated from deep data). We should move these algorithms to a new plugin system that's run by a new subtask, allowing these plugins to be run entirely separately from {{SingleFrameMeasurementTask}}. This will simplify some of the currently contorted logic required to make S/G classification happen after aperture correction, while making room for hierarchical inference algorithms like BFD and Bayesian S/G classification in the future. (We will not be able to support BFD immediately, as this will also require changes to our parallelization approach, but this will be a step in the right direction). This work should *probably* be delayed until after the HSC merge and [~rowen]'s rewrite of {{ProcessCcdTask}} are complete, but it's conceivable that this refactoring could solve emergent problems there and be worth doing earlier as a result."
"Update git-lfs documentation to work with git-lfs 1.1.0+ The git-lfs client (1.1.0+) does not support empty username and passwords. To work around this, users can store the appropriate credentials directly with the credential helper.",3,DM-4888,datamanagement,update git lfs documentation work git lfs 1.1.0 git lfs client 1.1.0 support username password work user store appropriate credential directly credential helper,"Update git-lfs documentation to work with git-lfs 1.1.0+ The git-lfs client (1.1.0+) does not support empty username and passwords. To work around this, users can store the appropriate credentials directly with the credential helper."
Update git-lfs repositories to point to the git-lfs documentation. Update git-lfs repositories to point to the git-lfs documentation.    All documentation should be generic and point to:    http://developer.lsst.io/en/latest/tools/git_lfs.html  ,1,DM-4889,datamanagement,update git lfs repository point git lfs documentation update git lfs repository point git lfs documentation documentation generic point http://developer.lsst.io/en/latest/tools/git_lfs.html,Update git-lfs repositories to point to the git-lfs documentation. Update git-lfs repositories to point to the git-lfs documentation. All documentation should be generic and point to: http://developer.lsst.io/en/latest/tools/git_lfs.html
"Write tutorial describing remote IPython + ds9 on lsst-dev [~mfisherlevine] recently figured out how to set up his system to run a remote IPython kernel on {{lsst-dev}} and interact with it from his laptop, including streaming image display from the remote system to a local instance of {{ds9}}.    He will write all this up so that others in the community can easily do the same.",2,DM-4893,datamanagement,write tutorial describe remote ipython ds9 lsst dev ~mfisherlevine recently figure set system run remote ipython kernel lsst dev interact laptop include stream image display remote system local instance ds9 write community easily,"Write tutorial describing remote IPython + ds9 on lsst-dev [~mfisherlevine] recently figured out how to set up his system to run a remote IPython kernel on {{lsst-dev}} and interact with it from his laptop, including streaming image display from the remote system to a local instance of {{ds9}}. He will write all this up so that others in the community can easily do the same."
"Ingest DECam/CBP data into LSST stack [~mfisherlevine] will ingest the data taken in DM-4892 into the LSST stack. Initial experiments indicate problems with:    * Bias subtraction  * Flat fielding  * Bad pixel masks    These may already be remedied by work on {{obs_decam}}; if not, he will file stories and fix them.",3,DM-4894,datamanagement,ingest decam cbp datum lsst stack ~mfisherlevine ingest datum take dm-4892 lsst stack initial experiment indicate problem bias subtraction flat fielding bad pixel mask remedie work obs_decam file story fix,"Ingest DECam/CBP data into LSST stack [~mfisherlevine] will ingest the data taken in DM-4892 into the LSST stack. Initial experiments indicate problems with: * Bias subtraction * Flat fielding * Bad pixel masks These may already be remedied by work on {{obs_decam}}; if not, he will file stories and fix them."
"Prepare calibration products for analysing DECam data Determine if existing bad pixel masks, flats, etc are adequate for analysing the DM-4893 DECam data, and, if not, provide alternatives.",3,DM-4895,datamanagement,prepare calibration product analyse decam datum determine exist bad pixel mask flat etc adequate analyse dm-4893 decam datum provide alternative,"Prepare calibration products for analysing DECam data Determine if existing bad pixel masks, flats, etc are adequate for analysing the DM-4893 DECam data, and, if not, provide alternatives."
"Qualitative exploration of the CBP/DECam data Having got the CBP/DECam data loaded into the stack, explore the parameter space and understand data.    This should result in a series of stories describing more detailed analysis with quantitative results.",8,DM-4897,datamanagement,qualitative exploration cbp decam datum having get cbp decam datum load stack explore parameter space understand datum result series story describe detailed analysis quantitative result,"Qualitative exploration of the CBP/DECam data Having got the CBP/DECam data loaded into the stack, explore the parameter space and understand data. This should result in a series of stories describing more detailed analysis with quantitative results."
"Use yaml configuration files to store camera-specific data ID and ref image information for validation testing. Currently there is {{validateCfht.py}} and {{validateDecam.py}} as code.  These differ in just having {{defaultData}} functions that specify the dataIds to consider and the dataIds to use as a reference for comparison.    Storing the information necessary to create these sets of dataIds in separate data files, to be stored as YAML would  1. Improve the separation of code and data  2. Clarify the usage and necessary information to run on a new or different set of data  3. Make it easier to run different subsets easily by specifying a different input file    The proposals is that {{validateCfht.py}} and {{validateDecam.py}} would disappear from {{bin}} and be replaced by just {{validate_drp.py}}.  The examples in {{examples/runCfhtTest.sh}} and {{examples/runDecamTest.sh}} will be updated to show the new usage.  The README will also be updated.",1,DM-4901,datamanagement,use yaml configuration file store camera specific data id ref image information validation testing currently validatecfht.py validatedecam.py code differ have defaultdata function specify consider dataids use reference comparison store information necessary create set dataids separate datum file store yaml improve separation code datum clarify usage necessary information run new different set datum easy run different subset easily specify different input file proposal validatecfht.py validatedecam.py disappear bin replace validate_drp.py example example runcfhttest.sh example rundecamtest.sh update new usage readme update,"Use yaml configuration files to store camera-specific data ID and ref image information for validation testing. Currently there is {{validateCfht.py}} and {{validateDecam.py}} as code. These differ in just having {{defaultData}} functions that specify the dataIds to consider and the dataIds to use as a reference for comparison. Storing the information necessary to create these sets of dataIds in separate data files, to be stored as YAML would 1. Improve the separation of code and data 2. Clarify the usage and necessary information to run on a new or different set of data 3. Make it easier to run different subsets easily by specifying a different input file The proposals is that {{validateCfht.py}} and {{validateDecam.py}} would disappear from {{bin}} and be replaced by just {{validate_drp.py}}. The examples in {{examples/runCfhtTest.sh}} and {{examples/runDecamTest.sh}} will be updated to show the new usage. The README will also be updated."
"Expand button hide/show, delete button hide/show, display title options, Expand button hide/show, delete button hide/show, display title options,  support pv.hideTitleDetail to control showing zoom level and rotation info (used by planck)  support external title bar (planck as well)  support checkbox on title bar (planck)  ",4,DM-4903,datamanagement,expand button hide delete button hide display title option expand button hide delete button hide display title option support pv.hidetitledetail control show zoom level rotation info planck support external title bar planck support checkbox title bar planck,"Expand button hide/show, delete button hide/show, display title options, Expand button hide/show, delete button hide/show, display title options, support pv.hideTitleDetail to control showing zoom level and rotation info (used by planck) support external title bar (planck as well) support checkbox on title bar (planck)"
"Buffer overrun in wcslib causes stack corruption The buffer 'msg' in wcsfix.c is used to report attempts by wcslib to re-format units found in fits files. It is allocated on the stack (in function 'unitfix') using a pre-processor macro defined size of 160 chars (set in wcserr.h). When attempting to run the function 'unitfix' in wcsfix, this buffer can overflow on some fits files (the raw files generated by HSC seem particularly prone to triggering this behavior) and results in the session being terminated on Ubuntu 14.04 as stack protection is turned on by default i.e. the stack crashes with a 'stack smashing detected' error. We have reported the bug to the creators of wcslib. As a temporary workaround, users affected by the bug should increase the default size of 'msg' by increasing WCSERR_MSG_LENGTH defined in wcserr.h      We are providing a small python example that demonstrates the problem. Run it as  python test.py <path to ci_hsc>/raw/<any fits file in this directory>    We are also providing a simple c program to demonstrate the bug. Compile it as  cc -fsanitize=address -g -I$WCSLIB_DIR/include/wcslib -o test test.c -L$WCSLIB_DIR/lib -lwcs (on Linux)  cc -fsanitize=address -g -L$WCSLIB_DIR/lib -lwcs -I$WCSLIB_DIR/include/wcslib -o test test.c (on Mac OS X)",2,DM-4904,datamanagement,buffer overrun wcslib cause stack corruption buffer msg report attempt wcslib format unit find fit file allocate stack function unitfix pre processor macro define size 160 char set wcserr.h attempt run function unitfix wcsfix buffer overflow fit file raw file generate hsc particularly prone trigger behavior result session terminate ubuntu 14.04 stack protection turn default i.e. stack crash stack smashing detect error report bug creator wcslib temporary workaround user affect bug increase default size msg increase wcserr_msg_length define wcserr.h provide small python example demonstrate problem run python test.py provide simple program demonstrate bug compile cc -fsanitize address -i$wcslib_dir include wcslib test test.c -l$wcslib_dir lib -lwcs linux cc -fsanitize address -l$wcslib_dir lib -lwcs -i$wcslib_dir include wcslib test test.c mac os,"Buffer overrun in wcslib causes stack corruption The buffer 'msg' in wcsfix.c is used to report attempts by wcslib to re-format units found in fits files. It is allocated on the stack (in function 'unitfix') using a pre-processor macro defined size of 160 chars (set in wcserr.h). When attempting to run the function 'unitfix' in wcsfix, this buffer can overflow on some fits files (the raw files generated by HSC seem particularly prone to triggering this behavior) and results in the session being terminated on Ubuntu 14.04 as stack protection is turned on by default i.e. the stack crashes with a 'stack smashing detected' error. We have reported the bug to the creators of wcslib. As a temporary workaround, users affected by the bug should increase the default size of 'msg' by increasing WCSERR_MSG_LENGTH defined in wcserr.h We are providing a small python example that demonstrates the problem. Run it as python test.py /raw/ We are also providing a simple c program to demonstrate the bug. Compile it as cc -fsanitize=address -g -I$WCSLIB_DIR/include/wcslib -o test test.c -L$WCSLIB_DIR/lib -lwcs (on Linux) cc -fsanitize=address -g -L$WCSLIB_DIR/lib -lwcs -I$WCSLIB_DIR/include/wcslib -o test test.c (on Mac OS X)"
"Cyber security infrastructure requirements Documenting cyber security operational requirements by LSST, particularly at the obs. site.",2,DM-4907,datamanagement,cyber security infrastructure requirement documenting cyber security operational requirement lsst particularly obs site,"Cyber security infrastructure requirements Documenting cyber security operational requirements by LSST, particularly at the obs. site."
"Security plan renewal Continuing work on cyber sec. plan renewal.  DM moving along, PO slated next.",1,DM-4908,datamanagement,security plan renewal continue work cyber sec plan renewal dm move po slate,"Security plan renewal Continuing work on cyber sec. plan renewal. DM moving along, PO slated next."
DM security meeting Security meeting/planning with LSST DM team at NCSA.,1,DM-4909,datamanagement,dm security meeting security meeting planning lsst dm team ncsa,DM security meeting Security meeting/planning with LSST DM team at NCSA.
"LSST IaM bi-weekly coordinating meeting Meeting between NCSA CSD group, NCSA LSST DM group, and other LSST groups.",1,DM-4912,datamanagement,lsst iam bi weekly coordinating meeting meeting ncsa csd group ncsa lsst dm group lsst group,"LSST IaM bi-weekly coordinating meeting Meeting between NCSA CSD group, NCSA LSST DM group, and other LSST groups."
"Add option for object name resolution For some object names resolved by NED, the position is not right. In this situation, it would be better to get the position from Simbad. Currently, Firefly offers two options: first NED, then Simbad; first Simbad, then NED. The third option to be added would be ""the best position according to the object type"".     It should check the object type returned by NED, making a decision whether to get position from Simbad instead; and vice versa. ",6,DM-4915,datamanagement,add option object resolution object name resolve ned position right situation well position simbad currently firefly offer option ned simbad simbad ned option add good position accord object type check object type return ned make decision position simbad instead vice versa,"Add option for object name resolution For some object names resolved by NED, the position is not right. In this situation, it would be better to get the position from Simbad. Currently, Firefly offers two options: first NED, then Simbad; first Simbad, then NED. The third option to be added would be ""the best position according to the object type"". It should check the object type returned by NED, making a decision whether to get position from Simbad instead; and vice versa."
"Test obs_decam with processed data Sometimes DECam-specific bugs only reveal in or affect the processed data. For example the bug of DM-4859 reveals in the {{postISRCCD}} products.  If the bugs are DECam-specific, some changes in {{obs_decam}} are likely needed.  It would be useful to have a more convenient way to test those changes. In this ticket I modify {{testdata_decam}} so that those data can be processed, and then allow wider options in the {{obs_decam}} unit tests.    I add {{testProcessCcd.py}} in {{obs_decam}} that runs {{processCcd.py}} with raw and calibration data in {{testdata_decam}}.  Besides a short sanity check, I add a test (testWcsPostIsr) that tests DM-4859. {{testWcsPostIsr}} fails without the DM-4859 fix, and passes with it.  ",3,DM-4916,datamanagement,test obs_decam process datum decam specific bug reveal affect process datum example bug dm-4859 reveal postisrccd product bug decam specific change obs_decam likely need useful convenient way test change ticket modify testdata_decam datum process allow wide option obs_decam unit test add testprocessccd.py obs_decam run processccd.py raw calibration datum testdata_decam short sanity check add test testwcspostisr test dm-4859 testwcspostisr fail dm-4859 fix pass,"Test obs_decam with processed data Sometimes DECam-specific bugs only reveal in or affect the processed data. For example the bug of DM-4859 reveals in the {{postISRCCD}} products. If the bugs are DECam-specific, some changes in {{obs_decam}} are likely needed. It would be useful to have a more convenient way to test those changes. In this ticket I modify {{testdata_decam}} so that those data can be processed, and then allow wider options in the {{obs_decam}} unit tests. I add {{testProcessCcd.py}} in {{obs_decam}} that runs {{processCcd.py}} with raw and calibration data in {{testdata_decam}}. Besides a short sanity check, I add a test (testWcsPostIsr) that tests DM-4859. {{testWcsPostIsr}} fails without the DM-4859 fix, and passes with it."
"Porting encodeURL of the java FitsDownlaodDialog code to javascript  When download an image,  the proper name needs to be resolved based on the URL and   the information about the image.  In Java code, it has the following three methods:  {code}   encodeUrl  makeFileName  makeTitleFileName  {code}    These method should be ported to javascript.  Thus, the javascript version of the FitsDownloadDialog will save the file in the same manner. ",2,DM-4917,datamanagement,port encodeurl java fitsdownlaoddialog code javascript download image proper need resolve base url information image java code follow method code encodeurl makefilename maketitlefilename code method port javascript javascript version fitsdownloaddialog save file manner,"Porting encodeURL of the java FitsDownlaodDialog code to javascript When download an image, the proper name needs to be resolved based on the URL and the information about the image. In Java code, it has the following three methods: {code} encodeUrl makeFileName makeTitleFileName {code} These method should be ported to javascript. Thus, the javascript version of the FitsDownloadDialog will save the file in the same manner."
"Test performance of vertical-partition joins in mysql We are planning to vertically partition some tables (for example Object). We should make sure such joins across say 5, 10 or 20 tables are not a problem for mysql from performance standpoint. The testing involves creating a wide table (say 200 columns) and testing a speed of full scan, then slicing that table vertically into different number of columns and using join to assemble the pieces together.",4,DM-4919,datamanagement,test performance vertical partition join mysql plan vertically partition table example object sure join 10 20 table problem mysql performance standpoint testing involve create wide table 200 column test speed scan slice table vertically different number column join assemble piece,"Test performance of vertical-partition joins in mysql We are planning to vertically partition some tables (for example Object). We should make sure such joins across say 5, 10 or 20 tables are not a problem for mysql from performance standpoint. The testing involves creating a wide table (say 200 columns) and testing a speed of full scan, then slicing that table vertically into different number of columns and using join to assemble the pieces together."
"Support Multi image fits and controls add toolbar: next, prev arrow buttons, title when multi image fits has image specific titles or title would be cube number.    Make sure the store will support multi images, add next, prev actions, etc",4,DM-4920,datamanagement,support multi image fit control add toolbar prev arrow button title multi image fit image specific title title cube number sure store support multi image add prev action etc,"Support Multi image fits and controls add toolbar: next, prev arrow buttons, title when multi image fits has image specific titles or title would be cube number. Make sure the store will support multi images, add next, prev actions, etc"
"Make obs_subaru build with OS X SIP Because of OS X SIP, {{obs_subaru}} fails to build on os x 10.11. In the {{hsc/SConscript}} file, the library environment variables need properly set, and scripts need to be delayed until the shebang rewriting occurs. ",1,DM-4921,datamanagement,obs_subaru build os sip os sip obs_subaru fail build os 10.11 hsc sconscript file library environment variable need properly set script need delay shebang rewrite occur,"Make obs_subaru build with OS X SIP Because of OS X SIP, {{obs_subaru}} fails to build on os x 10.11. In the {{hsc/SConscript}} file, the library environment variables need properly set, and scripts need to be delayed until the shebang rewriting occurs."
"want to see locations in trace when butler raises because multiple locations were found daf_persistence 11.0-2-g56eb0a1+1 gives the unhelpful error message:    {code}  > RuntimeError: Unable to retrieve bias for {'category': 'A', 'taiObs': '2015-12-22', 'visit': 7292, 'site': 'S', 'dateObs': '2015-12-22', 'filter': 'PFS-M', 'field': 'DARK', 'spectrograph': 2, 'ccd': 5}: No unique lookup for ['calibDate', 'calibVersion'] from {'category': 'A', 'taiObs': '2015-12-22', 'visit': 7292, 'site': 'S', 'dateObs': '2015-12-22', 'filter': 'PFS-M', 'field': 'DARK', 'spectrograph': 2, 'ccd': 5}: 2 matches  {code}    (the old butler did this too).  The user wants to know what the 2 matches were -- it's user error, but the user needs help and  printing the first few options (nicely formatted) is very useful.  I think I did this on the HSC side.      The butler code in question is actually in butlerUtils/python/lsst/daf/butlerUtils/mapping.py and my post-doc gave me the wrong package.    It's in need():  {code}  >         if len(lookups) != 1:  >             raise RuntimeError, ""No unique lookup for %s from %s: %d matches"" % (newProps, newId, len(lookups))  {code}",1,DM-4923,datamanagement,want location trace butler raise multiple location find daf_persistence 11.0 g56eb0a1 give unhelpful error message code runtimeerror unable retrieve bias category taiobs 2015 12 22 visit 7292 site dateobs 2015 12 22 filter pfs field dark spectrograph ccd unique lookup calibdate calibversion category taiobs 2015 12 22 visit 7292 site dateobs 2015 12 22 filter pfs field dark spectrograph ccd match code old butler user want know match user error user need help print option nicely format useful think hsc butler code question actually butlerutil python lsst daf butlerutil mapping.py post doc give wrong package need code len(lookup raise runtimeerror unique lookup match newprops newid len(lookup code,"want to see locations in trace when butler raises because multiple locations were found daf_persistence 11.0-2-g56eb0a1+1 gives the unhelpful error message: {code} > RuntimeError: Unable to retrieve bias for {'category': 'A', 'taiObs': '2015-12-22', 'visit': 7292, 'site': 'S', 'dateObs': '2015-12-22', 'filter': 'PFS-M', 'field': 'DARK', 'spectrograph': 2, 'ccd': 5}: No unique lookup for ['calibDate', 'calibVersion'] from {'category': 'A', 'taiObs': '2015-12-22', 'visit': 7292, 'site': 'S', 'dateObs': '2015-12-22', 'filter': 'PFS-M', 'field': 'DARK', 'spectrograph': 2, 'ccd': 5}: 2 matches {code} (the old butler did this too). The user wants to know what the 2 matches were -- it's user error, but the user needs help and printing the first few options (nicely formatted) is very useful. I think I did this on the HSC side. The butler code in question is actually in butlerUtils/python/lsst/daf/butlerUtils/mapping.py and my post-doc gave me the wrong package. It's in need(): {code} > if len(lookups) != 1: > raise RuntimeError, ""No unique lookup for %s from %s: %d matches"" % (newProps, newId, len(lookups)) {code}"
"Make FlagHandler, SafeCentroidExtractor usable from Python The {{meas_base}} framework includes {{SafeCentroidExtractor}}, a convenience routine for extracting a centroid from a source record, setting a consistent set of flags if that's not possible or if the centroid is in some way compromised. This consistent flag handling is made possible by the use of the {{FlagHandler}} class.    Unfortunately, {{FlagHandler}} is not meaningfully usable from Python, not least because it's impossible to define flags:  {code:python}  >>> import lsst.meas.base as measBase  >>> measBase.FlagDefinition(""flag"", ""doc"")  [...]  TypeError: __init__() takes exactly 1 argument (3 given)  >>> fd = measBase.FlagDefinition()  >>> fd.name = ""flag""  [...]  AttributeError: You cannot add attributes to <lsst.meas.base.baseLib.FlagDefinition; proxy of <Swig Object of type 'lsst::meas::base::FlagDefinition *' at 0x10a82b900> >  {code}    Looking further, even were we able to create {{FlagDefinitions}}, the {{FlagHandler}} is initialized with pointers to the beginning/end of a container of them, which seems like a stretch for Python code.    Please add Python support for these routines.",2,DM-4925,datamanagement,flaghandler safecentroidextractor usable python meas_base framework include safecentroidextractor convenience routine extract centroid source record set consistent set flag possible centroid way compromise consistent flag handling possible use flaghandler class unfortunately flaghandler meaningfully usable python impossible define flag code python import lsst.meas.base measbase measbase doc ... typeerror init take exactly argument give fd measbase flagdefinition fd.name flag ... attributeerror add attribute code look able create flagdefinitions flaghandler initialize pointer beginning end container like stretch python code add python support routine,"Make FlagHandler, SafeCentroidExtractor usable from Python The {{meas_base}} framework includes {{SafeCentroidExtractor}}, a convenience routine for extracting a centroid from a source record, setting a consistent set of flags if that's not possible or if the centroid is in some way compromised. This consistent flag handling is made possible by the use of the {{FlagHandler}} class. Unfortunately, {{FlagHandler}} is not meaningfully usable from Python, not least because it's impossible to define flags: {code:python} >>> import lsst.meas.base as measBase >>> measBase.FlagDefinition(""flag"", ""doc"") [...] TypeError: __init__() takes exactly 1 argument (3 given) >>> fd = measBase.FlagDefinition() >>> fd.name = ""flag"" [...] AttributeError: You cannot add attributes to  > {code} Looking further, even were we able to create {{FlagDefinitions}}, the {{FlagHandler}} is initialized with pointers to the beginning/end of a container of them, which seems like a stretch for Python code. Please add Python support for these routines."
"Centroids fall outside Footprints In DM-4882, we observed a number of centroids measured while running the {{lsst_dm_stack_demo}} routines fall outside their associated {{Footprints}}. This was seen with both the {{NaiveCentroid}} and the {{SdssCentroid}} centroiders.    For the purposes of DM-4882 we quieted the warnings arising from this, but we should investigate why this is happening and, if necessary, weed out small {{Footprints}} entirely.",8,DM-4926,datamanagement,centroid fall outside footprints dm-4882 observe number centroid measure run lsst_dm_stack_demo routine fall outside associate footprints see naivecentroid sdsscentroid centroider purpose dm-4882 quiet warning arise investigate happen necessary weed small footprints entirely,"Centroids fall outside Footprints In DM-4882, we observed a number of centroids measured while running the {{lsst_dm_stack_demo}} routines fall outside their associated {{Footprints}}. This was seen with both the {{NaiveCentroid}} and the {{SdssCentroid}} centroiders. For the purposes of DM-4882 we quieted the warnings arising from this, but we should investigate why this is happening and, if necessary, weed out small {{Footprints}} entirely."
"Fix intermittent testQdisp failure The mocks used in the executive class don't mock cancellation correctly and doing so would require significant effort. When Executive::squash() is called, the mocks threads are already running but waiting on the _go barrier. squash() calls JobQuery::cancel() for each thread and cancel() calls markComplete() for the job because a QueryResource has not been aquirred from xrootd. Once all the jobs are cancelled and _go is set to true, the ex.join() command doesn't wait for the jobs to complete since markComplete() has already been called for all of the jobs. If any of the jobs take longer to complete than the main thread, they call markComplete for an Executive that no longer exists and cause the test to fail.",1,DM-4928,datamanagement,fix intermittent testqdisp failure mock executive class mock cancellation correctly require significant effort executive::squash call mock thread run wait barrier squash call jobquery::cancel thread cancel call markcomplete job queryresource aquirre xrootd job cancel set true ex.join command wait job complete markcomplete call job job long complete main thread markcomplete executive long exist cause test fail,"Fix intermittent testQdisp failure The mocks used in the executive class don't mock cancellation correctly and doing so would require significant effort. When Executive::squash() is called, the mocks threads are already running but waiting on the _go barrier. squash() calls JobQuery::cancel() for each thread and cancel() calls markComplete() for the job because a QueryResource has not been aquirred from xrootd. Once all the jobs are cancelled and _go is set to true, the ex.join() command doesn't wait for the jobs to complete since markComplete() has already been called for all of the jobs. If any of the jobs take longer to complete than the main thread, they call markComplete for an Executive that no longer exists and cause the test to fail."
Fix build of MariaDB on OS X El Capitan The current MariaDB EUPS package does not build on OS X El Capitan because OS X no longer ships with OpenSSL developer files. MariaDB has a build option to use a bundled SSL library in preference to OpenSSL but the logic for automatically switching to this version breaks when the Anaconda OpenSSL libraries are present.,1,DM-4929,datamanagement,fix build mariadb os el capitan current mariadb eups package build os el capitan os long ship openssl developer file mariadb build option use bundled ssl library preference openssl logic automatically switch version break anaconda openssl library present,Fix build of MariaDB on OS X El Capitan The current MariaDB EUPS package does not build on OS X El Capitan because OS X no longer ships with OpenSSL developer files. MariaDB has a build option to use a bundled SSL library in preference to OpenSSL but the logic for automatically switching to this version breaks when the Anaconda OpenSSL libraries are present.
"Deploy 4 bare metal hosts for testing Base to Archive transfer implementation James needs to test network communication methodologies in an environment that mimics the expected real-world conditions. In order to minimize the complications with debugging, using bare metal machines in the first phase is preferred.    We can use 4 of the machines bought off the 2015 purchase or purchase new machines just for this purpose.",4,DM-4930,datamanagement,deploy bare metal host test base archive transfer implementation james need test network communication methodology environment mimic expect real world condition order minimize complication debugging bare metal machine phase prefer use machine buy 2015 purchase purchase new machine purpose,"Deploy 4 bare metal hosts for testing Base to Archive transfer implementation James needs to test network communication methodologies in an environment that mimics the expected real-world conditions. In order to minimize the complications with debugging, using bare metal machines in the first phase is preferred. We can use 4 of the machines bought off the 2015 purchase or purchase new machines just for this purpose."
Qserv build fails on El Capitan with missing OpenSSL Qserv does not build on OS X El Capitan due to the absence of OpenSSL include files. Apple now only ship the OpenSSL library (for backwards compatibility reasons). Qserv only uses SSL in two places to calculate digests (MD5 and SHA). This functionality is available in the Apple CommonCrypto library. Qserv digest code needs to be taught how to use CommonCrypto.,2,DM-4931,datamanagement,qserv build fail el capitan miss openssl qserv build os el capitan absence openssl include file apple ship openssl library backwards compatibility reason qserv use ssl place calculate digest md5 sha functionality available apple commoncrypto library qserv digest code need teach use commoncrypto,Qserv build fails on El Capitan with missing OpenSSL Qserv does not build on OS X El Capitan due to the absence of OpenSSL include files. Apple now only ship the OpenSSL library (for backwards compatibility reasons). Qserv only uses SSL in two places to calculate digests (MD5 and SHA). This functionality is available in the Apple CommonCrypto library. Qserv digest code needs to be taught how to use CommonCrypto.
"Track kernel panic issue The line that caused the kernel panic is in modules/mysql/MySqlConnection.cc line 151.  Currently the line is fine and is:          std::string const killSql = ""KILL QUERY "" + std::to_string(threadId);    This version of the line will occasionally cause the kernel panic (note the missing %1% that should be after KILL QUERY).          std::string killSql = boost::str(bo  ost::format(""KILL QUERY "") % threadId);  ",3,DM-4932,datamanagement,"track kernel panic issue line cause kernel panic module mysql mysqlconnection.cc line 151 currently line fine std::stre const killsql kill query std::to_string(threadid version line occasionally cause kernel panic note missing kill query std::stre killsql boost::str(bo ost::format(""kill query threadid","Track kernel panic issue The line that caused the kernel panic is in modules/mysql/MySqlConnection.cc line 151. Currently the line is fine and is: std::string const killSql = ""KILL QUERY "" + std::to_string(threadId); This version of the line will occasionally cause the kernel panic (note the missing %1% that should be after KILL QUERY). std::string killSql = boost::str(bo ost::format(""KILL QUERY "") % threadId);"
"Create a utility function do do spherical geometry averaging I would like to calculate a correct average and RMS for a set of RA, Dec positions.    Neither [~jbosch] nor [~price] knew of an easy, simple function to do that that existed in the stack.  [~price] suggested:    {code}  mean = sum(afwGeom.Extent3D(coord.toVector()) for coord in coordList, afwGeom.Point3D(0, 0, 0))  mean /= len(coordList)  mean = afwCoord.IcrsCoord(mean)  {code}    That makes sense, but it's a bit unobvious (it's obvious how it works, but would likely never occur to someone that they should do it that way in the stack).    Pedantically it's also not the best way to do a mean while preserving precision, but I don't anticipate that to be an issue in practice.    Creating a function that did this would provide clarity.  I don't know where that function should live.    Note: I know how to do this in Astropy.  I'm intentionally not using astropy here.  But part of the astropy dependency discussion is likely ""how much are we otherwise rewriting in the LSST stack"".",1,DM-4933,datamanagement,create utility function spherical geometry average like calculate correct average rms set ra dec position ~jbosch ~price know easy simple function exist stack ~price suggest code mean sum(afwgeom extent3d(coord.tovector coord coordlist afwgeom point3d(0 mean /= len(coordlist mean afwcoord icrscoord(mean code make sense bit unobvious obvious work likely occur way stack pedantically good way mean preserve precision anticipate issue practice create function provide clarity know function live note know astropy intentionally astropy astropy dependency discussion likely rewrite lsst stack,"Create a utility function do do spherical geometry averaging I would like to calculate a correct average and RMS for a set of RA, Dec positions. Neither [~jbosch] nor [~price] knew of an easy, simple function to do that that existed in the stack. [~price] suggested: {code} mean = sum(afwGeom.Extent3D(coord.toVector()) for coord in coordList, afwGeom.Point3D(0, 0, 0)) mean /= len(coordList) mean = afwCoord.IcrsCoord(mean) {code} That makes sense, but it's a bit unobvious (it's obvious how it works, but would likely never occur to someone that they should do it that way in the stack). Pedantically it's also not the best way to do a mean while preserving precision, but I don't anticipate that to be an issue in practice. Creating a function that did this would provide clarity. I don't know where that function should live. Note: I know how to do this in Astropy. I'm intentionally not using astropy here. But part of the astropy dependency discussion is likely ""how much are we otherwise rewriting in the LSST stack""."
on-going support to Camera team in visualization at UIUC Attend the weekly meeting and answer questions as needed,2,DM-4934,datamanagement,go support camera team visualization uiuc attend weekly meeting answer question need,on-going support to Camera team in visualization at UIUC Attend the weekly meeting and answer questions as needed
"Enable validateMatches in ci_hsc {{python/lsst/ci/hsc/validate.py}} in {{ci_hsc}} [says|https://github.com/lsst/ci_hsc/blob/69c7a62f675b8fb4164065d2c8c1621e296e40ad/python/lsst/ci/hsc/validate.py#L78]:  {code:python}      def validateMatches(self, dataId):          # XXX lsst.meas.astrom.readMatches is gone!          return  {code}  {{readMatches}} (or its successor) should be back in place as of DM-3633. Please enable this test.",2,DM-4936,datamanagement,enable validatematche ci_hsc python lsst ci hsc validate.py ci_hsc says|https://github.com lsst ci_hsc blob/69c7a62f675b8fb4164065d2c8c1621e296e40ad python lsst ci hsc validate.py#l78 code python def validatematches(self dataid xxx lsst.meas.astrom.readmatche go return code readmatche successor place dm-3633 enable test,"Enable validateMatches in ci_hsc {{python/lsst/ci/hsc/validate.py}} in {{ci_hsc}} [says|https://github.com/lsst/ci_hsc/blob/69c7a62f675b8fb4164065d2c8c1621e296e40ad/python/lsst/ci/hsc/validate.py#L78]: {code:python} def validateMatches(self, dataId): # XXX lsst.meas.astrom.readMatches is gone! return {code} {{readMatches}} (or its successor) should be back in place as of DM-3633. Please enable this test."
"multiple CVEs relevant to mariadb 10.1.9 and mysql Multiple CVEs have been released this week for mysql & mariadb.  The current eups product for mariadb is bundling 10.1.9, which is affected.  Several of the CVEs do not yet provide details, which typically means they are ""really bad"".    https://github.com/lsst/mariadb/blob/master/upstream/mariadb-10.1.9.tar.gz    https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0505  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0546  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0596  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0597  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0598  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0600  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0606  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0608  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0609  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0616  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-2047",1,DM-4937,datamanagement,multiple cf relevant mariadb 10.1.9 mysql multiple cf release week mysql mariadb current eup product mariadb bundle 10.1.9 affect cf provide detail typically mean bad https://github.com/lsst/mariadb/blob/master/upstream/mariadb-10.1.9.tar.gz https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0505 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0546 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0596 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0597 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0598 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0600 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0606 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0608 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0609 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-0616 https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2016-2047,"multiple CVEs relevant to mariadb 10.1.9 and mysql Multiple CVEs have been released this week for mysql & mariadb. The current eups product for mariadb is bundling 10.1.9, which is affected. Several of the CVEs do not yet provide details, which typically means they are ""really bad"". https://github.com/lsst/mariadb/blob/master/upstream/mariadb-10.1.9.tar.gz https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0505 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0546 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0596 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0597 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0598 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0600 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0606 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0608 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0609 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0616 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-2047"
"Update scisql to v0.3.5 In order to update MariaDB to v10.1.10 {{scisql}} needs to also be updated to deal with the hard-coded version checking. For the current version we get this error with the latest MariaDB:  {code}  :::::  [2016-01-28T16:51:40.539306Z]     user_function(self)  :::::  [2016-01-28T16:51:40.539334Z]   File ""/home/build0/lsstsw/build/scisql/wscript"", line 63, in configure  :::::  [2016-01-28T16:51:40.539346Z]     ctx.check_mysql()  :::::  [2016-01-28T16:51:40.539392Z]   File ""/home/build0/lsstsw/build/scisql/.waf-1.6.11-30618c54883417962c38f5d395f83584/waflib/Configure.py"", line 221, in fun  :::::  [2016-01-28T16:51:40.539410Z]     return f(*k,**kw)  :::::  [2016-01-28T16:51:40.539432Z]   File ""tools/mysql_waf.py"", line 85, in check_mysql  :::::  [2016-01-28T16:51:40.539451Z]     (ok, msg) = mysqlversion.check(version)  :::::  [2016-01-28T16:51:40.539473Z]   File ""tools/mysqlversion.py"", line 74, in check  :::::  [2016-01-28T16:51:40.539514Z]     if not comparison_op(version_nums, constraint_nums):  :::::  [2016-01-28T16:51:40.539547Z] UnboundLocalError: local variable 'constraint_nums' referenced before assignment  Failed during rebuild of DM stack.  {code}",1,DM-4938,datamanagement,"update scisql v0.3.5 order update mariadb v10.1.10 scisql need update deal hard code version checking current version error late mariadb code 2016 01 28t16:51:40.539306z user_function(self 2016 01 28t16:51:40.539334z file /home build0 lsstsw build scisql wscript line 63 configure 2016 01 28t16:51:40.539346z ctx.check_mysql 2016 01 28t16:51:40.539392z file /home build0 lsstsw build scisql/.waf-1.6.11 30618c54883417962c38f5d395f83584 waflib configure.py line 221 fun 2016 01 28t16:51:40.539410z return f(*k,**kw 2016 01 28t16:51:40.539432z file tool mysql_waf.py line 85 check_mysql 2016 01 28t16:51:40.539451z ok msg mysqlversion.check(version 2016 01 28t16:51:40.539473z file tool mysqlversion.py line 74 check 2016 01 28t16:51:40.539514z comparison_op(version_num constraint_nums 2016 01 28t16:51:40.539547z unboundlocalerror local variable constraint_num reference assignment fail rebuild dm stack code","Update scisql to v0.3.5 In order to update MariaDB to v10.1.10 {{scisql}} needs to also be updated to deal with the hard-coded version checking. For the current version we get this error with the latest MariaDB: {code} ::::: [2016-01-28T16:51:40.539306Z] user_function(self) ::::: [2016-01-28T16:51:40.539334Z] File ""/home/build0/lsstsw/build/scisql/wscript"", line 63, in configure ::::: [2016-01-28T16:51:40.539346Z] ctx.check_mysql() ::::: [2016-01-28T16:51:40.539392Z] File ""/home/build0/lsstsw/build/scisql/.waf-1.6.11-30618c54883417962c38f5d395f83584/waflib/Configure.py"", line 221, in fun ::::: [2016-01-28T16:51:40.539410Z] return f(*k,**kw) ::::: [2016-01-28T16:51:40.539432Z] File ""tools/mysql_waf.py"", line 85, in check_mysql ::::: [2016-01-28T16:51:40.539451Z] (ok, msg) = mysqlversion.check(version) ::::: [2016-01-28T16:51:40.539473Z] File ""tools/mysqlversion.py"", line 74, in check ::::: [2016-01-28T16:51:40.539514Z] if not comparison_op(version_nums, constraint_nums): ::::: [2016-01-28T16:51:40.539547Z] UnboundLocalError: local variable 'constraint_nums' referenced before assignment Failed during rebuild of DM stack. {code}"
IRSA developer mentoring effort IRSA is contributing to the Firefly package development.  we need to put in time to mentor the developers. ,2,DM-4939,datamanagement,irsa developer mentoring effort irsa contribute firefly package development need time mentor developer,IRSA developer mentoring effort IRSA is contributing to the Firefly package development. we need to put in time to mentor the developers.
IRSA developer mentoring effort IRSA is contributing to Firefly development. We need to mentor the new developers.,2,DM-4940,datamanagement,irsa developer mentoring effort irsa contribute firefly development need mentor new developer,IRSA developer mentoring effort IRSA is contributing to Firefly development. We need to mentor the new developers.
"Fix type inference and return types makeMaskedImage et al The {{makeMaskedImage}} function and cousins like {{makeExposure}} don't do the type inference they're supposed to do in C++, because they use the old {{typename Image<T>::Ptr}} approach instead of {{PTR(Image<T>)}}.    They also return raw pointers, which is dangerous.  They should be converted to return shared_ptrs.  Note that this will have to include adjusting or removing Swig code (probably {{%newobject}} statements) that deal with taking ownership of the raw pointers.",1,DM-4942,datamanagement,fix type inference return type makemaskedimage et al makemaskedimage function cousin like makeexposure type inference suppose c++ use old typename image::ptr approach instead ptr(image return raw pointer dangerous convert return shared_ptrs note include adjust remove swig code probably newobject statement deal take ownership raw pointer,"Fix type inference and return types makeMaskedImage et al The {{makeMaskedImage}} function and cousins like {{makeExposure}} don't do the type inference they're supposed to do in C++, because they use the old {{typename Image::Ptr}} approach instead of {{PTR(Image)}}. They also return raw pointers, which is dangerous. They should be converted to return shared_ptrs. Note that this will have to include adjusting or removing Swig code (probably {{%newobject}} statements) that deal with taking ownership of the raw pointers."
"butler should transparently allow files to be compressed or not see the conversation on c.l.o. at [https://community.lsst.org/t/how-does-the-butler-support-compression/502].    The summary is, when the mapper returns e.g. a non compressed file name e.g. {{foo.fits}}, that file may be compressed and the filename may reflect this e.g. in reality it might be named {{foo.fits.gz}}. On a posix system some component of the butler framework should discover this and transform the filename to the correct filename and pass that to the deserializer.    TBD if the list of allowed extensions is hard coded someplace (in a mapper subclass?) or specified another way, perhaps by the policy (could be for dataset type or globally).",6,DM-4944,datamanagement,butler transparently allow file compress conversation c.l.o https://community.lsst.org/t/how-does-the-butler-support-compression/502 summary mapper return e.g. non compress file e.g. foo.fits file compress filename reflect e.g. reality name foo.fits.gz posix system component butler framework discover transform filename correct filename pass deserializer tbd list allow extension hard code someplace mapper subclass specify way policy dataset type globally,"butler should transparently allow files to be compressed or not see the conversation on c.l.o. at [https://community.lsst.org/t/how-does-the-butler-support-compression/502]. The summary is, when the mapper returns e.g. a non compressed file name e.g. {{foo.fits}}, that file may be compressed and the filename may reflect this e.g. in reality it might be named {{foo.fits.gz}}. On a posix system some component of the butler framework should discover this and transform the filename to the correct filename and pass that to the deserializer. TBD if the list of allowed extensions is hard coded someplace (in a mapper subclass?) or specified another way, perhaps by the policy (could be for dataset type or globally)."
"afw Wcs object copying does not copy exactly A probable bug in WCSLIB is causing {{wcscopy}} to create copies of {{Wcs}} objects which are not the same as the object that was copied. In some cases when this object is passed to {{wcsset}} it fails, as the {{Wcs}} object contains impossible values.    This has behaviour is non-deterministic (failure is only seen occasionally). The error has only been observed on OSX, but we do not believe it to be operating system dependent (except insofar as different systems and compilers produce different memory layouts and hence different failure modes). This reliably causes {{ci_hsc}} to fail when running on a Mac.    Relevant lines in {{afw}} are {{image/Wcs.cc:140}} and the {{Wcs}} copy constructor in {{image/Wcs.cc:468}}    Additionally a bug has been found in {{image/Wcs.cc}} on line 485 where the flag property should be set on an element, and not on the object itself, ie {{_wcsInfo[i]->flag = -1;}}.",6,DM-4946,datamanagement,afw wcs object copying copy exactly probable bug wcslib cause wcscopy create copy wcs object object copy case object pass wcsset fail wcs object contain impossible value behaviour non deterministic failure see occasionally error observe osx believe operate system dependent insofar different system compiler produce different memory layout different failure mode reliably cause ci_hsc fail run mac relevant line afw image wcs.cc:140 wcs copy constructor image wcs.cc:468 additionally bug find image wcs.cc line 485 flag property set element object ie wcsinfo[i]->flag -1,"afw Wcs object copying does not copy exactly A probable bug in WCSLIB is causing {{wcscopy}} to create copies of {{Wcs}} objects which are not the same as the object that was copied. In some cases when this object is passed to {{wcsset}} it fails, as the {{Wcs}} object contains impossible values. This has behaviour is non-deterministic (failure is only seen occasionally). The error has only been observed on OSX, but we do not believe it to be operating system dependent (except insofar as different systems and compilers produce different memory layouts and hence different failure modes). This reliably causes {{ci_hsc}} to fail when running on a Mac. Relevant lines in {{afw}} are {{image/Wcs.cc:140}} and the {{Wcs}} copy constructor in {{image/Wcs.cc:468}} Additionally a bug has been found in {{image/Wcs.cc}} on line 485 where the flag property should be set on an element, and not on the object itself, ie {{_wcsInfo[i]->flag = -1;}}."
"Please improve the documentation for TransformTask and derivatives While working on DM-4629 (overhauling {{ProcessCcdTask}}) [~rowen] stumbled over {{TransformTask}}, which he wasn't previously familiar with. Existing Doxygen documentation covers what this task does, but lacks context as to why it's useful. Please provide a high-level overview of what the intention is here.",2,DM-4948,datamanagement,improve documentation transformtask derivative work dm-4629 overhaul processccdtask ~rowen stumble transformtask previously familiar existing doxygen documentation cover task lack context useful provide high level overview intention,"Please improve the documentation for TransformTask and derivatives While working on DM-4629 (overhauling {{ProcessCcdTask}}) [~rowen] stumbled over {{TransformTask}}, which he wasn't previously familiar with. Existing Doxygen documentation covers what this task does, but lacks context as to why it's useful. Please provide a high-level overview of what the intention is here."
Improve MySQL proxy code and add unit tests QServ's proxy needs some cleanup:    1. Standardize passing of q and qU parameters to methods  2. Comment removal may have never worked  3. Whitespace translation is likely buggy  4. A few unit tests verifying routing of queries would be nice  ,4,DM-4949,datamanagement,improve mysql proxy code add unit test qserv proxy need cleanup standardize passing qu parameter method comment removal work whitespace translation likely buggy unit test verify routing query nice,Improve MySQL proxy code and add unit tests QServ's proxy needs some cleanup: 1. Standardize passing of q and qU parameters to methods 2. Comment removal may have never worked 3. Whitespace translation is likely buggy 4. A few unit tests verifying routing of queries would be nice
"Add S3/Route53 project provisioning capabilities to ltd-keeper An **authenticated** user should be able to provision (and likewise, delete) an entire published software documentation project via ltd-keeper’s RESTful API. This includes creating an S3 bucket in SQuaRE’s AWS account and setting up Route 53 DNS. The user should also be able to delete a project. This ticket will add AWS affordances to ltd-keeper. DM-4950 will be responsible for hooking this functionality into the methods that service API calls.",3,DM-4951,datamanagement,add s3 route53 project provision capability ltd keeper authenticated user able provision likewise delete entire publish software documentation project ltd keeper restful api include create s3 bucket square aws account set route 53 dns user able delete project ticket add aws affordance ltd keeper dm-4950 responsible hook functionality method service api call,"Add S3/Route53 project provisioning capabilities to ltd-keeper An **authenticated** user should be able to provision (and likewise, delete) an entire published software documentation project via ltd-keeper s RESTful API. This includes creating an S3 bucket in SQuaRE s AWS account and setting up Route 53 DNS. The user should also be able to delete a project. This ticket will add AWS affordances to ltd-keeper. DM-4950 will be responsible for hooking this functionality into the methods that service API calls."
"delegate argument parsing to CmdLineTask instances Command-line argument parsing of data IDs for {{CmdLineTask}} s is currently defined at the class level, which means that we cannot make data ID definitions dependent on task configuration.  That in turn requires custom {{processCcd}} scripts for cameras that start processing at a level other than ""raw"" (SDSS, DECam with community pipeline ISR, possibly CFHT).    Instead, we should let {{CmdLineTask}} *instances* setup command-line parsing; after a {{CmdLineTask}} is constructed, it will have access to its final configuration tree, and can better choose how to parse its ID arguments.    I've assigned this to Process Middleware for now, since that's where it lives in the codebase, but it may make more sense to give this to [~rowen], [~price], or [~jbosch], just because we've already got enough familiarity with the code in question that we could do it quickly.  I'll leave that up to [~swinbank], [~krughoff], and [~mgelman2] to decide.",2,DM-4952,datamanagement,delegate argument parse cmdlinetask instance command line argument parse datum id cmdlinetask currently define class level mean data id definition dependent task configuration turn require custom processccd script camera start process level raw sdss decam community pipeline isr possibly cfht instead let cmdlinetask instance setup command line parsing cmdlinetask construct access final configuration tree well choose parse id argument assign process middleware live codebase sense ~rowen ~price ~jbosch get familiarity code question quickly leave ~swinbank ~krughoff ~mgelman2 decide,"delegate argument parsing to CmdLineTask instances Command-line argument parsing of data IDs for {{CmdLineTask}} s is currently defined at the class level, which means that we cannot make data ID definitions dependent on task configuration. That in turn requires custom {{processCcd}} scripts for cameras that start processing at a level other than ""raw"" (SDSS, DECam with community pipeline ISR, possibly CFHT). Instead, we should let {{CmdLineTask}} *instances* setup command-line parsing; after a {{CmdLineTask}} is constructed, it will have access to its final configuration tree, and can better choose how to parse its ID arguments. I've assigned this to Process Middleware for now, since that's where it lives in the codebase, but it may make more sense to give this to [~rowen], [~price], or [~jbosch], just because we've already got enough familiarity with the code in question that we could do it quickly. I'll leave that up to [~swinbank], [~krughoff], and [~mgelman2] to decide."
Update pyfits The final version of {{pyfits}} has just been released. This ticket covers updating to that version. This will be helpful in determining whether the migration to {{astropy.io.fits}} will be straightforward or complicated.,1,DM-4955,datamanagement,update pyfit final version pyfit release ticket cover update version helpful determine migration astropy.io.fits straightforward complicated,Update pyfits The final version of {{pyfits}} has just been released. This ticket covers updating to that version. This will be helpful in determining whether the migration to {{astropy.io.fits}} will be straightforward or complicated.
"Adapt SRD-based measurements of astrometric performance for validate_drp Adapt the SRD-based specifications for calculation of astrometric performance.  Follow the examples for AM1, AM2 as presented at    https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41785659    and detailed in DM-3057, DM-3064",4,DM-4956,datamanagement,adapt srd base measurement astrometric performance validate_drp adapt srd base specification calculation astrometric performance follow example am1 am2 present https://confluence.lsstcorp.org/pages/viewpage.action?pageid=41785659 detail dm-3057 dm-3064,"Adapt SRD-based measurements of astrometric performance for validate_drp Adapt the SRD-based specifications for calculation of astrometric performance. Follow the examples for AM1, AM2 as presented at https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41785659 and detailed in DM-3057, DM-3064"
Generate JSON output from validate_drp for inclusion in a test harness Generate JSON output from validate_drp for inclusion in a test harness.    Generate a file that summarizes the key metrics calculated by `validate_drp`.      Develop naming conventions that will make it easy to plug into the eventual harness being developed as part of DM-2050.,2,DM-4957,datamanagement,generate json output validate_drp inclusion test harness generate json output validate_drp inclusion test harness generate file summarize key metric calculate validate_drp develop naming convention easy plug eventual harness develop dm-2050,Generate JSON output from validate_drp for inclusion in a test harness Generate JSON output from validate_drp for inclusion in a test harness. Generate a file that summarizes the key metrics calculated by `validate_drp`. Develop naming conventions that will make it easy to plug into the eventual harness being developed as part of DM-2050.
"ci_hsc fails to execute tasks from with SCons on OSX 10.11/SIP The {{ci_hsc}} package executes a number of command line tasks directly from SCons based on {{Command}} directives in a {{SConstruct}} file. On an OSX 10.11 system with SIP enabled, there are two distinct problems which prevent the necessary environment being propagated to the tasks:  * -The {{scons}} executable starts with a {{#!/usr/bin/env python}}. Running through {{/usr/bin/env}} strips {{DYLD_LIBRARY_PATH}} from the environment.- (duplicates DM-4954)  * SCons executes command using the [{{sh}} shell on posix systems|https://bitbucket.org/scons/scons/src/09e1f0326b7678d1248dab88b28b456fd7d6fb54/src/engine/SCons/Platform/posix.py?at=default&fileviewer=file-view-default#posix.py-105]. By default, that means {{/bin/sh}} on a Mac, which, again, will strip {{DYLD_LIBRARY_PATH}}.    Please make it possible to run {{ci_hsc}} on such a system.",1,DM-4959,datamanagement,ci_hsc fail execute task scons osx 10.11 sip ci_hsc package execute number command line task directly scons base command directive sconstruct file osx 10.11 system sip enable distinct problem prevent necessary environment propagate task scon executable start /usr bin env python run /usr bin env strip dyld_library_path environment.- duplicate dm-4954 scon execute command sh shell posix systems|https://bitbucket.org scon scon src/09e1f0326b7678d1248dab88b28b456fd7d6fb54 src engine scons platform posix.py?at default&fileviewer file view default#posix.py-105 default mean /bin sh mac strip dyld_library_path possible run ci_hsc system,"ci_hsc fails to execute tasks from with SCons on OSX 10.11/SIP The {{ci_hsc}} package executes a number of command line tasks directly from SCons based on {{Command}} directives in a {{SConstruct}} file. On an OSX 10.11 system with SIP enabled, there are two distinct problems which prevent the necessary environment being propagated to the tasks: * -The {{scons}} executable starts with a {{#!/usr/bin/env python}}. Running through {{/usr/bin/env}} strips {{DYLD_LIBRARY_PATH}} from the environment.- (duplicates DM-4954) * SCons executes command using the [{{sh}} shell on posix systems|https://bitbucket.org/scons/scons/src/09e1f0326b7678d1248dab88b28b456fd7d6fb54/src/engine/SCons/Platform/posix.py?at=default&fileviewer=file-view-default#posix.py-105]. By default, that means {{/bin/sh}} on a Mac, which, again, will strip {{DYLD_LIBRARY_PATH}}. Please make it possible to run {{ci_hsc}} on such a system."
"LSST vs. HSC stack comparison: PSF estimation In order to determine the cause of the output differences between single frame processing runs of the same data using the LSST vs. HSC stacks (see figures attached to DM-4730), a detailed look at some of the image characterization steps is required.  This ticket involves a detailed investigation of the initial PSF estimation including:  {panel: title=LSST vs. HSC stack runs:}  - a comparison of the initial object detection (will likely involve looking at the initial background estimate as well as the specific assignment of footprints)  - which objects are selected as PSF candidates  - the initial PSF model (as a function of position)  {panel}",5,DM-4960,datamanagement,lsst vs. hsc stack comparison psf estimation order determine cause output difference single frame processing run datum lsst vs. hsc stack figure attach dm-4730 detailed look image characterization step require ticket involve detailed investigation initial psf estimation include panel title lsst vs. hsc stack run comparison initial object detection likely involve look initial background estimate specific assignment footprint object select psf candidate initial psf model function position panel,"LSST vs. HSC stack comparison: PSF estimation In order to determine the cause of the output differences between single frame processing runs of the same data using the LSST vs. HSC stacks (see figures attached to DM-4730), a detailed look at some of the image characterization steps is required. This ticket involves a detailed investigation of the initial PSF estimation including: {panel: title=LSST vs. HSC stack runs:} - a comparison of the initial object detection (will likely involve looking at the initial background estimate as well as the specific assignment of footprints) - which objects are selected as PSF candidates - the initial PSF model (as a function of position) {panel}"
"Obs_Subaru camera mapper has wrong deep_assembleCoadd_config When lsst switched to using SafeClipAssembleCoaddTask, the camera mapper for hsc was not updated accordingly. This causes ci_hsc to fail when it attempts to verify the config class type for the deep_coadd. Camera mapper should be updated accordingly",1,DM-4961,datamanagement,obs_subaru camera mapper wrong deep_assemblecoadd_config lsst switch safeclipassemblecoaddtask camera mapper hsc update accordingly cause ci_hsc fail attempt verify config class type deep_coadd camera mapper update accordingly,"Obs_Subaru camera mapper has wrong deep_assembleCoadd_config When lsst switched to using SafeClipAssembleCoaddTask, the camera mapper for hsc was not updated accordingly. This causes ci_hsc to fail when it attempts to verify the config class type for the deep_coadd. Camera mapper should be updated accordingly"
"January Operation Support Related Tasks Account cleanup process for existing infrastructure (Identify accounts, assign sponsors)    Reconcile inventory between NCSA and Aura (on going). Mock request was generated by Aura for dry run audit. Several machines have been found not included in inventory. Task to be completed in February.",3,DM-4962,datamanagement,january operation support related tasks account cleanup process exist infrastructure identify account assign sponsor reconcile inventory ncsa aura go mock request generate aura dry run audit machine find include inventory task complete february,"January Operation Support Related Tasks Account cleanup process for existing infrastructure (Identify accounts, assign sponsors) Reconcile inventory between NCSA and Aura (on going). Mock request was generated by Aura for dry run audit. Several machines have been found not included in inventory. Task to be completed in February."
"Investigate Roger as fallover for Nebula Investigate Roger OpenStack as fallback for Nebula during outages. Internally, this required technical and coordination meetings. Externally, this required interfacing with Square in order to facilitate a proper evaluation.     This task is ongoing. ",2,DM-4963,datamanagement,investigate roger fallover nebula investigate roger openstack fallback nebula outage internally require technical coordination meeting externally require interface square order facilitate proper evaluation task ongoing,"Investigate Roger as fallover for Nebula Investigate Roger OpenStack as fallback for Nebula during outages. Internally, this required technical and coordination meetings. Externally, this required interfacing with Square in order to facilitate a proper evaluation. This task is ongoing."
January AAA Tasks Attended local AAA meetings and reviewed documentation. ,1,DM-4964,datamanagement,january aaa tasks attend local aaa meeting review documentation,January AAA Tasks Attended local AAA meetings and reviewed documentation.
"January Tasks Security meeting with Paul, Bill, Eyrich to review goals and coordinate efforts.   Initial draft of the procurement plan. Waiting for hardware contract.   Updates to internal FY16 cost estimate spreadsheet and planning (not LDM-144).  Updating expected expenditures based on update quotes.   Meetings and discussions with OBFS with respect to new vendors within MHEC and procurement approval processing for FY16 components.",4,DM-4965,datamanagement,january tasks security meeting paul bill eyrich review goal coordinate effort initial draft procurement plan wait hardware contract update internal fy16 cost estimate spreadsheet planning ldm-144 update expect expenditure base update quote meeting discussion obfs respect new vendor mhec procurement approval processing fy16 component,"January Tasks Security meeting with Paul, Bill, Eyrich to review goals and coordinate efforts. Initial draft of the procurement plan. Waiting for hardware contract. Updates to internal FY16 cost estimate spreadsheet and planning (not LDM-144). Updating expected expenditures based on update quotes. Meetings and discussions with OBFS with respect to new vendors within MHEC and procurement approval processing for FY16 components."
January Tasks Mtg w/ IN2P3 re: ITIL implementation experiences.     Mtg w/ IN2P3 re: tape recall ordering,1,DM-4967,datamanagement,january tasks mtg w/ in2p3 itil implementation experience mtg w/ in2p3 tape recall order,January Tasks Mtg w/ IN2P3 re: ITIL implementation experiences. Mtg w/ IN2P3 re: tape recall ordering
"Jason January Tasks Activities this month include: IT sys admin meetings, LSST internal project meetings, conducting, coordinating, discussing interviews. Meeting with candidates. ICI coordination meeting (Randy). Discussion of work-to-be-done with onboarded teammates. Relaying task prioritization to IT for LSST-related activities. ",4,DM-4968,datamanagement,jason january tasks activity month include sys admin meeting lsst internal project meeting conduct coordinate discuss interview meet candidate ici coordination meeting randy discussion work onboarded teammate relay task prioritization lsst relate activity,"Jason January Tasks Activities this month include: IT sys admin meetings, LSST internal project meetings, conducting, coordinating, discussing interviews. Meeting with candidates. ICI coordination meeting (Randy). Discussion of work-to-be-done with onboarded teammates. Relaying task prioritization to IT for LSST-related activities."
DM Power Requirements Further discussions about power requirements at the Chilean DC.,2,DM-4969,datamanagement,dm power requirements discussion power requirement chilean dc,DM Power Requirements Further discussions about power requirements at the Chilean DC.
"Meetings, Jan 2016 verfication dataset meetings, TechTalk, RFD, local middleware-related meetings, etc",2,DM-4971,datamanagement,meeting jan 2016 verfication dataset meeting techtalk rfd local middleware relate meeting etc,"Meetings, Jan 2016 verfication dataset meetings, TechTalk, RFD, local middleware-related meetings, etc"
"LOE, Jan 2016 LSST local group meetings, postdoc meeting, other local meetings, etc",2,DM-4972,datamanagement,loe jan 2016 lsst local group meeting postdoc meeting local meeting etc,"LOE, Jan 2016 LSST local group meetings, postdoc meeting, other local meetings, etc"
"Reconsider high detection threshold in CharacterizeImageTask [~price] makes the [reasonable recommendation|https://community.lsst.org/t/why-was-detection-includethresholdmultiplier-10-for-old-processccdtask/500/6] that we consider providing PSF estimation with the N brightest sources in the image, rather than only detecting bright sources.    [~rowen] reasonably believes that this is beyond the scope of DM-4692, hence this new issue.    There should be very little new code needed here, but it may involve quite a bit of experimentation and validation.",8,DM-4973,datamanagement,reconsider high detection threshold characterizeimagetask ~price make reasonable recommendation|https://community.lsst.org detection includethresholdmultiplier-10 old processccdtask/500/6 consider provide psf estimation bright source image detect bright source ~rowen reasonably believe scope dm-4692 new issue little new code need involve bit experimentation validation,"Reconsider high detection threshold in CharacterizeImageTask [~price] makes the [reasonable recommendation|https://community.lsst.org/t/why-was-detection-includethresholdmultiplier-10-for-old-processccdtask/500/6] that we consider providing PSF estimation with the N brightest sources in the image, rather than only detecting bright sources. [~rowen] reasonably believes that this is beyond the scope of DM-4692, hence this new issue. There should be very little new code needed here, but it may involve quite a bit of experimentation and validation."
"upstream patches/deps from conda-lsst Where ever possible, missing dep information and patches from conda-lsst should be upstreamed.  The patches have already been observed to cause builds to fail due to upstream changes.",3,DM-4983,datamanagement,upstream patch dep conda lsst possible miss dep information patch conda lsst upstreamed patch observe cause build fail upstream change,"upstream patches/deps from conda-lsst Where ever possible, missing dep information and patches from conda-lsst should be upstreamed. The patches have already been observed to cause builds to fail due to upstream changes."
"Prepare for auth session at JTM Prepare for JTM session with a working title of “How Authentication/Authorization technology can be used to implement and enforce data access rights and operational processes for LSST"".  Prepare a final title and agenda for the session. Tuesday from 3:30pm - 5:00pm.",5,DM-4990,datamanagement,prepare auth session jtm prepare jtm session work title authentication authorization technology implement enforce datum access right operational process lsst prepare final title agenda session tuesday 3:30pm 5:00pm,"Prepare for auth session at JTM Prepare for JTM session with a working title of How Authentication/Authorization technology can be used to implement and enforce data access rights and operational processes for LSST"". Prepare a final title and agenda for the session. Tuesday from 3:30pm - 5:00pm."
"Save algorithm metadata in multiband.py The various {{Tasks}} in {{multiband.py}} do not attach the {{self.algMetadata}} instance attribute to their output tables before writing them out, so we aren't actually saving information like which radii were used for apertures.    We should also make sure this feature is maintained in the processCcd.py rewrite.",3,DM-4991,datamanagement,save algorithm metadata multiband.py tasks multiband.py attach self.algmetadata instance attribute output table write actually save information like radius aperture sure feature maintain processccd.py rewrite,"Save algorithm metadata in multiband.py The various {{Tasks}} in {{multiband.py}} do not attach the {{self.algMetadata}} instance attribute to their output tables before writing them out, so we aren't actually saving information like which radii were used for apertures. We should also make sure this feature is maintained in the processCcd.py rewrite."
"work flow of light curve visulizaiton Generate a description document of work flow that a scientist would go through in order to do time series research, visualize the light curve.",4,DM-4992,datamanagement,work flow light curve visulizaiton generate description document work flow scientist order time series research visualize light curve,"work flow of light curve visulizaiton Generate a description document of work flow that a scientist would go through in order to do time series research, visualize the light curve."
"review of dependency on the third party packages We need to periodically review the status of the third party software packages that Firefly depends on. Making a plan to do upgrade if needed.   package.json lists out the dependencies Firefly has on the third party software. The attached file was last modified 2016-02-09.    package.json_version lists the current version of the third party packages, major changes were indicated by (M). The attached file was created on 2016-02-29.     bq.      ""babel""     : ""5.8.34"",                           6.5.2 (M)      ""history""   : ""1.17.0"",                           2.0.0 (M)      ""icepick""   : ""0.2.0"",                            1.1.0 (M)                ""react-highcharts"": ""5.0.6"",                      7.0.0 (M)      ""react-redux"": ""3.1.2"",                           4.4.0 (M)      ""react-split-pane"": ""0.1.22"",                     2.0.1 (M)      ""redux-thunk"": ""0.1.0"",                           1.0.3 (M)      ""redux-logger"": ""1.0.9"",                          2.6.1 (M)      ""validator"" : ""4.5.0"",                            5.1.0 (M)      ""chai"": ""^2.3.0"",                                 3.5.0 (M)      ""esprima-fb"": ""^14001.1.0-dev-harmony-fb"",        15001.1001.0-dev-harmony-fb (M)      ""babel-eslint""      : ""^4.1.3"",                   5.0.0 (M)      ""babel-loader""      : ""^5.3.2"",                   6.2.4 (M)      ""babel-plugin-react-transform"": ""^1.1.0"",         2.0.0 (M)      ""babel-runtime""     : ""^5.8.20"",                  6.6.0 (M)      ""eslint""            : ""^1.10.3"",                  2.2.0 (M)      ""eslint-config-airbnb"": ""0.1.0"",                  6.0.2 (M) works with eslint 2.2.0      ""eslint-plugin-react"": ""^3.5.1"",                  4.1.0 (M)  works with eslint 2.2.0      ""extract-text-webpack-plugin"": ""^0.8.0"",          1.0.1 (M)      ""html-webpack-plugin"": ""^1.6.1"",                  2.9.0 (M)      ""karma-sinon-chai"": ""^0.3.0"",                     1.2.0 (M)      ""redux-devtools""    : ""^2.1.2"",                   3.3.1 (M)      ""webpack"": ""^1.8.2""                               1.12.14, 2.1.0 beta4 (M)          ",2,DM-4993,datamanagement,review dependency party package need periodically review status party software package firefly depend make plan upgrade need package.json list dependency firefly party software attached file modify 2016 02 09 package.json_version list current version party package major change indicate attached file create 2016 02 29 bq babel 5.8.34 6.5.2 history 1.17.0 2.0.0 icepick 0.2.0 1.1.0 react highchart 5.0.6 7.0.0 react redux 3.1.2 4.4.0 react split pane 0.1.22 2.0.1 redux thunk 0.1.0 1.0.3 redux logger 1.0.9 2.6.1 validator 4.5.0 5.1.0 chai ^2.3.0 3.5.0 esprima fb ^14001.1.0 dev harmony fb 15001.1001.0 dev harmony fb babel eslint ^4.1.3 5.0.0 babel loader ^5.3.2 6.2.4 babel plugin react transform ^1.1.0 2.0.0 babel runtime ^5.8.20 6.6.0 eslint 2.2.0 eslint config airbnb 0.1.0 6.0.2 work eslint 2.2.0 eslint plugin react ^3.5.1 4.1.0 work eslint 2.2.0 extract text webpack plugin ^0.8.0 1.0.1 html webpack plugin ^1.6.1 2.9.0 karma sinon chai ^0.3.0 1.2.0 redux devtool ^2.1.2 3.3.1 webpack ^1.8.2 1.12.14 2.1.0 beta4,"review of dependency on the third party packages We need to periodically review the status of the third party software packages that Firefly depends on. Making a plan to do upgrade if needed. package.json lists out the dependencies Firefly has on the third party software. The attached file was last modified 2016-02-09. package.json_version lists the current version of the third party packages, major changes were indicated by (M). The attached file was created on 2016-02-29. bq. ""babel"" : ""5.8.34"", 6.5.2 (M) ""history"" : ""1.17.0"", 2.0.0 (M) ""icepick"" : ""0.2.0"", 1.1.0 (M) ""react-highcharts"": ""5.0.6"", 7.0.0 (M) ""react-redux"": ""3.1.2"", 4.4.0 (M) ""react-split-pane"": ""0.1.22"", 2.0.1 (M) ""redux-thunk"": ""0.1.0"", 1.0.3 (M) ""redux-logger"": ""1.0.9"", 2.6.1 (M) ""validator"" : ""4.5.0"", 5.1.0 (M) ""chai"": ""^2.3.0"", 3.5.0 (M) ""esprima-fb"": ""^14001.1.0-dev-harmony-fb"", 15001.1001.0-dev-harmony-fb (M) ""babel-eslint"" : ""^4.1.3"", 5.0.0 (M) ""babel-loader"" : ""^5.3.2"", 6.2.4 (M) ""babel-plugin-react-transform"": ""^1.1.0"", 2.0.0 (M) ""babel-runtime"" : ""^5.8.20"", 6.6.0 (M) ""eslint"" : ""^1.10.3"", 2.2.0 (M) ""eslint-config-airbnb"": ""0.1.0"", 6.0.2 (M) works with eslint 2.2.0 ""eslint-plugin-react"": ""^3.5.1"", 4.1.0 (M) works with eslint 2.2.0 ""extract-text-webpack-plugin"": ""^0.8.0"", 1.0.1 (M) ""html-webpack-plugin"": ""^1.6.1"", 2.9.0 (M) ""karma-sinon-chai"": ""^0.3.0"", 1.2.0 (M) ""redux-devtools"" : ""^2.1.2"", 3.3.1 (M) ""webpack"": ""^1.8.2"" 1.12.14, 2.1.0 beta4 (M)"
"Design single-sign-on authentication system for webserv Outline a design of the authentication system (based on components provided by NCSA) that will support single sign-on. Current thinking involves two tokens: application token to certify the app is legitimate and to determine which users it can represent, and user token",6,DM-4994,datamanagement,design single sign authentication system webserv outline design authentication system base component provide ncsa support single sign current thinking involve token application token certify app legitimate determine user represent user token,"Design single-sign-on authentication system for webserv Outline a design of the authentication system (based on components provided by NCSA) that will support single sign-on. Current thinking involves two tokens: application token to certify the app is legitimate and to determine which users it can represent, and user token"
Extend webserv API to pass security tokens Extend the [API|https://confluence.lsstcorp.org/display/DM/AP] to pass security tokens.,8,DM-4995,datamanagement,extend webserv api pass security token extend api|https://confluence.lsstcorp.org display dm ap pass security token,Extend webserv API to pass security tokens Extend the [API|https://confluence.lsstcorp.org/display/DM/AP] to pass security tokens.
Update validate_drp for El Capitan validate_drp does not work on El Capitan due to SIP (System Integrity Protection) stripping DYLD_LIBRARY_PATH from shell scripts. The simple fix is to add  {code}  export DYLD_LIBRARY_PATH=${LSST_LIBRARY_PATH}  {code}  near the top of the scripts.,1,DM-4996,datamanagement,update validate_drp el capitan validate_drp work el capitan sip system integrity protection strip dyld_library_path shell script simple fix add code export dyld_library_path=${lsst_library_path code near script,Update validate_drp for El Capitan validate_drp does not work on El Capitan due to SIP (System Integrity Protection) stripping DYLD_LIBRARY_PATH from shell scripts. The simple fix is to add {code} export DYLD_LIBRARY_PATH=${LSST_LIBRARY_PATH} {code} near the top of the scripts.
"Benchmark dipole measurement (dipole fitting) Benchmark dipole measurement (dipole fitting), compare speed directly to psf-fit (and/or galaxy measurement) task. Runtime should be comparable (~factor of two?) - if not understand why. Evaluate new implementation vs. current impl. Accuracy?",8,DM-4997,datamanagement,benchmark dipole measurement dipole fitting benchmark dipole measurement dipole fitting compare speed directly psf fit and/or galaxy measurement task runtime comparable ~factor understand evaluate new implementation vs. current impl accuracy,"Benchmark dipole measurement (dipole fitting) Benchmark dipole measurement (dipole fitting), compare speed directly to psf-fit (and/or galaxy measurement) task. Runtime should be comparable (~factor of two?) - if not understand why. Evaluate new implementation vs. current impl. Accuracy?"
"Fix rotation for isr in obs_subaru Approximately half of the HSC CCDs are rotated 180 deg with respect to the others.  Two others have 90 deg rotations and another two have 270 deg rotations (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]) .  The raw images for the rotated CCDs thus need to be rotated to match the rotation of their associated calibration frames prior to applying the corrections.  This is accomplished by rotating the exposure using the *rotated* context manager function in {{obs_subaru}}'s *isr.py* and the *nQuarter* specification in the policy file for each CCD.  Currently, *rotated* uses {{afw}}'s *rotateImageBy90* (which apparently rotates in a counter-clockwise direction) to rotated the exposure by 4 - nQuarter turns.  This turns out to be the wrong rotation for the odd nQuarter CCDs as shown here:   !ccd100_nQuarter3.png|width=200!  top left = raw exposure as read in  top right = flatfield exposure as read in  bottom left = _incorrectly_ rotated raw exposure prior to flatfield correction",2,DM-4998,datamanagement,fix rotation isr obs_subaru approximately half hsc ccds rotate 180 deg respect 90 deg rotation 270 deg rotation hsc ccd layout|http://www.naoj.org observing instruments hsc ccdposition_20150804.png raw image rotate ccds need rotate match rotation associate calibration frame prior apply correction accomplish rotate exposure rotate context manager function obs_subaru isr.py nquarter specification policy file ccd currently rotate use afw rotateimageby90 apparently rotate counter clockwise direction rotate exposure nquarter turn turn wrong rotation odd nquarter ccds show ccd100_nquarter3.png|width=200 leave raw exposure read right flatfield exposure read left incorrectly rotate raw exposure prior flatfield correction,"Fix rotation for isr in obs_subaru Approximately half of the HSC CCDs are rotated 180 deg with respect to the others. Two others have 90 deg rotations and another two have 270 deg rotations (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]) . The raw images for the rotated CCDs thus need to be rotated to match the rotation of their associated calibration frames prior to applying the corrections. This is accomplished by rotating the exposure using the *rotated* context manager function in {{obs_subaru}}'s *isr.py* and the *nQuarter* specification in the policy file for each CCD. Currently, *rotated* uses {{afw}}'s *rotateImageBy90* (which apparently rotates in a counter-clockwise direction) to rotated the exposure by 4 - nQuarter turns. This turns out to be the wrong rotation for the odd nQuarter CCDs as shown here: !ccd100_nQuarter3.png|width=200! top left = raw exposure as read in top right = flatfield exposure as read in bottom left = _incorrectly_ rotated raw exposure prior to flatfield correction"
"Make ci_hsc resumable if ci_hsc fails for any reason, (or is cancelled) it must start from the beginning of processing again. This is because of the use of functools.partial to generate dynamic function. These differ enough in their byte code that scons thinks each build has a new function definition passed to the env.command function. Using lambda would suffer from the same problem. This ticket should change how the function signature is calculated such that scons can be resumed.    This work does not prevent this from being used as a ci tool, as the .scons directory can be deleted which will force the whole SConstruct file to run again.",2,DM-5002,datamanagement,ci_hsc resumable ci_hsc fail reason cancel start beginning processing use functools.partial generate dynamic function differ byte code scon think build new function definition pass env.command function lambda suffer problem ticket change function signature calculate scon resume work prevent ci tool directory delete force sconstruct file run,"Make ci_hsc resumable if ci_hsc fails for any reason, (or is cancelled) it must start from the beginning of processing again. This is because of the use of functools.partial to generate dynamic function. These differ enough in their byte code that scons thinks each build has a new function definition passed to the env.command function. Using lambda would suffer from the same problem. This ticket should change how the function signature is calculated such that scons can be resumed. This work does not prevent this from being used as a ci tool, as the .scons directory can be deleted which will force the whole SConstruct file to run again."
"Please trim config overrides in validate_drp validate_drp will test more of our code if it uses default config parameters wherever possible. To that effect I would like to ask you to eliminate all config overrides that are not essential and document the reasons for the remaining overrides.    For DECam there are no overrides that are different than the defaults, so the file can simply be emptied (for now).    For CFHT there are many overrides that are different, and an important question is whether the overrides in this package are better for CFHT data than the overrides in obs_cfht; if so, please move them to obs_cfht.    As a heads up: the default star selector is changing from ""secondMoment"" to ""objectSize"" in DM-4692 and I hope to allow that in validate_drp, since it works better and is better supported.    Sorry for the incorrect component, but validate_drp is not yet a supported component in JIRA (see DM-5004)",1,DM-5005,datamanagement,trim config override validate_drp validate_drp test code use default config parameter possible effect like ask eliminate config override essential document reason remain override decam override different default file simply empty cfht override different important question override package well cfht datum override obs_cfht obs_cfht head default star selector change secondmoment objectsize dm-4692 hope allow validate_drp work well well support sorry incorrect component validate_drp support component jira dm-5004,"Please trim config overrides in validate_drp validate_drp will test more of our code if it uses default config parameters wherever possible. To that effect I would like to ask you to eliminate all config overrides that are not essential and document the reasons for the remaining overrides. For DECam there are no overrides that are different than the defaults, so the file can simply be emptied (for now). For CFHT there are many overrides that are different, and an important question is whether the overrides in this package are better for CFHT data than the overrides in obs_cfht; if so, please move them to obs_cfht. As a heads up: the default star selector is changing from ""secondMoment"" to ""objectSize"" in DM-4692 and I hope to allow that in validate_drp, since it works better and is better supported. Sorry for the incorrect component, but validate_drp is not yet a supported component in JIRA (see DM-5004)"
"remove REUSE_DATAREPO in testCoadds in pipe_tasks When the test fails and the output directory is written but not populated, subsequent test executions fail every time until the output directory is deleted or REUSE_DATAREPO is set to False. This is misleading for users who don't know about this hidden feature.    Furthermore, the REUSE_DATAREPO=False feature is broken; setting it False causes NameError: global name 'DATAREPO_ROOT' is not defined.    It would be better if the test cleaned up after itself (deleted all outputs) every time. If it's really important to reuse the outputs then the dir should be cleaned up in the case of failed writes and/or corruption.    ",1,DM-5006,datamanagement,remove reuse_datarepo testcoadd pipe_task test fail output directory write populate subsequent test execution fail time output directory delete reuse_datarepo set false mislead user know hide feature furthermore reuse_datarepo false feature break set false cause nameerror global datarepo_root define well test clean delete output time important reuse output dir clean case fail write and/or corruption,"remove REUSE_DATAREPO in testCoadds in pipe_tasks When the test fails and the output directory is written but not populated, subsequent test executions fail every time until the output directory is deleted or REUSE_DATAREPO is set to False. This is misleading for users who don't know about this hidden feature. Furthermore, the REUSE_DATAREPO=False feature is broken; setting it False causes NameError: global name 'DATAREPO_ROOT' is not defined. It would be better if the test cleaned up after itself (deleted all outputs) every time. If it's really important to reuse the outputs then the dir should be cleaned up in the case of failed writes and/or corruption."
Resolve development issues by testing using WAN Emulator A test plan draft was written and some short meetings were held regarding the use of the WAN Emulator. The manuals for the Apposite Netropy 40G emulator were retrieved and read. The test plan draft for three test projects is attached.,5,DM-5011,datamanagement,resolve development issue test wan emulator test plan draft write short meeting hold use wan emulator manual apposite netropy 40 emulator retrieve read test plan draft test project attach,Resolve development issues by testing using WAN Emulator A test plan draft was written and some short meetings were held regarding the use of the WAN Emulator. The manuals for the Apposite Netropy 40G emulator were retrieved and read. The test plan draft for three test projects is attached.
"Convert Confluence DM Developer Guide to Sphinx (hack day)  This is a hack day sprint to convert all remaining content on https://confluence.lsstcorp.org/display/LDMDG to reStructuredText content in the Sphinx project at https://github.com/lsst-sqre/dm_dev_guide and published at http://developer.lsst.io.    The top priority for this sprint is to port all content into reST and have it tracked by Git.    h2. Sprint ground rules    # Before the sprint, clone {{https://github.com/lsst-sqre/dm_dev_guide.git}} and {{pip install -r requirements.txt}} in a Python 2.7 environment so that you can locally build the docs ({{make html}}).  # Claim a page from the list below by putting your name on it. Put a checkmark on the page when you’ve merged it to the ticket branch (see below).  # See http://developer.lsst.io/en/latest/docs/rst_styleguide.html for guidance on writing our style of reStructuredText. Pay attention to the [heading hierarchy|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#sections] and [labelling for internal links|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#internal-links-to-labels].  # If you use Pandoc to do an initial content conversion, you still need to go through the content line-by-line to standardize the reStructuredText. I personally recommend copy-and-pasting-and-formatting instead of using Pandoc.  # Your Git commit messages should include the URL of the original content from Confluence.  # Merge your work onto the {{tickets/DM-5013}} ticket branch. Rebase your personal work branch before merging. JSick is responsible for merging this ticket branch to {{master}}.  # Put a note at the top of the confluence page with the new URL; root is {{http://developer.lsst.io/en/latest/}}.    h2. Planned Developer Guide Table of Contents    We’re improving the organization of DM’s Developer Guide; there isn’t a 1:1 mapping of Confluence pages to developer.lsst.io pages. Below is a proposed section organization and page structure. These sections can still be refactored based on discussion during the hack day.    h3. Getting Started — /getting-started/    * ✅ *Onboarding Checklist* (Confluence: [Getting Started in DM|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM]). I’d like this to eventually be a quick checklist of things a new developer should do. It should be both a list of accounts the dev needs to have created, and a list of important developer guide pages to read next. The NCSA-specific material should be spun out. [[~jsick]]  * *Communication Tools* (new + DM Confluence [Communication and Links|https://confluence.lsstcorp.org/display/DM/Communication+and+Links]). I see this as being an overview of what methods DM uses to communicate, and what method should be chosen for any circumstance.  * *Finding Code on GitHub* (new). This should point out all of the GitHub organizations that a developer might come across (DM and LSST-wide), and point out important repositories within each organization. Replaces the confluence page [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories]    h3. Processes — /processes/    * ✅ *Team Culture and Conduct Standards* (confluence)  * ✅ *DM Development Workflow with Git, GitHub, JIRA and Jenkins* (new & Confluence: [git development guidelines for LSST|https://confluence.lsstcorp.org/display/LDMDG/git+development+guidelines+for+LSST] + [Git Commit Best Practices|https://confluence.lsstcorp.org/display/LDMDG/Git+Commit+Best+Practices] + [DM Branching Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Branching+Policy])  * ✅ *Discussion and Decision Making Process* (new & [confluence|https://confluence.lsstcorp.org/display/LDMDG/Discussion+and+Decision+Making+Process])  * ✅ *DM Wiki Use* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/DM+Wiki+Use]) [[~swinbank]]  * ✅ *Policy on Updating Doxygen* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Updating+Doxygen]); needs to be addressed with TCT. Inter-link with the developer workflow page. [[~jsick]] (we’re just re-pointing the Confluence page to the workflow document)  * ✅ *Transferring Code Between Packages* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Transferring+Code+Between+Packages]) [[~swinbank]]  * -*Policy on Changing a Baseline Requirement*- ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Changing+a+Baseline+Requirement])  * ✅ *Project Planning for Software Development* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Project+Planning+for+Software+Development]) [[~swinbank]]  * ✅ *JIRA Agile Usage* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/JIRA+Agile+Usage]) [[~swinbank]]  * -*Technical/Control Account Manager Guide*- ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=21397653]) (Do not port; see discussion below.)  * *Licensing* (new) Need a centralized page to discuss license and copyright policies; include boilerplate statements.    h3. Coding Guides — /coding/    * ✅ *Introduction* and note on stringency language (confluence: [DM Coding Style Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy])  * ✅ *DM Python Style Guide* (confluence: [Python Coding Standard|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard])  * ✅ *DM C++ Style Guide* (confluence pages: [C++ Coding Standard|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666] + [C++ General Recommendations|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908756] + [C++ Naming Conventions|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685] + [C++ Files|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908674] + [C++ Statements|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908706] + [C++ Layout and Comments|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908737] + [Policy on use of C++11/14|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399] + [On Using ‘Using’|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283856])  * Coding Style Linters (new; draft from confluence [C++ Coding Standards Compliance|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283861] and [Python Coding Standards Compliance|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standards+Compliance]  * ✅ *Using C++ Templates* ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190]); this page needs to severely edited or re-written, however.  * ✅ *Profiling* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Profiling|]). Also add a section ‘Using Valgrind with Python' (new) [[~jsick]]  * ✅ *Boost Usage* ([TRAC|https://dev.lsstcorp.org/trac/wiki/TCT/BoostUsageProposal]) [[~tjenness]]  * ✅ *Software Unit Test Policy* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Software+Unit+Test+Policy]) [[~swinbank]]  * ✅ *Unit Test Coverage Analysis* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Coverage+Analysis]) [[~swinbank]]  * ✅ *Unit Testing Private C++ Functions* ([trac|https://dev.lsstcorp.org/trac/wiki/UnitTestingPrivateFunctions]) [[~swinbank]]    h3. Writing Docs — /docs/    * *Introduction* (new): Overview of DM’s documentation needs; links resources on technical writing.  * *English Style Guide* (new): Supplement the [LSST Style Manual|https://www.lsstcorp.org/docushare/dsweb/Get/Document-13016/LSSTStyleManual.pdf] and provide English style guidance specific to DM. Capitalization of different heading levels; use of Chicago Manual of Style; a ‘this, not that’ table of spelling and word choices.  * ✅ *ReStructuredText Style Guide* (new)  * ✅ *Documenting Stack Packages* (new)  * ✅ *Documenting Python Code* (new)  * ✅ *Documenting C++ Code* (confluence, adapted from [Documentation Standards|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards]); needs improvement  * ✅ *Writing Technotes* (new; port README from [lsst-technote-bootstrap|https://github.com/lsst-sqre/lsst-technote-bootstrap/blob/master/README.rst])    h3. Developer Tools — /tools/    * ✅ *Git Setup and Best Practices* (new)  * ✅ *Using Git Large File Storage (LFS) for Data Repositories* (new)  * ✅ *JIRA Work Management Recipes* (new)  * ✅ *Emacs Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Emacs+Support+for+LSST+Development]). See DM-5045 for issue with Emacs config repo - [~jsick]  * ✅ *Vim Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Config+for+VIM]) - [~jsick]    h3. Developer Services — /services/    * ✅ *NCSA Nebula OpenStack Guide* (Confluence: [User Guide|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide] + [Starting an Instance|https://confluence.lsstcorp.org/display/LDMDG/Introduction+to+Starting+a+Nebula+Instance] + [Using Snapshots|https://confluence.lsstcorp.org/display/LDMDG/Start+an+Instance+using+a+base+snapshot+with+the+LSST+Stack]. Add the [Vagrant instructions too from SQR-002|http://sqr-002.lsst.io]? [[~jsick]]  * ✅ *Using lsst-dev* (Confluence: [notes Getting Started|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM] + [Developer Tools at NCSA|https://confluence.lsstcorp.org/display/LDMDG/Developer+Tools+at+NCSA]  * ✅ *Using the Bulk Transfer Server at NCSA* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Using+the+Bulk+Transfer+Server+at+NCSA]) [[~jsick]]    h3. Build, Test, Release — /build-ci/    * *Eups for LSST Developers* (new) [[~swinbank]]  * ✅ *The LSST Software Build Tool* → ‘Using lsstsw and lsst-build' ([confluence|https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool]); lsstsw and lsst-build documentation. [[~swinbank]]  * *Using DM’s Jenkins for Continuous Integration* (new) [~frossie]   * ✅ *Adding a New Package to the Build*([confluence|https://confluence.lsstcorp.org/display/LDMDG/Adding+a+new+package+to+the+build]) [[~swinbank]]  * ✅ *Distributing Third-Party Packages with Eups* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Distributing+third-party+packages+with+EUPS]) [[~swinbank]]  * ✅  *Triggering a Buildbot Build* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Triggering+a+Buildbot+Build]) [~frossie]  * ✅ *Buildbot Errors FAQ* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+FAQ+on+Errors]) [~frossie]  * * Buildbot configuration ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+Configuration+and+Setup] [~frossie]    * *Creating a new DM Stack Release* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Creating+a+new+DM+Stack+Release]); though this page or a modern equivalent should probably belong with the software docs? [~frossie]    _A lot of work should go into this section._ Have something about Scons? Or maybe that belongs in the doc of each relevant software product.    h2. Leftover Confluence pages    h3. The following pages should be moved to a separate Confluence space run by NCSA:    * [NCSA Nebula OpenStack Issues|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+Issues]  * [DM System Announcements|https://confluence.lsstcorp.org/display/LDMDG/DM+System+Announcements]  * [NCSA Development Servers|https://confluence.lsstcorp.org/display/LDMDG/DM+Development+Servers]    h3. The following pages are either not relevant, generally misplaced, or need to be updated/recalibrated:    * [Git Crash Course|https://confluence.lsstcorp.org/display/LDMDG/Git+Crash+Course]  * [Basic Git Operations|https://confluence.lsstcorp.org/display/LDMDG/Basic+Git+Operations]  * [Handling Git Push Problems|https://confluence.lsstcorp.org/display/LDMDG/Handling+Git+Push+Problems]  * [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories]; see the proposed “Finding Code on GitHub” page for a replacement.  * [Standards and Policies|https://confluence.lsstcorp.org/display/LDMDG/Standards+and+Policies]: this is a good TOC for the Confluence docs; but not longer needed for the new docs.  * [Documentation Guidelines|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Guidelines]. Some of this could be re-purposed into an intro to the ‘Writing Documentation’ section; some of this should go in a ‘Processes' page.  * [DM Acknowledgements of Use|https://confluence.lsstcorp.org/display/LDMDG/DM+Acknowledgements+of+Use]: this probably belongs in documentation for the software projects that actually used this work.",5,DM-5013,datamanagement,convert confluence dm developer guide sphinx hack day hack day sprint convert remain content https://confluence.lsstcorp.org/display/ldmdg restructuredtext content sphinx project https://github.com/lsst-sqre/dm_dev_guide publish http://developer.lsst.io priority sprint port content rest track git h2 sprint ground rule sprint clone https://github.com/lsst-sqre/dm_dev_guide.git pip install requirements.txt python 2.7 environment locally build doc html claim page list put checkmark page ve merge ticket branch http://developer.lsst.io/en/latest/docs/rst_styleguide.html guidance write style restructuredtext pay attention head hierarchy|http://developer.lsst.io en late doc rst_styleguide.html#section label internal links|http://developer.lsst.io en late doc rst_styleguide.html#internal link label use pandoc initial content conversion need content line line standardize restructuredtext personally recommend copy pasting formatting instead pandoc git commit message include url original content confluence merge work ticket dm-5013 ticket branch rebase personal work branch merge jsick responsible merge ticket branch master note confluence page new url root http://developer.lsst.io/en/latest/ h2 planned developer guide table contents improve organization dm developer guide isn 1:1 mapping confluence page developer.lsst.io page propose section organization page structure section refactore base discussion hack day h3 getting start /getting started/ onboarding checklist confluence getting start dm|https://confluence.lsstcorp.org display ldmdg getting+started+in+dm like eventually quick checklist thing new developer list account dev need create list important developer guide page read ncsa specific material spin ~jsick communication tools new dm confluence communication links|https://confluence.lsstcorp.org display dm communication+and+links overview method dm use communicate method choose circumstance find code github new point github organization developer come dm lsst wide point important repository organization replace confluence page lsst code repositories|https://confluence.lsstcorp.org display ldmdg lsst+code+repositorie h3 process /processes/ team culture conduct standards confluence dm development workflow git github jira jenkins new confluence git development guideline lsst|https://confluence.lsstcorp.org display ldmdg git+development+guidelines+for+lsst git commit best practices|https://confluence.lsstcorp.org display ldmdg git+commit+best+practices dm branch policy|https://confluence.lsstcorp.org display ldmdg dm+branching+policy discussion decision making process new confluence|https://confluence.lsstcorp.org display ldmdg discussion+and+decision+making+process dm wiki use confluence|https://confluence.lsstcorp.org display ldmdg dm+wiki+use ~swinbank policy updating doxygen confluence|https://confluence.lsstcorp.org display ldmdg policy+on+updating+doxygen need address tct inter link developer workflow page ~jsick point confluence page workflow document transfer code packages confluence|https://confluence.lsstcorp.org display ldmdg transferring+code+between+packages ~swinbank -*policy change baseline requirement*- confluence|https://confluence.lsstcorp.org display ldmdg policy+on+changing+a+baseline+requirement project planning software development confluence|https://confluence.lsstcorp.org display ldmdg project+planning+for+software+development ~swinbank jira agile usage confluence|https://confluence.lsstcorp.org display ldmdg jira+agile+usage ~swinbank -*technical control account manager guide*- confluence|https://confluence.lsstcorp.org page viewpage.action?pageid=21397653 port discussion licensing new need centralized page discuss license copyright policy include boilerplate statement h3 code guide /coding/ introduction note stringency language confluence dm coding style policy|https://confluence.lsstcorp.org display ldmdg dm+coding+style+policy dm python style guide confluence python coding standard|https://confluence.lsstcorp.org display ldmdg python+coding+standard dm c++ style guide confluence page c++ code standard|https://confluence.lsstcorp.org page viewpage.action?pageid=16908666 c++ general recommendations|https://confluence.lsstcorp.org page viewpage.action?pageid=16908756 c++ naming conventions|https://confluence.lsstcorp.org page viewpage.action?pageid=16908685 c++ files|https://confluence.lsstcorp.org page viewpage.action?pageid=16908674 c++ statements|https://confluence.lsstcorp.org page viewpage.action?pageid=16908706 c++ layout comments|https://confluence.lsstcorp.org page viewpage.action?pageid=16908737 policy use c++11/14|https://confluence.lsstcorp.org page viewpage.action?pageid=20283399 |https://confluence.lsstcorp.org page viewpage.action?pageid=20283856 code style linters new draft confluence c++ coding standards compliance|https://confluence.lsstcorp.org page viewpage.action?pageid=20283861 python coding standards compliance|https://confluence.lsstcorp.org display ldmdg python+coding+standards+compliance c++ templates confluence|https://confluence.lsstcorp.org page viewpage.action?pageid=20284190 page need severely edit write profiling confluence|https://confluence.lsstcorp.org display ldmdg profiling| add section valgrind python new ~jsick boost usage trac|https://dev.lsstcorp.org trac wiki tct boostusageproposal ~tjenness software unit test policy confluence|https://confluence.lsstcorp.org display ldmdg software+unit+test+policy ~swinbank unit test coverage analysis confluence|https://confluence.lsstcorp.org display ldmdg coverage+analysis ~swinbank unit testing private c++ functions trac|https://dev.lsstcorp.org trac wiki unittestingprivatefunctions ~swinbank h3 write docs /docs/ introduction new overview dm documentation need link resource technical writing english style guide new supplement lsst style manual|https://www.lsstcorp.org docushare dsweb document-13016 lsststylemanual.pdf provide english style guidance specific dm capitalization different heading level use chicago manual style table spelling word choice restructuredtext style guide new documenting stack packages new documenting python code new documenting c++ code confluence adapt documentation standards|https://confluence.lsstcorp.org display ldmdg documentation+standards need improvement write technotes new port readme lsst technote bootstrap|https://github.com lsst sqre lsst technote bootstrap blob master readme.rst h3 developer tools /tools/ git setup best practices new git large file storage lfs data repositories new jira work management recipes new emacs configuration confluence|https://confluence.lsstcorp.org display ldmdg emacs+support+for+lsst+development dm-5045 issue emacs config repo ~jsick vim configuration confluence|https://confluence.lsstcorp.org display ldmdg config+for+vim ~jsick h3 developer services /services/ ncsa nebula openstack guide confluence user guide|https://confluence.lsstcorp.org display ldmdg ncsa+nebula+openstack+user+guide start instance|https://confluence.lsstcorp.org display ldmdg introduction+to+starting+a+nebula+instance display ldmdg start+an+instance+using+a+base+snapshot+with+the+lsst+stack add vagrant instruction sqr-002|http://sqr-002.lsst.io ~jsick lsst dev confluence note get started|https://confluence.lsstcorp.org display ldmdg getting+started+in+dm developer tools ncsa|https://confluence.lsstcorp.org display ldmdg developer+tools+at+ncsa bulk transfer server ncsa confluence|https://confluence.lsstcorp.org display ldmdg using+the+bulk+transfer+server+at+ncsa ~jsick h3 build test release /build ci/ eup lsst developers new ~swinbank lsst software build tool lsstsw lsst build confluence|https://confluence.lsstcorp.org display ldmdg the+lsst+software+build+tool lsstsw lsst build documentation ~swinbank dm jenkins continuous integration new ~frossie add new package build*([confluence|https://confluence.lsstcorp.org display ldmdg adding+a+new+package+to+the+build ~swinbank distribute party packages eups confluence|https://confluence.lsstcorp.org display ldmdg distributing+third party+packages+with+eups ~swinbank trigger buildbot build confluence|https://confluence.lsstcorp.org display ldmdg triggering+a+buildbot+build ~frossie buildbot error faq confluence|https://confluence.lsstcorp.org display ldmdg buildbot+faq+on+errors ~frossie buildbot configuration confluence|https://confluence.lsstcorp.org display ldmdg buildbot+configuration+and+setup ~frossie create new dm stack release confluence|https://confluence.lsstcorp.org display ldmdg creating+a+new+dm+stack+release page modern equivalent probably belong software doc ~frossie lot work section scon maybe belong doc relevant software product h2 leftover confluence page h3 follow page move separate confluence space run ncsa ncsa nebula openstack issues|https://confluence.lsstcorp.org display ldmdg ncsa+nebula+openstack+issues dm system announcements|https://confluence.lsstcorp.org display ldmdg dm+system+announcement ncsa development servers|https://confluence.lsstcorp.org display ldmdg dm+development+server h3 follow page relevant generally misplace need update recalibrate git crash course|https://confluence.lsstcorp.org display ldmdg git+crash+course basic git operations|https://confluence.lsstcorp.org display ldmdg basic+git+operations handle git push problems|https://confluence.lsstcorp.org display ldmdg handling+git+push+problems lsst code repositories|https://confluence.lsstcorp.org display ldmdg lsst+code+repositories propose finding code github page replacement standards policies|https://confluence.lsstcorp.org display ldmdg standards+and+policies good toc confluence doc long need new doc documentation guidelines|https://confluence.lsstcorp.org display ldmdg documentation+guidelines purpose intro writing documentation section processes page dm acknowledgements use|https://confluence.lsstcorp.org display ldmdg dm+acknowledgements+of+use probably belong documentation software project actually work,"Convert Confluence DM Developer Guide to Sphinx (hack day) This is a hack day sprint to convert all remaining content on https://confluence.lsstcorp.org/display/LDMDG to reStructuredText content in the Sphinx project at https://github.com/lsst-sqre/dm_dev_guide and published at http://developer.lsst.io. The top priority for this sprint is to port all content into reST and have it tracked by Git. h2. Sprint ground rules # Before the sprint, clone {{https://github.com/lsst-sqre/dm_dev_guide.git}} and {{pip install -r requirements.txt}} in a Python 2.7 environment so that you can locally build the docs ({{make html}}). # Claim a page from the list below by putting your name on it. Put a checkmark on the page when you ve merged it to the ticket branch (see below). # See http://developer.lsst.io/en/latest/docs/rst_styleguide.html for guidance on writing our style of reStructuredText. Pay attention to the [heading hierarchy|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#sections] and [labelling for internal links|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#internal-links-to-labels]. # If you use Pandoc to do an initial content conversion, you still need to go through the content line-by-line to standardize the reStructuredText. I personally recommend copy-and-pasting-and-formatting instead of using Pandoc. # Your Git commit messages should include the URL of the original content from Confluence. # Merge your work onto the {{tickets/DM-5013}} ticket branch. Rebase your personal work branch before merging. JSick is responsible for merging this ticket branch to {{master}}. # Put a note at the top of the confluence page with the new URL; root is {{http://developer.lsst.io/en/latest/}}. h2. Planned Developer Guide Table of Contents We re improving the organization of DM s Developer Guide; there isn t a 1:1 mapping of Confluence pages to developer.lsst.io pages. Below is a proposed section organization and page structure. These sections can still be refactored based on discussion during the hack day. h3. Getting Started /getting-started/ * *Onboarding Checklist* (Confluence: [Getting Started in DM|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM]). I d like this to eventually be a quick checklist of things a new developer should do. It should be both a list of accounts the dev needs to have created, and a list of important developer guide pages to read next. The NCSA-specific material should be spun out. [[~jsick]] * *Communication Tools* (new + DM Confluence [Communication and Links|https://confluence.lsstcorp.org/display/DM/Communication+and+Links]). I see this as being an overview of what methods DM uses to communicate, and what method should be chosen for any circumstance. * *Finding Code on GitHub* (new). This should point out all of the GitHub organizations that a developer might come across (DM and LSST-wide), and point out important repositories within each organization. Replaces the confluence page [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories] h3. Processes /processes/ * *Team Culture and Conduct Standards* (confluence) * *DM Development Workflow with Git, GitHub, JIRA and Jenkins* (new & Confluence: [git development guidelines for LSST|https://confluence.lsstcorp.org/display/LDMDG/git+development+guidelines+for+LSST] + [Git Commit Best Practices|https://confluence.lsstcorp.org/display/LDMDG/Git+Commit+Best+Practices] + [DM Branching Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Branching+Policy]) * *Discussion and Decision Making Process* (new & [confluence|https://confluence.lsstcorp.org/display/LDMDG/Discussion+and+Decision+Making+Process]) * *DM Wiki Use* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/DM+Wiki+Use]) [[~swinbank]] * *Policy on Updating Doxygen* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Updating+Doxygen]); needs to be addressed with TCT. Inter-link with the developer workflow page. [[~jsick]] (we re just re-pointing the Confluence page to the workflow document) * *Transferring Code Between Packages* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Transferring+Code+Between+Packages]) [[~swinbank]] * -*Policy on Changing a Baseline Requirement*- ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Changing+a+Baseline+Requirement]) * *Project Planning for Software Development* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Project+Planning+for+Software+Development]) [[~swinbank]] * *JIRA Agile Usage* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/JIRA+Agile+Usage]) [[~swinbank]] * -*Technical/Control Account Manager Guide*- ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=21397653]) (Do not port; see discussion below.) * *Licensing* (new) Need a centralized page to discuss license and copyright policies; include boilerplate statements. h3. Coding Guides /coding/ * *Introduction* and note on stringency language (confluence: [DM Coding Style Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy]) * *DM Python Style Guide* (confluence: [Python Coding Standard|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard]) * *DM C++ Style Guide* (confluence pages: [C++ Coding Standard|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666] + [C++ General Recommendations|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908756] + [C++ Naming Conventions|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685] + [C++ Files|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908674] + [C++ Statements|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908706] + [C++ Layout and Comments|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908737] + [Policy on use of C++11/14|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399] + [On Using Using |https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283856]) * Coding Style Linters (new; draft from confluence [C++ Coding Standards Compliance|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283861] and [Python Coding Standards Compliance|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standards+Compliance] * *Using C++ Templates* ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190]); this page needs to severely edited or re-written, however. * *Profiling* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Profiling|]). Also add a section Using Valgrind with Python' (new) [[~jsick]] * *Boost Usage* ([TRAC|https://dev.lsstcorp.org/trac/wiki/TCT/BoostUsageProposal]) [[~tjenness]] * *Software Unit Test Policy* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Software+Unit+Test+Policy]) [[~swinbank]] * *Unit Test Coverage Analysis* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Coverage+Analysis]) [[~swinbank]] * *Unit Testing Private C++ Functions* ([trac|https://dev.lsstcorp.org/trac/wiki/UnitTestingPrivateFunctions]) [[~swinbank]] h3. Writing Docs /docs/ * *Introduction* (new): Overview of DM s documentation needs; links resources on technical writing. * *English Style Guide* (new): Supplement the [LSST Style Manual|https://www.lsstcorp.org/docushare/dsweb/Get/Document-13016/LSSTStyleManual.pdf] and provide English style guidance specific to DM. Capitalization of different heading levels; use of Chicago Manual of Style; a this, not that table of spelling and word choices. * *ReStructuredText Style Guide* (new) * *Documenting Stack Packages* (new) * *Documenting Python Code* (new) * *Documenting C++ Code* (confluence, adapted from [Documentation Standards|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards]); needs improvement * *Writing Technotes* (new; port README from [lsst-technote-bootstrap|https://github.com/lsst-sqre/lsst-technote-bootstrap/blob/master/README.rst]) h3. Developer Tools /tools/ * *Git Setup and Best Practices* (new) * *Using Git Large File Storage (LFS) for Data Repositories* (new) * *JIRA Work Management Recipes* (new) * *Emacs Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Emacs+Support+for+LSST+Development]). See DM-5045 for issue with Emacs config repo - [~jsick] * *Vim Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Config+for+VIM]) - [~jsick] h3. Developer Services /services/ * *NCSA Nebula OpenStack Guide* (Confluence: [User Guide|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide] + [Starting an Instance|https://confluence.lsstcorp.org/display/LDMDG/Introduction+to+Starting+a+Nebula+Instance] + [Using Snapshots|https://confluence.lsstcorp.org/display/LDMDG/Start+an+Instance+using+a+base+snapshot+with+the+LSST+Stack]. Add the [Vagrant instructions too from SQR-002|http://sqr-002.lsst.io]? [[~jsick]] * *Using lsst-dev* (Confluence: [notes Getting Started|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM] + [Developer Tools at NCSA|https://confluence.lsstcorp.org/display/LDMDG/Developer+Tools+at+NCSA] * *Using the Bulk Transfer Server at NCSA* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Using+the+Bulk+Transfer+Server+at+NCSA]) [[~jsick]] h3. Build, Test, Release /build-ci/ * *Eups for LSST Developers* (new) [[~swinbank]] * *The LSST Software Build Tool* Using lsstsw and lsst-build' ([confluence|https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool]); lsstsw and lsst-build documentation. [[~swinbank]] * *Using DM s Jenkins for Continuous Integration* (new) [~frossie] * *Adding a New Package to the Build*([confluence|https://confluence.lsstcorp.org/display/LDMDG/Adding+a+new+package+to+the+build]) [[~swinbank]] * *Distributing Third-Party Packages with Eups* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Distributing+third-party+packages+with+EUPS]) [[~swinbank]] * *Triggering a Buildbot Build* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Triggering+a+Buildbot+Build]) [~frossie] * *Buildbot Errors FAQ* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+FAQ+on+Errors]) [~frossie] * * Buildbot configuration ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+Configuration+and+Setup] [~frossie] * *Creating a new DM Stack Release* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Creating+a+new+DM+Stack+Release]); though this page or a modern equivalent should probably belong with the software docs? [~frossie] _A lot of work should go into this section._ Have something about Scons? Or maybe that belongs in the doc of each relevant software product. h2. Leftover Confluence pages h3. The following pages should be moved to a separate Confluence space run by NCSA: * [NCSA Nebula OpenStack Issues|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+Issues] * [DM System Announcements|https://confluence.lsstcorp.org/display/LDMDG/DM+System+Announcements] * [NCSA Development Servers|https://confluence.lsstcorp.org/display/LDMDG/DM+Development+Servers] h3. The following pages are either not relevant, generally misplaced, or need to be updated/recalibrated: * [Git Crash Course|https://confluence.lsstcorp.org/display/LDMDG/Git+Crash+Course] * [Basic Git Operations|https://confluence.lsstcorp.org/display/LDMDG/Basic+Git+Operations] * [Handling Git Push Problems|https://confluence.lsstcorp.org/display/LDMDG/Handling+Git+Push+Problems] * [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories]; see the proposed Finding Code on GitHub page for a replacement. * [Standards and Policies|https://confluence.lsstcorp.org/display/LDMDG/Standards+and+Policies]: this is a good TOC for the Confluence docs; but not longer needed for the new docs. * [Documentation Guidelines|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Guidelines]. Some of this could be re-purposed into an intro to the Writing Documentation section; some of this should go in a Processes' page. * [DM Acknowledgements of Use|https://confluence.lsstcorp.org/display/LDMDG/DM+Acknowledgements+of+Use]: this probably belongs in documentation for the software projects that actually used this work."
Set doRenorm default to False in AssembleCcdTask Change the default value of {{AssembleCcdConfig.doRenorm}} to {{False}} for the reasons given in RFC-157 and to implement that RFC.,1,DM-5014,datamanagement,set dorenorm default false assembleccdtask change default value assembleccdconfig.dorenorm false reason give rfc-157 implement rfc,Set doRenorm default to False in AssembleCcdTask Change the default value of {{AssembleCcdConfig.doRenorm}} to {{False}} for the reasons given in RFC-157 and to implement that RFC.
"Optionally report do-nothing config overrides As discussion on DM-4692 and in various HipChat rooms, it's too easy for camera-level config override files to contain many options that don't actually change anything, because they simply override the defaults with the same default values.  To aid in tracking these down and removing them, we should have an option in which {{CmdLineTask}} s (delegating to {{pex_config}}) refuse or warn about overrides that have no effect.    We should probably not make failing on do-nothing overrides the default behavior, but we could consider making warning the default behavior.  Mostly, I think it's important just to be able to find such options when wanted.",4,DM-5015,datamanagement,optionally report config override discussion dm-4692 hipchat room easy camera level config override file contain option actually change simply override default default value aid track remove option cmdlinetask delegate pex_config refuse warn override effect probably fail override default behavior consider make warn default behavior think important able find option want,"Optionally report do-nothing config overrides As discussion on DM-4692 and in various HipChat rooms, it's too easy for camera-level config override files to contain many options that don't actually change anything, because they simply override the defaults with the same default values. To aid in tracking these down and removing them, we should have an option in which {{CmdLineTask}} s (delegating to {{pex_config}}) refuse or warn about overrides that have no effect. We should probably not make failing on do-nothing overrides the default behavior, but we could consider making warning the default behavior. Mostly, I think it's important just to be able to find such options when wanted."
Modernize version check scripts in matplotlib and numpy packages The version check scripts in the stub {{matplotlib}} and {{numpy}} eups packages use old Python conventions. They should be updated to work with 2.7+.,1,DM-5018,datamanagement,modernize version check script matplotlib numpy package version check script stub matplotlib numpy eup package use old python convention update work 2.7,Modernize version check scripts in matplotlib and numpy packages The version check scripts in the stub {{matplotlib}} and {{numpy}} eups packages use old Python conventions. They should be updated to work with 2.7+.
FITS Visualizer porting: Expanded mode single - part 2 I split DM-4497 into two part so I can demonstrate code reviews. This part has paging controller & layout cleaned up.  This tickets is messy because it involves a lot of refactoring of the reducers.  Therefore I am going to end it and move the rest of the UI work to DM-5088.,4,DM-5019,datamanagement,fit visualizer porting expand mode single split dm-4497 demonstrate code review page controller layout clean ticket messy involve lot refactoring reducer go end rest ui work dm-5088,FITS Visualizer porting: Expanded mode single - part 2 I split DM-4497 into two part so I can demonstrate code reviews. This part has paging controller & layout cleaned up. This tickets is messy because it involves a lot of refactoring of the reducers. Therefore I am going to end it and move the rest of the UI work to DM-5088.
"Modernize python code in Qserv scons package The {{site_scons}} Python code is not using current project standards. For example, print is not a function, exceptions are not caught {{as e}}, {{map}} is called without storing the result and {{map/filter/lambda}} are used where list comprehensions would be clearer.    Most of these fixes are trivial with {{futurize}}.",1,DM-5022,datamanagement,modernize python code qserv scon package site_scon python code current project standard example print function exception catch map call store result map filter lambda list comprehension clear fix trivial futurize,"Modernize python code in Qserv scons package The {{site_scons}} Python code is not using current project standards. For example, print is not a function, exceptions are not caught {{as e}}, {{map}} is called without storing the result and {{map/filter/lambda}} are used where list comprehensions would be clearer. Most of these fixes are trivial with {{futurize}}."
"Fix dependencies for eups-packaged sqlalchemy Eups-packaged sqlalchemy lists {{mysqlclient}} as required dependency which is not really right. sqlalchemy does not directly depend on mysql client stuff, instead it determines at run time which python modules it needs to load depending on what exact driver client code is requesting (and {{mysqlclient}} does not actually provides python module so this dependency does not even make anything useful). So dependency on specific external package should be declared on client side and not in sqlalchemy, {{mysqlclient}} should be removed from sqlalchemy.table.",1,DM-5026,datamanagement,fix dependency eup package sqlalchemy eups package sqlalchemy list mysqlclient require dependency right sqlalchemy directly depend mysql client stuff instead determine run time python module need load depend exact driver client code request mysqlclient actually provide python module dependency useful dependency specific external package declare client sqlalchemy mysqlclient remove sqlalchemy.table,"Fix dependencies for eups-packaged sqlalchemy Eups-packaged sqlalchemy lists {{mysqlclient}} as required dependency which is not really right. sqlalchemy does not directly depend on mysql client stuff, instead it determines at run time which python modules it needs to load depending on what exact driver client code is requesting (and {{mysqlclient}} does not actually provides python module so this dependency does not even make anything useful). So dependency on specific external package should be declared on client side and not in sqlalchemy, {{mysqlclient}} should be removed from sqlalchemy.table."
Tests fail on Qserv on OS X El Capitan because of SIP OS X El Capitan introduced System Integrity Protection which leads to dangerous environment variables being stripped when executing trusted binaries. Since {{scons}} is launched using {{/usr/bin/env}} the tests that run do not get to see {{DYLD_LIBRARY_PATH}}. This causes them to fail.    The same fix that was applied to {{sconsUtils}} (copying the path information from {{LSST_LIBRARY_PATH}}) needs to be applied to the test execution code used by Qserv's private {{site_scons}} utility code.,2,DM-5030,datamanagement,test fail qserv os el capitan sip os el capitan introduce system integrity protection lead dangerous environment variable strip execute trust binary scon launch /usr bin env test run dyld_library_path cause fail fix apply sconsutils copy path information lsst_library_path need apply test execution code qserv private site_scon utility code,Tests fail on Qserv on OS X El Capitan because of SIP OS X El Capitan introduced System Integrity Protection which leads to dangerous environment variables being stripped when executing trusted binaries. Since {{scons}} is launched using {{/usr/bin/env}} the tests that run do not get to see {{DYLD_LIBRARY_PATH}}. This causes them to fail. The same fix that was applied to {{sconsUtils}} (copying the path information from {{LSST_LIBRARY_PATH}}) needs to be applied to the test execution code used by Qserv's private {{site_scons}} utility code.
"DAX & DB Docs (Fritz, March) * Document Data Distribution  * Create structure for DAX doc  * Bring over Provenance documentation from prov_prototype  * Update LDM-135 to reflect the updates to the storage/IO model  * Update LDM-152  * Fix LDM-135: 3.3.6.4 and 3.3.6.5 should be 3rd level, so 3.3.7 and 3.3.8  ",3,DM-5035,datamanagement,dax db docs fritz march document data distribution create structure dax doc bring provenance documentation prov_prototype update ldm-135 reflect update storage io model update ldm-152 fix ldm-135 3.3.6.4 3.3.6.5 3rd level 3.3.7 3.3.8,"DAX & DB Docs (Fritz, March) * Document Data Distribution * Create structure for DAX doc * Bring over Provenance documentation from prov_prototype * Update LDM-135 to reflect the updates to the storage/IO model * Update LDM-152 * Fix LDM-135: 3.3.6.4 and 3.3.6.5 should be 3rd level, so 3.3.7 and 3.3.8"
DAX & DB Docs (Mike) Document secondary index,5,DM-5040,datamanagement,dax db docs mike document secondary index,DAX & DB Docs (Mike) Document secondary index
"DAX & DB Docs (Serge) * Document spatial indexing  * Document database ingest  * Refresh ""Stored Procedures and Function"" in LDM-135",6,DM-5041,datamanagement,dax db docs serge document spatial indexing document database ingest refresh stored procedures function ldm-135,"DAX & DB Docs (Serge) * Document spatial indexing * Document database ingest * Refresh ""Stored Procedures and Function"" in LDM-135"
"update ""newinstall.sh"" nebula images & docker containers [~hchiang2] is looking for nebula images newer than {{w_2015_45}} (from the exploratory work in DM-4326) and [~gdaues] is interested in images with a complete {{lsst_distrib}} install for orchestration testing.  New builds should incorporate the pending change to {{newinstall.sh}} that converts from {{anaconda}} to {{miniconda}}.",6,DM-5049,datamanagement,update newinstall.sh nebula image docker container ~hchiang2 look nebula image new w_2015_45 exploratory work dm-4326 ~gdaues interested image complete lsst_distrib install orchestration testing new build incorporate pende change newinstall.sh convert anaconda miniconda,"update ""newinstall.sh"" nebula images & docker containers [~hchiang2] is looking for nebula images newer than {{w_2015_45}} (from the exploratory work in DM-4326) and [~gdaues] is interested in images with a complete {{lsst_distrib}} install for orchestration testing. New builds should incorporate the pending change to {{newinstall.sh}} that converts from {{anaconda}} to {{miniconda}}."
"SingleFrameVariancePlugin takes variance of entire image {{SingleFrameVariancePlugin}} takes the median variance of the entire image, rather than within an aperture around the source of interest.  A {{Footprint}} is constructed with the aperture, but it is unused.    This means that this plugin takes an excessive amount of run time (255/400 sec in a recent run of processCcd on HSC {{visit=1248 ccd=49}} with DM-4692).",1,DM-5050,datamanagement,singleframevarianceplugin take variance entire image singleframevarianceplugin take median variance entire image aperture source interest footprint construct aperture unused mean plugin take excessive run time 255/400 sec recent run processccd hsc visit=1248 ccd=49 dm-4692,"SingleFrameVariancePlugin takes variance of entire image {{SingleFrameVariancePlugin}} takes the median variance of the entire image, rather than within an aperture around the source of interest. A {{Footprint}} is constructed with the aperture, but it is unused. This means that this plugin takes an excessive amount of run time (255/400 sec in a recent run of processCcd on HSC {{visit=1248 ccd=49}} with DM-4692)."
Design replacement for A.net index files We need a simple way to hold index files that will be easy to use and simple to set up.,2,DM-5052,datamanagement,design replacement a.net index file need simple way hold index file easy use simple set,Design replacement for A.net index files We need a simple way to hold index files that will be easy to use and simple to set up.
Assess priority of Aprox/Interp upgrades. This is to assess the priority of a major approximation and interpolation refactor.,4,DM-5055,datamanagement,assess priority aprox interp upgrade assess priority major approximation interpolation refactor,Assess priority of Aprox/Interp upgrades. This is to assess the priority of a major approximation and interpolation refactor.
Assess the corrections that need to be imlemented The stack can do many of the corrections needed.  Assess the status of the current algorithms and identify any deficiencies.,4,DM-5057,datamanagement,assess correction need imlemente stack correction need assess status current algorithm identify deficiency,Assess the corrections that need to be imlemented The stack can do many of the corrections needed. Assess the status of the current algorithms and identify any deficiencies.
"Week end 1/09/16 Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 9, 2016.",1,DM-5068,datamanagement,week end 1/09/16 support lsst dev cluster openstack account week end january 2016,"Week end 1/09/16 Support for lsst-dev cluster, OpenStack, and accounts for week ending January 9, 2016."
"Week end 1/16/16 Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 16, 2016.",2,DM-5069,datamanagement,week end 1/16/16 support lsst dev cluster openstack account week end january 16 2016,"Week end 1/16/16 Support for lsst-dev cluster, OpenStack, and accounts for week ending January 16, 2016."
"Week end 1/23/16 Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 23, 2016.",2,DM-5070,datamanagement,week end 1/23/16 support lsst dev cluster openstack account week end january 23 2016,"Week end 1/23/16 Support for lsst-dev cluster, OpenStack, and accounts for week ending January 23, 2016."
"Week end 1/30/16 Support for lsst-dev cluster, OpenStack, and accounts  for week ending January 23, 2016.",2,DM-5071,datamanagement,week end 1/30/16 support lsst dev cluster openstack account week end january 23 2016,"Week end 1/30/16 Support for lsst-dev cluster, OpenStack, and accounts for week ending January 23, 2016."
New equipment setup and configuration (week end 1/23/16) * Finished setting up Mac vSphere infrastructure with Paul,2,DM-5072,datamanagement,new equipment setup configuration week end 1/23/16 finish set mac vsphere infrastructure paul,New equipment setup and configuration (week end 1/23/16) * Finished setting up Mac vSphere infrastructure with Paul
New equipment setup and configuration (week end 1/30/16) * Set up new lsst-dev7 as CentOS 7 server  * Continuing to set up IPMI on new test servers (working with Dell on issue with iDRAC license upgrade),3,DM-5073,datamanagement,new equipment setup configuration week end 1/30/16 set new lsst dev7 centos server continue set ipmi new test server work dell issue idrac license upgrade,New equipment setup and configuration (week end 1/30/16) * Set up new lsst-dev7 as CentOS 7 server * Continuing to set up IPMI on new test servers (working with Dell on issue with iDRAC license upgrade)
Decommissioning old equipment (week end 1/16/16) * Recovery of old LSST used equipment  ** Moved remaining surplussed last servers to wiping bench  ** Started wiping drives  ** Re-purposed 10 Dell 1950  ,2,DM-5074,datamanagement,decommission old equipment week end 1/16/16 recovery old lsst equipment moved remain surplusse server wipe bench start wipe drive purpose 10 dell 1950,Decommissioning old equipment (week end 1/16/16) * Recovery of old LSST used equipment ** Moved remaining surplussed last servers to wiping bench ** Started wiping drives ** Re-purposed 10 Dell 1950
Decommissioning old equipment (week end 1/23/16) * Complete the cleanup of last used NCSA systems,1,DM-5075,datamanagement,decommission old equipment week end 1/23/16 complete cleanup ncsa system,Decommissioning old equipment (week end 1/23/16) * Complete the cleanup of last used NCSA systems
Lenovo test server * Mount Lenovo test server in LSST1 rack. Install fiber card and networking. Test PXE boot to 10G nic.  * Work on getting Lenovo to PXE boot to 10G card  * Booted satisfactorily to 1GB interface – loaded Centos 7  ** Abruptly ends after Menu with 10GB card  ,3,DM-5077,datamanagement,lenovo test server mount lenovo test server lsst1 rack install fiber card networking test pxe boot 10 nic work get lenovo pxe boot 10 card boot satisfactorily gb interface load centos abruptly end menu 10 gb card,Lenovo test server * Mount Lenovo test server in LSST1 rack. Install fiber card and networking. Test PXE boot to 10G nic. * Work on getting Lenovo to PXE boot to 10G card * Booted satisfactorily to 1GB interface loaded Centos 7 ** Abruptly ends after Menu with 10GB card
"PcaPsf can hit an assertion failure This is bad for multiple reasons:  1. When multiprocessing, the assertion failure kills a single process, which prevents the final join of the multiple processes, so the job hangs forever.  2. The failure is not logged.  3. Hard assertions like this should only occur when we break the system integrity, which this does not (i.e., it's too big a hammer for the problem).    {code}  pprice@tiger-sumire:/tigress/pprice/dm-4692 $ eups list -s  afw                   tickets.DM-4692-gd8ad35cd96+1     b1901 setup  afwdata               2016_01.0         b1901 b1902 setup  astrometry_net        0.50.lsst2+5      b1901 b1902 setup  astrometry_net_data   sdss-dr9-fink-v5b         setup  base                  2016_01.0         b1901 b1902 setup  boost                 1.59.lsst5        b1901 b1902 setup  cfitsio               3360.lsst4        b1901 b1902 setup  coadd_chisquared      2016_01.0+6       b1901 setup  coadd_utils           2016_01.0+6       b1901 setup  daf_base              2016_01.0         b1901 b1902 setup  daf_butlerUtils       tickets.DM-4692-g048b33c50e+3     b1901 setup  daf_persistence       2016_01.0-1-gf47bb69+1    b1901 b1902 setup  display_ds9           2015_10.0+43      b1901 setup  doxygen               1.8.5.lsst1       b1901 b1902 setup  eigen                 3.2.5             b1901 b1902 setup  fftw                  3.3.4.lsst2       b1901 b1902 setup  geom                  10.0+50           b1901 b1902 setup  gsl                   1.16.lsst3        b1901 b1902 setup  ip_diffim             tickets.DM-4692-g543ea8fde5+3     b1901 setup  ip_isr                2016_01.0+6       b1901 setup  lsst_build            LOCAL:/tigress/pprice/lsstsw/lsst_build   setup  mariadbclient         master-gf2dee38289        b1901 b1902 setup  matplotlib            0.0.1+5           b1901 b1902 setup  meas_algorithms       tickets.DM-4692-g3d073a93d7+1     b1901 setup  meas_astrom           tickets.DM-4692-gbbf15418e6+1     b1901 setup  meas_base             LOCAL:/tigress/pprice/dm-4692/meas_base   setup  meas_deblender        2016_01.0+6       b1901 setup  minuit2               5.28.00.lsst2     b1901 b1902 setup  ndarray               10.1+58           b1901 b1902 setup  numpy                 0.0.1+5           b1901 b1902 setup  obs_subaru            LOCAL:/tigress/pprice/dm-4692/obs_subaru  setup  obs_test              tickets.DM-4692-g1533aee20f+1     b1901 setup  pex_config            2016_01.0         b1901 b1902 setup  pex_exceptions        2016_01.0         b1901 b1902 setup  pex_logging           2016_01.0         b1901 b1902 setup  pex_policy            2016_01.0         b1901 b1902 setup  pipe_base             2016_01.0+6       b1901 setup  pipe_tasks            LOCAL:/tigress/pprice/dm-4692/pipe_tasks  setup  psfex                 2016_01.0         b1901 b1902 setup  pyfits                3.4.0             b1901 b1902 setup  python                0.0.3             b1901 b1902 setup  python_d2to1          0.2.12            b1901 b1902 setup  pyyaml                3.11.lsst1        b1901 b1902 setup  scons                 2.3.5             b1901 b1902 setup  sconsUtils            2016_01.0         b1901 b1902 setup  skymap                2016_01.0+6       b1901 setup  skypix                10.0+347          b1901 setup  stsci_distutils       0.3.7-1-gb22a065  b1901 b1902 setup  swig                  3.0.2.lsst1       b1901 b1902 setup  utils                 2016_01.0         b1901 b1902 setup  wcslib                5.13.lsst1        b1901 b1902 setup  xpa                   2.1.15.lsst3      b1901 b1902 setup    pprice@tiger-sumire:/tigress/pprice/dm-4692 $ processCcd.py /tigress/HSC/HSC --rerun price/dm-4692 --rerun price/dm-4692 --id visit=1248 ccd=100 --clobber-config  /tigress/pprice/lsstsw/miniconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.    warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')  : Loading config overrride file '/tigress/pprice/dm-4692/obs_subaru/config/processCcd.py'  WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM (    File ""src/Utils.cc"", line 42, in std::string lsst::utils::getPackageDir(const std::string&)      Package meas_extensions_shapeHSM not found {0}  lsst::pex::exceptions::NotFoundError: 'Package meas_extensions_shapeHSM not found'  ): disabling HSM shape measurements  : Loading config overrride file '/tigress/pprice/dm-4692/obs_subaru/config/hsc/processCcd.py'  : input=/tigress/HSC/HSC  : calib=None  : output=/tigress/HSC/HSC/rerun/price/dm-4692  CameraMapper: Loading registry registry from /tigress/HSC/HSC/rerun/price/dm-4692/_parent/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3  processCcd: Processing {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0}  processCcd.isr: Performing ISR on sensor {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0}  processCcd.isr: Applying linearity corrections to Ccd 100  processCcd.isr.crosstalk: Applying crosstalk correction  processCcd.isr: Set 0 BAD pixels to 3147.74  processCcd.isr: Flattened sky level: 3847.800781 +/- 2114.507723  processCcd.isr: Measuring sky levels in 8x16 grids: 3884.324645  processCcd.isr: Sky flatness in 8x16 grids - pp: 15293.248379 rms: 1173.423587  processCcd.isr: Setting rough magnitude zero point: 34.678409  processCcd.charImage: Processing {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0}  processCcd.charImage.repair: Identified 6044 cosmic rays.  processCcd.charImage.detectAndMeasure.detection: Detected 127 positive sources to 5 sigma.  processCcd.charImage.detectAndMeasure.detection: Resubtracting the background after object detection  processCcd.charImage.detectAndMeasure.measurement: Measuring 127 sources (127 parents, 0 children)   processCcd.charImage.measurePsf: Measuring PSF  /tigress/pprice/lsstsw/stack/Linux64/meas_algorithms/tickets.DM-4692-g3d073a93d7+1/python/lsst/meas/algorithms/objectSizeStarSelector.py:354: RuntimeWarning: invalid value encountered in less    bad = numpy.logical_or(bad, width < self._widthMin)  /tigress/pprice/lsstsw/stack/Linux64/meas_algorithms/tickets.DM-4692-g3d073a93d7+1/python/lsst/meas/algorithms/objectSizeStarSelector.py:355: RuntimeWarning: invalid value encountered in greater    bad = numpy.logical_or(bad, width > self._widthMax)  processCcd.charImage.measurePsf: PSF star selector found 6 candidates  meas.algorithms.psfDeterminer WARNING: You only have 3 eigen images (you asked for 4): reducing number of eigen components  meas.algorithms.psfDeterminer WARNING: You only have 1 eigen images (you asked for 3): reducing number of eigen components  meas.algorithms.psfDeterminer WARNING: You only have 1 eigen images (you asked for 2): reducing number of eigen components  python: /tigress/pprice/lsstsw/stack/Linux64/eigen/3.2.5/include/Eigen/src/Core/Redux.h:202: static Eigen::internal::redux_impl<Func, Derived, 3, 0>::Scalar Eigen::internal::redux_impl<Func, Derived, 3, 0>::run(const Derived&, const Func&) [with Func = Eigen::internal::scalar_max_op<double>; Derived = Eigen::CwiseUnaryOp<Eigen::internal::scalar_abs_op<double>, const Eigen::Matrix<double, -1, -1> >; Eigen::internal::redux_impl<Func, Derived, 3, 0>::Scalar = double]: Assertion `size && ""you are using an empty matrix""' failed.  Aborted  {code}    Note:  * This occurred while testing DM-4692.  The LOCAL pipe_tasks and obs_subaru are on that ticket branch.  The LOCAL meas_base is for the fix from DM-5050.  * One root cause of the bad PSF modeling may be bad rotations in the application of the calibs ([~lauren] is looking into that; don't know if there's a ticket number), but this should never happen regardless.",1,DM-5078,datamanagement,pcapsf hit assertion failure bad multiple reason multiprocesse assertion failure kill single process prevent final join multiple process job hang forever failure log hard assertion like occur break system integrity i.e. big hammer problem code pprice@tiger sumire:/tigress pprice dm-4692 eup list afw ticket dm-4692 gd8ad35cd96 b1901 setup afwdata 2016_01.0 b1901 b1902 setup astrometry_net 0.50.lsst2 b1901 b1902 setup astrometry_net_data sdss dr9 fink v5b setup base 2016_01.0 b1901 b1902 setup boost 1.59.lsst5 b1901 b1902 setup cfitsio 3360.lsst4 b1901 b1902 setup coadd_chisquare 2016_01.0 b1901 setup coadd_util 2016_01.0 b1901 setup daf_base 2016_01.0 b1901 b1902 setup daf_butlerutil ticket dm-4692 g048b33c50e+3 b1901 setup daf_persistence 2016_01.0 gf47bb69 b1901 b1902 setup display_ds9 2015_10.0 43 b1901 setup doxygen 1.8.5.lsst1 b1901 b1902 setup eigen 3.2.5 b1901 b1902 setup fftw 3.3.4.lsst2 b1901 b1902 setup geom 10.0 50 b1901 b1902 setup gsl 1.16.lsst3 b1901 b1902 setup ip_diffim ticket dm-4692 g543ea8fde5 b1901 setup ip_isr 2016_01.0 b1901 setup lsst_build local:/tigress pprice lsstsw lsst_build setup mariadbclient master gf2dee38289 b1901 b1902 setup matplotlib 0.0.1 b1901 b1902 setup meas_algorithm ticket dm-4692 g3d073a93d7 b1901 setup meas_astrom ticket dm-4692 gbbf15418e6 b1901 setup meas_base local:/tigress pprice dm-4692 meas_base setup meas_deblender 2016_01.0 b1901 setup minuit2 5.28.00.lsst2 b1901 b1902 setup ndarray 10.1 58 b1901 b1902 setup numpy 0.0.1 b1901 b1902 setup obs_subaru local:/tigress pprice dm-4692 obs_subaru setup obs_test ticket dm-4692 g1533aee20f+1 b1901 setup pex_config 2016_01.0 b1901 b1902 setup pex_exception 2016_01.0 b1901 b1902 setup pex_logge 2016_01.0 b1901 b1902 setup pex_policy 2016_01.0 b1901 b1902 setup pipe_base 2016_01.0 b1901 setup pipe_task local:/tigress pprice dm-4692 pipe_task setup psfex 2016_01.0 b1901 b1902 setup pyfit 3.4.0 b1901 b1902 setup python 0.0.3 b1901 b1902 setup python_d2to1 0.2.12 b1901 b1902 setup pyyaml 3.11.lsst1 b1901 b1902 setup scon 2.3.5 b1901 b1902 setup sconsutil 2016_01.0 b1901 b1902 setup skymap 2016_01.0 b1901 setup skypix 10.0 347 b1901 setup stsci_distutil 0.3.7 gb22a065 b1901 b1902 setup swig 3.0.2.lsst1 b1901 b1902 setup util 2016_01.0 b1901 b1902 setup wcslib 5.13.lsst1 b1901 b1902 setup xpa 2.1.15.lsst3 b1901 b1902 setup pprice@tiger sumire:/tigress pprice dm-4692 processccd.py hsc hsc price dm-4692 price dm-4692 --id visit=1248 ccd=100 --clobber config /tigress pprice lsstsw miniconda lib python2.7 site package matplotlib font_manager.py:273 userwarning matplotlib build font cache fc list moment warnings.warn('matplotlib build font cache fc list moment loading config overrride file /tigress pprice dm-4692 obs_subaru config processccd.py warning unable use psfex module name extensions.psfex.psfexpsfdetermin import lsst.meas.extensions.photometrykron disable kron measurement enable shapehsm file src utils.cc line 42 std::stre lsst::utils::getpackagedir(const std::stre package meas_extensions_shapehsm find lsst::pex::exceptions::notfounderror package meas_extensions_shapehsm find disable hsm shape measurement loading config overrride file /tigress pprice dm-4692 obs_subaru config hsc processccd.py input=/tigress hsc hsc calib output=/tigress hsc hsc rerun price dm-4692 cameramapper loading registry registry hsc hsc rerun price dm-4692/_parent registry.sqlite3 cameramapper loading calibregistry registry hsc hsc calib calibregistry.sqlite3 processccd processing taiobs 2014 03 28 point 817 visit 1248 dateobs 2014 03 28 filter hsc field ssp_udeep_cosmo ccd 100 exptime 270.0 processccd.isr perform isr sensor taiobs 2014 03 28 point 817 visit 1248 dateobs 2014 03 28 filter hsc field ssp_udeep_cosmo ccd 100 exptime 270.0 processccd.isr apply linearity correction ccd 100 processccd.isr.crosstalk apply crosstalk correction processccd.isr set bad pixel 3147.74 processccd.isr flattened sky level 3847.800781 /- 2114.507723 processccd.isr measure sky level 8x16 grid 3884.324645 processccd.isr sky flatness 8x16 grid pp 15293.248379 rm 1173.423587 processccd.isr set rough magnitude zero point 34.678409 processccd.charimage processing taiobs 2014 03 28 point 817 visit 1248 dateobs 2014 03 28 filter hsc field ssp_udeep_cosmo ccd 100 exptime 270.0 processccd.charimage.repair identify 6044 cosmic ray processccd.charimage.detectandmeasure.detection detect 127 positive source sigma processccd.charimage.detectandmeasure.detection resubtracte background object detection processccd.charimage.detectandmeasure.measurement measure 127 source 127 parent child processccd.charimage.measurepsf measure psf pprice lsstsw stack linux64 meas_algorithm ticket dm-4692 g3d073a93d7 python lsst meas algorithm objectsizestarselector.py:354 runtimewarning invalid value encounter bad numpy.logical_or(bad width self._widthmin /tigress pprice lsstsw stack linux64 meas_algorithm ticket dm-4692 g3d073a93d7 python lsst meas algorithm objectsizestarselector.py:355 runtimewarning invalid value encounter great bad numpy.logical_or(bad width self._widthmax processccd.charimage.measurepsf psf star selector find candidate meas.algorithms.psfdeterminer warning eigen image ask reduce number eigen component meas.algorithms.psfdeterminer warning eigen image ask reduce number eigen component meas.algorithms.psfdeterminer warning eigen image ask reduce number eigen component python /tigress pprice lsstsw stack linux64 eigen/3.2.5 include eigen src core redux.h:202 static eigen::internal::redux_impl::scalar eigen::internal::redux_impl::run(const derived const func func eigen::internal::scalar_max_op derived eigen::cwiseunaryop const eigen::matrix eigen::internal::redux_impl::scalar double assertion size matrix fail aborted code note occur test dm-4692 local pipe_task obs_subaru ticket branch local meas_base fix dm-5050 root cause bad psf modeling bad rotation application calibs ~lauren look know ticket number happen regardless,"PcaPsf can hit an assertion failure This is bad for multiple reasons: 1. When multiprocessing, the assertion failure kills a single process, which prevents the final join of the multiple processes, so the job hangs forever. 2. The failure is not logged. 3. Hard assertions like this should only occur when we break the system integrity, which this does not (i.e., it's too big a hammer for the problem). {code} pprice@tiger-sumire:/tigress/pprice/dm-4692 $ eups list -s afw tickets.DM-4692-gd8ad35cd96+1 b1901 setup afwdata 2016_01.0 b1901 b1902 setup astrometry_net 0.50.lsst2+5 b1901 b1902 setup astrometry_net_data sdss-dr9-fink-v5b setup base 2016_01.0 b1901 b1902 setup boost 1.59.lsst5 b1901 b1902 setup cfitsio 3360.lsst4 b1901 b1902 setup coadd_chisquared 2016_01.0+6 b1901 setup coadd_utils 2016_01.0+6 b1901 setup daf_base 2016_01.0 b1901 b1902 setup daf_butlerUtils tickets.DM-4692-g048b33c50e+3 b1901 setup daf_persistence 2016_01.0-1-gf47bb69+1 b1901 b1902 setup display_ds9 2015_10.0+43 b1901 setup doxygen 1.8.5.lsst1 b1901 b1902 setup eigen 3.2.5 b1901 b1902 setup fftw 3.3.4.lsst2 b1901 b1902 setup geom 10.0+50 b1901 b1902 setup gsl 1.16.lsst3 b1901 b1902 setup ip_diffim tickets.DM-4692-g543ea8fde5+3 b1901 setup ip_isr 2016_01.0+6 b1901 setup lsst_build LOCAL:/tigress/pprice/lsstsw/lsst_build setup mariadbclient master-gf2dee38289 b1901 b1902 setup matplotlib 0.0.1+5 b1901 b1902 setup meas_algorithms tickets.DM-4692-g3d073a93d7+1 b1901 setup meas_astrom tickets.DM-4692-gbbf15418e6+1 b1901 setup meas_base LOCAL:/tigress/pprice/dm-4692/meas_base setup meas_deblender 2016_01.0+6 b1901 setup minuit2 5.28.00.lsst2 b1901 b1902 setup ndarray 10.1+58 b1901 b1902 setup numpy 0.0.1+5 b1901 b1902 setup obs_subaru LOCAL:/tigress/pprice/dm-4692/obs_subaru setup obs_test tickets.DM-4692-g1533aee20f+1 b1901 setup pex_config 2016_01.0 b1901 b1902 setup pex_exceptions 2016_01.0 b1901 b1902 setup pex_logging 2016_01.0 b1901 b1902 setup pex_policy 2016_01.0 b1901 b1902 setup pipe_base 2016_01.0+6 b1901 setup pipe_tasks LOCAL:/tigress/pprice/dm-4692/pipe_tasks setup psfex 2016_01.0 b1901 b1902 setup pyfits 3.4.0 b1901 b1902 setup python 0.0.3 b1901 b1902 setup python_d2to1 0.2.12 b1901 b1902 setup pyyaml 3.11.lsst1 b1901 b1902 setup scons 2.3.5 b1901 b1902 setup sconsUtils 2016_01.0 b1901 b1902 setup skymap 2016_01.0+6 b1901 setup skypix 10.0+347 b1901 setup stsci_distutils 0.3.7-1-gb22a065 b1901 b1902 setup swig 3.0.2.lsst1 b1901 b1902 setup utils 2016_01.0 b1901 b1902 setup wcslib 5.13.lsst1 b1901 b1902 setup xpa 2.1.15.lsst3 b1901 b1902 setup pprice@tiger-sumire:/tigress/pprice/dm-4692 $ processCcd.py /tigress/HSC/HSC --rerun price/dm-4692 --rerun price/dm-4692 --id visit=1248 ccd=100 --clobber-config /tigress/pprice/lsstsw/miniconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.') : Loading config overrride file '/tigress/pprice/dm-4692/obs_subaru/config/processCcd.py' WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements Cannot enable shapeHSM ( File ""src/Utils.cc"", line 42, in std::string lsst::utils::getPackageDir(const std::string&) Package meas_extensions_shapeHSM not found {0} lsst::pex::exceptions::NotFoundError: 'Package meas_extensions_shapeHSM not found' ): disabling HSM shape measurements : Loading config overrride file '/tigress/pprice/dm-4692/obs_subaru/config/hsc/processCcd.py' : input=/tigress/HSC/HSC : calib=None : output=/tigress/HSC/HSC/rerun/price/dm-4692 CameraMapper: Loading registry registry from /tigress/HSC/HSC/rerun/price/dm-4692/_parent/registry.sqlite3 CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3 processCcd: Processing {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0} processCcd.isr: Performing ISR on sensor {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0} processCcd.isr: Applying linearity corrections to Ccd 100 processCcd.isr.crosstalk: Applying crosstalk correction processCcd.isr: Set 0 BAD pixels to 3147.74 processCcd.isr: Flattened sky level: 3847.800781 +/- 2114.507723 processCcd.isr: Measuring sky levels in 8x16 grids: 3884.324645 processCcd.isr: Sky flatness in 8x16 grids - pp: 15293.248379 rms: 1173.423587 processCcd.isr: Setting rough magnitude zero point: 34.678409 processCcd.charImage: Processing {'taiObs': '2014-03-28', 'pointing': 817, 'visit': 1248, 'dateObs': '2014-03-28', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 100, 'expTime': 270.0} processCcd.charImage.repair: Identified 6044 cosmic rays. processCcd.charImage.detectAndMeasure.detection: Detected 127 positive sources to 5 sigma. processCcd.charImage.detectAndMeasure.detection: Resubtracting the background after object detection processCcd.charImage.detectAndMeasure.measurement: Measuring 127 sources (127 parents, 0 children) processCcd.charImage.measurePsf: Measuring PSF /tigress/pprice/lsstsw/stack/Linux64/meas_algorithms/tickets.DM-4692-g3d073a93d7+1/python/lsst/meas/algorithms/objectSizeStarSelector.py:354: RuntimeWarning: invalid value encountered in less bad = numpy.logical_or(bad, width < self._widthMin) /tigress/pprice/lsstsw/stack/Linux64/meas_algorithms/tickets.DM-4692-g3d073a93d7+1/python/lsst/meas/algorithms/objectSizeStarSelector.py:355: RuntimeWarning: invalid value encountered in greater bad = numpy.logical_or(bad, width > self._widthMax) processCcd.charImage.measurePsf: PSF star selector found 6 candidates meas.algorithms.psfDeterminer WARNING: You only have 3 eigen images (you asked for 4): reducing number of eigen components meas.algorithms.psfDeterminer WARNING: You only have 1 eigen images (you asked for 3): reducing number of eigen components meas.algorithms.psfDeterminer WARNING: You only have 1 eigen images (you asked for 2): reducing number of eigen components python: /tigress/pprice/lsstsw/stack/Linux64/eigen/3.2.5/include/Eigen/src/Core/Redux.h:202: static Eigen::internal::redux_impl::Scalar Eigen::internal::redux_impl::run(const Derived&, const Func&) [with Func = Eigen::internal::scalar_max_op; Derived = Eigen::CwiseUnaryOp, const Eigen::Matrix >; Eigen::internal::redux_impl::Scalar = double]: Assertion `size && ""you are using an empty matrix""' failed. Aborted {code} Note: * This occurred while testing DM-4692. The LOCAL pipe_tasks and obs_subaru are on that ticket branch. The LOCAL meas_base is for the fix from DM-5050. * One root cause of the bad PSF modeling may be bad rotations in the application of the calibs ([~lauren] is looking into that; don't know if there's a ticket number), but this should never happen regardless."
"PropagateVisitFlags doesn't work with other pipeline components {{PropagateVisitFlags}}, which was recently ported over from HSC on DM-4878, doesn't work due to some inconsistencies with earlier packages/tasks:   - The default fields to transfer have new names: ""calib_psfCandidate"" and ""calib_psfUsed""   - We're not currently transferring these fields from icSrc to src, so those fields aren't present in src anyway.  I propose we just match against icSrc for now, since it has all of the fields we're concerned with.   - It makes a call to {{afw.table.ExposureCatalog.subsetContaining(Point, Wcs, bool)}}, which apparently exists in C++ but not in Python; I'll look into seeing which HSC commits may have been missed in that port.",1,DM-5084,datamanagement,propagatevisitflags work pipeline component propagatevisitflags recently port hsc dm-4878 work inconsistency early package task default field transfer new name calib_psfcandidate calib_psfuse currently transfer field icsrc src field present src propose match icsrc field concerned make afw.table exposurecatalog.subsetcontaining(point wcs bool apparently exist c++ python look seeing hsc commit miss port,"PropagateVisitFlags doesn't work with other pipeline components {{PropagateVisitFlags}}, which was recently ported over from HSC on DM-4878, doesn't work due to some inconsistencies with earlier packages/tasks: - The default fields to transfer have new names: ""calib_psfCandidate"" and ""calib_psfUsed"" - We're not currently transferring these fields from icSrc to src, so those fields aren't present in src anyway. I propose we just match against icSrc for now, since it has all of the fields we're concerned with. - It makes a call to {{afw.table.ExposureCatalog.subsetContaining(Point, Wcs, bool)}}, which apparently exists in C++ but not in Python; I'll look into seeing which HSC commits may have been missed in that port."
"Please add a package that includes obs_decam, obs_cfht and all validation_data datasets It would be very helpful to have an lsstsw package that added all supported obs_* packages (certainly including obs_cfht and obs_decam, and I hope obs_subaru) and all validation_data_* packages. This could be something other than lsst_apps, but I'm not sure what to call it.",1,DM-5085,datamanagement,add package include obs_decam obs_cfht validation_data dataset helpful lsstsw package add support obs package certainly include obs_cfht obs_decam hope obs_subaru validation_data package lsst_apps sure,"Please add a package that includes obs_decam, obs_cfht and all validation_data datasets It would be very helpful to have an lsstsw package that added all supported obs_* packages (certainly including obs_cfht and obs_decam, and I hope obs_subaru) and all validation_data_* packages. This could be something other than lsst_apps, but I'm not sure what to call it."
"Enable aperture correction on coadd processing Aperture corrections are now coadded, so we can enable aperture corrections in measurements done on coadds.",1,DM-5086,datamanagement,enable aperture correction coadd process aperture correction coadde enable aperture correction measurement coadd,"Enable aperture correction on coadd processing Aperture corrections are now coadded, so we can enable aperture corrections in measurements done on coadds."
"Add auto play,select which dialog, close button working,  to expanded mode Add the auto play to expanded mode.  Add the choose which dialog to expanded mode. Make close button work.    I am breaking this up the expanded mode ticketa because the task is getting so big and ticket DM-5019 involved reducer refactoring.  Also the refactoring needs to get into the dev branch.",4,DM-5088,datamanagement,add auto play select dialog close button working expand mode add auto play expand mode add choose dialog expand mode close button work break expand mode ticketa task get big ticket dm-5019 involve reducer refactoring refactoring need dev branch,"Add auto play,select which dialog, close button working, to expanded mode Add the auto play to expanded mode. Add the choose which dialog to expanded mode. Make close button work. I am breaking this up the expanded mode ticketa because the task is getting so big and ticket DM-5019 involved reducer refactoring. Also the refactoring needs to get into the dev branch."
Add task discovery on command line activator I'll add a way to specify on the command line the path or the package to discover for CmdLineTask or SuperTasks,4,DM-5089,datamanagement,add task discovery command line activator add way specify command line path package discover cmdlinetask supertasks,Add task discovery on command line activator I'll add a way to specify on the command line the path or the package to discover for CmdLineTask or SuperTasks
Investigate alternative for networkx before RFC I'll make sure I explored other alternatives before creating a RFC for adding networkx which by itself require other packages. This is needed for the pipe_flow_x work. I tried one stand-alone package before pygraphviz but then decided to migrate to networkx as it is more complete and allow other possible future features,4,DM-5090,datamanagement,investigate alternative networkx rfc sure explore alternative create rfc add networkx require package need pipe_flow_x work try stand package pygraphviz decide migrate networkx complete allow possible future feature,Investigate alternative for networkx before RFC I'll make sure I explored other alternatives before creating a RFC for adding networkx which by itself require other packages. This is needed for the pipe_flow_x work. I tried one stand-alone package before pygraphviz but then decided to migrate to networkx as it is more complete and allow other possible future features
Security plan renewal Renewal of the LSST security plan.  Starts with DM.,3,DM-5092,datamanagement,security plan renewal renewal lsst security plan start dm,Security plan renewal Renewal of the LSST security plan. Starts with DM.
HSC backport: Set BAD mask for dead amps instead of SAT This is a port of [HSC-1095|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1095] and a leftover commit from [HSC-1231|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1231]: [isr: don't perform overscan subtraction on bad amps|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d6fe6cf5c4ecadebd5a344d163e1f1e60137c7e4] (noted in DM-3942).,3,DM-5094,datamanagement,hsc backport set bad mask dead amp instead sat port hsc-1095|https://hsc jira.astro.princeton.edu jira browse hsc-1095 leftover commit hsc-1231|https://hsc jira.astro.princeton.edu jira browse hsc-1231 isr perform overscan subtraction bad amps|https://github.com hypersuprime cam obs_subaru commit d6fe6cf5c4ecadebd5a344d163e1f1e60137c7e4 note dm-3942,HSC backport: Set BAD mask for dead amps instead of SAT This is a port of [HSC-1095|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1095] and a leftover commit from [HSC-1231|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1231]: [isr: don't perform overscan subtraction on bad amps|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d6fe6cf5c4ecadebd5a344d163e1f1e60137c7e4] (noted in DM-3942).
Redirect confluence based pages to new developer guide. Delete and apply redirects to all migrated pages in old Confluence-based Developer Guide,1,DM-5095,datamanagement,redirect confluence base page new developer guide delete apply redirect migrate page old confluence base developer guide,Redirect confluence based pages to new developer guide. Delete and apply redirects to all migrated pages in old Confluence-based Developer Guide
"Make validateDrp a Task. Make validateDrp a Task so   1. it can easily be run from the command line or programmatically.  2. it can import the standard command line arguments  3. it can be logged in the same way.    This eventually should fit into DM-2050, and DM-3859.",2,DM-5096,datamanagement,validatedrp task validatedrp task easily run command line programmatically import standard command line argument log way eventually fit dm-2050 dm-3859,"Make validateDrp a Task. Make validateDrp a Task so 1. it can easily be run from the command line or programmatically. 2. it can import the standard command line arguments 3. it can be logged in the same way. This eventually should fit into DM-2050, and DM-3859."
"Update validate_drp to use TransformTask to store calibrated measurements Currently validate_drp uses some manual crude addition of calibration information and constructs new schemas to store this information.  This is essentially what TransformTask is meant for.  Using this would simplify the code, make it less fragile, and ideally eventually integrate more transparently with future calibration improvements or redefinitions of how zeropoints are tracked..    1. Learn how to use TransformTask.  Note DM-4948 is the doc task for this.  2. Adapt the code.  3. Verify unchanged results on existing validation_data_decam and validation_data_cfht.",2,DM-5097,datamanagement,update validate_drp use transformtask store calibrate measurement currently validate_drp use manual crude addition calibration information construct new schema store information essentially transformtask mean simplify code fragile ideally eventually integrate transparently future calibration improvement redefinition zeropoint track learn use transformtask note dm-4948 doc task adapt code verify unchanged result exist validation_data_decam validation_data_cfht,"Update validate_drp to use TransformTask to store calibrated measurements Currently validate_drp uses some manual crude addition of calibration information and constructs new schemas to store this information. This is essentially what TransformTask is meant for. Using this would simplify the code, make it less fragile, and ideally eventually integrate more transparently with future calibration improvements or redefinitions of how zeropoints are tracked.. 1. Learn how to use TransformTask. Note DM-4948 is the doc task for this. 2. Adapt the code. 3. Verify unchanged results on existing validation_data_decam and validation_data_cfht."
Add tests to validate_drp to verify SRD calculations and utility function behavior The current validate_drp is woefully lacking in tests.    1. The key SRD metrics definitely need to have test cases that verify the calculation of these important metrics.  2. Overall the utility functions would benefit from testing.,4,DM-5098,datamanagement,add test validate_drp verify srd calculation utility function behavior current validate_drp woefully lack test key srd metric definitely need test case verify calculation important metric overall utility function benefit test,Add tests to validate_drp to verify SRD calculations and utility function behavior The current validate_drp is woefully lacking in tests. 1. The key SRD metrics definitely need to have test cases that verify the calculation of these important metrics. 2. Overall the utility functions would benefit from testing.
"Polish IN2P3 cluster upgrade to CentOS7 What remains:    - problem with Docker 1.9.1+overlay+xfs => switch to Docker 1.10.1? Then switch back from devicemapper to overlay?  - problem with qserv uid: go back to 1000, instead of 1008?",4,DM-5099,datamanagement,polish in2p3 cluster upgrade centos7 remain problem docker 1.9.1+overlay+xfs switch docker 1.10.1 switch devicemapper overlay problem qserv uid 1000 instead 1008,"Polish IN2P3 cluster upgrade to CentOS7 What remains: - problem with Docker 1.9.1+overlay+xfs => switch to Docker 1.10.1? Then switch back from devicemapper to overlay? - problem with qserv uid: go back to 1000, instead of 1008?"
Docs for ltd-keeper Create a documentation project within ltd-keeper that documents the RESTful API while it is being developed. This will allow the [SQR-006|http://sqr-006.lsst.io] technote to have a place to link to for detailed information.,1,DM-5100,datamanagement,docs ltd keeper create documentation project ltd keeper document restful api develop allow sqr-006|http://sqr-006.lsst.io technote place link detailed information,Docs for ltd-keeper Create a documentation project within ltd-keeper that documents the RESTful API while it is being developed. This will allow the [SQR-006|http://sqr-006.lsst.io] technote to have a place to link to for detailed information.
"Fix --id examples in processCcd.py and friends to correctly show ""ccd=1^2"". The required '^' convention for lists of things, e.g. {{ccd}}, {{filter}}, {{visit}} and such is surprising.  But, worse, the documentation is currently wrong in its examples and presents several {{ccd=1,2}}, {{patch=1,2}} examples.    * Fix the {{--id}} examples in {{pipe_tasks}} and other uses of processCcd.py in obs_* packages to correctly match the required syntax.    Here's the current list in {{pipe_tasks}}, but check other packages as well.    {code}  [serenity tasks] grep '[0-9],[0-9]' *.py | grep '""'  assembleCoadd.py:                               help=""data ID, e.g. --id tract=12345 patch=1,2"",  coaddBase.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2"",  imageDifference.py:        parser.add_id_argument(""--id"", ""calexp"", help=""data ID, e.g. --id visit=12345 ccd=1,2"")  makeDiscreteSkyMap.py:            boxI = afwGeom.Box2I(afwGeom.Point2I(0,0), afwGeom.Extent2I(md.get(""NAXIS1""), md.get(""NAXIS2"")))  multiBand.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2 filter=r"",  multiBand.py:                               help=""data ID, e.g. --id tract=12345 patch=1,2 filter=g^r^i"")  multiBand.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2 filter=r"",  processCoadd.py:        parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2"",  {code}",1,DM-5101,datamanagement,"fix --id example processccd.py friend correctly ccd=1 require convention list thing e.g. ccd filter visit surprising bad documentation currently wrong example present ccd=1,2 patch=1,2 example fix --id example pipe_tasks use processccd.py obs package correctly match require syntax current list pipe_tasks check package code serenity task grep 9],[0 .py grep assemblecoadd.py help=""data id e.g. --id tract=12345 patch=1,2 coaddbase.py parser.add_id_argument(""--id deepcoadd help=""data id e.g. --id tract=12345 patch=1,2 imagedifference.py parser.add_id_argument(""--id calexp help=""data id e.g. --id visit=12345 ccd=1,2 makediscreteskymap.py boxi afwgeom box2i(afwgeom point2i(0,0 afwgeom extent2i(md.get(""naxis1 md.get(""naxis2 multiband.py parser.add_id_argument(""--id deepcoadd help=""data id e.g. --id tract=12345 patch=1,2 filter multiband.py help=""data id e.g. --id tract=12345 patch=1,2 filter g^r^i multiband.py parser.add_id_argument(""--id deepcoadd help=""data id e.g. --id tract=12345 patch=1,2 filter processcoadd.py parser.add_id_argument(""--id deepcoadd help=""data id e.g. --id tract=12345 patch=1,2 code","Fix --id examples in processCcd.py and friends to correctly show ""ccd=1^2"". The required '^' convention for lists of things, e.g. {{ccd}}, {{filter}}, {{visit}} and such is surprising. But, worse, the documentation is currently wrong in its examples and presents several {{ccd=1,2}}, {{patch=1,2}} examples. * Fix the {{--id}} examples in {{pipe_tasks}} and other uses of processCcd.py in obs_* packages to correctly match the required syntax. Here's the current list in {{pipe_tasks}}, but check other packages as well. {code} [serenity tasks] grep '[0-9],[0-9]' *.py | grep '""' assembleCoadd.py: help=""data ID, e.g. --id tract=12345 patch=1,2"", coaddBase.py: parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2"", imageDifference.py: parser.add_id_argument(""--id"", ""calexp"", help=""data ID, e.g. --id visit=12345 ccd=1,2"") makeDiscreteSkyMap.py: boxI = afwGeom.Box2I(afwGeom.Point2I(0,0), afwGeom.Extent2I(md.get(""NAXIS1""), md.get(""NAXIS2""))) multiBand.py: parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2 filter=r"", multiBand.py: help=""data ID, e.g. --id tract=12345 patch=1,2 filter=g^r^i"") multiBand.py: parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2 filter=r"", processCoadd.py: parser.add_id_argument(""--id"", ""deepCoadd"", help=""data ID, e.g. --id tract=12345 patch=1,2"", {code}"
"Rewrite integration test queries with spatial constraint returning empty results Some queries in the integration test suite return empty results, here's how to catch them:  {code:bash}  # this should be done for alll tests cases  egrep ""^0$"" ~/qserv-run/2016_02/tmp/qservTest_case02/outputs/mysql/*  # empty results files have also to be tracked  {code}    There parameters should be fixed to query a region containing data (use select * on object).",5,DM-5102,datamanagement,rewrite integration test query spatial constraint return result query integration test suite return result catch code bash alll test case egrep ^0 ~/qserv run/2016_02 tmp qservtest_case02 outputs mysql/ result file track code parameter fix query region contain datum use select object,"Rewrite integration test queries with spatial constraint returning empty results Some queries in the integration test suite return empty results, here's how to catch them: {code:bash} # this should be done for alll tests cases egrep ""^0$"" ~/qserv-run/2016_02/tmp/qservTest_case02/outputs/mysql/* # empty results files have also to be tracked {code} There parameters should be fixed to query a region containing data (use select * on object)."
"Add scans for DRx-1 to the model Per RFC-134 we need to support scans for DRx-1. This story involves building this into the model, costing it, and changing the baseline.",6,DM-5103,datamanagement,add scan drx-1 model rfc-134 need support scan drx-1 story involve build model cost change baseline,"Add scans for DRx-1 to the model Per RFC-134 we need to support scans for DRx-1. This story involves building this into the model, costing it, and changing the baseline."
"Add scans for DRP-produced Dia* tables to the model Per RFC-133, we need to support scans on DiaObject table, possibly Dia*Source tables as well. This story involves adding it to the model, costing it and adding it to the baseline.",6,DM-5104,datamanagement,add scan drp produce dia table model rfc-133 need support scan diaobject table possibly dia*source table story involve add model cost add baseline,"Add scans for DRP-produced Dia* tables to the model Per RFC-133, we need to support scans on DiaObject table, possibly Dia*Source tables as well. This story involves adding it to the model, costing it and adding it to the baseline."
"new conda 'mkl' dependent packages break meas_base tests Continuum release/rebuilt a number of packages last friday to depend on the the Intel MKL library.     https://www.continuum.io/blog/developer-blog/anaconda-25-release-now-mkl-optimizations    There are [new feature named] versions that continue to use openblas but the MKL versions appear to be installed by default.  This causes at least multiple {{meas_base}} tests to fail.After extensive testing, I have confirmed that the meas_base tests do not fail with the equivalent 'nomkl' package.  In addition, mkl is closed source software that requires you to accept and download a license file or it is time-bombed to stop working after a trial period.      {code:java}      docker-centos-7: [ 36/36 ]  meas_base 2015_10.0-9-g6daf04b+7 ...      docker-centos-7:      docker-centos-7: ***** error: from /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/build.log:      docker-centos-7: tests/sincPhotSums.py      docker-centos-7:      docker-centos-7: tests/measureSources.py      docker-centos-7:      docker-centos-7: tests/testApertureFlux.py      docker-centos-7:      docker-centos-7: tests/testJacobian.py      docker-centos-7:      docker-centos-7: tests/testScaledApertureFlux.py      docker-centos-7:      docker-centos-7: The following tests failed:      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/sincPhotSums.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/measureSources.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testApertureFlux.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testJacobian.py.failed      docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testScaledApertureFlux.py.failed      docker-centos-7: 5 tests failed  {code}  (the exact cause of the test failures was not investigated as this should not have happened)    This change has also broken the ability to import an existing conda env from 2016-02-05 or earlier that uses scipy due to some sort of package version resolution problem.  Explicit declaring it as the scipy package without mkl fixes the resolution problem.    There is a new 'nomkl' package, when installed, any subsequent package installations will default to versions without mkl.  However, this does not fix any already installed packages.    I am traumatized by the lack of reproducible  build envs even within a few days of each other.  After discussion in the Tucson office, I'm going to pin the lsstsw and newinstall.sh conda package versions with a commitment from square to update them on a monthly basis.  I already have a test version of lsstsw/bin/deploy that defaults to a bundled package but with a option flag to use bleeding edge.  ",4,DM-5105,datamanagement,new conda mkl dependent package break meas_base test continuum release rebuild number package friday depend intel mkl library https://www.continuum.io/blog/developer-blog/anaconda-25-release-now-mkl-optimization new feature name version continue use openbla mkl version appear instal default cause multiple meas_base test fail extensive testing confirm meas_base test fail equivalent nomkl package addition mkl close source software require accept download license file time bomb stop work trial period code java docker centos-7 36/36 meas_base 2015_10.0 g6daf04b+7 docker centos-7 docker centos-7 error /opt lsst software stack eupsbuilddir linux64 meas_base-2015_10.0 g6daf04b+7 build.log docker centos-7 test sincphotsums.py docker centos-7 docker centos-7 test measuresources.py docker centos-7 docker centos-7 test testapertureflux.py docker centos-7 docker centos-7 test testjacobian.py docker centos-7 docker centos-7 test testscaledapertureflux.py docker centos-7 docker centos-7 follow test fail docker centos-7 /opt lsst software stack eupsbuilddir linux64 meas_base-2015_10.0 g6daf04b+7 meas_base-2015_10.0 g6daf04b+7 tests/.tests sincphotsums.py.faile docker centos-7 /opt lsst software stack eupsbuilddir linux64 meas_base-2015_10.0 g6daf04b+7 meas_base-2015_10.0 g6daf04b+7 tests/.test measuresources.py.faile docker centos-7 /opt lsst software stack eupsbuilddir linux64 meas_base-2015_10.0 g6daf04b+7 meas_base-2015_10.0 g6daf04b+7 tests/.test testapertureflux.py.faile docker centos-7 /opt lsst software stack eupsbuilddir linux64 meas_base-2015_10.0 g6daf04b+7 meas_base-2015_10.0 g6daf04b+7 tests/.test testjacobian.py.faile docker centos-7 /opt lsst software stack eupsbuilddir linux64 meas_base-2015_10.0 g6daf04b+7 meas_base-2015_10.0 g6daf04b+7 tests/.test testscaledapertureflux.py.faile docker centos-7 test fail code exact cause test failure investigate happen change break ability import exist conda env 2016 02 05 early use scipy sort package version resolution problem explicit declare scipy package mkl fix resolution problem new nomkl package instal subsequent package installation default version mkl fix instal package traumatize lack reproducible build envs day discussion tucson office go pin lsstsw newinstall.sh conda package version commitment square update monthly basis test version lsstsw bin deploy default bundled package option flag use bleeding edge,"new conda 'mkl' dependent packages break meas_base tests Continuum release/rebuilt a number of packages last friday to depend on the the Intel MKL library. https://www.continuum.io/blog/developer-blog/anaconda-25-release-now-mkl-optimizations There are [new feature named] versions that continue to use openblas but the MKL versions appear to be installed by default. This causes at least multiple {{meas_base}} tests to fail.After extensive testing, I have confirmed that the meas_base tests do not fail with the equivalent 'nomkl' package. In addition, mkl is closed source software that requires you to accept and download a license file or it is time-bombed to stop working after a trial period. {code:java} docker-centos-7: [ 36/36 ] meas_base 2015_10.0-9-g6daf04b+7 ... docker-centos-7: docker-centos-7: ***** error: from /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/build.log: docker-centos-7: tests/sincPhotSums.py docker-centos-7: docker-centos-7: tests/measureSources.py docker-centos-7: docker-centos-7: tests/testApertureFlux.py docker-centos-7: docker-centos-7: tests/testJacobian.py docker-centos-7: docker-centos-7: tests/testScaledApertureFlux.py docker-centos-7: docker-centos-7: The following tests failed: docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/sincPhotSums.py.failed docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/measureSources.py.failed docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testApertureFlux.py.failed docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testJacobian.py.failed docker-centos-7: /opt/lsst/software/stack/EupsBuildDir/Linux64/meas_base-2015_10.0-9-g6daf04b+7/meas_base-2015_10.0-9-g6daf04b+7/tests/.tests/testScaledApertureFlux.py.failed docker-centos-7: 5 tests failed {code} (the exact cause of the test failures was not investigated as this should not have happened) This change has also broken the ability to import an existing conda env from 2016-02-05 or earlier that uses scipy due to some sort of package version resolution problem. Explicit declaring it as the scipy package without mkl fixes the resolution problem. There is a new 'nomkl' package, when installed, any subsequent package installations will default to versions without mkl. However, this does not fix any already installed packages. I am traumatized by the lack of reproducible build envs even within a few days of each other. After discussion in the Tucson office, I'm going to pin the lsstsw and newinstall.sh conda package versions with a commitment from square to update them on a monthly basis. I already have a test version of lsstsw/bin/deploy that defaults to a bundled package but with a option flag to use bleeding edge."
"newinstall.sh fails with ""eups: command not found"" [~jgates] has reported the following output when running {{newinstall.sh}} on el6.    {code:java}  Installing EUPS (v2.0.1)... done.  setup: No module named utils  Installing Miniconda2 Python Distribution ...   newinstall.sh: line 277: eups: command not found  {code}    Clearly, a command failure which should have been fatal was ignored.  ",2,DM-5106,datamanagement,newinstall.sh fail eup command find ~jgates report following output run newinstall.sh el6 code java instal eups v2.0.1 setup module name util instal miniconda2 python distribution newinstall.sh line 277 eup command find code clearly command failure fatal ignore,"newinstall.sh fails with ""eups: command not found"" [~jgates] has reported the following output when running {{newinstall.sh}} on el6. {code:java} Installing EUPS (v2.0.1)... done. setup: No module named utils Installing Miniconda2 Python Distribution ... newinstall.sh: line 277: eups: command not found {code} Clearly, a command failure which should have been fatal was ignored."
"Fix effective coordinates for defects in obs_subaru The defects as defined in {{obs_subaru}} (in the {{hsc/defects/20NN-NN-NN/defects.dat}} files) are defined in a coordinate system where pixel (0, 0) is the lower left pixel.  However, the LSST stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics.  As such, the defect positions are being misinterpreted for the rotated CCDs in HSC (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]).  This needs to be remedied.",2,DM-5107,datamanagement,fix effective coordinate defect obs_subaru defect define obs_subaru hsc defects/20nn nn nn defects.dat file define coordinate system pixel low left pixel lsst stack use interpretation prefer maintain coordinate system tie electronic defect position misinterpret rotate ccds hsc hsc ccd layout|http://www.naoj.org observing instruments hsc ccdposition_20150804.png need remedie,"Fix effective coordinates for defects in obs_subaru The defects as defined in {{obs_subaru}} (in the {{hsc/defects/20NN-NN-NN/defects.dat}} files) are defined in a coordinate system where pixel (0, 0) is the lower left pixel. However, the LSST stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics. As such, the defect positions are being misinterpreted for the rotated CCDs in HSC (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]). This needs to be remedied."
"Offset in gaussian-psf in ci_hsc I'm seeing what looks like an aperture correction problem in psf-gaussian on {{ci_hsc}} coadds.  This gets in the way of our ability to do star/galaxy classification, and suggests potentially more serious problems elsewhere.  ",2,DM-5109,datamanagement,offset gaussian psf ci_hsc see look like aperture correction problem psf gaussian ci_hsc coadd get way ability star galaxy classification suggest potentially problem,"Offset in gaussian-psf in ci_hsc I'm seeing what looks like an aperture correction problem in psf-gaussian on {{ci_hsc}} coadds. This gets in the way of our ability to do star/galaxy classification, and suggests potentially more serious problems elsewhere."
"Add intelligence to `validate_drp` so it does ""A Reasonable Thing"" on an unknown output repo validate_drp current takes as input both a repository and a configuration file.  The configuration file contains information to construct the list of dataIds to analyze.    However, these dataIds could be extracted from the repo itself, in cases where the desired is to analyze the entire repo.      1.  Add a function that loads the set of dataIds from the repo. (/)  2.  Select reasonable defaults for the additional parameters specified in the config file. (/)  3.  Design how to handle multiple filters. (/)",5,DM-5120,datamanagement,add intelligence validate_drp reasonable thing unknown output repo validate_drp current take input repository configuration file configuration file contain information construct list analyze dataids extract repo case desire analyze entire repo add function load set dataids repo select reasonable default additional parameter specify config file design handle multiple filter,"Add intelligence to `validate_drp` so it does ""A Reasonable Thing"" on an unknown output repo validate_drp current takes as input both a repository and a configuration file. The configuration file contains information to construct the list of dataIds to analyze. However, these dataIds could be extracted from the repo itself, in cases where the desired is to analyze the entire repo. 1. Add a function that loads the set of dataIds from the repo. (/) 2. Select reasonable defaults for the additional parameters specified in the config file. (/) 3. Design how to handle multiple filters. (/)"
"Add multiple-filter capabilities to `validate_drp` Design and refactor `validate_drp` to produce results for multiple filters.    1. Decide on the syntax for the YAML configuration file that denotes the multiple filters.  E.g., which visit goes with what filter? (/)  2. Organize the running of multiple filters in `validate.run` to sequentially generate statistics and plots for each filter. (/)  3. Add a filter designation to the default output prefix. (/)    Note: matching objects *across* filters is out-of-scope for this ticket.",1,DM-5121,datamanagement,add multiple filter capability validate_drp design refactor validate_drp produce result multiple filter decide syntax yaml configuration file denote multiple filter e.g. visit go filter organize running multiple filter validate.run sequentially generate statistic plot filter add filter designation default output prefix note matching object filter scope ticket,"Add multiple-filter capabilities to `validate_drp` Design and refactor `validate_drp` to produce results for multiple filters. 1. Decide on the syntax for the YAML configuration file that denotes the multiple filters. E.g., which visit goes with what filter? (/) 2. Organize the running of multiple filters in `validate.run` to sequentially generate statistics and plots for each filter. (/) 3. Add a filter designation to the default output prefix. (/) Note: matching objects *across* filters is out-of-scope for this ticket."
"LOAD DATA LOCAL does not work with mariadb After we un-messed mariadb-mysqlclient we see errors now when trying to run integration tests:  {noformat}    File ""/usr/local/home/salnikov/dm-yyy/lib/python/lsst/qserv/wmgr/client.py"", line 683, in _request      raise ServerError(exc.response.status_code, exc.response.text)  ServerError: Server returned error: 500 (body: ""{""exception"": ""OperationalError"", ""message"": ""(_mysql_exceptions.OperationalError) (1148, 'The used command is not allowed with this MariaDB version') [SQL: 'LOAD DATA LOCAL INFILE %(file)s INTO TABLE qservTest_case01_mysql.LeapSeconds FIELDS TERMINATED BY %(delimiter)s ENCLOSED BY %(enclose)s                          ESCAPED BY %(escape)s LINES TERMINATED BY %(terminate)s'] [parameters: {'terminate': u'\\n', 'delimiter': u'\\t', 'enclose': u'', 'file': '/home/salnikov/qserv-run/2016_02/tmp/tmpWeAj6u/tabledata.dat', 'escape': u'\\\\'}]""}"")  2016-02-10 14:17:40,836 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : qserv-data-loader.py -v --config=/usr/local/home/salnikov/testdata-repo/datasets/case01/data/common.cfg --host=127.0.0.1 --port=5012 --secret=/home/salnikov/qserv-run/2016_02/etc/wmgr.secret --delete-tables --chunks-dir=/home/salnikov/qserv-run/2016_02/tmp/qserv_data_loader/LeapSeconds --no-css --skip-partition --one-table qservTest_case01_mysql LeapSeconds /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.schema /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.tsv.gz  {noformat}    It looks like mariadb client by default disables LOCAL option for data loading and it needs to be explicitly enabled.  ",1,DM-5122,datamanagement,"load datum local work mariadb un mess mariadb mysqlclient error try run integration test noformat file /usr local home salnikov dm yyy lib python lsst qserv wmgr client.py line 683 request raise servererror(exc.response.status_code exc.response.text servererror server return error 500 body exception operationalerror message mysql_exception operationalerror 1148 command allow mariadb version sql load datum local infile file)s table qservtest_case01_mysql leapseconds fields terminate delimiter)s enclosed enclose)s escaped escape)s lines terminate terminate)s parameter terminate u'\\n delimiter u'\\t enclose file /home salnikov qserv run/2016_02 tmp tmpweaj6u tabledata.dat escape u'\\\\ 2016 02 10 14:17:40,836 lsst.qserv.admin.common critical error code return command qserv-data-loader.py -v local home salnikov testdata repo dataset case01 data common.cfg --port=5012 --secret=/home salnikov qserv run/2016_02 etc wmgr.secret --delete table dir=/home salnikov qserv run/2016_02 tmp qserv_data_loader leapseconds --no css --skip partition --one table qservtest_case01_mysql leapseconds /usr local home salnikov testdata repo dataset case01 data leapseconds.schema /usr local home salnikov testdata repo dataset case01 data leapseconds.tsv.gz noformat look like mariadb client default disable local option datum loading need explicitly enable","LOAD DATA LOCAL does not work with mariadb After we un-messed mariadb-mysqlclient we see errors now when trying to run integration tests: {noformat} File ""/usr/local/home/salnikov/dm-yyy/lib/python/lsst/qserv/wmgr/client.py"", line 683, in _request raise ServerError(exc.response.status_code, exc.response.text) ServerError: Server returned error: 500 (body: ""{""exception"": ""OperationalError"", ""message"": ""(_mysql_exceptions.OperationalError) (1148, 'The used command is not allowed with this MariaDB version') [SQL: 'LOAD DATA LOCAL INFILE %(file)s INTO TABLE qservTest_case01_mysql.LeapSeconds FIELDS TERMINATED BY %(delimiter)s ENCLOSED BY %(enclose)s ESCAPED BY %(escape)s LINES TERMINATED BY %(terminate)s'] [parameters: {'terminate': u'\\n', 'delimiter': u'\\t', 'enclose': u'', 'file': '/home/salnikov/qserv-run/2016_02/tmp/tmpWeAj6u/tabledata.dat', 'escape': u'\\\\'}]""}"") 2016-02-10 14:17:40,836 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : qserv-data-loader.py -v --config=/usr/local/home/salnikov/testdata-repo/datasets/case01/data/common.cfg --host=127.0.0.1 --port=5012 --secret=/home/salnikov/qserv-run/2016_02/etc/wmgr.secret --delete-tables --chunks-dir=/home/salnikov/qserv-run/2016_02/tmp/qserv_data_loader/LeapSeconds --no-css --skip-partition --one-table qservTest_case01_mysql LeapSeconds /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.schema /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.tsv.gz {noformat} It looks like mariadb client by default disables LOCAL option for data loading and it needs to be explicitly enabled."
"Adapt all HSC calibration data to LSST camera geometry In the [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png], approximately half of the HSC CCDs are rotated 180 deg with respect to the others, two others have 90 deg rotations and another two have 270 deg rotations.  The HSC camera geometry defined a coordinate system where pixel (0, 0) is always the lower-left corner.  However, the new camera geometry in the LSST stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics.  As such, accommodations have had to be made for the rotated CCDs on {{obs_subaru}}.  See DM-4998 and DM-5107 in particular for details.  The need for these accommodations, and the accommodations themselves, should be removed.  This entails a re-ingestion of the HSC calibration data files (BIAS, DARK, FLAT, etc.) as well as a redefinition of the defects files in {{obs_subaru}}.",4,DM-5124,datamanagement,adapt hsc calibration datum lsst camera geometry hsc ccd layout|http://www.naoj.org observing instruments hsc ccdposition_20150804.png approximately half hsc ccds rotate 180 deg respect 90 deg rotation 270 deg rotation hsc camera geometry define coordinate system pixel lower leave corner new camera geometry lsst stack use interpretation prefer maintain coordinate system tie electronic accommodation rotate ccds obs_subaru dm-4998 dm-5107 particular detail need accommodation accommodation remove entail ingestion hsc calibration datum file bias dark flat etc redefinition defect file obs_subaru,"Adapt all HSC calibration data to LSST camera geometry In the [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png], approximately half of the HSC CCDs are rotated 180 deg with respect to the others, two others have 90 deg rotations and another two have 270 deg rotations. The HSC camera geometry defined a coordinate system where pixel (0, 0) is always the lower-left corner. However, the new camera geometry in the LSST stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics. As such, accommodations have had to be made for the rotated CCDs on {{obs_subaru}}. See DM-4998 and DM-5107 in particular for details. The need for these accommodations, and the accommodations themselves, should be removed. This entails a re-ingestion of the HSC calibration data files (BIAS, DARK, FLAT, etc.) as well as a redefinition of the defects files in {{obs_subaru}}."
"qserv fails when it mixes mariadb and mariadbclient directories When I tried to run qserv-configure after installing qserv 2016_01-7-gbd0349f I got this error:  {noformat}  2016-02-10 16:03:16,915 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh  {noformat}    Running script configure/mysql.sh:  {noformat}  $ sh -x /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh    + echo '-- Installing mysql database files.'  -- Installing mysql database files.  + /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov  + echo 'ERROR : mysql_install_db failed, exiting'  ERROR : mysql_install_db failed, exiting  + exit 1  {noformat}    and     {noformat}  $ /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov    FATAL ERROR: Could not find mysqld    The following directories were searched:        /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/libexec      /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/sbin      /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/bin  {noformat}    So it looks for mysqld in mariadbclient, the same directory as mysql_install_db script, mysql_install_db should be actually running from mariadb.  ",1,DM-5125,datamanagement,"qserv fail mix mariadb mariadbclient directory try run qserv configure instal qserv 2016_01 gbd0349f get error noformat 2016 02 10 16:03:16,915 lsst.qserv.admin.common critical error code return command /home salnikov qserv run/2016_02 tmp configure mysql.sh noformat running script configure mysql.sh noformat sh -x /home salnikov qserv run/2016_02 tmp configure mysql.sh echo instal mysql database file instal mysql database file /u2 salnikov stack linux64 mariadbclient/10.1.11 script mysql_install_db --basedir=/u2 salnikov stack linux64 mariadbclient/10.1.11 --defaults file=/home salnikov qserv run/2016_02 etc my.cnf salnikov echo error mysql_install_db fail exit error mysql_install_db fail exit exit noformat noformat /u2 salnikov stack linux64 mariadbclient/10.1.11 script mysql_install_db --basedir=/u2 salnikov stack linux64 mariadbclient/10.1.11 --defaults file=/home salnikov qserv run/2016_02 etc my.cnf salnikov fatal error find mysqld follow directory search /u2 salnikov stack linux64 mariadbclient/10.1.11 libexec salnikov stack linux64 mariadbclient/10.1.11 sbin salnikov stack linux64 mariadbclient/10.1.11 bin noformat look mysqld mariadbclient directory mysql_install_db script mysql_install_db actually run mariadb","qserv fails when it mixes mariadb and mariadbclient directories When I tried to run qserv-configure after installing qserv 2016_01-7-gbd0349f I got this error: {noformat} 2016-02-10 16:03:16,915 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh {noformat} Running script configure/mysql.sh: {noformat} $ sh -x /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh + echo '-- Installing mysql database files.' -- Installing mysql database files. + /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov + echo 'ERROR : mysql_install_db failed, exiting' ERROR : mysql_install_db failed, exiting + exit 1 {noformat} and {noformat} $ /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov FATAL ERROR: Could not find mysqld The following directories were searched: /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/libexec /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/sbin /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/bin {noformat} So it looks for mysqld in mariadbclient, the same directory as mysql_install_db script, mysql_install_db should be actually running from mariadb."
"Cost adding the support for Object / DiaObject joins in Qserv Per RFC-133, we should support Object / DiaObject joins. That requires changes to query analyzer (and possibly elsewhere), currently we only support self-joins on objectId for director table. We'd need to either make DiaObject a director table and allow director-director joins, or allow director-child joins. This story involves costing how much effort it will be to implement it (and making a straw-man proposal how to implement it)",2,DM-5128,datamanagement,cost add support object diaobject join qserv rfc-133 support object diaobject join require change query analyzer possibly currently support self join objectid director table need diaobject director table allow director director join allow director child join story involve cost effort implement make straw man proposal implement,"Cost adding the support for Object / DiaObject joins in Qserv Per RFC-133, we should support Object / DiaObject joins. That requires changes to query analyzer (and possibly elsewhere), currently we only support self-joins on objectId for director table. We'd need to either make DiaObject a director table and allow director-director joins, or allow director-child joins. This story involves costing how much effort it will be to implement it (and making a straw-man proposal how to implement it)"
"Create InputField for generic use cases. Create a composable, validating InputField so it can use outside of the form/submit use-case.",2,DM-5129,datamanagement,create inputfield generic use case create composable validate inputfield use outside form submit use case,"Create InputField for generic use cases. Create a composable, validating InputField so it can use outside of the form/submit use-case."
"B-F correction breaks non-HSC custom ISR, ci_hsc The addition of brighter-fatter correction on DM-4837 breaks obs_cfht's custom ISR, since it slightly changes an internal ISR API by addding an argument that isn't expected by the obs_cfht version.  It also breaks ci_hsc, since the B-F kernel file isn't included in the calibrations packaged there.  ",1,DM-5130,datamanagement,correction break non hsc custom isr ci_hsc addition bright fatter correction dm-4837 break obs_cfht custom isr slightly change internal isr api addde argument expect obs_cfht version break ci_hsc kernel file include calibration package,"B-F correction breaks non-HSC custom ISR, ci_hsc The addition of brighter-fatter correction on DM-4837 breaks obs_cfht's custom ISR, since it slightly changes an internal ISR API by addding an argument that isn't expected by the obs_cfht version. It also breaks ci_hsc, since the B-F kernel file isn't included in the calibrations packaged there."
"obs_subaru install with eups distrib fails Thus:  {code}  $ eups distrib install -t w_2016_06 obs_subaru  ...    [ 52/52 ]  obs_subaru 5.0.0.1-60-ge4efae7+2 ...    ***** error: from /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/build.log:  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/hscRepository.py"", line 91, in setUp      self.repoPath = createDataRepository(""lsst.obs.hsc.HscMapper"", rawPath)    File ""tests/hscRepository.py"", line 63, in createDataRepository      check_call([ingest_cmd, repoPath] + glob(os.path.join(inputPath, ""*.fits.gz"")))    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 540, in check_call      raise CalledProcessError(retcode, cmd)  CalledProcessError: Command '['/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/bin/hscIngestImages.py', '/var/folders/jp/lqz3n0m17nqft7bwtw3b8n380000gp/T/tmptUSKuf', '/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/testdata_subaru/master-gf9ba9abdbe/hsc/raw/HSCA90402512.fits.gz']' returned non-zero exit status 1    ----------------------------------------------------------------------  Ran 8 tests in 9.928s    FAILED (errors=7)  The following tests failed:  /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/tests/.tests/hscRepository.py.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  + exit -4  {code}  Please fix it.",1,DM-5132,datamanagement,"obs_subaru install eup distrib fail code eup distrib install w_2016_06 obs_subaru 52/52 obs_subaru 5.0.0.1 60 ge4efae7 error /users jds projects astronomy lsst stack eupsbuilddir darwinx86 obs_subaru-5.0.0.1 60 ge4efae7 build.log traceback recent file test hscrepository.py line 91 setup self.repopath createdatarepository(""lsst.obs.hsc hscmapper rawpath file test hscrepository.py line 63 createdatarepository check_call([ingest_cmd repopath glob(os.path.join(inputpath .fits.gz file /opt local library frameworks python.framework versions/2.7 lib python2.7 subprocess.py line 540 check_call raise calledprocesserror(retcode cmd calledprocesserror command /users jds projects astronomy lsst stack eupsbuilddir darwinx86 obs_subaru-5.0.0.1 60 ge4efae7 obs_subaru-5.0.0.1 60 ge4efae7 bin hscingestimages.py /var folder jp lqz3n0m17nqft7bwtw3b8n380000gp tmptuskuf /users jds projects astronomy lsst stack darwinx86 testdata_subaru master gf9ba9abdbe hsc raw hsca90402512.fits.gz return non zero exit status ---------------------------------------------------------------------- ran test 9.928s failed errors=7 follow test fail /users jds projects astronomy lsst stack eupsbuilddir darwinx86 obs_subaru-5.0.0.1 60 ge4efae7 obs_subaru-5.0.0.1 60 ge4efae7 tests/.test hscrepository.py.failed test fail scon checkteststatus error scon building terminate error exit code fix","obs_subaru install with eups distrib fails Thus: {code} $ eups distrib install -t w_2016_06 obs_subaru ... [ 52/52 ] obs_subaru 5.0.0.1-60-ge4efae7+2 ... ***** error: from /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/build.log: ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/hscRepository.py"", line 91, in setUp self.repoPath = createDataRepository(""lsst.obs.hsc.HscMapper"", rawPath) File ""tests/hscRepository.py"", line 63, in createDataRepository check_call([ingest_cmd, repoPath] + glob(os.path.join(inputPath, ""*.fits.gz""))) File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 540, in check_call raise CalledProcessError(retcode, cmd) CalledProcessError: Command '['/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/bin/hscIngestImages.py', '/var/folders/jp/lqz3n0m17nqft7bwtw3b8n380000gp/T/tmptUSKuf', '/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/testdata_subaru/master-gf9ba9abdbe/hsc/raw/HSCA90402512.fits.gz']' returned non-zero exit status 1 ---------------------------------------------------------------------- Ran 8 tests in 9.928s FAILED (errors=7) The following tests failed: /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/tests/.tests/hscRepository.py.failed 1 tests failed scons: *** [checkTestStatus] Error 1 scons: building terminated because of errors. + exit -4 {code} Please fix it."
"Make ci_hsc buildable by Jenkins 1. Make sure {{ci_hsc}} is buildable by {{lsstsw}} / {{lsst_build}}  (/)  2. Add {{ci_hsc}} to lsstsw/etc/repos.yaml so that one can request that Jenkins builds it.  (/)  3. Verify that the test in {{ci_hsc}} fails on known broken tags and passes on known successful tags. (/)    No dependencies will be added to {{lsst_sims}} or {{lsst_distib}}.  This is meant to provide the ability to request that Jenkins do these builds and to fail if something has broken them.    This will later be expanded to new packages {{ci_cfht}}, {{ci_decam}}, and {{ci_sim}}.    The key goal is to make sure one hasn't broken obs_ packages in their butler interface or in their processCcd    Additional Notes and Thoughts from HipChat Discussion  [~ktl]  Sounds good to me; we might have an ""lsst_ci"" top-level metapackage depending on all of them which is what Jenkins would run regularly.     If the goal is to test obs_ packages, then my first instinct would be to put that in the obs_ package.  Longer term goal to test the stack with different precursor datasets.  If this is testing obs_ packages on a slower cadence than the built-in tests, it's OK for that to be a separate package.    [~jbosch]  Eventually, I think we need to run a CI dataset for each camera, then run some camera generic tests on each of those, then run some camera-specific tests on each of those.  So we don't want to go too far down a road in which all tests are camera-specific, but maybe we don't have a choice until we have some better unifying framework for them.  I've certainly been putting some checks in {{ci_hsc}} that would be valid for all other cameras, if we had a CI package for them that went through to coadd processing.",2,DM-5135,datamanagement,ci_hsc buildable jenkins sure ci_hsc buildable lsstsw lsst_build add ci_hsc lsstsw etc repos.yaml request jenkins build verify test ci_hsc fail know broken tag pass know successful tag dependency add lsst_sims lsst_distib mean provide ability request jenkins build fail break later expand new package ci_cfht ci_decam ci_sim key goal sure break obs package butler interface processccd additional note thoughts hipchat discussion ~ktl sound good lsst_ci level metapackage depend jenkins run regularly goal test obs package instinct obs package longer term goal test stack different precursor dataset test ob package slow cadence build test ok separate package ~jbosch eventually think need run ci dataset camera run camera generic test run camera specific test want far road test camera specific maybe choice well unifying framework certainly put check ci_hsc valid camera ci package go coadd processing,"Make ci_hsc buildable by Jenkins 1. Make sure {{ci_hsc}} is buildable by {{lsstsw}} / {{lsst_build}} (/) 2. Add {{ci_hsc}} to lsstsw/etc/repos.yaml so that one can request that Jenkins builds it. (/) 3. Verify that the test in {{ci_hsc}} fails on known broken tags and passes on known successful tags. (/) No dependencies will be added to {{lsst_sims}} or {{lsst_distib}}. This is meant to provide the ability to request that Jenkins do these builds and to fail if something has broken them. This will later be expanded to new packages {{ci_cfht}}, {{ci_decam}}, and {{ci_sim}}. The key goal is to make sure one hasn't broken obs_ packages in their butler interface or in their processCcd Additional Notes and Thoughts from HipChat Discussion [~ktl] Sounds good to me; we might have an ""lsst_ci"" top-level metapackage depending on all of them which is what Jenkins would run regularly. If the goal is to test obs_ packages, then my first instinct would be to put that in the obs_ package. Longer term goal to test the stack with different precursor datasets. If this is testing obs_ packages on a slower cadence than the built-in tests, it's OK for that to be a separate package. [~jbosch] Eventually, I think we need to run a CI dataset for each camera, then run some camera generic tests on each of those, then run some camera-specific tests on each of those. So we don't want to go too far down a road in which all tests are camera-specific, but maybe we don't have a choice until we have some better unifying framework for them. I've certainly been putting some checks in {{ci_hsc}} that would be valid for all other cameras, if we had a CI package for them that went through to coadd processing."
"Increase key_buffer_size I just looked at my qserv-run/etc/my.cnf and I don't see us setting key_buffer_size there. Looking at mysqld run as part of qserv I can see it is set to 128 MB. That is pretty low given we are planning to do lots of joins. Please add an entry in my.cnf that sets it to something higher with a comment that ""~20% of available RAM is recommended"".",1,DM-5136,datamanagement,increase key_buffer_size look qserv run etc my.cnf set key_buffer_size look mysqld run qserv set 128 mb pretty low give plan lot join add entry my.cnf set high comment ~20 available ram recommend,"Increase key_buffer_size I just looked at my qserv-run/etc/my.cnf and I don't see us setting key_buffer_size there. Looking at mysqld run as part of qserv I can see it is set to 128 MB. That is pretty low given we are planning to do lots of joins. Please add an entry in my.cnf that sets it to something higher with a comment that ""~20% of available RAM is recommended""."
on-going support to Camera team in visualization (Feb. 2016)  Attend the weekly meeting and answer questions as needed.  Help with the Python and JS debug ,4,DM-5137,datamanagement,go support camera team visualization feb. 2016 attend weekly meeting answer question need help python js debug,on-going support to Camera team in visualization (Feb. 2016) Attend the weekly meeting and answer questions as needed. Help with the Python and JS debug
Update apr and apr_util {{apr}} and {{apr-util}} are outdated and lagging behind the versions on RHEL6. They should be updated as agreed in RFC-76.,1,DM-5139,datamanagement,update apr apr_util apr apr util outdate lag version rhel6 update agree rfc-76,Update apr and apr_util {{apr}} and {{apr-util}} are outdated and lagging behind the versions on RHEL6. They should be updated as agreed in RFC-76.
"Move luaxmlrpc to lsst-dm/legacy- We no longer need luaxmlrpc because we run czar inside proxy. We should move it to lsst-dm/legacy-, and remove mentioning it in readme.",1,DM-5140,datamanagement,luaxmlrpc lsst dm legacy- long need luaxmlrpc run czar inside proxy lsst dm legacy- remove mention readme,"Move luaxmlrpc to lsst-dm/legacy- We no longer need luaxmlrpc because we run czar inside proxy. We should move it to lsst-dm/legacy-, and remove mentioning it in readme."
"DM Power Requirements Justification The power requirements for the base site appeared to have increased greatly since LSE-239 or LDM-144 v140. Significant effort was spent digging through LDM-144 for precise rack counts, rack weights, rack power. Further time was spent on the analysis of why the power requirement is greater then expected. This involved analyzing swing space power requirements, max swing space needed, investigation into what LSE-239 refers to 'expansion' (turns out to be alert processing), attributing alert processing power requirements to the base (LDM-144 contributes to archive site but contingency is still in place for base site operations), comparing peak and steady state power needs. Also discussions around reinforcing the floors for greater rack weights.  ",6,DM-5142,datamanagement,dm power requirements justification power requirement base site appear increase greatly lse-239 ldm-144 v140 significant effort spend dig ldm-144 precise rack count rack weight rack power time spend analysis power requirement great expect involve analyze swing space power requirement max swing space need investigation lse-239 refer expansion turn alert processing attribute alert processing power requirement base ldm-144 contribute archive site contingency place base site operation compare peak steady state power need discussion reinforce floor great rack weight,"DM Power Requirements Justification The power requirements for the base site appeared to have increased greatly since LSE-239 or LDM-144 v140. Significant effort was spent digging through LDM-144 for precise rack counts, rack weights, rack power. Further time was spent on the analysis of why the power requirement is greater then expected. This involved analyzing swing space power requirements, max swing space needed, investigation into what LSE-239 refers to 'expansion' (turns out to be alert processing), attributing alert processing power requirements to the base (LDM-144 contributes to archive site but contingency is still in place for base site operations), comparing peak and steady state power needs. Also discussions around reinforcing the floors for greater rack weights."
"Jason Feb Tasks Weeks 1&2 - Interviews, Team mtgs, uptime institute tier discussions: 1.5 pts  Weeks 3&4 - Team mtgs, ICI meetings, set/prioritize IT goals 4 pts",6,DM-5143,datamanagement,jason feb tasks weeks 1&2 interviews team mtgs uptime institute tier discussion 1.5 pt week 3&4 team mtgs ici meeting set prioritize goal pt,"Jason Feb Tasks Weeks 1&2 - Interviews, Team mtgs, uptime institute tier discussions: 1.5 pts Weeks 3&4 - Team mtgs, ICI meetings, set/prioritize IT goals 4 pts"
"Jason Feb Educational Activities Learning DM stack deployment and layout, reading on redesign of butler 1.5",2,DM-5144,datamanagement,jason feb educational activities learn dm stack deployment layout read redesign butler 1.5,"Jason Feb Educational Activities Learning DM stack deployment and layout, reading on redesign of butler 1.5"
"Provide usable repos in {{validation_data_*}} packages. Re-interpreted ticket:  1. Provide already-initialized repositories in the `validation_data_cfht`, `validation_data_decam`, and `validation_data_hsc` packages alongside the raw data.  The goal is to allow both easy quick-start analyses as well as comparisons of output steps from processCcd.py and friends at each step of the processing. (/)  2. Add (Cfht,Decam,HSC).list files to provide for easy processing of the available dataIds in the example data. (/)  3. Update README files to explain available data.  (/)    [Original request:]  In validation_drp when I run examples/runXTest.sh I find that any data I had saved in CFHT or DECam is lost, even if I have carefully renamed it. This is very dangerous and I lost a lot of work due to it. At a bare minimum please do NOT touch any directories not named ""input"" or ""output"".    Lower priority requests that I hope you will consider:  - Have the the input repo be entirely contained in the validation_data_X packages, ready to use ""as is"". That would simplify the use of those packages by other code. It would also simplify validate_drp, and it would just leave the output repo to generate (which already has a link back to the input repo).  - Have runXTest.sh accept a single argument: the path to the output. (The path to the input is not necessary if you implement the first suggestion).",3,DM-5147,datamanagement,provide usable repos validation_data package interpret ticket provide initialize repository validation_data_cfht validation_data_decam validation_data_hsc package alongside raw data goal allow easy quick start analysis comparison output step processccd.py friend step processing add cfht decam hsc).list file provide easy processing available dataids example data update readme file explain available datum original request validation_drp run example runxtest.sh find datum save cfht decam lose carefully rename dangerous lose lot work bare minimum touch directory name input output low priority request hope consider input repo entirely contain validation_data_x package ready use simplify use package code simplify validate_drp leave output repo generate link input repo runxtest.sh accept single argument path output path input necessary implement suggestion,"Provide usable repos in {{validation_data_*}} packages. Re-interpreted ticket: 1. Provide already-initialized repositories in the `validation_data_cfht`, `validation_data_decam`, and `validation_data_hsc` packages alongside the raw data. The goal is to allow both easy quick-start analyses as well as comparisons of output steps from processCcd.py and friends at each step of the processing. (/) 2. Add (Cfht,Decam,HSC).list files to provide for easy processing of the available dataIds in the example data. (/) 3. Update README files to explain available data. (/) [Original request:] In validation_drp when I run examples/runXTest.sh I find that any data I had saved in CFHT or DECam is lost, even if I have carefully renamed it. This is very dangerous and I lost a lot of work due to it. At a bare minimum please do NOT touch any directories not named ""input"" or ""output"". Lower priority requests that I hope you will consider: - Have the the input repo be entirely contained in the validation_data_X packages, ready to use ""as is"". That would simplify the use of those packages by other code. It would also simplify validate_drp, and it would just leave the output repo to generate (which already has a link back to the input repo). - Have runXTest.sh accept a single argument: the path to the output. (The path to the input is not necessary if you implement the first suggestion)."
"IN2P3 cluster worker nodes failed to start due to Innodb error Next error happens when starting mariadb on worker (with existing data from 35TB dataset, which were generated by mysql):  {code:bash}  2016-02-13 22:02:36 139632684558144 [Note] InnoDB: Completed initialization of buffer pool  2016-02-13 22:02:36 139632684558144 [ERROR] InnoDB: auto-extending data file ./ibdata1 is of a different size 640 pages (rounded down to MB) than specified in the .cnf file: initial 768 pages, max 0 (relevant if non-zero) pages!  2016-02-13 22:02:36 139632684558144 [ERROR] InnoDB: Could not open or create the system tablespace. If you tried to add new data files to the system tablespace, and it failed here, you should now edit innodb_data_file_path in my.cnf back to what it was, and remove the new ibdata files InnoDB created in this failed attempt. InnoDB only wrote those files full of zeros, but did not yet use them in any way. But be careful: do not remove old data files which contain your precious data!  2016-02-13 22:02:36 139632684558144 [ERROR] Plugin 'InnoDB' init function returned error.  2016-02-13 22:02:36 139632684558144 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.  2016-02-13 22:02:36 139632684558144 [Note] Plugin 'FEEDBACK' is disabled.  2016-02-13 22:02:36 139632684558144 [ERROR] Unknown/unsupported storage engine: InnoDB  2016-02-13 22:02:36 139632684558144 [ERROR] Aborting  {code}",4,DM-5148,datamanagement,in2p3 cluster worker nodes fail start innodb error error happen start mariadb worker exist datum 35 tb dataset generate mysql code bash 2016 02 13 22:02:36 139632684558144 note innodb complete initialization buffer pool 2016 02 13 22:02:36 139632684558144 error innodb auto extend datum file ./ibdata1 different size 640 page round mb specify .cnf file initial 768 page max relevant non zero page 2016 02 13 22:02:36 139632684558144 error innodb open create system tablespace try add new datum file system tablespace fail edit innodb_data_file_path my.cnf remove new ibdata file innodb create fail attempt innodb write file zero use way careful remove old datum file contain precious datum 2016 02 13 22:02:36 139632684558144 error plugin innodb init function return error 2016 02 13 22:02:36 139632684558144 error plugin innodb registration storage engine fail 2016 02 13 22:02:36 139632684558144 note plugin feedback disable 2016 02 13 22:02:36 139632684558144 error unknown unsupported storage engine innodb 2016 02 13 22:02:36 139632684558144 error aborting code,"IN2P3 cluster worker nodes failed to start due to Innodb error Next error happens when starting mariadb on worker (with existing data from 35TB dataset, which were generated by mysql): {code:bash} 2016-02-13 22:02:36 139632684558144 [Note] InnoDB: Completed initialization of buffer pool 2016-02-13 22:02:36 139632684558144 [ERROR] InnoDB: auto-extending data file ./ibdata1 is of a different size 640 pages (rounded down to MB) than specified in the .cnf file: initial 768 pages, max 0 (relevant if non-zero) pages! 2016-02-13 22:02:36 139632684558144 [ERROR] InnoDB: Could not open or create the system tablespace. If you tried to add new data files to the system tablespace, and it failed here, you should now edit innodb_data_file_path in my.cnf back to what it was, and remove the new ibdata files InnoDB created in this failed attempt. InnoDB only wrote those files full of zeros, but did not yet use them in any way. But be careful: do not remove old data files which contain your precious data! 2016-02-13 22:02:36 139632684558144 [ERROR] Plugin 'InnoDB' init function returned error. 2016-02-13 22:02:36 139632684558144 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed. 2016-02-13 22:02:36 139632684558144 [Note] Plugin 'FEEDBACK' is disabled. 2016-02-13 22:02:36 139632684558144 [ERROR] Unknown/unsupported storage engine: InnoDB 2016-02-13 22:02:36 139632684558144 [ERROR] Aborting {code}"
Create an easy place to add tests to ci_hsc Create a single file where tests for validating source can be added. The tests will be duck typed to a class method and be registered to the corresponding validation class with a decorator.,1,DM-5150,datamanagement,create easy place add test ci_hsc create single file test validate source add test duck type class method register corresponding validation class decorator,Create an easy place to add tests to ci_hsc Create a single file where tests for validating source can be added. The tests will be duck typed to a class method and be registered to the corresponding validation class with a decorator.
Process a tiny set of raw DECam Stripe 82 data Process some DECam data to gain familiarity with process execution and learn to debug issues,8,DM-5153,datamanagement,process tiny set raw decam stripe 82 datum process decam datum gain familiarity process execution learn debug issue,Process a tiny set of raw DECam Stripe 82 data Process some DECam data to gain familiarity with process execution and learn to debug issues
"Please document MemoryTestCase {{lsst.utils.tests.MemoryTestCase}} is used extensively throughout our test suite, but it is lacking in documentation and it's not clear under what circumstances its use is required or encouraged. Please add appropriate documentation to the [Software Unit Test Policy |http://developer.lsst.io/en/latest/coding/unit_test_policy.html].    See also [this thread on clo|https://community.lsst.org/t/what-is-the-policy-for-using-lsst-utils-tests-memorytestcase].",1,DM-5156,datamanagement,document memorytestcase lsst.utils.test memorytestcase extensively test suite lack documentation clear circumstance use require encourage add appropriate documentation software unit test policy |http://developer.lsst.io en late code unit_test_policy.html thread clo|https://community.lsst.org policy lsst util test memorytestcase,"Please document MemoryTestCase {{lsst.utils.tests.MemoryTestCase}} is used extensively throughout our test suite, but it is lacking in documentation and it's not clear under what circumstances its use is required or encouraged. Please add appropriate documentation to the [Software Unit Test Policy |http://developer.lsst.io/en/latest/coding/unit_test_policy.html]. See also [this thread on clo|https://community.lsst.org/t/what-is-the-policy-for-using-lsst-utils-tests-memorytestcase]."
"Record CCD, visit of input catalog in `validate_drp` 1. Record the CCD and `visit` of the individual source in the catalog so that it is available for later analysis.  3. Update `analyzeData` to use these newly available CCD and `visit` information in the catalog.  ",1,DM-5160,datamanagement,record ccd visit input catalog validate_drp record ccd visit individual source catalog available later analysis update analyzedata use newly available ccd visit information catalog,"Record CCD, visit of input catalog in `validate_drp` 1. Record the CCD and `visit` of the individual source in the catalog so that it is available for later analysis. 3. Update `analyzeData` to use these newly available CCD and `visit` information in the catalog."
"HSC backport: Support a full background model when detecting cosmic rays This is a port of the following two standalone HSC commits:    [Support a full background model when detecting cosmic rays|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/3bae328e0fff4b2a02267e97cc1e53b5bbe431cb]  {code}  If there are strong gradients (e.g. M31's nucleus) we need to do more than  treat the background as a constant.  However, this requires making a copy  of the data so the background-is-a-constant model is preserved as a special  case  {code}  [Fixed cosmicRay() in RepairTask for the case background is subtracted.|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/2cdb7c606270d84c7a05baf9949ff5724463fa6b]  {code}  When the background is subtracted with finer binsize, new exposure  will be created and cosmic rays will be detected on that exposure.  But the image of that exposure was not properly returned back.  {code}  ",1,DM-5161,datamanagement,hsc backport support background model detect cosmic ray port follow standalone hsc commit support background model detect cosmic rays|https://github.com hypersuprime cam pipe_tasks commit/3bae328e0fff4b2a02267e97cc1e53b5bbe431cb code strong gradient e.g. m31 nucleus need treat background constant require make copy datum background constant model preserve special case code fixed cosmicray repairtask case background subtracted.|https://github.com hypersuprime cam pipe_tasks commit/2cdb7c606270d84c7a05baf9949ff5724463fa6b code background subtract finer binsize new exposure create cosmic ray detect exposure image exposure properly return code,"HSC backport: Support a full background model when detecting cosmic rays This is a port of the following two standalone HSC commits: [Support a full background model when detecting cosmic rays|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/3bae328e0fff4b2a02267e97cc1e53b5bbe431cb] {code} If there are strong gradients (e.g. M31's nucleus) we need to do more than treat the background as a constant. However, this requires making a copy of the data so the background-is-a-constant model is preserved as a special case {code} [Fixed cosmicRay() in RepairTask for the case background is subtracted.|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/2cdb7c606270d84c7a05baf9949ff5724463fa6b] {code} When the background is subtracted with finer binsize, new exposure will be created and cosmic rays will be detected on that exposure. But the image of that exposure was not properly returned back. {code}"
"Audit the LSST and HSC codebases for differences We've already merged a lot of code from HSC to LSST, and are optimistic that we've captured most of the big ticket items. However, we need to perform a thorough comparison of the codebases to check there's nothing we're missing. Please do that, and file tickets in the DM-3560 and DM-3568 epics to describe outstanding work.",4,DM-5162,datamanagement,audit lsst hsc codebase difference merge lot code hsc lsst optimistic capture big ticket item need perform thorough comparison codebase check miss file ticket dm-3560 dm-3568 epic describe outstanding work,"Audit the LSST and HSC codebases for differences We've already merged a lot of code from HSC to LSST, and are optimistic that we've captured most of the big ticket items. However, we need to perform a thorough comparison of the codebases to check there's nothing we're missing. Please do that, and file tickets in the DM-3560 and DM-3568 epics to describe outstanding work."
Modify System layout to support expanded views Each of the visualizers needs to expand to full screen.  We need to modify our current layout system so each and expand and collapse so that the old view is restored. The system needs to be flexible enough so an 'expanded version' of the component can be used.,4,DM-5163,datamanagement,modify system layout support expand view visualizer need expand screen need modify current layout system expand collapse old view restore system need flexible expand version component,Modify System layout to support expanded views Each of the visualizers needs to expand to full screen. We need to modify our current layout system so each and expand and collapse so that the old view is restored. The system needs to be flexible enough so an 'expanded version' of the component can be used.
Tests in daf_persistence should skip properly Some of the tests in {{daf_persistence}} have a couple of problems that cause difficulties with modern test frameworks:  # unittest is not being used at all in some cases  # Skipping is done with a print and a {{sys.exit}}    They need to be modernized.,2,DM-5164,datamanagement,test daf_persistence skip properly test daf_persistence couple problem cause difficulty modern test framework unitt case skipping print sys.exit need modernize,Tests in daf_persistence should skip properly Some of the tests in {{daf_persistence}} have a couple of problems that cause difficulties with modern test frameworks: # unittest is not being used at all in some cases # Skipping is done with a print and a {{sys.exit}} They need to be modernized.
Analyze catalog-comparison CmdLineTasks Analyze the QA CmdLineTask collection being generated by [~lauren] sufficiently well to determine the interface requirements needed to represent them as Supertasks.    Does not include actually designing that interface.,6,DM-5166,datamanagement,analyze catalog comparison cmdlinetask analyze qa cmdlinetask collection generate ~lauren sufficiently determine interface requirement need represent supertasks include actually design interface,Analyze catalog-comparison CmdLineTasks Analyze the QA CmdLineTask collection being generated by [~lauren] sufficiently well to determine the interface requirements needed to represent them as Supertasks. Does not include actually designing that interface.
Standup Fastly infrastructure for LSST the Docs LSST the Docs will use Fastly to serve docs out of an S3 bucket with well-formatted URLs thanks to routing at the Varnish layer. See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for an overview of the desired setup and http://sqr-006.lsst.io for an overview of LSST the Docs. Specific outcomes are:    * Create S3 bucket for LTD  * Create Fastly account (may be a demo account pending negotiations with Fastly)  * Basic configurations for Fastly account  * Research pricing/configure a TLS certificate for *.lsst.io domains  * Set up base VCL configuration on Fastly.  ,3,DM-5167,datamanagement,standup fastly infrastructure lsst docs lsst docs use fastly serve doc s3 bucket format url thank route varnish layer https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html overview desire setup http://sqr-006.lsst.io overview lsst docs specific outcome create s3 bucket ltd create fastly account demo account pende negotiation fastly basic configuration fastly account research pricing configure tls certificate .lsst.io domain set base vcl configuration fastly,Standup Fastly infrastructure for LSST the Docs LSST the Docs will use Fastly to serve docs out of an S3 bucket with well-formatted URLs thanks to routing at the Varnish layer. See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for an overview of the desired setup and http://sqr-006.lsst.io for an overview of LSST the Docs. Specific outcomes are: * Create S3 bucket for LTD * Create Fastly account (may be a demo account pending negotiations with Fastly) * Basic configurations for Fastly account * Research pricing/configure a TLS certificate for *.lsst.io domains * Set up base VCL configuration on Fastly.
"Fastly API interactions for LSST the Docs Using Fastly’s API, have ltd-keeper setup new builds and editions:    - Add {{Surrogate-Key}} to headers of objects uploaded to S3 (happens on ltd-mason side)  - Configure Varnish to serve specific bucket directories as specific domains (DM-4951 has added Route 53 interactions to ltd-keeper)  - Purge content when editions switch or content is deleted.    DM-5167 is covering non-API driven work to configure fastly.    See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for a write-up on serving static site via fastly. See also http://sqr-006.lsst.io for an overview of LSST the Docs.",8,DM-5169,datamanagement,fastly api interaction lsst docs fastly api ltd keeper setup new build edition add surrogate key header object upload s3 happen ltd mason configure varnish serve specific bucket directory specific domain dm-4951 add route 53 interaction ltd keeper purge content edition switch content delete dm-5167 cover non api drive work configure fastly https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html write serve static site fastly http://sqr-006.lsst.io overview lsst docs,"Fastly API interactions for LSST the Docs Using Fastly s API, have ltd-keeper setup new builds and editions: - Add {{Surrogate-Key}} to headers of objects uploaded to S3 (happens on ltd-mason side) - Configure Varnish to serve specific bucket directories as specific domains (DM-4951 has added Route 53 interactions to ltd-keeper) - Purge content when editions switch or content is deleted. DM-5167 is covering non-API driven work to configure fastly. See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for a write-up on serving static site via fastly. See also http://sqr-006.lsst.io for an overview of LSST the Docs."
"Manipulating masks is confusing A possible bug exists in afwImage.Exposure.getMaskedImage(). This function returns a copy of the Exposure's masked image, and not the actual maskedImage owned by the Exposure. This means that any changes made to the mask are done only on the copy, and are not reflected in the Exposure's maskImage. The intended behavior seems to be that a shallow copy be returned with pointers to all the original objects (such as the mask). This however does not seem to be the case, a deep copy is always made instead. Verify that the intended behavior is indeed happening. Steps to reproduce    {code:python}  coaddExposure = afwImage.ExposureF()  coaddExposure.getMaskedImage().getMask().addMaskPlane('TEST')  print(coaddExposure.getMaskedImage().getMask().getMaskPlaneDict().asdict())  m = coaddExposure.getMaskedImage().getMask()  print(m.getMaskPlaneDict().asdict())  m.removeAndClearMaskPlane('TEST')  print(m.getMaskPlaneDict().asdict())  print(coaddExposure.getMaskedImage().getMask().getMaskPlaneDict().asdict())  {code}     A second concern, though not necessarily a bug, is that adding and removing mask planes is confusing due to inconsistent manipulation of global state. For example:  {code}  In [1]: import lsst.afw.image as afwImage    # Create two separate Masks  In [2]: mask1 = afwImage.MaskU()  In [3]: mask2 = afwImage.MaskU()    # Neither Mask contains a ""TEST"" plane  In [4]: 'TEST' in mask1.getMaskPlaneDict()  Out[4]: False  In [5]: 'TEST' in mask2.getMaskPlaneDict()  Out[5]: False    # Adding a plane to one updates a shared list of planes, so it appears in the other  In [6]: mask1.addMaskPlane('TEST')  Out[6]: 9  In [7]: 'TEST' in mask1.getMaskPlaneDict()  Out[7]: True  In [8]: 'TEST' in mask2.getMaskPlaneDict()  Out[8]: True    # But deleting a plane from one affects only that particular Mask and not the global state  In [9]: mask1.removeAndClearMaskPlane('TEST')  In [10]: 'TEST' in mask1.getMaskPlaneDict()  Out[10]: False  In [11]: 'TEST' in mask2.getMaskPlaneDict()  Out[11]: True  {code}",4,DM-5174,datamanagement,manipulating mask confuse possible bug exist afwimage exposure.getmaskedimage function return copy exposure mask image actual maskedimage own exposure mean change mask copy reflect exposure maskimage intended behavior shallow copy return pointer original object mask case deep copy instead verify intended behavior happen step reproduce code python coaddexposure afwimage exposuref coaddexposure.getmaskedimage().getmask().addmaskplane('test print(coaddexposure.getmaskedimage().getmask().getmaskplanedict().asdict coaddexposure.getmaskedimage().getmask print(m.getmaskplanedict().asdict m.removeandclearmaskplane('test print(m.getmaskplanedict().asdict print(coaddexposure.getmaskedimage().getmask().getmaskplanedict().asdict code second concern necessarily bug add remove mask plane confuse inconsistent manipulation global state example code import lsst.afw.image afwimage create separate mask mask1 afwimage masku mask2 afwimage masku mask contain test plane test mask1.getmaskplanedict out[4 false test mask2.getmaskplanedict out[5 false add plane update share list plane appear mask1.addmaskplane('test out[6 test mask1.getmaskplanedict out[7 true test mask2.getmaskplanedict out[8 true delete plane affect particular mask global state mask1.removeandclearmaskplane('test 10 test mask1.getmaskplanedict out[10 false 11 test mask2.getmaskplanedict out[11 true code,"Manipulating masks is confusing A possible bug exists in afwImage.Exposure.getMaskedImage(). This function returns a copy of the Exposure's masked image, and not the actual maskedImage owned by the Exposure. This means that any changes made to the mask are done only on the copy, and are not reflected in the Exposure's maskImage. The intended behavior seems to be that a shallow copy be returned with pointers to all the original objects (such as the mask). This however does not seem to be the case, a deep copy is always made instead. Verify that the intended behavior is indeed happening. Steps to reproduce {code:python} coaddExposure = afwImage.ExposureF() coaddExposure.getMaskedImage().getMask().addMaskPlane('TEST') print(coaddExposure.getMaskedImage().getMask().getMaskPlaneDict().asdict()) m = coaddExposure.getMaskedImage().getMask() print(m.getMaskPlaneDict().asdict()) m.removeAndClearMaskPlane('TEST') print(m.getMaskPlaneDict().asdict()) print(coaddExposure.getMaskedImage().getMask().getMaskPlaneDict().asdict()) {code} A second concern, though not necessarily a bug, is that adding and removing mask planes is confusing due to inconsistent manipulation of global state. For example: {code} In [1]: import lsst.afw.image as afwImage # Create two separate Masks In [2]: mask1 = afwImage.MaskU() In [3]: mask2 = afwImage.MaskU() # Neither Mask contains a ""TEST"" plane In [4]: 'TEST' in mask1.getMaskPlaneDict() Out[4]: False In [5]: 'TEST' in mask2.getMaskPlaneDict() Out[5]: False # Adding a plane to one updates a shared list of planes, so it appears in the other In [6]: mask1.addMaskPlane('TEST') Out[6]: 9 In [7]: 'TEST' in mask1.getMaskPlaneDict() Out[7]: True In [8]: 'TEST' in mask2.getMaskPlaneDict() Out[8]: True # But deleting a plane from one affects only that particular Mask and not the global state In [9]: mask1.removeAndClearMaskPlane('TEST') In [10]: 'TEST' in mask1.getMaskPlaneDict() Out[10]: False In [11]: 'TEST' in mask2.getMaskPlaneDict() Out[11]: True {code}"
Add CSS information for shared scans to integration test data. Some tables int the integration tests need to be flagged as needing to be locked in memory and given a scan rating.,1,DM-5175,datamanagement,add css information share scan integration test datum table int integration test need flag need lock memory give scan rating,Add CSS information for shared scans to integration test data. Some tables int the integration tests need to be flagged as needing to be locked in memory and given a scan rating.
lsstsw deploy on OS X fails in miniconda install Testing the fixes for the {{deploy}} script in DM-4359 it seems that the part of the script installing {{miniconda}} no longer works on OS X because the list of packages to be installed has been derived from a Linux system and not all the Linux packages have OS X equivalents. There needs to be a per-OS list of packages. The default OS X list seems to be:  {code}  # This file may be used to create an environment using:  # $ conda create --name <env> --file <this file>  # platform: osx-64  astropy=1.1.1=np110py27_0  conda=3.19.1=py27_0  conda-env=2.4.5=py27_0  cycler=0.9.0=py27_0  cython=0.23.4=py27_1  freetype=2.5.5=0  libpng=1.6.17=0  matplotlib=1.5.1=np110py27_0  nomkl=1.0=0  numpy=1.10.4=py27_nomkl_0  openssl=1.0.2f=0  pandas=0.17.1=np110py27_0  pip=8.0.2=py27_0  pycosat=0.6.1=py27_0  pycrypto=2.6.1=py27_0  pyparsing=2.0.3=py27_0  pyqt=4.11.4=py27_1  python=2.7.11=0  python-dateutil=2.4.2=py27_0  pytz=2015.7=py27_0  pyyaml=3.11=py27_1  qt=4.8.7=1  readline=6.2=2  requests=2.9.1=py27_0  scipy=0.17.0=np110py27_nomkl_0  setuptools=19.6.2=py27_0  sip=4.16.9=py27_0  six=1.10.0=py27_0  sqlalchemy=1.0.11=py27_0  sqlite=3.9.2=0  tk=8.5.18=0  wheel=0.29.0=py27_0  yaml=0.1.6=0  zlib=1.2.8=0  {code},1,DM-5178,datamanagement,lsstsw deploy os fail miniconda install test fix deploy script dm-4359 script instal miniconda long work os list package instal derive linux system linux package os equivalent need os list package default os list code file create environment conda create platform astropy=1.1.1 np110py27_0 conda=3.19.1 py27_0 conda env=2.4.5 py27_0 cycler=0.9.0 py27_0 cython=0.23.4 py27_1 libpng=1.6.17=0 matplotlib=1.5.1 np110py27_0 nomkl=1.0=0 numpy=1.10.4 py27_nomkl_0 openssl=1.0.2f=0 pandas=0.17.1 np110py27_0 pip=8.0.2 py27_0 pycosat=0.6.1 py27_0 pycrypto=2.6.1 py27_0 pyparsing=2.0.3 py27_0 pyqt=4.11.4 py27_1 python=2.7.11=0 python dateutil=2.4.2 py27_0 pytz=2015.7 py27_0 pyyaml=3.11 py27_1 qt=4.8.7=1 readline=6.2=2 requests=2.9.1 py27_0 scipy=0.17.0 np110py27_nomkl_0 setuptools=19.6.2 py27_0 sip=4.16.9 py27_0 six=1.10.0 py27_0 sqlalchemy=1.0.11 py27_0 sqlite=3.9.2=0 tk=8.5.18=0 wheel=0.29.0 py27_0 yaml=0.1.6=0 code,lsstsw deploy on OS X fails in miniconda install Testing the fixes for the {{deploy}} script in DM-4359 it seems that the part of the script installing {{miniconda}} no longer works on OS X because the list of packages to be installed has been derived from a Linux system and not all the Linux packages have OS X equivalents. There needs to be a per-OS list of packages. The default OS X list seems to be: {code} # This file may be used to create an environment using: # $ conda create --name  --file  # platform: osx-64 astropy=1.1.1=np110py27_0 conda=3.19.1=py27_0 conda-env=2.4.5=py27_0 cycler=0.9.0=py27_0 cython=0.23.4=py27_1 freetype=2.5.5=0 libpng=1.6.17=0 matplotlib=1.5.1=np110py27_0 nomkl=1.0=0 numpy=1.10.4=py27_nomkl_0 openssl=1.0.2f=0 pandas=0.17.1=np110py27_0 pip=8.0.2=py27_0 pycosat=0.6.1=py27_0 pycrypto=2.6.1=py27_0 pyparsing=2.0.3=py27_0 pyqt=4.11.4=py27_1 python=2.7.11=0 python-dateutil=2.4.2=py27_0 pytz=2015.7=py27_0 pyyaml=3.11=py27_1 qt=4.8.7=1 readline=6.2=2 requests=2.9.1=py27_0 scipy=0.17.0=np110py27_nomkl_0 setuptools=19.6.2=py27_0 sip=4.16.9=py27_0 six=1.10.0=py27_0 sqlalchemy=1.0.11=py27_0 sqlite=3.9.2=0 tk=8.5.18=0 wheel=0.29.0=py27_0 yaml=0.1.6=0 zlib=1.2.8=0 {code}
miniconda2 eups package fails to install on OS X The {{miniconda2}} eups package attempts to install the relevant conda packages by downloading a list from the {{lsstsw}} repository. This fails for the same reason that {{lsstsw}} fails in DM-5178 in that the list of packages is not OS-specific. This means that {{newinstall.sh}} does not work any more on OS X.,1,DM-5179,datamanagement,miniconda2 eup package fail install os miniconda2 eup package attempt install relevant conda package download list lsstsw repository fail reason lsstsw fail dm-5178 list package os specific mean newinstall.sh work os x.,miniconda2 eups package fails to install on OS X The {{miniconda2}} eups package attempts to install the relevant conda packages by downloading a list from the {{lsstsw}} repository. This fails for the same reason that {{lsstsw}} fails in DM-5178 in that the list of packages is not OS-specific. This means that {{newinstall.sh}} does not work any more on OS X.
Implement Lock plot button on toolbar * Write a button on the toolbar that monitors the active plot view's group and shows the locked or unlocked icons  * Add an action and reducer functions the will toggle the lock state of the group.,6,DM-5185,datamanagement,implement lock plot button toolbar write button toolbar monitor active plot view group show locked unlocked icon add action reducer function toggle lock state group,Implement Lock plot button on toolbar * Write a button on the toolbar that monitors the active plot view's group and shows the locked or unlocked icons * Add an action and reducer functions the will toggle the lock state of the group.
"Add Xrdssi plugin configuration file Xrdssi plugin configuration file could be useful for sharedscan.  to pass plugin configuration file path to xrootd  http://xrootd.org/doc/dev42/xrd_config.htm#_Passing_Plug-In_Command (use -+xrdssi)  to get this argument from C++  http://xrootd.org/doc/dev42/ssi_reference.htm#_Toc431242001    then an add-hoc config file parser needs to be used (not to be xrootd dependant), json/yaml could be used.",6,DM-5186,datamanagement,add xrdssi plugin configuration file xrdssi plugin configuration file useful sharedscan pass plugin configuration file path xrootd http://xrootd.org/doc/dev42/xrd_config.htm#_passing_plug-in_command use -+xrdssi argument c++ http://xrootd.org/doc/dev42/ssi_reference.htm#_toc431242001 add hoc config file parser need xrootd dependant json yaml,"Add Xrdssi plugin configuration file Xrdssi plugin configuration file could be useful for sharedscan. to pass plugin configuration file path to xrootd http://xrootd.org/doc/dev42/xrd_config.htm#_Passing_Plug-In_Command (use -+xrdssi) to get this argument from C++ http://xrootd.org/doc/dev42/ssi_reference.htm#_Toc431242001 then an add-hoc config file parser needs to be used (not to be xrootd dependant), json/yaml could be used."
"Set Qserv master in env variable for Docker containers This would allow use of pre-configured container on all clusters, indeed the only parameter which currently change in cluster install is master fqdn.  See http://xrootd.org/doc/dev42/Syntax_config.htm  and  {code}  if defined ?~EXPORTPATH    set exportpath = $EXPORTPATH    else    set exportpath = /tmp    fi    all.export $exportpath    {code}",3,DM-5187,datamanagement,set qserv master env variable docker container allow use pre configure container cluster parameter currently change cluster install master fqdn http://xrootd.org/doc/dev42/syntax_config.htm code define set exportpath exportpath set exportpath /tmp fi all.export exportpath code,"Set Qserv master in env variable for Docker containers This would allow use of pre-configured container on all clusters, indeed the only parameter which currently change in cluster install is master fqdn. See http://xrootd.org/doc/dev42/Syntax_config.htm and {code} if defined ?~EXPORTPATH set exportpath = $EXPORTPATH else set exportpath = /tmp fi all.export $exportpath {code}"
attend the bi-weekly meeting authentication and authorization discussion attend the bi-weekly meeting authentication and authorization discussion. provide input and feedback to IAM. ,4,DM-5193,datamanagement,attend bi weekly meeting authentication authorization discussion attend bi weekly meeting authentication authorization discussion provide input feedback iam,attend the bi-weekly meeting authentication and authorization discussion attend the bi-weekly meeting authentication and authorization discussion. provide input and feedback to IAM.
"swift API availability? The downtime announcement email for {{Nebula unavailable Feb 9-10}} mentioned a ""roadmap"" for swift.  I have checked and post maintenance, there is not a swift endpoint available in the catalog.  Is there a time line for availability?",1,DM-5196,datamanagement,swift api availability downtime announcement email nebula unavailable feb 10 mention roadmap swift check post maintenance swift endpoint available catalog time line availability,"swift API availability? The downtime announcement email for {{Nebula unavailable Feb 9-10}} mentioned a ""roadmap"" for swift. I have checked and post maintenance, there is not a swift endpoint available in the catalog. Is there a time line for availability?"
"Test and robustify shapelet PSF approximations The CModel code ported from HSC only works as well as the ShapeletPsfApproximation algorithm that runs before it, but we've switched on the LSST side to a more flexible algorithm that isn't as nearly as battle-tested as what's been running on the HSC side, and there are some concerning indications from [~pgee]'s work that it can be catastrophically slow on some reasonable PSFs.  On this issue, I'll run it on some real HSC data and try to improve it, even if that means reducing the flexibility back to what was on the HSC side in some ways.",8,DM-5197,datamanagement,test robustify shapelet psf approximation cmodel code port hsc work shapeletpsfapproximation algorithm run switch lsst flexible algorithm nearly battle test run hsc concern indication ~pgee work catastrophically slow reasonable psf issue run real hsc datum try improve mean reduce flexibility hsc way,"Test and robustify shapelet PSF approximations The CModel code ported from HSC only works as well as the ShapeletPsfApproximation algorithm that runs before it, but we've switched on the LSST side to a more flexible algorithm that isn't as nearly as battle-tested as what's been running on the HSC side, and there are some concerning indications from [~pgee]'s work that it can be catastrophically slow on some reasonable PSFs. On this issue, I'll run it on some real HSC data and try to improve it, even if that means reducing the flexibility back to what was on the HSC side in some ways."
FITS Visualizer porting: Statistics - part 2 - drawing overlay & 3 color support drawing overlay 3 Color Support,8,DM-5198,datamanagement,fit visualizer porting statistic draw overlay color support draw overlay color support,FITS Visualizer porting: Statistics - part 2 - drawing overlay & 3 color support drawing overlay 3 Color Support
"instance I/O errors The kernel dmesg for Instance {{bbfd7458-6dd6-4412-a8ba-8d417c3df56b}} has started reporting thousands of block I/O errors and these are starting to trickle up as a filesystem I/O errors.  I suspect this is likely a hypervisor I/O issue.    {code}  [687301.556430] Buffer I/O error on device dm-3, logical block 3768490  [687301.556433] Buffer I/O error on device dm-3, logical block 3768491  [687301.556436] Buffer I/O error on device dm-3, logical block 3768492  {code}    {code}  $ openstack server show bbfd7458-6dd6-4412-a8ba-8d417c3df56b  +--------------------------------------+-----------------------------------------------------------------------+  | Field                                | Value                                                                 |  +--------------------------------------+-----------------------------------------------------------------------+  | OS-DCF:diskConfig                    | MANUAL                                                                |  | OS-EXT-AZ:availability_zone          | nova                                                                  |  | OS-EXT-STS:power_state               | 1                                                                     |  | OS-EXT-STS:task_state                | None                                                                  |  | OS-EXT-STS:vm_state                  | active                                                                |  | OS-SRV-USG:launched_at               | 2016-02-11T23:36:25.000000                                            |  | OS-SRV-USG:terminated_at             | None                                                                  |  | accessIPv4                           |                                                                       |  | accessIPv6                           |                                                                       |  | addresses                            | LSST-net=172.16.1.115, 141.142.209.121                                |  | config_drive                         |                                                                       |  | created                              | 2016-02-11T23:36:12Z                                                  |  | flavor                               | m1.xlarge (5)                                                         |  | hostId                               | f7fbf308022d02f52e1111c91cf578d852784d290d0e0ddb0d69635c              |  | id                                   | bbfd7458-6dd6-4412-a8ba-8d417c3df56b                                  |  | image                                | centos-7-docker-20151116230205 (59a2a478-11ab-41c5-affc-29706d38d65a) |  | key_name                             | vagrant-generated-comshorc                                            |  | name                                 | el7-docker-jhoblitt                                                   |  | os-extended-volumes:volumes_attached | []                                                                    |  | progress                             | 0                                                                     |  | project_id                           | 8c1ba1e0b84d486fbe7a665c30030113                                      |  | properties                           |                                                                       |  | security_groups                      | [{'name': 'default'}, {'name': 'remote SSH'}]                         |  | status                               | ACTIVE                                                                |  | updated                              | 2016-02-11T23:36:25Z                                                  |  | user_id                              | 83bf259d1f0c4f458e03f9002f9b4008                                      |  +--------------------------------------+-----------------------------------------------------------------------+  {code}",1,DM-5200,datamanagement,instance error kernel dmesg instance bbfd7458 6dd6 4412 a8ba-8d417c3df56b start report thousand block error start trickle filesystem error suspect likely hypervisor issue code 687301.556430 buffer error device dm-3 logical block 3768490 687301.556433 buffer error device dm-3 logical block 3768491 687301.556436 buffer error device dm-3 logical block 3768492 code code openstack server bbfd7458 6dd6 4412 a8ba-8d417c3df56b field value os dcf diskconfig manual os ext az availability_zone nova os ext sts power_state os ext sts task_state os ext sts vm_state active os srv usg launched_at 2016 02 11t23:36:25.000000 os srv usg terminated_at accessipv4 accessipv6 address lsst net=172.16.1.115 141.142.209.121 config_drive create 2016 02 11t23:36:12z flavor m1.xlarge hostid f7fbf308022d02f52e1111c91cf578d852784d290d0e0ddb0d69635c bbfd7458 6dd6 4412 a8ba-8d417c3df56b image centos-7 docker-20151116230205 59a2a478 11ab-41c5 affc-29706d38d65a key_name vagrant generate comshorc el7 docker jhoblitt os extend volume volumes_attache progress project_id 8c1ba1e0b84d486fbe7a665c30030113 property security_group default remote ssh status active update 2016 02 11t23:36:25z user_id 83bf259d1f0c4f458e03f9002f9b4008 code,"instance I/O errors The kernel dmesg for Instance {{bbfd7458-6dd6-4412-a8ba-8d417c3df56b}} has started reporting thousands of block I/O errors and these are starting to trickle up as a filesystem I/O errors. I suspect this is likely a hypervisor I/O issue. {code} [687301.556430] Buffer I/O error on device dm-3, logical block 3768490 [687301.556433] Buffer I/O error on device dm-3, logical block 3768491 [687301.556436] Buffer I/O error on device dm-3, logical block 3768492 {code} {code} $ openstack server show bbfd7458-6dd6-4412-a8ba-8d417c3df56b +--------------------------------------+-----------------------------------------------------------------------+ | Field | Value | +--------------------------------------+-----------------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-STS:power_state | 1 | | OS-EXT-STS:task_state | None | | OS-EXT-STS:vm_state | active | | OS-SRV-USG:launched_at | 2016-02-11T23:36:25.000000 | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | LSST-net=172.16.1.115, 141.142.209.121 | | config_drive | | | created | 2016-02-11T23:36:12Z | | flavor | m1.xlarge (5) | | hostId | f7fbf308022d02f52e1111c91cf578d852784d290d0e0ddb0d69635c | | id | bbfd7458-6dd6-4412-a8ba-8d417c3df56b | | image | centos-7-docker-20151116230205 (59a2a478-11ab-41c5-affc-29706d38d65a) | | key_name | vagrant-generated-comshorc | | name | el7-docker-jhoblitt | | os-extended-volumes:volumes_attached | [] | | progress | 0 | | project_id | 8c1ba1e0b84d486fbe7a665c30030113 | | properties | | | security_groups | [{'name': 'default'}, {'name': 'remote SSH'}] | | status | ACTIVE | | updated | 2016-02-11T23:36:25Z | | user_id | 83bf259d1f0c4f458e03f9002f9b4008 | +--------------------------------------+-----------------------------------------------------------------------+ {code}"
Remove LOGF macros from log package We have removed all uses of LOGF macros from qserv and as far as I know no other clients use those macros. It's time to clean up log package itself from those macros.,1,DM-5202,datamanagement,remove logf macro log package remove use logf macro qserv far know client use macro time clean log package macro,Remove LOGF macros from log package We have removed all uses of LOGF macros from qserv and as far as I know no other clients use those macros. It's time to clean up log package itself from those macros.
Add support for 3 Color Most of this is done.  I just need to plot a few 3 color images and work out the bugs.,4,DM-5203,datamanagement,add support color need plot color image work bug,Add support for 3 Color Most of this is done. I just need to plot a few 3 color images and work out the bugs.
"Remove remaining LOGF macros from qserv There are still few cases of LOGF macros in qserv, have to replace them all.",1,DM-5204,datamanagement,remove remain logf macro qserv case logf macro qserv replace,"Remove remaining LOGF macros from qserv There are still few cases of LOGF macros in qserv, have to replace them all."
"Please do not write garbage to the FITS EQUINOX The equinox is not relevant when dealing with ICRS coordinates.    When {{afw}} manipulates {{Wcs}} objects, it simply doesn't bother initializing the {{equinox}} field of its {{_wcsInfo}} struct when dealing with an ICRS system.    When {{afw}} persists the {{Wcs}} to FITS, it blindly writes whatever happens to be in that uninitialized field to the FITS header. Thus, we end up with something like:  {code}  EQUINOX =      9.87654321E+107 / Equinox of coordinates  {code}  This should be no problem, since, per the [FITS standard|http://fits.gsfc.nasa.gov/standard30/fits_standard30aa.pdf] (page 30), the {{EQUINOX}} is ""not applicable"" if they {{RADESYS}} is {{ICRS}}. The reader should thus ignore this value.    However, [SAOimage DS9|http://ds9.si.edu] version 7.4.1 (the latest release at time of writing) does _not_ ignore the {{EQUINOX}}. Rather, it refuses to handle the WCS for the image. Note that version 7.3 of DS9 does not seem to have the same issue.    While this does seem to be a bug in DS9, it's easy enough to work around by simply not writing {{EQUINOX}}.",1,DM-5206,datamanagement,write garbage fits equinox equinox relevant deal icr coordinate afw manipulate wcs object simply bother initialize equinox field wcsinfo struct deal icrs system afw persist wcs fits blindly write happen uninitialized field fit header end like code equinox 9.87654321e+107 equinox coordinate code problem fit standard|http://fits.gsfc.nasa.gov standard30 fits_standard30aa.pdf page 30 equinox applicable radesys icrs reader ignore value saoimage ds9|http://ds9.si.edu version 7.4.1 late release time writing ignore equinox refuse handle wcs image note version 7.3 ds9 issue bug ds9 easy work simply write equinox,"Please do not write garbage to the FITS EQUINOX The equinox is not relevant when dealing with ICRS coordinates. When {{afw}} manipulates {{Wcs}} objects, it simply doesn't bother initializing the {{equinox}} field of its {{_wcsInfo}} struct when dealing with an ICRS system. When {{afw}} persists the {{Wcs}} to FITS, it blindly writes whatever happens to be in that uninitialized field to the FITS header. Thus, we end up with something like: {code} EQUINOX = 9.87654321E+107 / Equinox of coordinates {code} This should be no problem, since, per the [FITS standard|http://fits.gsfc.nasa.gov/standard30/fits_standard30aa.pdf] (page 30), the {{EQUINOX}} is ""not applicable"" if they {{RADESYS}} is {{ICRS}}. The reader should thus ignore this value. However, [SAOimage DS9|http://ds9.si.edu] version 7.4.1 (the latest release at time of writing) does _not_ ignore the {{EQUINOX}}. Rather, it refuses to handle the WCS for the image. Note that version 7.3 of DS9 does not seem to have the same issue. While this does seem to be a bug in DS9, it's easy enough to work around by simply not writing {{EQUINOX}}."
"Evaluate MariaDB GSSAPI Authentication Plugin As a follow-on to DM-4315, deploy the new [Maria DB GSSAPI Authentication Plugin|https://mariadb.com/kb/en/mariadb/gssapi-authentication-plugin/] in the IAM testbed for Kerberos ticket-based authentication, to provide single sign-on access.",2,DM-5208,datamanagement,evaluate mariadb gssapi authentication plugin follow dm-4315 deploy new maria db gssapi authentication plugin|https://mariadb.com kb en mariadb gssapi authentication plugin/ iam testbe kerberos ticket base authentication provide single sign access,"Evaluate MariaDB GSSAPI Authentication Plugin As a follow-on to DM-4315, deploy the new [Maria DB GSSAPI Authentication Plugin|https://mariadb.com/kb/en/mariadb/gssapi-authentication-plugin/] in the IAM testbed for Kerberos ticket-based authentication, to provide single sign-on access."
Run Qserv multinodes integration tests inside Travis This aims at preparing integration of this procedure inside Jenkins CI,4,DM-5218,datamanagement,run qserv multinode integration test inside travis aim prepare integration procedure inside jenkins ci,Run Qserv multinodes integration tests inside Travis This aims at preparing integration of this procedure inside Jenkins CI
Add configured requirements parameters.  Pass/Fail test. 1. Add pass/fail routine to report success/fail against metrics.  Do this for    * SRD  (/)    * Configured metrics  (/)    2. Add pass/fail reporting to running of `validate.drp.run`  (/),4,DM-5219,datamanagement,add configure requirement parameter pass fail test add pass fail routine report success fail metric srd configure metric add pass fail report run validate.drp.run,Add configured requirements parameters. Pass/Fail test. 1. Add pass/fail routine to report success/fail against metrics. Do this for * SRD (/) * Configured metrics (/) 2. Add pass/fail reporting to running of `validate.drp.run` (/)
"Add a ci_hsc daily build Please add a daily build of `ci_hsc` to the Jenkins system.    This does not need to explicitly build `lsst_distrib` or `lsst_sims`.  The only product to list is `ci_hsc`.    In the slightly near future, I anticipate that this build will be replaced by a daily build of a metapackage `lsst_ci`.",5,DM-5222,datamanagement,add ci_hsc daily build add daily build ci_hsc jenkins system need explicitly build lsst_distrib lsst_sims product list ci_hsc slightly near future anticipate build replace daily build metapackage lsst_ci,"Add a ci_hsc daily build Please add a daily build of `ci_hsc` to the Jenkins system. This does not need to explicitly build `lsst_distrib` or `lsst_sims`. The only product to list is `ci_hsc`. In the slightly near future, I anticipate that this build will be replaced by a daily build of a metapackage `lsst_ci`."
"Segfault in shapeHSM centroid extractor [~boutigny] reports a segfault in {{meas_extenstions_shapeHSM}}. He provides the following backtrace:  {code}  Program received signal SIGSEGV, Segmentation fault.  0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement<lsst::afw::table::Flag> (this=0x21c8d60, key=...)  at include/lsst/afw/table/BaseRecord.h:61  61	typename Field<T>::Element * getElement(Key<T> const & key) {  (gdb) bt  #0 0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement<lsst::afw::table::Flag> (this=0x21c8d60, key=...)  at include/lsst/afw/table/BaseRecord.h:61  #1 0x00007fffdc8775f2 in set<lsst::afw::table::Flag, bool> (value=<synthetic pointer>, key=..., this=0x21c8d60)  at /home/boutigny/CFHT/lsstsw/stack/Linux64/afw/11.0-8-g38426eb/include/lsst/afw/table/BaseRecord.h:137  #2 setValue (value=true, i=0, record=..., this=0x1da2500) at include/lsst/meas/base/FlagHandler.h:73  #3 lsst::meas::base::SafeCentroidExtractor::operator() (this=<optimized out>, record=..., flags=...)  at src/InputUtilities.cc:134  #4 0x00007fffd03655c6 in lsst::meas::extensions::shapeHSM::HsmPsfMomentsAlgorithm::measure (this=0x1da2410,   source=..., exposure=...) at src/HsmMoments.cc:115  #5 0x00007fffd06708d5 in _wrap_HsmPsfMomentsAlgorithm_measure (args=0x7fffccc67b90)  at python/lsst/meas/extensions/shapeHSM/hsmLib_wrap.cc:14337  #6 0x00007ffff7aee37f in ext_do_call (nk=-859407472, na=<optimized out>, flags=<optimized out>,   pp_stack=0x7fffffff7d18, func=0x7fffd0c21878) at Python/ceval.c:4345  #7 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2720  #8 0x00007ffff7aefdbe in PyEval_EvalCodeEx (co=0x7fffd0a9ceb0, globals=<optimized out>, locals=<optimized out>,   args=<optimized out>, argcount=3, kws=0x7fffccd43b08, kwcount=0, defs=0x0, defcount=0, closure=0x0)  at Python/ceval.c:3267  {code}    See the discussion at DM-4780.",2,DM-5247,datamanagement,segfault shapehsm centroid extractor ~boutigny report segfault meas_extenstions_shapehsm provide following backtrace code program receive signal sigsegv segmentation fault 0x00007fffe7043156 lsst::afw::table::baserecord::getelement this=0x21c8d60 key= include lsst afw table baserecord.h:61 61 typename field::element getelement(key const key gdb bt 0x00007fffe7043156 lsst::afw::table::baserecord::getelement this=0x21c8d60 key= include lsst afw table baserecord.h:61 0x00007fffdc8775f2 set value= key= this=0x21c8d60 /home boutigny cfht lsstsw stack linux64 afw/11.0 g38426eb include lsst afw table baserecord.h:137 setvalue value true i=0 record= this=0x1da2500 include lsst meas base flaghandler.h:73 lsst::meas::base::safecentroidextractor::operator this= record= flags= src inpututilities.cc:134 0x00007fffd03655c6 lsst::meas::extensions::shapehsm::hsmpsfmomentsalgorithm::measure this=0x1da2410 source= exposure= src hsmmoments.cc:115 0x00007fffd06708d5 wrap_hsmpsfmomentsalgorithm_measure args=0x7fffccc67b90 python lsst mea extension shapehsm hsmlib_wrap.cc:14337 0x00007ffff7aee37f ext_do_call nk=-859407472 na= flags= pp_stack=0x7fffffff7d18 func=0x7fffd0c21878 python ceval.c:4345 pyeval_evalframeex f= throwflag= python ceval.c:2720 0x00007ffff7aefdbe pyeval_evalcodeex co=0x7fffd0a9ceb0 globals= locals= args= argcount=3 kws=0x7fffccd43b08 kwcount=0 defs=0x0 defcount=0 closure=0x0 python ceval.c:3267 code discussion dm-4780,"Segfault in shapeHSM centroid extractor [~boutigny] reports a segfault in {{meas_extenstions_shapeHSM}}. He provides the following backtrace: {code} Program received signal SIGSEGV, Segmentation fault. 0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement (this=0x21c8d60, key=...) at include/lsst/afw/table/BaseRecord.h:61 61 typename Field::Element * getElement(Key const & key) { (gdb) bt #0 0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement (this=0x21c8d60, key=...) at include/lsst/afw/table/BaseRecord.h:61 #1 0x00007fffdc8775f2 in set (value=, key=..., this=0x21c8d60) at /home/boutigny/CFHT/lsstsw/stack/Linux64/afw/11.0-8-g38426eb/include/lsst/afw/table/BaseRecord.h:137 #2 setValue (value=true, i=0, record=..., this=0x1da2500) at include/lsst/meas/base/FlagHandler.h:73 #3 lsst::meas::base::SafeCentroidExtractor::operator() (this=, record=..., flags=...) at src/InputUtilities.cc:134 #4 0x00007fffd03655c6 in lsst::meas::extensions::shapeHSM::HsmPsfMomentsAlgorithm::measure (this=0x1da2410, source=..., exposure=...) at src/HsmMoments.cc:115 #5 0x00007fffd06708d5 in _wrap_HsmPsfMomentsAlgorithm_measure (args=0x7fffccc67b90) at python/lsst/meas/extensions/shapeHSM/hsmLib_wrap.cc:14337 #6 0x00007ffff7aee37f in ext_do_call (nk=-859407472, na=, flags=, pp_stack=0x7fffffff7d18, func=0x7fffd0c21878) at Python/ceval.c:4345 #7 PyEval_EvalFrameEx (f=, throwflag=) at Python/ceval.c:2720 #8 0x00007ffff7aefdbe in PyEval_EvalCodeEx (co=0x7fffd0a9ceb0, globals=, locals=, args=, argcount=3, kws=0x7fffccd43b08, kwcount=0, defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3267 {code} See the discussion at DM-4780."
Implement background gradient fit in pre-sub. images for dipole fit Add a linear background gradient fit to the integrated pre-subtraction and dipole fitter (for testing).  This will eventually be implemented in the measurement plugin.,2,DM-5248,datamanagement,implement background gradient fit pre sub image dipole fit add linear background gradient fit integrate pre subtraction dipole fitter testing eventually implement measurement plugin,Implement background gradient fit in pre-sub. images for dipole fit Add a linear background gradient fit to the integrated pre-subtraction and dipole fitter (for testing). This will eventually be implemented in the measurement plugin.
lsstsw breakage with spaces in paths There are still some issues relating to using {{lsstsw}} to build the stack when spaces are in the path to the {{$LSSTSW}} location. This is a fine thing to sort out on Rodeo Day...,1,DM-5251,datamanagement,lsstsw breakage space path issue relate lsstsw build stack space path lsstsw location fine thing sort rodeo day,lsstsw breakage with spaces in paths There are still some issues relating to using {{lsstsw}} to build the stack when spaces are in the path to the {{$LSSTSW}} location. This is a fine thing to sort out on Rodeo Day...
"Base ""bright star"" cut on S/N instead of magnitudes The astrometry histogram generated by validateDrp.py conflates astrometric and photometric calibration because it uses magnitude for brightness, and this relies on the accuracy of the photometric calibration. [~ctslater] suggests (and I agree) that brightness should be based on signal to noise ratio, thus making the astrometry histogram independent of photometric calibration.  ",2,DM-5252,datamanagement,base bright star cut instead magnitude astrometry histogram generate validatedrp.py conflate astrometric photometric calibration use magnitude brightness rely accuracy photometric calibration ~ctslater suggest agree brightness base signal noise ratio make astrometry histogram independent photometric calibration,"Base ""bright star"" cut on S/N instead of magnitudes The astrometry histogram generated by validateDrp.py conflates astrometric and photometric calibration because it uses magnitude for brightness, and this relies on the accuracy of the photometric calibration. [~ctslater] suggests (and I agree) that brightness should be based on signal to noise ratio, thus making the astrometry histogram independent of photometric calibration."
Modernize python in lsst_build The python in {{lsst_build}} uses old-style print and exception handling. These should be updated to the current standard.,1,DM-5264,datamanagement,modernize python lsst_build python lsst_build use old style print exception handling update current standard,Modernize python in lsst_build The python in {{lsst_build}} uses old-style print and exception handling. These should be updated to the current standard.
"Turn on bias-jump fix for all CCDs  The overscan fix to handle bias jump in an amplifier done in DM-4366 introduced a new config parameter {{overscanBiasJumpBKP}}, and the fix is applied for CCDs on the backplanes specified in {{overscanBiasJumpBKP}}.  Previously, the default is to only fix CCDs on backplanes next to the focus chips. But [~mfisherlevine] also see the bias jump features in other CCDs.  It would make more sense to turn it on for all CCDs by default. ",1,DM-5265,datamanagement,turn bias jump fix ccds overscan fix handle bias jump amplifier dm-4366 introduce new config parameter overscanbiasjumpbkp fix apply ccd backplane specify overscanbiasjumpbkp previously default fix ccd backplane focus chip ~mfisherlevine bias jump feature ccd sense turn ccd default,"Turn on bias-jump fix for all CCDs The overscan fix to handle bias jump in an amplifier done in DM-4366 introduced a new config parameter {{overscanBiasJumpBKP}}, and the fix is applied for CCDs on the backplanes specified in {{overscanBiasJumpBKP}}. Previously, the default is to only fix CCDs on backplanes next to the focus chips. But [~mfisherlevine] also see the bias jump features in other CCDs. It would make more sense to turn it on for all CCDs by default."
Provide comparison routines for comparing two repos of the same data Adapt the HSC capabilities from DM-4730 as represented on pipe_tasks u/laurenm/DM-4730 (as prepared by [~lauren] and [~price])  into generally available {{pipe_tasks}} routines for comparison of two different repositories of the same data.  The intended use case is comparing two different algorithms or configurations on the same data and providing individual source-measurement to source-measurement comparisons for debugging new algorithms and understanding the behavior.,3,DM-5270,datamanagement,provide comparison routine compare repos datum adapt hsc capability dm-4730 represent pipe_task laurenm dm-4730 prepare ~lauren ~price generally available pipe_tasks routine comparison different repository datum intend use case compare different algorithm configuration datum provide individual source measurement source measurement comparison debug new algorithm understand behavior,Provide comparison routines for comparing two repos of the same data Adapt the HSC capabilities from DM-4730 as represented on pipe_tasks u/laurenm/DM-4730 (as prepared by [~lauren] and [~price]) into generally available {{pipe_tasks}} routines for comparison of two different repositories of the same data. The intended use case is comparing two different algorithms or configurations on the same data and providing individual source-measurement to source-measurement comparisons for debugging new algorithms and understanding the behavior.
Audit SuprimeCam policy and update to current standards {{obs_subaru}}'s {{policy/SuprimecamMapper.paf}} contains a number of entries that look wrong (e.g. {{deep_forcedPhotCoadd_metadata}} should be {{deepCoadd_forced_config}}) or do not apply to LSST (e.g. {{stack_config}}) and doesn't contain a number of entries that might be expected (e.g. {{transformed_src}}).    Please ensure that this file is updated to comply with current expectations.,1,DM-5271,datamanagement,audit suprimecam policy update current standard obs_subaru policy suprimecammapper.paf contain number entry look wrong e.g. deep_forcedphotcoadd_metadata deepcoadd_forced_config apply lsst e.g. stack_config contain number entry expect e.g. transformed_src ensure file update comply current expectation,Audit SuprimeCam policy and update to current standards {{obs_subaru}}'s {{policy/SuprimecamMapper.paf}} contains a number of entries that look wrong (e.g. {{deep_forcedPhotCoadd_metadata}} should be {{deepCoadd_forced_config}}) or do not apply to LSST (e.g. {{stack_config}}) and doesn't contain a number of entries that might be expected (e.g. {{transformed_src}}). Please ensure that this file is updated to comply with current expectations.
"rename meas_simastrom to jointcal and flatten namespace Moving meas_simastrom from lsst_france/ to lsst/ also resulted in a name change per RFC-123, and a namespace flattening: it's not derived from meas; it's a task. This is the necessary first step in getting it integrated into the stack.",1,DM-5273,datamanagement,rename meas_simastrom jointcal flatten namespace move meas_simastrom lsst_france/ lsst/ result change rfc-123 namespace flattening derive mea task necessary step get integrate stack,"rename meas_simastrom to jointcal and flatten namespace Moving meas_simastrom from lsst_france/ to lsst/ also resulted in a name change per RFC-123, and a namespace flattening: it's not derived from meas; it's a task. This is the necessary first step in getting it integrated into the stack."
Filtering from XY Plot table view (JS) Allow to filter in a selected area from XY Plot.,4,DM-5274,datamanagement,filter xy plot table view js allow filter select area xy plot,Filtering from XY Plot table view (JS) Allow to filter in a selected area from XY Plot.
"make floating point exception handling cross-platform (or remove it) jointcal currently has a couple of trapfpe() functions that wrap feenableexcept, which doesn't exist on OSX. Were these an important part of error handling in meas_simastrom, or can I just remove them?",2,DM-5275,datamanagement,float point exception handle cross platform remove jointcal currently couple trapfpe function wrap feenableexcept exist osx important error handling meas_simastrom remove,"make floating point exception handling cross-platform (or remove it) jointcal currently has a couple of trapfpe() functions that wrap feenableexcept, which doesn't exist on OSX. Were these an important part of error handling in meas_simastrom, or can I just remove them?"
"plan to upgrade the third party packages The following packages need to be reviewed and maybe upgraded.      ""babel""     : ""5.8.34"",                           6.5.2 (M)      ""history""   : ""1.17.0"",                           2.0.0 (M)      ""icepick""   : ""0.2.0"",                            1.1.0 (M)                ""react-highcharts"": ""5.0.6"",                      7.0.0 (M)      ""react-redux"": ""3.1.2"",                           4.4.0 (M)      ""react-split-pane"": ""0.1.22"",                     2.0.1 (M)      ""redux-thunk"": ""0.1.0"",                           1.0.3 (M)      ""redux-logger"": ""1.0.9"",                          2.6.1 (M)      ""validator"" : ""4.5.0"",                            5.1.0 (M)      ""chai"": ""^2.3.0"",                                 3.5.0 (M)      ""esprima-fb"": ""^14001.1.0-dev-harmony-fb"",        15001.1001.0-dev-harmony-fb (M)      ""babel-eslint""      : ""^4.1.3"",                   5.0.0 (M)      ""babel-loader""      : ""^5.3.2"",                   6.2.4 (M)      ""babel-plugin-react-transform"": ""^1.1.0"",         2.0.0 (M)      ""babel-runtime""     : ""^5.8.20"",                  6.6.0 (M)      ""eslint""            : ""^1.10.3"",                  2.2.0 (M)      ""eslint-config-airbnb"": ""0.1.0"",                  6.0.2 (M) works with eslint 2.2.0      ""eslint-plugin-react"": ""^3.5.1"",                  4.1.0 (M)  works with eslint 2.2.0      ""extract-text-webpack-plugin"": ""^0.8.0"",          1.0.1 (M)      ""html-webpack-plugin"": ""^1.6.1"",                  2.9.0 (M)      ""karma-sinon-chai"": ""^0.3.0"",                     1.2.0 (M)      ""redux-devtools""    : ""^2.1.2"",                   3.3.1 (M)      ""webpack"": ""^1.8.2""                               1.12.14, 2.1.0 beta4 (M)  ",2,DM-5276,datamanagement,plan upgrade party package follow package need review maybe upgrade babel 5.8.34 6.5.2 history 1.17.0 2.0.0 icepick 0.2.0 1.1.0 react highchart 5.0.6 7.0.0 react redux 3.1.2 4.4.0 react split pane 0.1.22 2.0.1 redux thunk 0.1.0 1.0.3 redux logger 1.0.9 2.6.1 validator 4.5.0 5.1.0 chai ^2.3.0 3.5.0 esprima fb ^14001.1.0 dev harmony fb 15001.1001.0 dev harmony fb babel eslint ^4.1.3 5.0.0 babel loader ^5.3.2 6.2.4 babel plugin react transform ^1.1.0 2.0.0 babel runtime ^5.8.20 6.6.0 eslint 2.2.0 eslint config airbnb 0.1.0 6.0.2 work eslint 2.2.0 eslint plugin react ^3.5.1 4.1.0 work eslint 2.2.0 extract text webpack plugin ^0.8.0 1.0.1 html webpack plugin ^1.6.1 2.9.0 karma sinon chai ^0.3.0 1.2.0 redux devtool ^2.1.2 3.3.1 webpack ^1.8.2 1.12.14 2.1.0 beta4,"plan to upgrade the third party packages The following packages need to be reviewed and maybe upgraded. ""babel"" : ""5.8.34"", 6.5.2 (M) ""history"" : ""1.17.0"", 2.0.0 (M) ""icepick"" : ""0.2.0"", 1.1.0 (M) ""react-highcharts"": ""5.0.6"", 7.0.0 (M) ""react-redux"": ""3.1.2"", 4.4.0 (M) ""react-split-pane"": ""0.1.22"", 2.0.1 (M) ""redux-thunk"": ""0.1.0"", 1.0.3 (M) ""redux-logger"": ""1.0.9"", 2.6.1 (M) ""validator"" : ""4.5.0"", 5.1.0 (M) ""chai"": ""^2.3.0"", 3.5.0 (M) ""esprima-fb"": ""^14001.1.0-dev-harmony-fb"", 15001.1001.0-dev-harmony-fb (M) ""babel-eslint"" : ""^4.1.3"", 5.0.0 (M) ""babel-loader"" : ""^5.3.2"", 6.2.4 (M) ""babel-plugin-react-transform"": ""^1.1.0"", 2.0.0 (M) ""babel-runtime"" : ""^5.8.20"", 6.6.0 (M) ""eslint"" : ""^1.10.3"", 2.2.0 (M) ""eslint-config-airbnb"": ""0.1.0"", 6.0.2 (M) works with eslint 2.2.0 ""eslint-plugin-react"": ""^3.5.1"", 4.1.0 (M) works with eslint 2.2.0 ""extract-text-webpack-plugin"": ""^0.8.0"", 1.0.1 (M) ""html-webpack-plugin"": ""^1.6.1"", 2.9.0 (M) ""karma-sinon-chai"": ""^0.3.0"", 1.2.0 (M) ""redux-devtools"" : ""^2.1.2"", 3.3.1 (M) ""webpack"": ""^1.8.2"" 1.12.14, 2.1.0 beta4 (M)"
replace buildbot with jenkins job(s) Removing buildbot and replacing it with jenkins would provide a number of benefits    * one less dashboard for developers to know about / interact with  * one less system for SQRE to maintain  * lessening the cost of refactoring the CI drivers scripts as synchronized updates to two CI system configurations would no longer be necessary    It should also be easy to go one step further and try to eliminate the need for developers to manually log into the {{lsstsw}} account on {{lsst-dev}} to publish eups distrib packages. ,3,DM-5277,datamanagement,replace buildbot jenkins job(s remove buildbot replace jenkin provide number benefit dashboard developer know interact system sqre maintain lessen cost refactore ci driver script synchronize update ci system configuration long necessary easy step try eliminate need developer manually log lsstsw account lsst dev publish eup distrib package,replace buildbot with jenkins job(s) Removing buildbot and replacing it with jenkins would provide a number of benefits * one less dashboard for developers to know about / interact with * one less system for SQRE to maintain * lessening the cost of refactoring the CI drivers scripts as synchronized updates to two CI system configurations would no longer be necessary It should also be easy to go one step further and try to eliminate the need for developers to manually log into the {{lsstsw}} account on {{lsst-dev}} to publish eups distrib packages.
"Attend JTM Joint Technical Meeting 2/22-24, Santa Cruz",6,DM-5278,datamanagement,attend jtm joint technical meeting 2/22 24 santa cruz,"Attend JTM Joint Technical Meeting 2/22-24, Santa Cruz"
arrays not properly transmitted Sending a property set with an array as one of the entries only passes the last element of the array.,1,DM-5279,datamanagement,array properly transmit send property set array entry pass element array,arrays not properly transmitted Sending a property set with an array as one of the entries only passes the last element of the array.
"Port HSC afw changesets to LSST We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 2c12255372bde846ba0429b5b542960e57d169f0, 0aec617e0ea604cde85105de3dade279a4fe10df: Footprint::overlapsMask  * 76f3706f6688b23d5b0c71e66af3e94095a9f821: copyWithinFootprint: respect image size  * f49676d7f1348f9de8ca21ee633e0c25473251ae: Implemented Linear and ZScale transformations, HSC-1206, 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04  * c369a8ad53962aba950f7710210be4b23f45a523: utils: make Mosaic.nImage a property  * Maybe 0a2647a4f57addc3b3adb347da995fa0d36b43cb: Add display.utils function to plot the bboxes of inputs to a coadd in ds9  * 386a4b71d974d9e5672fe8690d0db3e56c9fb40f: Box2?.getCorners  * f3d3029c561b957069cbf280b62ea8e37447c068: Calib: add operator*= for scaling zeropoint  * 6c1845474f28f528a95190eeb88f095b11999078: Check in #3092 (iterable coord) directly on master  * f3d3029c561b957069cbf280b62ea8e37447c068: Calib: add operator*= for scaling zeropoint  * 5d1934cc9fe7d8c43aa8f9318a1ac9a3ce85e94e, 0d1ab12db604d5e42a5d72f028411a64294283ce: Footprint merging  * b8578746d69920bc1e1089cca4b4acb230f0e8d5: Interpolate: add support for ndarray  * 3de3339aa075f869d73a5bc66fc65dbee8ae3d16: Fix unit test fallout from Interpolate std::vector change  * 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04, f73544e15abd2760bf84794798cf4b84e97e938d: Added xSize, ySize, and rescaleFactor arguments to {make,write}RGB; HSC-1207  * 88d838b74898d9572bbc8c46121da029958c1c72: rgb: fix makeRGB so it can replace saturated pixels and produce an image  * 254d7248ea20d98de481d968f6503d1610b16ae7: Remove tests of writing rgb images to jpeg and tiff  * 3252a40ad55222b882acf14d2f7cf0f3fe80f585: Added MatchControl and implemented it for matchXY and MatchRaDec  * 81c6063a32883b748f3770b7124d74ced7b480f5: Implement and test includeMismatches option in MatchControl object  * fd4c0baec617155fac0816607a5acba88e8970f5: Add support for renaming without replacing the full field in SchemaMapper  * 79337bb6d1ee3a0b73bcd2b2d0ca506a44d3fa56: Handle empty Footprints when normalising  ",4,DM-5280,datamanagement,port hsc afw changeset lsst identify dm-5162 changeset need port hsc lsst 2c12255372bde846ba0429b5b542960e57d169f0 0aec617e0ea604cde85105de3dade279a4fe10df footprint::overlapsmask 76f3706f6688b23d5b0c71e66af3e94095a9f821 copywithinfootprint respect image size f49676d7f1348f9de8ca21ee633e0c25473251ae implement linear zscale transformation hsc-1206 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04 c369a8ad53962aba950f7710210be4b23f45a523 util mosaic.nimage property maybe 0a2647a4f57addc3b3adb347da995fa0d36b43cb add display.util function plot bboxe input coadd ds9 386a4b71d974d9e5672fe8690d0db3e56c9fb40f box2?.getcorners f3d3029c561b957069cbf280b62ea8e37447c068 calib add operator*= scale zeropoint 6c1845474f28f528a95190eeb88f095b11999078 check 3092 iterable coord directly master f3d3029c561b957069cbf280b62ea8e37447c068 calib add operator*= scale zeropoint 5d1934cc9fe7d8c43aa8f9318a1ac9a3ce85e94e 0d1ab12db604d5e42a5d72f028411a64294283ce footprint merge b8578746d69920bc1e1089cca4b4acb230f0e8d5 interpolate add support ndarray 3de3339aa075f869d73a5bc66fc65dbee8ae3d16 fix unit test fallout interpolate std::vector change 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04 f73544e15abd2760bf84794798cf4b84e97e938d added xsize ysize rescalefactor argument write}rgb hsc-1207 88d838b74898d9572bbc8c46121da029958c1c72 rgb fix makergb replace saturated pixel produce image 254d7248ea20d98de481d968f6503d1610b16ae7 remove test write rgb image jpeg tiff 3252a40ad55222b882acf14d2f7cf0f3fe80f585 added matchcontrol implement matchxy matchradec 81c6063a32883b748f3770b7124d74ced7b480f5 implement test includemismatche option matchcontrol object fd4c0baec617155fac0816607a5acba88e8970f5 add support rename replace field schemamapper 79337bb6d1ee3a0b73bcd2b2d0ca506a44d3fa56 handle footprints normalise,"Port HSC afw changesets to LSST We identified in DM-5162 several changesets that still need to be ported from HSC to LSST: * 2c12255372bde846ba0429b5b542960e57d169f0, 0aec617e0ea604cde85105de3dade279a4fe10df: Footprint::overlapsMask * 76f3706f6688b23d5b0c71e66af3e94095a9f821: copyWithinFootprint: respect image size * f49676d7f1348f9de8ca21ee633e0c25473251ae: Implemented Linear and ZScale transformations, HSC-1206, 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04 * c369a8ad53962aba950f7710210be4b23f45a523: utils: make Mosaic.nImage a property * Maybe 0a2647a4f57addc3b3adb347da995fa0d36b43cb: Add display.utils function to plot the bboxes of inputs to a coadd in ds9 * 386a4b71d974d9e5672fe8690d0db3e56c9fb40f: Box2?.getCorners * f3d3029c561b957069cbf280b62ea8e37447c068: Calib: add operator*= for scaling zeropoint * 6c1845474f28f528a95190eeb88f095b11999078: Check in #3092 (iterable coord) directly on master * f3d3029c561b957069cbf280b62ea8e37447c068: Calib: add operator*= for scaling zeropoint * 5d1934cc9fe7d8c43aa8f9318a1ac9a3ce85e94e, 0d1ab12db604d5e42a5d72f028411a64294283ce: Footprint merging * b8578746d69920bc1e1089cca4b4acb230f0e8d5: Interpolate: add support for ndarray * 3de3339aa075f869d73a5bc66fc65dbee8ae3d16: Fix unit test fallout from Interpolate std::vector change * 08a7740ed756ee7b2c845b4ce6aed8d9a0f50d04, f73544e15abd2760bf84794798cf4b84e97e938d: Added xSize, ySize, and rescaleFactor arguments to {make,write}RGB; HSC-1207 * 88d838b74898d9572bbc8c46121da029958c1c72: rgb: fix makeRGB so it can replace saturated pixels and produce an image * 254d7248ea20d98de481d968f6503d1610b16ae7: Remove tests of writing rgb images to jpeg and tiff * 3252a40ad55222b882acf14d2f7cf0f3fe80f585: Added MatchControl and implemented it for matchXY and MatchRaDec * 81c6063a32883b748f3770b7124d74ced7b480f5: Implement and test includeMismatches option in MatchControl object * fd4c0baec617155fac0816607a5acba88e8970f5: Add support for renaming without replacing the full field in SchemaMapper * 79337bb6d1ee3a0b73bcd2b2d0ca506a44d3fa56: Handle empty Footprints when normalising"
"Port HSC skymap, shapelet changesets to LSST We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * skymap:  ** f83f71718eac5307d575d3113ee3757a63a16de2: Set vertex list in ExplicitTractInfo.    * shapelet:  ** bb928df3fc2fafe5183e0d075da19994f0af4fc7: Let the value to normalize to be specified in [Multi]ShapeletFunction  ",1,DM-5281,datamanagement,port hsc skymap shapelet changeset lsst identify dm-5162 changeset need port hsc lsst skymap set vertex list explicittractinfo shapelet bb928df3fc2fafe5183e0d075da19994f0af4fc7 let value normalize specify multi]shapeletfunction,"Port HSC skymap, shapelet changesets to LSST We identified in DM-5162 some changesets that still need to be ported from HSC to LSST: * skymap: ** f83f71718eac5307d575d3113ee3757a63a16de2: Set vertex list in ExplicitTractInfo. * shapelet: ** bb928df3fc2fafe5183e0d075da19994f0af4fc7: Let the value to normalize to be specified in [Multi]ShapeletFunction"
"Port HSC meas_algorithms changesets to LSST We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 1293a31c19c238ba2c2acd8f67ec1be742764b66: BinnedWcs  * 9f392b134502f6e4fbbd8759806b15f89a267e5a: detection: additional debugging plots for background  * ad74fe8595ec523d6269e36ec2db051534bf3e9a: Add initial.flags.pixel.interpolated.any to ObjectSizeStarSelectorConfig.badFlags  * 69f5db0eba69225cff917fa4c96a94dc8b765aa0, 4a0d59e191fc40d3091b56b20cf27ede4e0c23ab: Check for bad PSF measurements (HSC-1153)  * a54b1ac52678025d3317e8a379c2849d3eb567ba: pcaPsfDeterminer: catch case of no good PSF candidates in debugging  * c4fcab3251e6f41da2248d63fdf28c0bf80e30f8: Indent seems to be wrong for debug display  * 2a889c17d47c879dbb4345bafba6aed9869b5984, f3e42cc03ab8a4f1b28d9e0852619cbdbf3b7018: Make IdSpanCompar more deterministic  * f99eb46f484609673b45290eaaba47688d7b4a24: CR code has to take care of 'NO_DATA' mask  * 6f6b786bce8ca34bf4c67f75f965130dea027147: Handle small numbers of psfCandidates (HSC-1176)  * d744e6514feaf67b87068ac502bca677306f9fc2: tests: add test for MeasureApCorrTask  * 65f617089038fe19179fca4f959bf23ea061a6b8, 1b7e3cc48ed347b0afa31e81c821b38f87d18d64: Test case for measurement of negative objects    There are also a couple of issues that were identified in the DM-5162 review:  * Delete tests/config/MeasureSources.py --- mere configuration, old-style measurement  * testPsfDetermination has method 'xtestRejectBlends'",4,DM-5282,datamanagement,port hsc meas_algorithm changeset lsst identify dm-5162 changeset need port hsc lsst 1293a31c19c238ba2c2acd8f67ec1be742764b66 binnedwcs 9f392b134502f6e4fbbd8759806b15f89a267e5a detection additional debug plot background ad74fe8595ec523d6269e36ec2db051534bf3e9a add initial.flags.pixel.interpolated.any objectsizestarselectorconfig.badflags 69f5db0eba69225cff917fa4c96a94dc8b765aa0 4a0d59e191fc40d3091b56b20cf27ede4e0c23ab check bad psf measurement hsc-1153 a54b1ac52678025d3317e8a379c2849d3eb567ba pcapsfdeterminer catch case good psf candidate debug c4fcab3251e6f41da2248d63fdf28c0bf80e30f8 indent wrong debug display 2a889c17d47c879dbb4345bafba6aed9869b5984 f3e42cc03ab8a4f1b28d9e0852619cbdbf3b7018 idspancompar deterministic f99eb46f484609673b45290eaaba47688d7b4a24 cr code care no_data mask 6f6b786bce8ca34bf4c67f75f965130dea027147 handle small number psfcandidates hsc-1176 d744e6514feaf67b87068ac502bca677306f9fc2 test add test measureapcorrtask 65f617089038fe19179fca4f959bf23ea061a6b8 1b7e3cc48ed347b0afa31e81c821b38f87d18d64 test case measurement negative object couple issue identify dm-5162 review delete test config measuresources.py mere configuration old style measurement testpsfdetermination method xtestrejectblends,"Port HSC meas_algorithms changesets to LSST We identified in DM-5162 several changesets that still need to be ported from HSC to LSST: * 1293a31c19c238ba2c2acd8f67ec1be742764b66: BinnedWcs * 9f392b134502f6e4fbbd8759806b15f89a267e5a: detection: additional debugging plots for background * ad74fe8595ec523d6269e36ec2db051534bf3e9a: Add initial.flags.pixel.interpolated.any to ObjectSizeStarSelectorConfig.badFlags * 69f5db0eba69225cff917fa4c96a94dc8b765aa0, 4a0d59e191fc40d3091b56b20cf27ede4e0c23ab: Check for bad PSF measurements (HSC-1153) * a54b1ac52678025d3317e8a379c2849d3eb567ba: pcaPsfDeterminer: catch case of no good PSF candidates in debugging * c4fcab3251e6f41da2248d63fdf28c0bf80e30f8: Indent seems to be wrong for debug display * 2a889c17d47c879dbb4345bafba6aed9869b5984, f3e42cc03ab8a4f1b28d9e0852619cbdbf3b7018: Make IdSpanCompar more deterministic * f99eb46f484609673b45290eaaba47688d7b4a24: CR code has to take care of 'NO_DATA' mask * 6f6b786bce8ca34bf4c67f75f965130dea027147: Handle small numbers of psfCandidates (HSC-1176) * d744e6514feaf67b87068ac502bca677306f9fc2: tests: add test for MeasureApCorrTask * 65f617089038fe19179fca4f959bf23ea061a6b8, 1b7e3cc48ed347b0afa31e81c821b38f87d18d64: Test case for measurement of negative objects There are also a couple of issues that were identified in the DM-5162 review: * Delete tests/config/MeasureSources.py --- mere configuration, old-style measurement * testPsfDetermination has method 'xtestRejectBlends'"
"Port HSC daf_butlerUtils changesets to LSST We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * daee24edba01b01a0412df7f9b4cf70be5b10860: CameraMapper: allow a default filter name to be provided  * e3fee95d6a1850dd2309d3ebe4e3ef3ffe38eef0: CameraMapper: normalize path names, and remove leading double slash  * 476b6ddccd9d0cceb2b89ca34bee7d0fdcd70694: preserve timestamps in cameraMapper.backup()  * b2491ef60e5e23afa7d9f0297f257e694aa1af35: Only attempt to update Wcs if it's available  * 9f62bcce588fa9abc8e1e44ff2f0275e5230f629: Registry: hold registry cache for a single thread only (HSC-1035)  * 412f03b95b7a5e82003ab33a61bd43adbf465188: Registry: use a pool of registries to avoid having too many open files",2,DM-5283,datamanagement,port hsc daf_butlerutil changeset lsst identify dm-5162 changeset need port hsc lsst daee24edba01b01a0412df7f9b4cf70be5b10860 cameramapper allow default filter provide e3fee95d6a1850dd2309d3ebe4e3ef3ffe38eef0 cameramapper normalize path name remove lead double slash 476b6ddccd9d0cceb2b89ca34bee7d0fdcd70694 preserve timestamp cameramapper.backup b2491ef60e5e23afa7d9f0297f257e694aa1af35 attempt update wcs available 9f62bcce588fa9abc8e1e44ff2f0275e5230f629 registry hold registry cache single thread hsc-1035 412f03b95b7a5e82003ab33a61bd43adbf465188 registry use pool registry avoid have open file,"Port HSC daf_butlerUtils changesets to LSST We identified in DM-5162 several changesets that still need to be ported from HSC to LSST: * daee24edba01b01a0412df7f9b4cf70be5b10860: CameraMapper: allow a default filter name to be provided * e3fee95d6a1850dd2309d3ebe4e3ef3ffe38eef0: CameraMapper: normalize path names, and remove leading double slash * 476b6ddccd9d0cceb2b89ca34bee7d0fdcd70694: preserve timestamps in cameraMapper.backup() * b2491ef60e5e23afa7d9f0297f257e694aa1af35: Only attempt to update Wcs if it's available * 9f62bcce588fa9abc8e1e44ff2f0275e5230f629: Registry: hold registry cache for a single thread only (HSC-1035) * 412f03b95b7a5e82003ab33a61bd43adbf465188: Registry: use a pool of registries to avoid having too many open files"
"Port HSC meas_extensions_simpleShape package to LSST HSC uses a package, meas_extensions_simpleShape, which needs to be ported to LSST.  The package is used for basic shape measurements for determining focus, and also serves as a simple guide for writing measurement plugins.",3,DM-5284,datamanagement,port hsc meas_extensions_simpleshape package lsst hsc use package meas_extensions_simpleshape need port lsst package basic shape measurement determine focus serve simple guide write measurement plugin,"Port HSC meas_extensions_simpleShape package to LSST HSC uses a package, meas_extensions_simpleShape, which needs to be ported to LSST. The package is used for basic shape measurements for determining focus, and also serves as a simple guide for writing measurement plugins."
Port HSC meas_extensions_multiShapelet changesets to LSST We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST:    * bf5f753133ae4b41357f9789ff4763949ebb6ffb: FitPsf: reduce outerOrder to 1  * a54d6cbd41baf916fac2a1bb235a8502af14edfd: Provide explicit instantiations for the sake of clang 6.0 on os/x 10.9  * a53ac9e5cdb678a3f8ef633110d7d4cc5ac84f15: FitProfileAlgorithm: bail if the PSF flux failed  ,1,DM-5285,datamanagement,port hsc meas_extensions_multishapelet changeset lsst identify dm-5162 changeset need port hsc lsst bf5f753133ae4b41357f9789ff4763949ebb6ffb fitpsf reduce outerorder a54d6cbd41baf916fac2a1bb235a8502af14edfd provide explicit instantiation sake clang 6.0 os 10.9 a53ac9e5cdb678a3f8ef633110d7d4cc5ac84f15 fitprofilealgorithm bail psf flux fail,Port HSC meas_extensions_multiShapelet changesets to LSST We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST: * bf5f753133ae4b41357f9789ff4763949ebb6ffb: FitPsf: reduce outerOrder to 1 * a54d6cbd41baf916fac2a1bb235a8502af14edfd: Provide explicit instantiations for the sake of clang 6.0 on os/x 10.9 * a53ac9e5cdb678a3f8ef633110d7d4cc5ac84f15: FitProfileAlgorithm: bail if the PSF flux failed
"Port HSC meas_deblender changesets to LSST We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST:    * a8cf6c22df14494d6dcf2d7354c695cba9506301: Clarify tiny footprint limit  * 624790aa63a38fb7a328ebc21abfd1b10503aa26: config: change default strayFluxRule  * db7d705de93b43a5f32f771c716b1c5c7368d124: consolidate failed peak logic and downgrade warning    We also identified a few differences that should be resolved:  * clipStrayFluxFraction defaults to 0.01 for LSST, 0.001 for HSC  * Stray file, src/Baseline.cc.orig, on LSST side  ",1,DM-5286,datamanagement,port hsc meas_deblender changeset lsst identify dm-5162 changeset need port hsc lsst a8cf6c22df14494d6dcf2d7354c695cba9506301 clarify tiny footprint limit 624790aa63a38fb7a328ebc21abfd1b10503aa26 config change default strayfluxrule consolidate fail peak logic downgrade warn identify difference resolve clipstrayfluxfraction default 0.01 lsst 0.001 hsc stray file src baseline.cc.orig lsst,"Port HSC meas_deblender changesets to LSST We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST: * a8cf6c22df14494d6dcf2d7354c695cba9506301: Clarify tiny footprint limit * 624790aa63a38fb7a328ebc21abfd1b10503aa26: config: change default strayFluxRule * db7d705de93b43a5f32f771c716b1c5c7368d124: consolidate failed peak logic and downgrade warning We also identified a few differences that should be resolved: * clipStrayFluxFraction defaults to 0.01 for LSST, 0.001 for HSC * Stray file, src/Baseline.cc.orig, on LSST side"
Port HSC ip_isr changesets to LSST We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * f1cee734998f1faf86c02af42ea599b077847eeb: IsrTask: allow fallback to a different filter when loading calibrations  * 89cd629bb8e1a72a545176311b1ef659358d95af: saturationDetection: apply to overscan as well as image  ,1,DM-5287,datamanagement,port hsc ip_isr changeset lsst identify dm-5162 changeset need port hsc lsst f1cee734998f1faf86c02af42ea599b077847eeb isrtask allow fallback different filter load calibration 89cd629bb8e1a72a545176311b1ef659358d95af saturationdetection apply overscan image,Port HSC ip_isr changesets to LSST We identified in DM-5162 some changesets that still need to be ported from HSC to LSST: * f1cee734998f1faf86c02af42ea599b077847eeb: IsrTask: allow fallback to a different filter when loading calibrations * 89cd629bb8e1a72a545176311b1ef659358d95af: saturationDetection: apply to overscan as well as image
"Port HSC pipe_tasks changesets to LSST We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * 31ab5f02f7722650ad0a0eb4e2f7f8b3e0073366, 0c9a4a06bfb34ed26c72109131ef9f4a8c8f237a: multiBand: save background-subtracted coadd as deepCoadd_calexp  * e99e140feafe28e6f034143e8ee2ae58e9a9358d: Rejig interface for DetectCoaddSourcesTask to provide non-dataRef-centric API  * 829ee0cdd605ed027af1fada4446b715d9a5180d: multiband: activate sky objects  * MeasureMergedCoaddSources.doMatchSources defaults to False  * ProcessImageConfig.doWriteHeavyFootprintsInSources defaults to False ?  * 56666e8feba6893ac95fd4982d3e0daf6baf2d34: WcsSelectImagesTask: catch imagePoly is None    We also noticed some differences:    * * CalibrateConfig.setDefaults doesn't call parent  * CalibrateTask.run isn't returning apCorrMap  * reserveFraction=-1 instead of 0.2  ",3,DM-5288,datamanagement,port hsc pipe_task changeset lsst identify dm-5162 changeset need port hsc lsst 31ab5f02f7722650ad0a0eb4e2f7f8b3e0073366 0c9a4a06bfb34ed26c72109131ef9f4a8c8f237a save background subtract coadd deepcoadd_calexp e99e140feafe28e6f034143e8ee2ae58e9a9358d rejig interface detectcoaddsourcestask provide non dataref centric api 829ee0cdd605ed027af1fada4446b715d9a5180d multiband activate sky object measuremergedcoaddsources.domatchsource default false processimageconfig.dowriteheavyfootprintsinsource default false 56666e8feba6893ac95fd4982d3e0daf6baf2d34 wcsselectimagestask catch imagepoly notice difference calibrateconfig.setdefault parent calibratetask.run return reservefraction=-1 instead 0.2,"Port HSC pipe_tasks changesets to LSST We identified in DM-5162 some changesets that still need to be ported from HSC to LSST: * 31ab5f02f7722650ad0a0eb4e2f7f8b3e0073366, 0c9a4a06bfb34ed26c72109131ef9f4a8c8f237a: multiBand: save background-subtracted coadd as deepCoadd_calexp * e99e140feafe28e6f034143e8ee2ae58e9a9358d: Rejig interface for DetectCoaddSourcesTask to provide non-dataRef-centric API * 829ee0cdd605ed027af1fada4446b715d9a5180d: multiband: activate sky objects * MeasureMergedCoaddSources.doMatchSources defaults to False * ProcessImageConfig.doWriteHeavyFootprintsInSources defaults to False ? * 56666e8feba6893ac95fd4982d3e0daf6baf2d34: WcsSelectImagesTask: catch imagePoly is None We also noticed some differences: * * CalibrateConfig.setDefaults doesn't call parent * CalibrateTask.run isn't returning apCorrMap * reserveFraction=-1 instead of 0.2"
"Port HSC obs_subaru changesets to LSST We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters  * 69d35a890234e37c1142ddbeff43e62fe36e6c45: Set radius for flux.naive, adjust comment for flux.sinc  * 8ea54d10f5ae56f8b6f244bca76d5796ae015216: config: disable sigma clipping in coadd assembly  * 8d2f4a02d0d668fc82e853b633444d8e0fe80010: config: reduce coadd subregionSize  * e36bd1b4410812ca314f50c01f899d92acc0e7a5: config: set pixelScale for jacobian correction  * Remove processCcdOnsiteDb.py, processStack.py  * Rename stacker.py to coaddDriver.py or whatever Nate chooses in DM-3369  * 49e9f5dcf16490f6be6438b89b17911a0cd35fb2: Fixed obvious errors caused by introducing VignetteConfig  * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters  * daa43eeac46e8708de6f37feeb5d5d16a3caca11: HscMapper: set unit exposure time for dark  * 77ff7c89d56bed94bca4f320f839dbd20fbab641: Set BAD mask for dead amps instead of SAT    We also noticed the following need to be done:    * Forced photometry configuration (CCDs and Coadds)  * Sanitize config of OBS_SUBARU_DIR (use getPackageDir)  * multiband config files need ""root"" --> ""config""  * No astrometry in measureCoaddSources  * Narrow bands missing from priority list  * detectCoaddSources removed from multiband  * Move filterMap from config/processCcd.py into own file",5,DM-5289,datamanagement,port hsc obs_subaru changeset lsst identify dm-5162 changeset need port hsc lsst 8948917de4579e032c7bbb2c8316014446e3841b config add astrometry filter map hsc narrow band filter 69d35a890234e37c1142ddbeff43e62fe36e6c45 set radius flux.naive adjust comment flux.sinc 8ea54d10f5ae56f8b6f244bca76d5796ae015216 config disable sigma clip coadd assembly 8d2f4a02d0d668fc82e853b633444d8e0fe80010 config reduce coadd subregionsize e36bd1b4410812ca314f50c01f899d92acc0e7a5 config set pixelscale jacobian correction remove processccdonsitedb.py processstack.py rename stacker.py coadddriver.py nate choose dm-3369 49e9f5dcf16490f6be6438b89b17911a0cd35fb2 fix obvious error cause introduce vignetteconfig 8948917de4579e032c7bbb2c8316014446e3841b config add astrometry filter map hsc narrow band filter daa43eeac46e8708de6f37feeb5d5d16a3caca11 hscmapper set unit exposure time dark 77ff7c89d56bed94bca4f320f839dbd20fbab641 set bad mask dead amp instead sat notice follow need force photometry configuration ccds coadds sanitize config obs_subaru_dir use getpackagedir multiband config file need root config astrometry measurecoaddsource narrow band miss priority list detectcoaddsource remove multiband filtermap config processccd.py file,"Port HSC obs_subaru changesets to LSST We identified in DM-5162 several changesets that still need to be ported from HSC to LSST: * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters * 69d35a890234e37c1142ddbeff43e62fe36e6c45: Set radius for flux.naive, adjust comment for flux.sinc * 8ea54d10f5ae56f8b6f244bca76d5796ae015216: config: disable sigma clipping in coadd assembly * 8d2f4a02d0d668fc82e853b633444d8e0fe80010: config: reduce coadd subregionSize * e36bd1b4410812ca314f50c01f899d92acc0e7a5: config: set pixelScale for jacobian correction * Remove processCcdOnsiteDb.py, processStack.py * Rename stacker.py to coaddDriver.py or whatever Nate chooses in DM-3369 * 49e9f5dcf16490f6be6438b89b17911a0cd35fb2: Fixed obvious errors caused by introducing VignetteConfig * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters * daa43eeac46e8708de6f37feeb5d5d16a3caca11: HscMapper: set unit exposure time for dark * 77ff7c89d56bed94bca4f320f839dbd20fbab641: Set BAD mask for dead amps instead of SAT We also noticed the following need to be done: * Forced photometry configuration (CCDs and Coadds) * Sanitize config of OBS_SUBARU_DIR (use getPackageDir) * multiband config files need ""root"" --> ""config"" * No astrometry in measureCoaddSources * Narrow bands missing from priority list * detectCoaddSources removed from multiband * Move filterMap from config/processCcd.py into own file"
Add z-index for dialogs components Some of the outside modules that we have brought in have a z-index.  We need to make sure that our dialog components stay on top of them.,2,DM-5290,datamanagement,add index dialog component outside module bring index need sure dialog component stay,Add z-index for dialogs components Some of the outside modules that we have brought in have a z-index. We need to make sure that our dialog components stay on top of them.
"Docker-ready configuration system for LTD Keeper To deploy LTD Keeper in a Docker container (DM-5194), it’s best practice to handle all configurations through environment variables. In DM-4950, LTD Keeper was configured through files for test and dev/deployment profiles. What we should do is continue to allow hard-coded configurations for test and dev environments, but have a third fully fledged configuration environment that’s driven entirely by environment variables.    The environment variables should allow fine grained configuration (for example, to turn off calls to individual external services for testing).    This should also resolve how to deal with Google Container Engine/Kubernetes auth flow works with environment variables, config files, and profiles.",1,DM-5291,datamanagement,docker ready configuration system ltd keeper deploy ltd keeper docker container dm-5194 good practice handle configuration environment variable dm-4950 ltd keeper configure file test dev deployment profile continue allow hard code configuration test dev environment fully fledge configuration environment drive entirely environment variable environment variable allow fine grain configuration example turn call individual external service testing resolve deal google container engine kubernetes auth flow work environment variable config file profile,"Docker-ready configuration system for LTD Keeper To deploy LTD Keeper in a Docker container (DM-5194), it s best practice to handle all configurations through environment variables. In DM-4950, LTD Keeper was configured through files for test and dev/deployment profiles. What we should do is continue to allow hard-coded configurations for test and dev environments, but have a third fully fledged configuration environment that s driven entirely by environment variables. The environment variables should allow fine grained configuration (for example, to turn off calls to individual external services for testing). This should also resolve how to deal with Google Container Engine/Kubernetes auth flow works with environment variables, config files, and profiles."
"ImageDifferenceTask: Refactor Image DifferenceTask The original DM-3704 was to refactor all ImageDifference task. This issue was split into 3 tasks:  1) Split image difference task into two tasks (1) to generate an image difference, and (2) to run detection and measurement on it: processDiffim.py  2) Refactor the Image Difference portion  3) Refactor the processDiffim portion    This ticket refactors the new task that just generates and image difference. ",8,DM-5294,datamanagement,imagedifferencetask refactor image differencetask original dm-3704 refactor imagedifference task issue split task split image difference task task generate image difference run detection measurement processdiffim.py refactor image difference portion refactor processdiffim portion ticket refactor new task generate image difference,"ImageDifferenceTask: Refactor Image DifferenceTask The original DM-3704 was to refactor all ImageDifference task. This issue was split into 3 tasks: 1) Split image difference task into two tasks (1) to generate an image difference, and (2) to run detection and measurement on it: processDiffim.py 2) Refactor the Image Difference portion 3) Refactor the processDiffim portion This ticket refactors the new task that just generates and image difference."
"Make jointcal buildable under CI Once jointcal is part of the stack, we need to get it under continuous integration, buildable by Jenkins, etc. There is only one unittest in the package currently, but at least getting that test built and run will catch a number of basic problems.    This requires having its dependencies (CHOLMOD from SuiteSparse) under CI as well.    As part of this, it would be good to have at least one ""integration test"" that runs jointcal as part of processCcd, to catch problems that appear when that interface changes.",8,DM-5297,datamanagement,jointcal buildable ci jointcal stack need continuous integration buildable jenkins etc unitt package currently get test build run catch number basic problem require have dependency cholmod suitesparse ci good integration test run jointcal processccd catch problem appear interface change,"Make jointcal buildable under CI Once jointcal is part of the stack, we need to get it under continuous integration, buildable by Jenkins, etc. There is only one unittest in the package currently, but at least getting that test built and run will catch a number of basic problems. This requires having its dependencies (CHOLMOD from SuiteSparse) under CI as well. As part of this, it would be good to have at least one ""integration test"" that runs jointcal as part of processCcd, to catch problems that appear when that interface changes."
Document simple simulator Document the simple simulator produced in DM-4899.  This will also involve some refactoring and adding unit tests to make it usable by others in the group.,8,DM-5298,datamanagement,document simple simulator document simple simulator produce dm-4899 involve refactoring add unit test usable group,Document simple simulator Document the simple simulator produced in DM-4899. This will also involve some refactoring and adding unit tests to make it usable by others in the group.
manage jenkins core + plugin versions There have been a couple of issues that have arisen when deploying test instances vs updating an existing instance due to slight differences between plugin versions.  This would be avoided by putting all plugin versions under change control.    Including:  * The versions of all jenkins components need to be explicitly specified  * The stored job {{config.xml}}'s should be updated to reflect plugin version changes  * The hipchat notification configuration should be updated to fix breakage caused by the production core/plugin update earlier this week  ,5,DM-5302,datamanagement,manage jenkins core plugin version couple issue arise deploy test instance vs update exist instance slight difference plugin version avoid put plugin version change control include version jenkin component need explicitly specify store job config.xml update reflect plugin version change hipchat notification configuration update fix breakage cause production core plugin update early week,manage jenkins core + plugin versions There have been a couple of issues that have arisen when deploying test instances vs updating an existing instance due to slight differences between plugin versions. This would be avoided by putting all plugin versions under change control. Including: * The versions of all jenkins components need to be explicitly specified * The stored job {{config.xml}}'s should be updated to reflect plugin version changes * The hipchat notification configuration should be updated to fix breakage caused by the production core/plugin update earlier this week
Enable automated publication of qserv-dev release This would allow integration tests in CI not to break when some Qserv dependencies change. Indeed CI uses a Docker container which include qserv-dev to build current Qserv version.,4,DM-5311,datamanagement,enable automate publication qserv dev release allow integration test ci break qserv dependency change ci use docker container include qserv dev build current qserv version,Enable automated publication of qserv-dev release This would allow integration tests in CI not to break when some Qserv dependencies change. Indeed CI uses a Docker container which include qserv-dev to build current Qserv version.
"Additional vertical partitioning tests Test potential improvements in many-vertical-shards test (20,50) run-times with query optimizer settings.",5,DM-5312,datamanagement,"additional vertical partition test test potential improvement vertical shard test 20,50 run time query optimizer setting","Additional vertical partitioning tests Test potential improvements in many-vertical-shards test (20,50) run-times with query optimizer settings."
"Implement unique query-id generation There are currently two separate query IDs defined for queries in czar code:  - ""user query ID"" - defined in {{Czar::submitQuery()}}, used for constructing table names for result table and message table  - ""QMeta query ID"" - ID obtained from QMeta after registering the query (by {{UserQuerySelect::_qMetaRegister()}})    Currently user query ID is used by the rest of the czar code to track the processing of this query, QMeta ID is not used yet for anything except QMeta registration and updates.     QMeta ID will be used for async query identification and there is no actual reason to keep two IDs around, so we should replace user query ID with the QMeta-generated one everywhere. One minor issue is that currently message table name is built and table is locked before we register query in QMeta. Need to understand it and see if we can reverse that logic.",4,DM-5314,datamanagement,implement unique query id generation currently separate query id define query czar code user query id define czar::submitquery construct table name result table message table qmeta query id id obtain qmeta register query userqueryselect::_qmetaregister currently user query id rest czar code track processing query qmeta id qmeta registration update qmeta id async query identification actual reason id replace user query id qmeta generate minor issue currently message table build table lock register query qmeta need understand reverse logic,"Implement unique query-id generation There are currently two separate query IDs defined for queries in czar code: - ""user query ID"" - defined in {{Czar::submitQuery()}}, used for constructing table names for result table and message table - ""QMeta query ID"" - ID obtained from QMeta after registering the query (by {{UserQuerySelect::_qMetaRegister()}}) Currently user query ID is used by the rest of the czar code to track the processing of this query, QMeta ID is not used yet for anything except QMeta registration and updates. QMeta ID will be used for async query identification and there is no actual reason to keep two IDs around, so we should replace user query ID with the QMeta-generated one everywhere. One minor issue is that currently message table name is built and table is locked before we register query in QMeta. Need to understand it and see if we can reverse that logic."
"Begin exploratory TAP implementation within dbserv This is a quick coding foray, to try to shake loose unforseen implementation dependencies or speed-bumps with TAP integration.    Time-boxed at 4 points to fit into a single sprint with Brian's current resource loading -- this is intended to be only a clarifying start.",4,DM-5318,datamanagement,begin exploratory tap implementation dbserv quick code foray try shake loose unforseen implementation dependency speed bump tap integration time box point fit single sprint brian current resource loading intend clarifying start,"Begin exploratory TAP implementation within dbserv This is a quick coding foray, to try to shake loose unforseen implementation dependencies or speed-bumps with TAP integration. Time-boxed at 4 points to fit into a single sprint with Brian's current resource loading -- this is intended to be only a clarifying start."
Fix mariadb CI patch package is missing in docker container used by travis-CI.,1,DM-5319,datamanagement,fix mariadb ci patch package miss docker container travis ci,Fix mariadb CI patch package is missing in docker container used by travis-CI.
"Make Bright Object Masks compatible with all cameras Currently all of the logic that goes into using bright object masks falls into obs_subaru and pipe_tasks. This ticket should move parts (such as the bright object mask class) out of obs_subaru, into a camera agnostic location. The work should also duplicate relevant camera configurations and parameter overrides in the other camera packages. Bright object masks were originally introduced in DM-4831",2,DM-5320,datamanagement,bright object masks compatible camera currently logic go bright object mask fall obs_subaru pipe_task ticket part bright object mask class obs_subaru camera agnostic location work duplicate relevant camera configuration parameter override camera package bright object mask originally introduce dm-4831,"Make Bright Object Masks compatible with all cameras Currently all of the logic that goes into using bright object masks falls into obs_subaru and pipe_tasks. This ticket should move parts (such as the bright object mask class) out of obs_subaru, into a camera agnostic location. The work should also duplicate relevant camera configurations and parameter overrides in the other camera packages. Bright object masks were originally introduced in DM-4831"
"MeasureApCorrTask should use slot_CalibFlux as default ref flux {{MeasureApCorrTask}} uses ""base_CircularApertureFlux_17_0"" as its default reference flux. It should use ""slot_CalibFlux"" instead.    Also check obs_sdss packages for overrides that can be removed; obs_sdss certainly has one in {{config/processCcdTask.py}}",1,DM-5321,datamanagement,measureapcorrtask use slot_calibflux default ref flux measureapcorrtask use base_circularapertureflux_17_0 default reference flux use slot_calibflux instead check obs_sdss package override remove obs_sdss certainly config processccdtask.py,"MeasureApCorrTask should use slot_CalibFlux as default ref flux {{MeasureApCorrTask}} uses ""base_CircularApertureFlux_17_0"" as its default reference flux. It should use ""slot_CalibFlux"" instead. Also check obs_sdss packages for overrides that can be removed; obs_sdss certainly has one in {{config/processCcdTask.py}}"
Remove any redundant or unused datasets Please remove any redundant or unused dataset names from policy files throughout the stack.,1,DM-5322,datamanagement,remove redundant unused dataset remove redundant unused dataset name policy file stack,Remove any redundant or unused datasets Please remove any redundant or unused dataset names from policy files throughout the stack.
estimateBackground should not make a deep copy of the exposure Implement RFC-155: change {{estimateBackground}} as follows:  - Always subtract the background  - Modify the exposure in place  - Replace {{estimateBackground}} with the run method of a new task {{SubtractBackgroundTask}}  - Replace {{getBackground}} (which fits a background) with {{SubtractBackgroundTask.fitBackground}},4,DM-5323,datamanagement,estimatebackground deep copy exposure implement rfc-155 change estimatebackground follow subtract background modify exposure place replace estimatebackground run method new task subtractbackgroundtask replace getbackground fit background subtractbackgroundtask.fitbackground,estimateBackground should not make a deep copy of the exposure Implement RFC-155: change {{estimateBackground}} as follows: - Always subtract the background - Modify the exposure in place - Replace {{estimateBackground}} with the run method of a new task {{SubtractBackgroundTask}} - Replace {{getBackground}} (which fits a background) with {{SubtractBackgroundTask.fitBackground}}
"Add ExposureIdInfo class Implement RFC-146: add ExposureIdInfo class to daf_butlerUtils    This will be implemented in daf_butlerUtils as part of DM-4692, with a unit test in obs_test because daf_butlerUtils has no camera mapper or camera repo in its test directory.",4,DM-5330,datamanagement,add exposureidinfo class implement rfc-146 add exposureidinfo class daf_butlerutil implement daf_butlerutil dm-4692 unit test obs_test daf_butlerutil camera mapper camera repo test directory,"Add ExposureIdInfo class Implement RFC-146: add ExposureIdInfo class to daf_butlerUtils This will be implemented in daf_butlerUtils as part of DM-4692, with a unit test in obs_test because daf_butlerUtils has no camera mapper or camera repo in its test directory."
Add usesMatches to star selectors Implement RFC-126 add usesMatches to star selectors    This will be implemented as part of DM-4692,4,DM-5331,datamanagement,add usesmatche star selector implement rfc-126 add usesmatche star selector implement dm-4692,Add usesMatches to star selectors Implement RFC-126 add usesMatches to star selectors This will be implemented as part of DM-4692
"GWT Conversion: Table results container Create a result container for table data.  This task is composed of:  - create actions, action creators and reducing functions  - dynamically add/remove table from view  - support expanded mode  - TabPanel support for deleting tabs.",6,DM-5334,datamanagement,gwt conversion table result container create result container table datum task compose create action action creator reduce function dynamically add remove table view support expand mode tabpanel support delete tab,"GWT Conversion: Table results container Create a result container for table data. This task is composed of: - create actions, action creators and reducing functions - dynamically add/remove table from view - support expanded mode - TabPanel support for deleting tabs."
"Fix minor issues in docker procedure - params.sh was missing at configuration  - startup.py wasn't importing correctly module ""utils""  - remove unused parameters in params.sh",1,DM-5336,datamanagement,fix minor issue docker procedure params.sh miss configuration startup.py import correctly module util remove unused parameter params.sh,"Fix minor issues in docker procedure - params.sh was missing at configuration - startup.py wasn't importing correctly module ""utils"" - remove unused parameters in params.sh"
"Planning for GPFS, etc. * Gathered filesize statistics from existing NFS for planning GPFS  * Assisted with GPFS client setup on test servers  * Reviewed infrastructure changes for Jason",2,DM-5337,datamanagement,plan gpfs etc gather filesize statistic exist nfs plan gpfs assist gpfs client setup test server review infrastructure change jason,"Planning for GPFS, etc. * Gathered filesize statistics from existing NFS for planning GPFS * Assisted with GPFS client setup on test servers * Reviewed infrastructure changes for Jason"
"Week end 2/07/16 Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 7, 2016.",5,DM-5338,datamanagement,week end 2/07/16 support lsst dev cluster openstack account week end february 2016,"Week end 2/07/16 Support for lsst-dev cluster, OpenStack, and accounts for week ending February 7, 2016."
"Week end 2/14/16 Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 14, 2016.",6,DM-5339,datamanagement,week end 2/14/16 support lsst dev cluster openstack account week end february 14 2016,"Week end 2/14/16 Support for lsst-dev cluster, OpenStack, and accounts for week ending February 14, 2016."
"Week end 2/21/16 Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 21, 2016.",2,DM-5340,datamanagement,week end 2/21/16 support lsst dev cluster openstack account week end february 21 2016,"Week end 2/21/16 Support for lsst-dev cluster, OpenStack, and accounts for week ending February 21, 2016."
"Week end 2/28/16 Support for lsst-dev cluster, OpenStack, and accounts  for week ending February 28, 2016.  ",4,DM-5341,datamanagement,week end 2/28/16 support lsst dev cluster openstack account week end february 28 2016,"Week end 2/28/16 Support for lsst-dev cluster, OpenStack, and accounts for week ending February 28, 2016."
"Jason Feb Tasks Procurement activities to prepare ""procurement plan activity 1"". PDU, rack, network selection and review. Refresh quotes for compute, storage, rack, pdu, electrical. Refresh, finalize, present (internally) and review design for FY16 infrastructure.    Draft and review of Procurement Plan Activity 1 document.",6,DM-5342,datamanagement,jason feb tasks procurement activity prepare procurement plan activity pdu rack network selection review refresh quote compute storage rack pdu electrical refresh finalize present internally review design fy16 infrastructure draft review procurement plan activity document,"Jason Feb Tasks Procurement activities to prepare ""procurement plan activity 1"". PDU, rack, network selection and review. Refresh quotes for compute, storage, rack, pdu, electrical. Refresh, finalize, present (internally) and review design for FY16 infrastructure. Draft and review of Procurement Plan Activity 1 document."
"New equipment setup and configuration (week end 2/07/16) * Updated lsst-dev7 with few missing pieces after initial user testing  * Setup 3 of 8 lsst-test servers  * Confirmed IPMI setup on new test servers (working with Dell on issue with 1 iDRAC license upgrade)  ** Completed and verified IPMI setups  *** Installed licenses for lsst-test1 - lsst-test6  *** re-associated IPMI lsst-test1m last-tsst6m with the correct systems.  *** Installed CentOS7 on lsst-test1 - lsst-test6. (in progress)  * UPS setup  ** Setup table with location of systems in 3003 racks  ** Setup apcusbd on lsst-stor141, lsst-stor142, lsst-stor143, lsst-stor144, lsst20, lsst13",3,DM-5343,datamanagement,new equipment setup configuration week end 2/07/16 update lsst dev7 miss piece initial user testing setup lsst test server confirmed ipmi setup new test server work dell issue idrac license upgrade complete verify ipmi setup instal license lsst test1 lsst test6 associate ipmi lsst test1 tsst6 correct system installed centos7 lsst test1 lsst test6 progress ups setup setup table location system 3003 rack setup apcusbd lsst stor141 lsst stor142 lsst stor143 lsst stor144 lsst20 lsst13,"New equipment setup and configuration (week end 2/07/16) * Updated lsst-dev7 with few missing pieces after initial user testing * Setup 3 of 8 lsst-test servers * Confirmed IPMI setup on new test servers (working with Dell on issue with 1 iDRAC license upgrade) ** Completed and verified IPMI setups *** Installed licenses for lsst-test1 - lsst-test6 *** re-associated IPMI lsst-test1m last-tsst6m with the correct systems. *** Installed CentOS7 on lsst-test1 - lsst-test6. (in progress) * UPS setup ** Setup table with location of systems in 3003 racks ** Setup apcusbd on lsst-stor141, lsst-stor142, lsst-stor143, lsst-stor144, lsst20, lsst13"
"New equipment setup and configuration (week end 2/14/16) * Still pushing at Dell to fix broken iDRAC license  * Added 5 systems to RSA OTP system  * Completed the setup of lsst-test1, lsst-test4, lsst-test5, lsst-test6  ** Reinstalled lsst-test1 to correct error in puppet install, Completed CentOS7 install, Installed puppet and ran puppet  ** Corrected network error on lsst-test4, Completed CentOS7 install, Installed puppet and ran puppet  ** Completed the setup of Installed lsst-test5, Completed CentOS7 install, Installed puppet and ran puppet  ** Installed lsst-test6, Completed CentOS7 install, Installed puppet and ran puppet",1,DM-5345,datamanagement,new equipment setup configuration week end 2/14/16 push dell fix break idrac license add system rsa otp system complete setup lsst test1 lsst test4 lsst test5 lsst test6 reinstall lsst test1 correct error puppet install complete centos7 install instal puppet run puppet correct network error lsst test4 completed centos7 install instal puppet run puppet complete setup installed lsst test5 complete centos7 install instal puppet run puppet instal lsst test6 completed centos7 install instal puppet run puppet,"New equipment setup and configuration (week end 2/14/16) * Still pushing at Dell to fix broken iDRAC license * Added 5 systems to RSA OTP system * Completed the setup of lsst-test1, lsst-test4, lsst-test5, lsst-test6 ** Reinstalled lsst-test1 to correct error in puppet install, Completed CentOS7 install, Installed puppet and ran puppet ** Corrected network error on lsst-test4, Completed CentOS7 install, Installed puppet and ran puppet ** Completed the setup of Installed lsst-test5, Completed CentOS7 install, Installed puppet and ran puppet ** Installed lsst-test6, Completed CentOS7 install, Installed puppet and ran puppet"
"Add tests for recent improvements to CModel In DM-4768 we ported a number of improvements to CModel from HSC. However, these were not accompanied by test cases. Please add them.",3,DM-5347,datamanagement,add test recent improvement cmodel dm-4768 port number improvement cmodel hsc accompany test case add,"Add tests for recent improvements to CModel In DM-4768 we ported a number of improvements to CModel from HSC. However, these were not accompanied by test cases. Please add them."
"Get rid of ProcessCcdSdssTask and ProcessCcdDecamTask Update {{ProcessCcdTask}} so that it can be used with different datasete types as appropriate for the ISR task. This will allow us to get rid of obs-specific variants {{ProcessCcdSdssTask}} and {{ProcessCcdDecamTask}}    The plan is to change {{ProcessCcdTask}} as follows:  - set {{doMakeDataRefList=False}} in the call to {{add_id_argument}}  - get the dataset type from the ISR task (default to ""raw"") and set it in data container  - make the dataRef list by calling {{makeDataRefList}} on the data container    Question for DECam folks: do you want two executable scripts for DECam (one that processes data from the community pipeline and one that performs ISR)? Or do you prefer one exectutable (in which case you switch between performing ISR and reading the output of the community pipeline output by retargeting the ISR task)? If you prefer one binary, then which should be the default: perform ISR or read the output of the community pipeline?",2,DM-5348,datamanagement,rid processccdsdsstask update processccdtask different datasete type appropriate isr task allow rid obs specific variant processccdsdsstask processccddecamtask plan change processccdtask follow set domakedatareflist false add_id_argument dataset type isr task default raw set datum container dataref list call makedatareflist datum container question decam folk want executable script decam process datum community pipeline perform isr prefer exectutable case switch perform isr read output community pipeline output retargete isr task prefer binary default perform isr read output community pipeline,"Get rid of ProcessCcdSdssTask and ProcessCcdDecamTask Update {{ProcessCcdTask}} so that it can be used with different datasete types as appropriate for the ISR task. This will allow us to get rid of obs-specific variants {{ProcessCcdSdssTask}} and {{ProcessCcdDecamTask}} The plan is to change {{ProcessCcdTask}} as follows: - set {{doMakeDataRefList=False}} in the call to {{add_id_argument}} - get the dataset type from the ISR task (default to ""raw"") and set it in data container - make the dataRef list by calling {{makeDataRefList}} on the data container Question for DECam folks: do you want two executable scripts for DECam (one that processes data from the community pipeline and one that performs ISR)? Or do you prefer one exectutable (in which case you switch between performing ISR and reading the output of the community pipeline output by retargeting the ISR task)? If you prefer one binary, then which should be the default: perform ISR or read the output of the community pipeline?"
"Revise LSE-140 to account for recent changes to calibration instrumentation Produce a revision of LSE-140, the DM - to - auxiliary instrumentation ICD, taking into account recent changes to the calibration instrumentation.",5,DM-5349,datamanagement,revise account recent change calibration instrumentation produce revision lse-140 dm auxiliary instrumentation icd take account recent change calibration instrumentation,"Revise LSE-140 to account for recent changes to calibration instrumentation Produce a revision of LSE-140, the DM - to - auxiliary instrumentation ICD, taking into account recent changes to the calibration instrumentation."
"Establish goals and create EA framework for LSE-140 update Deliverable: together with [~pingraham], identify the changes needed and develop initial content in EA.",2,DM-5350,datamanagement,establish goal create ea framework lse-140 update deliverable ~pingraham identify change need develop initial content ea,"Establish goals and create EA framework for LSE-140 update Deliverable: together with [~pingraham], identify the changes needed and develop initial content in EA."
Upgrade minuit2 Minuit2 5.34.14 came out in 2014. The current version in the stack is 5.28 from 2010. Minuit2 is annotated on the DM third party software page as being approved for 6-monthly uprev. Minuit2 is only used by AFW.    Release notes for 5.34.14:    * Several fixes and improvements have been put between this verion and the previous stand-alone one (5.28). Main new features is now the support for using the {{ROOT::Math::Minimizer}} interface via the class {{ROOT::Math::Minuit2Minimizer}} also in the standalone version. A new test has been added ({{test/MnSim/demoMinimizer.cxx}}) to show this new functionality  * Other major improvements is in the control of the error messages. One can now use the class {{MnPrint::SetLevel(int value)}} to control the output level. The same can be achieved by calling {{ROOT::Math::Minuit2Minimizer::SetPrintLevel}}.,1,DM-5353,datamanagement,upgrade minuit2 minuit2 5.34.14 come 2014 current version stack 5.28 2010 minuit2 annotate dm party software page approve monthly uprev minuit2 afw release note 5.34.14 fix improvement verion previous stand 5.28 main new feature support root::math::minimizer interface class root::math::minuit2minimizer standalone version new test add test mnsim demominimizer.cxx new functionality major improvement control error message use class mnprint::setlevel(int value control output level achieve call root::math::minuit2minimizer::setprintlevel,Upgrade minuit2 Minuit2 5.34.14 came out in 2014. The current version in the stack is 5.28 from 2010. Minuit2 is annotated on the DM third party software page as being approved for 6-monthly uprev. Minuit2 is only used by AFW. Release notes for 5.34.14: * Several fixes and improvements have been put between this verion and the previous stand-alone one (5.28). Main new features is now the support for using the {{ROOT::Math::Minimizer}} interface via the class {{ROOT::Math::Minuit2Minimizer}} also in the standalone version. A new test has been added ({{test/MnSim/demoMinimizer.cxx}}) to show this new functionality * Other major improvements is in the control of the error messages. One can now use the class {{MnPrint::SetLevel(int value)}} to control the output level. The same can be achieved by calling {{ROOT::Math::Minuit2Minimizer::SetPrintLevel}}.
meas_algorithms uses packages that are not listed in table file {{meas_algorithms}} directly uses the following packages not expressed in the table file:  * Minuit2  * daf_persistence  * daf_base  * pex_config  * pex_exceptions  * pex_policy  ,1,DM-5355,datamanagement,meas_algorithm use package list table file meas_algorithms directly use follow package express table file minuit2 daf_persistence daf_base pex_config pex_exception pex_policy,meas_algorithms uses packages that are not listed in table file {{meas_algorithms}} directly uses the following packages not expressed in the table file: * Minuit2 * daf_persistence * daf_base * pex_config * pex_exceptions * pex_policy
"Test consistency of Shear Measurements with different Psfs DM-1136 was done with a single Psf, partly to avoid some of the problems we found with PsfShapeletApprox.  In this issue, I will look at consistency of the measurement for different Psfs.",8,DM-5356,datamanagement,test consistency shear measurements different psfs dm-1136 single psf partly avoid problem find psfshapeletapprox issue look consistency measurement different psfs,"Test consistency of Shear Measurements with different Psfs DM-1136 was done with a single Psf, partly to avoid some of the problems we found with PsfShapeletApprox. In this issue, I will look at consistency of the measurement for different Psfs."
Test error estimation with bootstrap resampling Test the error estimation code using bootstrap resampling.,6,DM-5357,datamanagement,test error estimation bootstrap resample test error estimation code bootstrap resampling,Test error estimation with bootstrap resampling Test the error estimation code using bootstrap resampling.
Move supertask code our from pipe_base Create a new package {{pipe_supertask}} and move all supertask code and activator there. Will soon create a poll to pick a better name.,3,DM-5358,datamanagement,supertask code pipe_base create new package pipe_supertask supertask code activator soon create poll pick well,Move supertask code our from pipe_base Create a new package {{pipe_supertask}} and move all supertask code and activator there. Will soon create a poll to pick a better name.
"Update DMTN-002 to reflect last changes Need to update documentation with latest changes on {{pipe_base}}, {{pipe_supertask}} and {{pipe_flow}}",1,DM-5359,datamanagement,update dmtn-002 reflect change need update documentation late change pipe_base pipe_supertask pipe_flow,"Update DMTN-002 to reflect last changes Need to update documentation with latest changes on {{pipe_base}}, {{pipe_supertask}} and {{pipe_flow}}"
Update {{pipe_flow}} Update {{pipe_flow}} to change dependencies and examples to reflect migration to {{pipe_supertask}},1,DM-5360,datamanagement,update pipe_flow update pipe_flow change dependency example reflect migration pipe_supertask,Update {{pipe_flow}} Update {{pipe_flow}} to change dependencies and examples to reflect migration to {{pipe_supertask}}
Image Select Panel: Support add or modify of plot previously the image select panel would only modify a plot.  Now give it the ability to add a plot.,8,DM-5364,datamanagement,image select panel support add modify plot previously image select panel modify plot ability add plot,Image Select Panel: Support add or modify of plot previously the image select panel would only modify a plot. Now give it the ability to add a plot.
"Enable CC-IN2P3/Qserv team communication in order to prepare for Pan-STARRS large scale tests The goal of this ticket is to enable communication between CC-IN2P3 and Qserv team in order to prepare for Pan-STARRS data ingestion into Qserv. This data ingestion step is necessary for the large scale tests of Qserv foreseen for summer 2016.    Specifically, we need to understand:    - What is the size of the data set to be imported to CC-IN2P3?  - Where the Pan-STARRS data set to be imported is currently located?  - What mechanisms will the host of Pan-STARRS data make available to CC-IN2P3 for downloading the data set?  - Does the envisaged ingestion mechanism into Qserv requires that the data transit through the Qserv master server or will each Qserv worker be able to ingest its own chunk of data?  - After the ingestion process is finished, do we need to keep a copy of the ingested data out of Qserv?      Given the size of the dataset likely involved in this process, this project will probably require that we (both Qserv and CC-IN2P3 experts) set up specific mechanisms and equipment for efficient transport, storage and ingestion of these data. Timely planning and several testing campaigns seem necessary for this project to make progress.      ",8,DM-5365,datamanagement,enable cc in2p3 qserv team communication order prepare pan starrs large scale test goal ticket enable communication cc in2p3 qserv team order prepare pan starrs datum ingestion qserv datum ingestion step necessary large scale test qserv foresee summer 2016 specifically need understand size datum set import cc in2p3 pan starrs datum set import currently locate mechanism host pan starrs datum available cc in2p3 download datum set envisage ingestion mechanism qserv require datum transit qserv master server qserv worker able ingest chunk datum ingestion process finish need copy ingest datum qserv give size dataset likely involve process project probably require qserv cc in2p3 expert set specific mechanism equipment efficient transport storage ingestion datum timely planning testing campaign necessary project progress,"Enable CC-IN2P3/Qserv team communication in order to prepare for Pan-STARRS large scale tests The goal of this ticket is to enable communication between CC-IN2P3 and Qserv team in order to prepare for Pan-STARRS data ingestion into Qserv. This data ingestion step is necessary for the large scale tests of Qserv foreseen for summer 2016. Specifically, we need to understand: - What is the size of the data set to be imported to CC-IN2P3? - Where the Pan-STARRS data set to be imported is currently located? - What mechanisms will the host of Pan-STARRS data make available to CC-IN2P3 for downloading the data set? - Does the envisaged ingestion mechanism into Qserv requires that the data transit through the Qserv master server or will each Qserv worker be able to ingest its own chunk of data? - After the ingestion process is finished, do we need to keep a copy of the ingested data out of Qserv? Given the size of the dataset likely involved in this process, this project will probably require that we (both Qserv and CC-IN2P3 experts) set up specific mechanisms and equipment for efficient transport, storage and ingestion of these data. Timely planning and several testing campaigns seem necessary for this project to make progress."
"Change default value of MeasApCorrConfig.refFluxName to slot_CalibFlux The default value of {{MeasApCorrConfig.refFluxName}} is presently ""base_CircularApertureFlux_17_0"". This should be changed to ""slot_CalibFlux"". That is what the slot is intended for. The slot usually points to ""base_CircularApertureFlux_17_0"", but {{obs_sdss}}, at least, overrides this.    Additional jobs:  - Update {{obs_sdss}} {{config/processCcd.py}} to remove the override for this value, since it will no longer be needed.  - Check for and remove unnecessary overrides in other obs_ packages",1,DM-5367,datamanagement,change default value measapcorrconfig.reffluxname slot_calibflux default value measapcorrconfig.reffluxname presently base_circularapertureflux_17_0 change slot_calibflux slot intend slot usually point base_circularapertureflux_17_0 obs_sdss override additional job update obs_sdss config processccd.py remove override value long need check remove unnecessary override ob package,"Change default value of MeasApCorrConfig.refFluxName to slot_CalibFlux The default value of {{MeasApCorrConfig.refFluxName}} is presently ""base_CircularApertureFlux_17_0"". This should be changed to ""slot_CalibFlux"". That is what the slot is intended for. The slot usually points to ""base_CircularApertureFlux_17_0"", but {{obs_sdss}}, at least, overrides this. Additional jobs: - Update {{obs_sdss}} {{config/processCcd.py}} to remove the override for this value, since it will no longer be needed. - Check for and remove unnecessary overrides in other obs_ packages"
Use modern TAP package declarations for all EUPS third party packages In DM-4670 the TAP-ness of the packages was declared using a {{.tap_package}} file. The modern fix is to use a {{$TAP_PACKAGE}} environment variable in the {{eupspkg.cfg.sh}} file. This is how {{pyyaml}} was implemented.,1,DM-5368,datamanagement,use modern tap package declaration eups party package dm-4670 tap ness package declare .tap_package file modern fix use tap_package environment variable eupspkg.cfg.sh file pyyaml implement,Use modern TAP package declarations for all EUPS third party packages In DM-4670 the TAP-ness of the packages was declared using a {{.tap_package}} file. The modern fix is to use a {{$TAP_PACKAGE}} environment variable in the {{eupspkg.cfg.sh}} file. This is how {{pyyaml}} was implemented.
"Create {{lsst_ci}} package as a continuous integration build target Create an {{lsst_ci}} package to be built for the continuous integration testing.    Plan:  1. Create empty package that has dependencies on {{obs_cfht}}, {{obs_decam}}, {{obs_subaru}}, {{testdata_cfht}}, {{testdata_decam}}, {{testdata_subaru}}. (/)  2.  Ensure above builds. (/)  3.  Add {{obs_lsstSim}} and ensure that it builds. (/)    The following were moved to DM-5381:  [ [~tjenness] : How can I get strikethrough to work in the following list?]  3. Add dependencies on {{validation_data_cfht}} and {{validation_data_decam}}, and {{validate_drp}}.  4. Run CFHT, DECam quick examples in {{validate_drp}}.  5. Test for successful running of the above examples.  Fail and trigger Jenkins FAILURE message if these examples fail.  6. Check performance of CFHT, DECam runs against reference numbers.  Fail if there is a significant regression.  7. Decide how to include {{ci_hsc}}, which currently can take at least 30 minutes to process the image data.--",1,DM-5370,datamanagement,create lsst_ci package continuous integration build target create lsst_ci package build continuous integration testing plan create package dependency obs_cfht obs_decam obs_subaru testdata_cfht testdata_decam testdata_subaru ensure build add obs_lsstsim ensure build follow move dm-5381 ~tjenness strikethrough work follow list add dependency validation_data_cfht validation_data_decam validate_drp run cfht decam quick example validate_drp test successful running example fail trigger jenkins failure message example fail check performance cfht decam run reference number fail significant regression decide include ci_hsc currently 30 minute process image data.--,"Create {{lsst_ci}} package as a continuous integration build target Create an {{lsst_ci}} package to be built for the continuous integration testing. Plan: 1. Create empty package that has dependencies on {{obs_cfht}}, {{obs_decam}}, {{obs_subaru}}, {{testdata_cfht}}, {{testdata_decam}}, {{testdata_subaru}}. (/) 2. Ensure above builds. (/) 3. Add {{obs_lsstSim}} and ensure that it builds. (/) The following were moved to DM-5381: [ [~tjenness] : How can I get strikethrough to work in the following list?] 3. Add dependencies on {{validation_data_cfht}} and {{validation_data_decam}}, and {{validate_drp}}. 4. Run CFHT, DECam quick examples in {{validate_drp}}. 5. Test for successful running of the above examples. Fail and trigger Jenkins FAILURE message if these examples fail. 6. Check performance of CFHT, DECam runs against reference numbers. Fail if there is a significant regression. 7. Decide how to include {{ci_hsc}}, which currently can take at least 30 minutes to process the image data.--"
"Fix obs_* packages and ci tests broken by DM-4683 The butler changes in DM-4683, in particular the removal of {{.mapper}} from the interface exposed by a {{Butler}} object, broken {{obs_cfht}}, {{obs_decam}}, and {{ci_hsc}}.    This issue will fix those changes, and search for additional broken things.    This work is proceeding in conjunction with DM-5370 to test that the CI system, e.g. {{lsst_ci}}, is sensitive to these breakages and fixes.",1,DM-5372,datamanagement,fix obs package ci test break dm-4683 butler change dm-4683 particular removal .mapper interface expose butler object break obs_cfht obs_decam ci_hsc issue fix change search additional broken thing work proceed conjunction dm-5370 test ci system e.g. lsst_ci sensitive breakage fix,"Fix obs_* packages and ci tests broken by DM-4683 The butler changes in DM-4683, in particular the removal of {{.mapper}} from the interface exposed by a {{Butler}} object, broken {{obs_cfht}}, {{obs_decam}}, and {{ci_hsc}}. This issue will fix those changes, and search for additional broken things. This work is proceeding in conjunction with DM-5370 to test that the CI system, e.g. {{lsst_ci}}, is sensitive to these breakages and fixes."
"Add to baseline a dedicated replica of L1 database just for scans Per RFC-133, users will sometimes need to do full table scan through L1 catalogs, and our baseline does not allow for full scans on the L1 catalog. It'd be good to maintain a replica of L1 for such scans. This story involves changing LDM-141 and adding hardware for the replica. ",4,DM-5374,datamanagement,add baseline dedicated replica l1 database scan rfc-133 user need table scan l1 catalog baseline allow scan l1 catalog good maintain replica l1 scan story involve change ldm-141 add hardware replica,"Add to baseline a dedicated replica of L1 database just for scans Per RFC-133, users will sometimes need to do full table scan through L1 catalogs, and our baseline does not allow for full scans on the L1 catalog. It'd be good to maintain a replica of L1 for such scans. This story involves changing LDM-141 and adding hardware for the replica."
Remotely attend JTM 2016 sessions SSIA.  The final hours of the final day were very valuable.,6,DM-5379,datamanagement,remotely attend jtm 2016 session ssia final hour final day valuable,Remotely attend JTM 2016 sessions SSIA. The final hours of the final day were very valuable.
"Create {{lsst_qa}} package as a daily build target for regression testing 1. Add dependencies on {{validation_data_cfht}} and {{validation_data_decam}}, and {{validation_data_hsc}}.  (/)  2. Add dependency on {{validate_drp}}.  (/)  3. Run CFHT, DECam quick examples in {{validate_drp}}.  (/)  4. Test for successful running of the above examples.  (/)  5. Implement in a testing framework.  6. Check performance of CFHT, DECam runs against reference numbers. Fail if there is a significant regression.  (/)  7. Include {{ci_hsc}}, which currently takes 1000 seconds  to process the image data.  8. Check performance of HSC runs against reference numbers.  Fail if there is a significant regression.",4,DM-5381,datamanagement,create lsst_qa package daily build target regression testing add dependency validation_data_cfht validation_data_decam validation_data_hsc add dependency validate_drp run cfht decam quick example validate_drp test successful running example implement testing framework check performance cfht decam run reference number fail significant regression include ci_hsc currently take 1000 second process image datum check performance hsc run reference number fail significant regression,"Create {{lsst_qa}} package as a daily build target for regression testing 1. Add dependencies on {{validation_data_cfht}} and {{validation_data_decam}}, and {{validation_data_hsc}}. (/) 2. Add dependency on {{validate_drp}}. (/) 3. Run CFHT, DECam quick examples in {{validate_drp}}. (/) 4. Test for successful running of the above examples. (/) 5. Implement in a testing framework. 6. Check performance of CFHT, DECam runs against reference numbers. Fail if there is a significant regression. (/) 7. Include {{ci_hsc}}, which currently takes 1000 seconds to process the image data. 8. Check performance of HSC runs against reference numbers. Fail if there is a significant regression."
"Port SdssShape changes from HSC meas_algorithms to LSST meas_base In porting {{meas_algorithm}} changes from HSC to LSST, modifications to the {{SdssShape}} algorithm were discovered. These changes should be transferred to LSST.",3,DM-5384,datamanagement,port sdssshape change hsc meas_algorithm lsst meas_base porting meas_algorithm change hsc lsst modification sdssshape algorithm discover change transfer lsst,"Port SdssShape changes from HSC meas_algorithms to LSST meas_base In porting {{meas_algorithm}} changes from HSC to LSST, modifications to the {{SdssShape}} algorithm were discovered. These changes should be transferred to LSST."
"calib_psfReserved is only defined when candidate reservation is activated The schema should in general not be a function of whether particular features are enabled or disabled so that users can have confidence looking for columns.  However, {{MeasurePsfTask}} only creates the {{calib_psfReserved}} column when {{reserveFraction > 0}}.  This causes warnings when attempting to propagate flags from calibration catalogs to deep catalogs.",1,DM-5385,datamanagement,calib_psfreserve define candidate reservation activate schema general function particular feature enable disabled user confidence look column measurepsftask create calib_psfreserve column reservefraction cause warning attempt propagate flag calibration catalog deep catalog,"calib_psfReserved is only defined when candidate reservation is activated The schema should in general not be a function of whether particular features are enabled or disabled so that users can have confidence looking for columns. However, {{MeasurePsfTask}} only creates the {{calib_psfReserved}} column when {{reserveFraction > 0}}. This causes warnings when attempting to propagate flags from calibration catalogs to deep catalogs."
"Filter  editor A dialog to edit all the filters on the data for table and XY plot.     AND, OR conditions?    implemented:  * display column's units and descriptions  * add single column filter with auto-correction  * add free-hand filters field with validation and auto-correction  * reset, clear filters as well.  * 'Column' is sticky... scrolling left/right will not affect it.    ",8,DM-5387,datamanagement,filter editor dialog edit filter datum table xy plot condition implement display column unit description add single column filter auto correction add free hand filter field validation auto correction reset clear filter column sticky scrolling leave right affect,"Filter editor A dialog to edit all the filters on the data for table and XY plot. AND, OR conditions? implemented: * display column's units and descriptions * add single column filter with auto-correction * add free-hand filters field with validation and auto-correction * reset, clear filters as well. * 'Column' is sticky... scrolling left/right will not affect it."
"GWT Conversion: Dropdown Container Create drop down container to display search panel, catalog search panel, image search panel, etc.     ",4,DM-5389,datamanagement,gwt conversion dropdown container create drop container display search panel catalog search panel image search panel etc,"GWT Conversion: Dropdown Container Create drop down container to display search panel, catalog search panel, image search panel, etc."
"JavaScript loading/caching plan We need to ensure that the latest version of the application(javascript) is loaded. Conditions: 1. once loaded, it should be cached by the browser. 2. name of the script has to be a static, so it can be referenced by api user. 3. it also has to load dependencies(gwt scripts) after the main script is loaded.  To do this, we created a tiny firefly_loader.js script whose role is to load the main script and then its dependencies. firefly_loader.js is configured to never cache so that the latest main script is always picked up. The main script is appended with a unique hash on every build.  This ensures that the browser will pick up the new script the very first time, and then cache it for future use. ",2,DM-5390,datamanagement,javascript loading cache plan need ensure late version application(javascript load condition load cache browser script static reference api user load dependencies(gwt script main script load create tiny firefly_loader.js script role load main script dependency firefly_loader.js configure cache late main script pick main script append unique hash build ensure browser pick new script time cache future use,"JavaScript loading/caching plan We need to ensure that the latest version of the application(javascript) is loaded. Conditions: 1. once loaded, it should be cached by the browser. 2. name of the script has to be a static, so it can be referenced by api user. 3. it also has to load dependencies(gwt scripts) after the main script is loaded. To do this, we created a tiny firefly_loader.js script whose role is to load the main script and then its dependencies. firefly_loader.js is configured to never cache so that the latest main script is always picked up. The main script is appended with a unique hash on every build. This ensures that the browser will pick up the new script the very first time, and then cache it for future use."
"Please stop leaving repoCfg.yaml files around After a recent change to {{daf_persistence}} and possibly other packages I'm finding that many packages leave {{repoCfg.yaml}} files lying around after they run unit tests.    I'm not sure what is best to do about these files. If they are temporary, as I am guessing, then I think we need some way to clean them up when the tests that generated them have run. If they are intended to be permanent (which would be surprising for auto-generated files) then they should probably be committed?    I hope we can do better than adding them to .gitignore.",1,DM-5392,datamanagement,stop leave repocfg.yaml file recent change daf_persistence possibly package find package leave repocfg.yaml file lie run unit test sure good file temporary guess think need way clean test generate run intend permanent surprising auto generate file probably commit hope well add,"Please stop leaving repoCfg.yaml files around After a recent change to {{daf_persistence}} and possibly other packages I'm finding that many packages leave {{repoCfg.yaml}} files lying around after they run unit tests. I'm not sure what is best to do about these files. If they are temporary, as I am guessing, then I think we need some way to clean them up when the tests that generated them have run. If they are intended to be permanent (which would be surprising for auto-generated files) then they should probably be committed? I hope we can do better than adding them to .gitignore."
"Investigate boost compiler warnings and update boost to v1.60 As reported in comments in DM-1304 clang now triggers many warnings with Boost v1.59:  {code}  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/archive/detail/check.hpp:148:5: warning: unused typedef 'STATIC_WARNING_LINE148' [-Wunused-local-typedef]      BOOST_STATIC_WARNING(typex::value);      ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:100:33: note: expanded from macro 'BOOST_STATIC_WARNING'  #define BOOST_STATIC_WARNING(B) BOOST_SERIALIZATION_BSW(B, __LINE__)                                  ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:99:7: note: expanded from macro 'BOOST_SERIALIZATION_BSW'      > BOOST_JOIN(STATIC_WARNING_LINE, L) BOOST_STATIC_ASSERT_UNUSED_ATTRIBUTE;         ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:544:28: note: expanded from macro 'BOOST_JOIN'  #define BOOST_JOIN( X, Y ) BOOST_DO_JOIN( X, Y )                             ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:545:31: note: expanded from macro 'BOOST_DO_JOIN'  #define BOOST_DO_JOIN( X, Y ) BOOST_DO_JOIN2(X,Y)                                ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:546:32: note: expanded from macro 'BOOST_DO_JOIN2'  #define BOOST_DO_JOIN2( X, Y ) X##Y                                 ^  <scratch space>:25:1: note: expanded from here  STATIC_WARNING_LINE148  ^  {code}  v1.60 is the current version so we should see if these warnings have been fixed in that version.",2,DM-5394,datamanagement,investigate boost compiler warning update boost v1.60 report comment dm-1304 clang trigger warning boost v1.59 code /users rowen uw lsst lsstsw stack darwinx86 boost/1.59.lsst5 include boost archive detail check.hpp:148:5 warn unused typedef static_warning_line148 -wunused local typedef boost_static_warning(typex::value rowen uw lsst lsstsw stack darwinx86 boost/1.59.lsst5 include boost serialization static_warning.hpp:100:33 note expand macro boost_static_warning define boost_static_warning(b boost_serialization_bsw(b line rowen uw lsst lsstsw stack darwinx86 boost/1.59.lsst5 include boost serialization static_warning.hpp:99:7 note expand macro boost_serialization_bsw boost_join(static_warning_line rowen uw lsst lsstsw stack darwinx86 boost/1.59.lsst5 include boost config suffix.hpp:544:28 note expand macro boost_join define boost_join boost_do_join rowen uw lsst lsstsw stack darwinx86 boost/1.59.lsst5 include boost config suffix.hpp:545:31 note expand macro boost_do_join define boost_do_join boost_do_join2(x rowen uw lsst lsstsw stack darwinx86 boost/1.59.lsst5 include boost config suffix.hpp:546:32 note expand macro boost_do_join2 define boost_do_join2 x##y 25:1 note expand static_warning_line148 code v1.60 current version warning fix version,"Investigate boost compiler warnings and update boost to v1.60 As reported in comments in DM-1304 clang now triggers many warnings with Boost v1.59: {code} /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/archive/detail/check.hpp:148:5: warning: unused typedef 'STATIC_WARNING_LINE148' [-Wunused-local-typedef] BOOST_STATIC_WARNING(typex::value); ^ /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:100:33: note: expanded from macro 'BOOST_STATIC_WARNING' #define BOOST_STATIC_WARNING(B) BOOST_SERIALIZATION_BSW(B, __LINE__) ^ /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:99:7: note: expanded from macro 'BOOST_SERIALIZATION_BSW' > BOOST_JOIN(STATIC_WARNING_LINE, L) BOOST_STATIC_ASSERT_UNUSED_ATTRIBUTE; ^ /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:544:28: note: expanded from macro 'BOOST_JOIN' #define BOOST_JOIN( X, Y ) BOOST_DO_JOIN( X, Y ) ^ /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:545:31: note: expanded from macro 'BOOST_DO_JOIN' #define BOOST_DO_JOIN( X, Y ) BOOST_DO_JOIN2(X,Y) ^ /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:546:32: note: expanded from macro 'BOOST_DO_JOIN2' #define BOOST_DO_JOIN2( X, Y ) X##Y ^ :25:1: note: expanded from here STATIC_WARNING_LINE148 ^ {code} v1.60 is the current version so we should see if these warnings have been fixed in that version."
"Cleanup jointcal Before we start digging into jointcal, it'd be good to get the whitespace/oldpython/indentation/lint/etc. questions sorted out. This ticket is for that.",2,DM-5400,datamanagement,cleanup jointcal start dig jointcal good whitespace oldpython indentation lint etc question sort ticket,"Cleanup jointcal Before we start digging into jointcal, it'd be good to get the whitespace/oldpython/indentation/lint/etc. questions sorted out. This ticket is for that."
Make cluster deployment scripts more generic and enable ccqserv100...124 These scripts will be improved (i.e. more genericity) and integrated inside Qserv code. Qserv will be deployed on ccqserv100 to ccqserv125,3,DM-5402,datamanagement,cluster deployment script generic enable ccqserv100 124 script improve i.e. genericity integrate inside qserv code qserv deploy ccqserv100,Make cluster deployment scripts more generic and enable ccqserv100...124 These scripts will be improved (i.e. more genericity) and integrated inside Qserv code. Qserv will be deployed on ccqserv100 to ccqserv125
Developer Guide Content & Maintenance Backlog Epic General maintenance and original content for the DM Developer Guide (http://developer.lsst.io) based on needs during the cycle.,7,DM-5403,datamanagement,developer guide content maintenance backlog epic general maintenance original content dm developer guide http://developer.lsst.io base need cycle,Developer Guide Content & Maintenance Backlog Epic General maintenance and original content for the DM Developer Guide (http://developer.lsst.io) based on needs during the cycle.
"Re-enable CModel forced measurement on CCDs Recent changes from the HSC side (DM-4768) were implemented in a hurry, and break CModel forced measurement when the reference WCS is different from the measurement WCS (as is the case with forced measurement on CCDs).  This was considered an acceptable temporarily, since forced CCD measurement is currently severely limited by our lack of deblending, but we'll need to fix it eventually.    The fix is trivial from an algorithmic standpoint but may require a bit of refactoring (at least changing some function signatures; maybe more).    This should include re-enabling the different-WCS complexity in testCModelPlugins.py,",2,DM-5405,datamanagement,enable cmodel force measurement ccds recent change hsc dm-4768 implement hurry break cmodel force measurement reference wcs different measurement wcs case force measurement ccd consider acceptable temporarily force ccd measurement currently severely limit lack deblending need fix eventually fix trivial algorithmic standpoint require bit refactoring change function signature maybe include enable different wcs complexity testcmodelplugins.py,"Re-enable CModel forced measurement on CCDs Recent changes from the HSC side (DM-4768) were implemented in a hurry, and break CModel forced measurement when the reference WCS is different from the measurement WCS (as is the case with forced measurement on CCDs). This was considered an acceptable temporarily, since forced CCD measurement is currently severely limited by our lack of deblending, but we'll need to fix it eventually. The fix is trivial from an algorithmic standpoint but may require a bit of refactoring (at least changing some function signatures; maybe more). This should include re-enabling the different-WCS complexity in testCModelPlugins.py,"
"Require fields listed in icSourceFieldsToCopy to be present {{CalibrateTask}} presently treats config field {{icSourceFieldsToCopy}} as a list of fields to copy *if present*. This was required because one of the standard fields to copy was usually missing. However, [~price] fixed that problem in DM-5385. Now we can raise an exception if any field listed is missing (though I propose to continue ignoring {{icSourceFieldsToCopy}} if isSourceCatalog is not provided).",1,DM-5406,datamanagement,require field list icsourcefieldstocopy present calibratetask presently treat config field icsourcefieldstocopy list field copy present require standard field copy usually miss ~price fix problem dm-5385 raise exception field list miss propose continue ignore icsourcefieldstocopy issourcecatalog provide,"Require fields listed in icSourceFieldsToCopy to be present {{CalibrateTask}} presently treats config field {{icSourceFieldsToCopy}} as a list of fields to copy *if present*. This was required because one of the standard fields to copy was usually missing. However, [~price] fixed that problem in DM-5385. Now we can raise an exception if any field listed is missing (though I propose to continue ignoring {{icSourceFieldsToCopy}} if isSourceCatalog is not provided)."
"Rename datasets to utilize butler aliases Now that the butler has alias features that can allow for some degree of dataset substitutability, we should consider renaming (or adding aliases) for our existing datasets to make the naming consistent and analysis code more generic.    This work should be proceeded by an RFC with a proposal for the new names and a migration plan.    It *might* make sense to defer this until the high-level pipeline descriptions are more mature and we can choose relatively future-proof names, but hopefully the alias features will also make migration easy enough that this doesn't matter a lot.",4,DM-5407,datamanagement,rename dataset utilize butler alias butler alia feature allow degree dataset substitutability consider rename add alias exist dataset naming consistent analysis code generic work proceed rfc proposal new name migration plan sense defer high level pipeline description mature choose relatively future proof name hopefully alia feature migration easy matter lot,"Rename datasets to utilize butler aliases Now that the butler has alias features that can allow for some degree of dataset substitutability, we should consider renaming (or adding aliases) for our existing datasets to make the naming consistent and analysis code more generic. This work should be proceeded by an RFC with a proposal for the new names and a migration plan. It *might* make sense to defer this until the high-level pipeline descriptions are more mature and we can choose relatively future-proof names, but hopefully the alias features will also make migration easy enough that this doesn't matter a lot."
"Qserv do not return very same BLOB field than MySQL Enabling query {{qserv_testdata/datasets/case01/queries/0007.2_fetchSourceByObjIdSelectBLOB.sql.FIXME}} will reveal this bug.    Qserv chunk table contains next BLOB:  {code:bash}  mysql --socket /home/dev/qserv-run/git/var/lib/mysql/mysql.sock --user=root --password=changeme qservTest_case01_qserv -e ""select blobField from Source_6630 where SourceId=29809239313746172;"" > 29809239313746172.chunk6630    vi 29809239313746172.chunk6630    blobField    ^D^B\0^W\0\0\0^B\0^K\0~B\0first_fieldsecond_field(�q.\\�  {code}    But Qserv returns:  {code}  ^D^B\0^W\0\0\0^B\0^K\0~B\0first_fieldsecond_field(�q.�  {code}    See DM-991 for additional informations.  ",8,DM-5408,datamanagement,qserv return blob field mysql enabling query qserv_testdata dataset case01 queries/0007.2_fetchsourcebyobjidselectblob.sql fixme reveal bug qserv chunk table contain blob code bash mysql /home dev qserv run git var lib mysql mysql.sock root --password changeme qservtest_case01_qserv select blobfield source_6630 sourceid=29809239313746172 29809239313746172.chunk6630 vi 29809239313746172.chunk6630 blobfield ^d^b\0^w\0\0\0^b\0^k\0 b\0first_fieldsecond_field q.\\ code qserv return code ^d^b\0^w\0\0\0^b\0^k\0 b\0first_fieldsecond_field q. code dm-991 additional information,"Qserv do not return very same BLOB field than MySQL Enabling query {{qserv_testdata/datasets/case01/queries/0007.2_fetchSourceByObjIdSelectBLOB.sql.FIXME}} will reveal this bug. Qserv chunk table contains next BLOB: {code:bash} mysql --socket /home/dev/qserv-run/git/var/lib/mysql/mysql.sock --user=root --password=changeme qservTest_case01_qserv -e ""select blobField from Source_6630 where SourceId=29809239313746172;"" > 29809239313746172.chunk6630 vi 29809239313746172.chunk6630 blobField ^D^B\0^W\0\0\0^B\0^K\0~B\0first_fieldsecond_field( q.\\ {code} But Qserv returns: {code} ^D^B\0^W\0\0\0^B\0^K\0~B\0first_fieldsecond_field( q. {code} See DM-991 for additional informations."
"DecamIngestTask is mis-calling openRegistry `DecamIngestTask` is mis-calling `lsst.pipe.tasks.RegistryTask`. Line 59:    {code}  with self.register.openRegistry(args.butler, create=args.create, dryrun=args.dryrun) as registry:  {code}  {{openRegistry}} is expecting a directory name, not a butler object for the first argument    Thanks to [~wmwood-vasey] for diagnosing this.",1,DM-5410,datamanagement,decamingesttask mis call openregistry decamingesttask mis call lsst.pipe.task registrytask line 59 code self.register.openregistry(args.butler create args.create dryrun args.dryrun registry code openregistry expect directory butler object argument thank ~wmwood vasey diagnose,"DecamIngestTask is mis-calling openRegistry `DecamIngestTask` is mis-calling `lsst.pipe.tasks.RegistryTask`. Line 59: {code} with self.register.openRegistry(args.butler, create=args.create, dryrun=args.dryrun) as registry: {code} {{openRegistry}} is expecting a directory name, not a butler object for the first argument Thanks to [~wmwood-vasey] for diagnosing this."
"Test new dipole fitting task on real data Test new task on real data (which data, TBD); inspect results by eye and compare with existing DipoleMeasurementTask output. This is necessary prior to incorporation into the imageDifference command-line task.    This test may also indicate that further optimizations are necessary (DM-5721).",6,DM-5412,datamanagement,test new dipole fitting task real datum test new task real datum data tbd inspect result eye compare exist dipolemeasurementtask output necessary prior incorporation imagedifference command line task test indicate optimization necessary dm-5721,"Test new dipole fitting task on real data Test new task on real data (which data, TBD); inspect results by eye and compare with existing DipoleMeasurementTask output. This is necessary prior to incorporation into the imageDifference command-line task. This test may also indicate that further optimizations are necessary (DM-5721)."
"Incorporate new DipoleFitTask into imageDifference command-line task alongside existing DipoleMeasurementTask Incorporate the new task into the command-line task. The goal of this ticket is to implement DipoleFitTask along-side the existing DipoleMeasurementTask, eventually to replace it.    This is likely to have additional stories added, including testing, possibly as part of DM-5412.",6,DM-5413,datamanagement,incorporate new dipolefittask imagedifference command line task alongside exist dipolemeasurementtask incorporate new task command line task goal ticket implement dipolefittask exist dipolemeasurementtask eventually replace likely additional story add include testing possibly dm-5412,"Incorporate new DipoleFitTask into imageDifference command-line task alongside existing DipoleMeasurementTask Incorporate the new task into the command-line task. The goal of this ticket is to implement DipoleFitTask along-side the existing DipoleMeasurementTask, eventually to replace it. This is likely to have additional stories added, including testing, possibly as part of DM-5412."
"Create buildable SuiteSparse external package To get jointcal to build in the stack, we need to satisfy the SuiteSparse dependency by creating an external package for SuiteSparse.    Assuming it builds cleanly, this should satisfy the remaining requirement of RFC-153, now that the licensing question has been answered there.",2,DM-5414,datamanagement,create buildable suitesparse external package jointcal build stack need satisfy suitesparse dependency create external package suitesparse assume build cleanly satisfy remain requirement rfc-153 licensing question answer,"Create buildable SuiteSparse external package To get jointcal to build in the stack, we need to satisfy the SuiteSparse dependency by creating an external package for SuiteSparse. Assuming it builds cleanly, this should satisfy the remaining requirement of RFC-153, now that the licensing question has been answered there."
Ci Deploy and Distribution Improvements part IV This is a bucket epic for ongoing improvements to the CI system,8,DM-5416,datamanagement,ci deploy distribution improvements iv bucket epic ongoing improvement ci system,Ci Deploy and Distribution Improvements part IV This is a bucket epic for ongoing improvements to the CI system
"ci_hsc fails test requiring >95% of PSF stars to be stars on the coadd Since the first week of March 2016, ci_hsc fails its test that requires that >95% of the PSF stars be identified as stars in the coadd.  I suspect this is related to the DM-4692 merge.    Here is a sample job that fails:  https://ci.lsst.codes/job/stack-os-matrix/9084/label=centos-6/console    The relevant snippet of the failure is:    {code}  [2016-03-10T17:12:06.667778Z] : Validating dataset measureCoaddSources_config for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:06.697383Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3  [2016-03-10T17:12:06.697615Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3  [2016-03-10T17:12:07.716310Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3  [2016-03-10T17:12:07.716443Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3  [2016-03-10T17:12:08.663566Z] : measureCoaddSources_config exists: PASS  [2016-03-10T17:12:08.721051Z] : measureCoaddSources_config readable (<class 'lsst.pipe.tasks.multiBand.MeasureMergedCoaddSourcesConfig'>): PASS  [2016-03-10T17:12:08.721077Z] : Validating dataset measureCoaddSources_metadata for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:08.721249Z] : measureCoaddSources_metadata exists: PASS  [2016-03-10T17:12:08.721663Z] : measureCoaddSources_metadata readable (<class 'lsst.daf.base.baseLib.PropertySet'>): PASS  [2016-03-10T17:12:08.721715Z] : Validating dataset deepCoadd_meas_schema for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:08.721878Z] : deepCoadd_meas_schema exists: PASS  [2016-03-10T17:12:08.726703Z] : deepCoadd_meas_schema readable (<class 'lsst.afw.table.tableLib.SourceCatalog'>): PASS  [2016-03-10T17:12:08.726834Z] : Validating source output for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:10.203469Z] : Number of sources (7595 > 100): PASS  [2016-03-10T17:12:10.204166Z] : calib_psfCandidate field exists in deepCoadd_meas catalog: PASS  [2016-03-10T17:12:10.204772Z] : calib_psfUsed field exists in deepCoadd_meas catalog: PASS  [2016-03-10T17:12:10.205468Z] : Aperture correction fields for base_PsfFlux are present.: PASS  [2016-03-10T17:12:10.206159Z] : Aperture correction fields for base_GaussianFlux are present.: PASS  [2016-03-10T17:12:10.207193Z]  FATAL: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0): FAIL  [2016-03-10T17:12:10.207455Z] scons: *** [.scons/measure-HSC-R] AssertionError : Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0)  [2016-03-10T17:12:10.207481Z] Traceback (most recent call last):  [2016-03-10T17:12:10.207525Z]   File ""/home/build0/lsstsw/stack/Linux64/scons/2.3.5/lib/scons/SCons/Action.py"", line 1063, in execute  [2016-03-10T17:12:10.207556Z]     result = self.execfunction(target=target, source=rsources, env=env)  [2016-03-10T17:12:10.207593Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 133, in scons  [2016-03-10T17:12:10.207611Z]     return self.run(*args, **kwargs)  [2016-03-10T17:12:10.207646Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 122, in run  [2016-03-10T17:12:10.207663Z]     self.validateSources(dataId)  [2016-03-10T17:12:10.207732Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 191, in validateSources  [2016-03-10T17:12:10.207749Z]     0.95*psfStars.sum()  [2016-03-10T17:12:10.207786Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 52, in assertGreater  [2016-03-10T17:12:10.207816Z]     self.assertTrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2)  [2016-03-10T17:12:10.207853Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 43, in assertTrue  [2016-03-10T17:12:10.207877Z]     raise AssertionError(""Failed test: %s"" % description)  [2016-03-10T17:12:10.207919Z] AssertionError: Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0)  [2016-03-10T17:12:10.209935Z] scons: building terminated because of errors.  {code}    This is the test that fails    https://github.com/lsst/ci_hsc/blob/74303a818eb5049a2015b5e885df2781053748c9/python/lsst/ci/hsc/validate.py#L169  {code}  class MeasureValidation(Validation):      _datasets = [""measureCoaddSources_config"", ""measureCoaddSources_metadata"", ""deepCoadd_meas_schema""]      _sourceDataset = ""deepCoadd_meas""      _matchDataset = ""deepCoadd_srcMatch""        def validateSources(self, dataId):          catalog = Validation.validateSources(self, dataId)          self.assertTrue(""calib_psfCandidate field exists in deepCoadd_meas catalog"",                          ""calib_psfCandidate"" in catalog.schema)          self.assertTrue(""calib_psfUsed field exists in deepCoadd_meas catalog"",                          ""calib_psfUsed"" in catalog.schema)          self.checkApertureCorrections(catalog)          # Check that at least 95% of the stars we used to model the PSF end up classified as stars          # on the coadd.  We certainly need much more purity than that to build good PSF models, but          # this should verify that flag propagation, aperture correction, and extendendess are all          # running and configured reasonably (but it may not be sensitive enough to detect subtle          # bugs).          psfStars = catalog.get(""calib_psfUsed"")          extStars = catalog.get(""base_ClassificationExtendedness_value"") < 0.5          self.assertGreater(              ""95% of sources used to build the PSF are classified as stars on the coadd"",              numpy.logical_and(extStars, psfStars).sum(),              0.95*psfStars.sum()          )  {code}    Note that the assertion failure messages is a bit confusing.  It should say  ""Fewer than 95% of the sources used to build the PSF are classified as stars on the coadd.""",1,DM-5419,datamanagement,"ci_hsc fail test require 95 psf star star coadd week march 2016 ci_hsc fail test require 95 psf star identify star coadd suspect relate dm-4692 merge sample job fail https://ci.lsst.codes/job/stack-os-matrix/9084/label=centos-6/console relevant snippet failure code 2016 03 10t17:12:06.667778z validate dataset measurecoaddsources_config filter hsc tract patch 5,4 2016 03 10t17:12:06.697383z cameramapper loading registry registry /home build0 lsstsw build ci_hsc data registry.sqlite3 2016 03 10t17:12:06.697615z cameramapper loading calibregistry registry /home build0 lsstsw build ci_hsc data calib calibregistry.sqlite3 2016 03 10t17:12:07.716310z cameramapper loading registry registry /home build0 lsstsw build ci_hsc data registry.sqlite3 2016 03 10t17:12:07.716443z cameramapper loading calibregistry registry /home build0 lsstsw build ci_hsc data calib calibregistry.sqlite3 2016 03 10t17:12:08.663566z measurecoaddsources_config exist pass 2016 03 10t17:12:08.721051z measurecoaddsources_config readable pass 2016 03 10t17:12:08.721077z validate dataset measurecoaddsources_metadata filter hsc tract patch 5,4 2016 03 10t17:12:08.721249z measurecoaddsources_metadata exist pass 2016 03 10t17:12:08.721663z measurecoaddsources_metadata readable pass 2016 03 10t17:12:08.721715z validate dataset deepcoadd_meas_schema filter hsc tract patch 5,4 2016 03 10t17:12:08.721878z deepcoadd_meas_schema exist pass 2016 03 10t17:12:08.726703z deepcoadd_meas_schema readable pass 2016 03 10t17:12:08.726834z validate source output filter hsc tract patch 5,4 2016 03 10t17:12:10.203469z number source 7595 100 pass 2016 03 10t17:12:10.204166z calib_psfcandidate field exist deepcoadd_meas catalog pass 2016 03 10t17:12:10.204772z calib_psfuse field exist deepcoadd_meas catalog pass 2016 03 10t17:12:10.205468z aperture correction field base_psfflux present pass 2016 03 10t17:12:10.206159z aperture correction field base_gaussianflux present pass 2016 03 10t17:12:10.207193z fatal 95 source build psf classify star coadd fail 2016 03 10t17:12:10.207455z scon .scons measure hsc assertionerror fail test 95 source build psf classify star coadd 2016 03 10t17:12:10.207481z traceback recent 2016 03 10t17:12:10.207525z file /home build0 lsstsw stack linux64 scons/2.3.5 lib scon scons action.py line 1063 execute 2016 03 10t17:12:10.207556z result self.execfunction(target target source rsource env env 2016 03 10t17:12:10.207593z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 133 scon 2016 03 10t17:12:10.207611z return self.run(*args kwargs 2016 03 10t17:12:10.207646z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 122 run 2016 03 10t17:12:10.207663z self.validatesources(dataid 2016 03 10t17:12:10.207732z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 191 validatesource 2016 03 10t17:12:10.207749z 0.95*psfstars.sum 2016 03 10t17:12:10.207786z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 52 assertgreater 2016 03 10t17:12:10.207816z self.asserttrue(description num1 num2 num1 num2 2016 03 10t17:12:10.207853z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 43 asserttrue 2016 03 10t17:12:10.207877z raise assertionerror(""failed test description 2016 03 10t17:12:10.207919z assertionerror fail test 95 source build psf classify star coadd 2016 03 10t17:12:10.209935z scon building terminate error code test fail https://github.com/lsst/ci_hsc/blob/74303a818eb5049a2015b5e885df2781053748c9/python/lsst/ci/hsc/validate.py#l169 code class measurevalidation(validation dataset measurecoaddsources_config measurecoaddsources_metadata deepcoadd_meas_schema sourcedataset deepcoadd_mea matchdataset deepcoadd_srcmatch def validatesources(self dataid catalog validation.validatesources(self dataid self.asserttrue(""calib_psfcandidate field exist deepcoadd_meas catalog calib_psfcandidate catalog.schema self.asserttrue(""calib_psfuse field exist deepcoadd_meas catalog calib_psfuse catalog.schema self.checkaperturecorrections(catalog check 95 star model psf end classify star coadd certainly need purity build good psf model verify flag propagation aperture correction extendendess run configure reasonably sensitive detect subtle bug psfstars catalog.get(""calib_psfuse extstar catalog.get(""base_classificationextendedness_value 0.5 self.assertgreater 95 source build psf classify star coadd psfstars).sum 0.95*psfstars.sum code note assertion failure message bit confusing few 95 source build psf classify star coadd","ci_hsc fails test requiring >95% of PSF stars to be stars on the coadd Since the first week of March 2016, ci_hsc fails its test that requires that >95% of the PSF stars be identified as stars in the coadd. I suspect this is related to the DM-4692 merge. Here is a sample job that fails: https://ci.lsst.codes/job/stack-os-matrix/9084/label=centos-6/console The relevant snippet of the failure is: {code} [2016-03-10T17:12:06.667778Z] : Validating dataset measureCoaddSources_config for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'} [2016-03-10T17:12:06.697383Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3 [2016-03-10T17:12:06.697615Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3 [2016-03-10T17:12:07.716310Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3 [2016-03-10T17:12:07.716443Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3 [2016-03-10T17:12:08.663566Z] : measureCoaddSources_config exists: PASS [2016-03-10T17:12:08.721051Z] : measureCoaddSources_config readable (): PASS [2016-03-10T17:12:08.721077Z] : Validating dataset measureCoaddSources_metadata for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'} [2016-03-10T17:12:08.721249Z] : measureCoaddSources_metadata exists: PASS [2016-03-10T17:12:08.721663Z] : measureCoaddSources_metadata readable (): PASS [2016-03-10T17:12:08.721715Z] : Validating dataset deepCoadd_meas_schema for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'} [2016-03-10T17:12:08.721878Z] : deepCoadd_meas_schema exists: PASS [2016-03-10T17:12:08.726703Z] : deepCoadd_meas_schema readable (): PASS [2016-03-10T17:12:08.726834Z] : Validating source output for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'} [2016-03-10T17:12:10.203469Z] : Number of sources (7595 > 100): PASS [2016-03-10T17:12:10.204166Z] : calib_psfCandidate field exists in deepCoadd_meas catalog: PASS [2016-03-10T17:12:10.204772Z] : calib_psfUsed field exists in deepCoadd_meas catalog: PASS [2016-03-10T17:12:10.205468Z] : Aperture correction fields for base_PsfFlux are present.: PASS [2016-03-10T17:12:10.206159Z] : Aperture correction fields for base_GaussianFlux are present.: PASS [2016-03-10T17:12:10.207193Z] FATAL: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0): FAIL [2016-03-10T17:12:10.207455Z] scons: *** [.scons/measure-HSC-R] AssertionError : Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0) [2016-03-10T17:12:10.207481Z] Traceback (most recent call last): [2016-03-10T17:12:10.207525Z] File ""/home/build0/lsstsw/stack/Linux64/scons/2.3.5/lib/scons/SCons/Action.py"", line 1063, in execute [2016-03-10T17:12:10.207556Z] result = self.execfunction(target=target, source=rsources, env=env) [2016-03-10T17:12:10.207593Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 133, in scons [2016-03-10T17:12:10.207611Z] return self.run(*args, **kwargs) [2016-03-10T17:12:10.207646Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 122, in run [2016-03-10T17:12:10.207663Z] self.validateSources(dataId) [2016-03-10T17:12:10.207732Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 191, in validateSources [2016-03-10T17:12:10.207749Z] 0.95*psfStars.sum() [2016-03-10T17:12:10.207786Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 52, in assertGreater [2016-03-10T17:12:10.207816Z] self.assertTrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2) [2016-03-10T17:12:10.207853Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 43, in assertTrue [2016-03-10T17:12:10.207877Z] raise AssertionError(""Failed test: %s"" % description) [2016-03-10T17:12:10.207919Z] AssertionError: Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0) [2016-03-10T17:12:10.209935Z] scons: building terminated because of errors. {code} This is the test that fails https://github.com/lsst/ci_hsc/blob/74303a818eb5049a2015b5e885df2781053748c9/python/lsst/ci/hsc/validate.py#L169 {code} class MeasureValidation(Validation): _datasets = [""measureCoaddSources_config"", ""measureCoaddSources_metadata"", ""deepCoadd_meas_schema""] _sourceDataset = ""deepCoadd_meas"" _matchDataset = ""deepCoadd_srcMatch"" def validateSources(self, dataId): catalog = Validation.validateSources(self, dataId) self.assertTrue(""calib_psfCandidate field exists in deepCoadd_meas catalog"", ""calib_psfCandidate"" in catalog.schema) self.assertTrue(""calib_psfUsed field exists in deepCoadd_meas catalog"", ""calib_psfUsed"" in catalog.schema) self.checkApertureCorrections(catalog) # Check that at least 95% of the stars we used to model the PSF end up classified as stars # on the coadd. We certainly need much more purity than that to build good PSF models, but # this should verify that flag propagation, aperture correction, and extendendess are all # running and configured reasonably (but it may not be sensitive enough to detect subtle # bugs). psfStars = catalog.get(""calib_psfUsed"") extStars = catalog.get(""base_ClassificationExtendedness_value"") < 0.5 self.assertGreater( ""95% of sources used to build the PSF are classified as stars on the coadd"", numpy.logical_and(extStars, psfStars).sum(), 0.95*psfStars.sum() ) {code} Note that the assertion failure messages is a bit confusing. It should say ""Fewer than 95% of the sources used to build the PSF are classified as stars on the coadd."""
"Switch PropagateVisitFlags to use src instead of icSrc On DM-5084 [~jbosch] switched PropagateVisitFlags to match against icSrc instead of src because we weren't yet matching `icSrc` to `src` in ProcessCcdTask.  That's now been done on DM-4692, so we can revert this.    After doing so, please verify with ci_hsc that this is working, as that's where the only test of this feature lives.",2,DM-5424,datamanagement,switch propagatevisitflag use src instead icsrc dm-5084 ~jbosch switch propagatevisitflags match icsrc instead src match icsrc src processccdtask dm-4692 revert verify ci_hsc work test feature live,"Switch PropagateVisitFlags to use src instead of icSrc On DM-5084 [~jbosch] switched PropagateVisitFlags to match against icSrc instead of src because we weren't yet matching `icSrc` to `src` in ProcessCcdTask. That's now been done on DM-4692, so we can revert this. After doing so, please verify with ci_hsc that this is working, as that's where the only test of this feature lives."
"Provide an easy way to set Coord fields of a source catalog We sometimes need to set the coord fields of a source catalog, e.g. when fitting a new WCS or when studying an `icSrc` catalog (whose Coord field is not set). It would be nice to have a central, easily found way to do this. Right now we have the following as a static method of `TanSipWcsTask`, which works fine but is in a poor location:    {code}      def updateSourceCoords(wcs, sourceList):          """"""Update coords in a collection of sources, given a WCS          """"""          if len(sourceList) < 1:              return          schema = sourceList[1].schema          srcCoordKey = afwTable.CoordKey(schema[""coord""])          for src in sourceList:              src.set(srcCoordKey, wcs.pixelToSky(src.getCentroid()))  {code}    The other direction is also useful for reference catalogs, though from a practical standpoint the only user is probably `meas_astrom`. Even so, I suggest that this be made publicly available in the same way. Again, this is presently a static method of `FitTanSipWcsTask`:    {code}      def updateRefCentroids(wcs, refList):          """"""Update centroids in a collection of reference objects, given a WCS          """"""          if len(refList) < 1:              return          schema = refList[0].schema          coordKey = afwTable.CoordKey(schema[""coord""])          centroidKey = afwTable.Point2DKey(schema[""centroid""])          for refObj in refList:              refObj.set(centroidKey, wcs.skyToPixel(refObj.get(coordKey)))  {code}    I hope this can remain Python code, but admit that the extra speed of C++ might come in handy in some cases. In any case, once the function is in a central location we can implement it in C++ if we find the need.",2,DM-5425,datamanagement,"provide easy way set coord field source catalog need set coord field source catalog e.g. fit new wcs study icsrc catalog coord field set nice central easily find way right following static method tansipwcstask work fine poor location code def updatesourcecoords(wcs sourcelist update coord collection source give wcs len(sourcelist return schema sourcelist[1].schema srccoordkey afwtable coordkey(schema[""coord src sourcelist src.set(srccoordkey wcs.pixeltosky(src.getcentroid code direction useful reference catalog practical standpoint user probably meas_astrom suggest publicly available way presently static method fittansipwcstask code def updaterefcentroids(wcs reflist update centroid collection reference object give wcs len(reflist return schema reflist[0].schema coordkey afwtable coordkey(schema[""coord afwtable point2dkey(schema[""centroid refobj reflist refobj.set(centroidkey wcs.skytopixel(refobj.get(coordkey code hope remain python code admit extra speed c++ come handy case case function central location implement c++ find need","Provide an easy way to set Coord fields of a source catalog We sometimes need to set the coord fields of a source catalog, e.g. when fitting a new WCS or when studying an `icSrc` catalog (whose Coord field is not set). It would be nice to have a central, easily found way to do this. Right now we have the following as a static method of `TanSipWcsTask`, which works fine but is in a poor location: {code} def updateSourceCoords(wcs, sourceList): """"""Update coords in a collection of sources, given a WCS """""" if len(sourceList) < 1: return schema = sourceList[1].schema srcCoordKey = afwTable.CoordKey(schema[""coord""]) for src in sourceList: src.set(srcCoordKey, wcs.pixelToSky(src.getCentroid())) {code} The other direction is also useful for reference catalogs, though from a practical standpoint the only user is probably `meas_astrom`. Even so, I suggest that this be made publicly available in the same way. Again, this is presently a static method of `FitTanSipWcsTask`: {code} def updateRefCentroids(wcs, refList): """"""Update centroids in a collection of reference objects, given a WCS """""" if len(refList) < 1: return schema = refList[0].schema coordKey = afwTable.CoordKey(schema[""coord""]) centroidKey = afwTable.Point2DKey(schema[""centroid""]) for refObj in refList: refObj.set(centroidKey, wcs.skyToPixel(refObj.get(coordKey))) {code} I hope this can remain Python code, but admit that the extra speed of C++ might come in handy in some cases. In any case, once the function is in a central location we can implement it in C++ if we find the need."
"SingleFrameVariancePlugin can give numpy warnings SingleFrameVariancePlugin can produce the following numpy warning, with no hint as to where the problem is coming from:  {code}  /Users/rowen/UW/LSST/lsstsw/miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(""Mean of empty slice."", RuntimeWarning)  {code}  I tracked it down by adding the following code to the calling code:  {code}  import warnings  with warnings.catch_warnings():      warnings.filterwarnings('error')  {code}    It would be nice if the measurement plugin handled this situation more gracefully, such as turning the warning into an exception or testing for it and handling it.    One way to reproduce this problem is to run {{tests/testProcessCcd.py}} in {{pipe_tasks}}. However, it is commonly seen when running {{processCcd}} on other data, as well.",2,DM-5427,datamanagement,"singleframevarianceplugin numpy warning produce follow numpy warning hint problem come code /users rowen uw lsst lsstsw miniconda lib python2.7 site package numpy core/_methods.py:59 runtimewarning mean slice warnings.warn(""mean slice runtimewarning code track add follow code call code code import warning warnings.catch_warning warnings.filterwarnings('error code nice measurement plugin handle situation gracefully turn warning exception testing handle way reproduce problem run test testprocessccd.py pipe_task commonly see run processccd datum","SingleFrameVariancePlugin can give numpy warnings SingleFrameVariancePlugin can produce the following numpy warning, with no hint as to where the problem is coming from: {code} /Users/rowen/UW/LSST/lsstsw/miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice. warnings.warn(""Mean of empty slice."", RuntimeWarning) {code} I tracked it down by adding the following code to the calling code: {code} import warnings with warnings.catch_warnings(): warnings.filterwarnings('error') {code} It would be nice if the measurement plugin handled this situation more gracefully, such as turning the warning into an exception or testing for it and handling it. One way to reproduce this problem is to run {{tests/testProcessCcd.py}} in {{pipe_tasks}}. However, it is commonly seen when running {{processCcd}} on other data, as well."
"ObjectSizeStarSelector can produce numpy warnings `ObjectSizeStarSelector` can produce the following numpy warning:   {code}  RuntimeWarning: invalid value encountered in less  {code}  This occurs at the following point in the code:  {code}          for i in range(nCluster):              # Only compute func if some points are available; otherwise, default to NaN.              pointsInCluster = (clusterId == i)              if numpy.any(pointsInCluster):                  centers[i] = func(yvec[pointsInCluster])  {code}  where `func` has been assigned to `numpy.mean`. When I have seen this occur I have found that `dist` is an array of `nan`    I suggest that the star selector handle this situation more gracefully, e.g. by reporting an appropriate exception or handling the data in an appropriate way. If logging a message would be helpful, then please do that (and if RFC-154 is adopted, a log will be available).    One way to reproduce this is to run `tests/testProcessCcd.py` in `pipe_tasks`. However, I often see it when running `processCcd.py` on other data, as well.",2,DM-5428,datamanagement,objectsizestarselector produce numpy warning objectsizestarselector produce follow numpy warning code runtimewarning invalid value encounter code occur following point code code range(ncluster compute func point available default nan. pointsincluster clusterid numpy.any(pointsincluster centers[i func(yvec[pointsincluster code func assign numpy.mean see occur find dist array nan suggest star selector handle situation gracefully e.g. report appropriate exception handle datum appropriate way log message helpful rfc-154 adopt log available way reproduce run test testprocessccd.py pipe_task run processccd.py datum,"ObjectSizeStarSelector can produce numpy warnings `ObjectSizeStarSelector` can produce the following numpy warning: {code} RuntimeWarning: invalid value encountered in less {code} This occurs at the following point in the code: {code} for i in range(nCluster): # Only compute func if some points are available; otherwise, default to NaN. pointsInCluster = (clusterId == i) if numpy.any(pointsInCluster): centers[i] = func(yvec[pointsInCluster]) {code} where `func` has been assigned to `numpy.mean`. When I have seen this occur I have found that `dist` is an array of `nan` I suggest that the star selector handle this situation more gracefully, e.g. by reporting an appropriate exception or handling the data in an appropriate way. If logging a message would be helpful, then please do that (and if RFC-154 is adopted, a log will be available). One way to reproduce this is to run `tests/testProcessCcd.py` in `pipe_tasks`. However, I often see it when running `processCcd.py` on other data, as well."
"Changes to galaxy_shear_experiments Python code This ticket describes changes which were made to the test runner and analysis scripts during the Dec 2015 - Feb 2016 period.  Most of these changes were made as a part of moving to a large computing cluster, where both the units of work and the output file organization had to be changed to make parallelization possible.    The large number of tests run during this period and the need to more efficiently analyze and compare also introduced some changed to the analysis and plot modules.    Since these changes do not pertain to any single test (though many were done during Dm-1136), I have put them on a separate ticket.",5,DM-5431,datamanagement,change galaxy_shear_experiment python code ticket describe change test runner analysis script dec 2015 feb 2016 period change move large computing cluster unit work output file organization change parallelization possible large number test run period need efficiently analyze compare introduce change analysis plot module change pertain single test dm-1136 separate ticket,"Changes to galaxy_shear_experiments Python code This ticket describes changes which were made to the test runner and analysis scripts during the Dec 2015 - Feb 2016 period. Most of these changes were made as a part of moving to a large computing cluster, where both the units of work and the output file organization had to be changed to make parallelization possible. The large number of tests run during this period and the need to more efficiently analyze and compare also introduced some changed to the analysis and plot modules. Since these changes do not pertain to any single test (though many were done during Dm-1136), I have put them on a separate ticket."
"Provide a shared stack on lsst-dev & other relevant systems Following the discussion in RFC-156, ensure that a documented, fast, easy to initialize shared stack is available for developers to use on shared systems, certainly to include {{lsst-dev}}.",3,DM-5435,datamanagement,provide share stack lsst dev relevant system follow discussion rfc-156 ensure document fast easy initialize share stack available developer use share system certainly include lsst dev,"Provide a shared stack on lsst-dev & other relevant systems Following the discussion in RFC-156, ensure that a documented, fast, easy to initialize shared stack is available for developers to use on shared systems, certainly to include {{lsst-dev}}."
Create unit test for ip_isr fallbackfilter DM-5287 introduced a configuration option that allows specifying a fallback filter in the event that getting a specific butler product fails. Currently there is no test for this functionality. One should be created which tests all the logical paths. This may involve just adapting or mimicking another test that already exists.,2,DM-5436,datamanagement,create unit test ip_isr fallbackfilter dm-5287 introduce configuration option allow specify fallback filter event get specific butler product fail currently test functionality create test logical path involve adapt mimic test exist,Create unit test for ip_isr fallbackfilter DM-5287 introduced a configuration option that allows specifying a fallback filter in the event that getting a specific butler product fails. Currently there is no test for this functionality. One should be created which tests all the logical paths. This may involve just adapting or mimicking another test that already exists.
"Move tests/negative.py from meas_algorithms to meas_base Porting code from HSC to LSST brought over a unit test into meas_algorithms for functionality that exists in meas_base in LSST. This is due to the refactoring of code into meas_base on the LSST some while ago. This unit test currently runs with code from meas_algorithms, which means it can not simply be moved, as meas_base comes before meas_algorithms in the build order. This work may involve rewriting the unit test to use different code, or evaluating if it is worth bringing that functionality to meas_base along with the test. The code in question is the detection task.",2,DM-5437,datamanagement,test negative.py meas_algorithm meas_base porting code hsc lsst bring unit test meas_algorithm functionality exist meas_base lsst refactoring code meas_base lsst ago unit test currently run code meas_algorithms mean simply move meas_base come meas_algorithm build order work involve rewrite unit test use different code evaluate worth bring functionality meas_base test code question detection task,"Move tests/negative.py from meas_algorithms to meas_base Porting code from HSC to LSST brought over a unit test into meas_algorithms for functionality that exists in meas_base in LSST. This is due to the refactoring of code into meas_base on the LSST some while ago. This unit test currently runs with code from meas_algorithms, which means it can not simply be moved, as meas_base comes before meas_algorithms in the build order. This work may involve rewriting the unit test to use different code, or evaluating if it is worth bringing that functionality to meas_base along with the test. The code in question is the detection task."
Data Backbone ConOps Data backbone first edit : 1pt (week 1)  Data backbone second edit : 1 pt (week 2)  Third edition : 1 pt week 3&4,3,DM-5438,datamanagement,data backbone conops data backbone edit 1pt week datum backbone second edit pt week edition pt week 3&4,Data Backbone ConOps Data backbone first edit : 1pt (week 1) Data backbone second edit : 1 pt (week 2) Third edition : 1 pt week 3&4
Add scipy as a stack dependency Adding scipy as a stack dependency is still a nebulous term to me.  David is going to follow up on how to do this exactly (it's already in conda_packages.txt).,2,DM-5446,datamanagement,add scipy stack dependency add scipy stack dependency nebulous term david go follow exactly conda_packages.txt,Add scipy as a stack dependency Adding scipy as a stack dependency is still a nebulous term to me. David is going to follow up on how to do this exactly (it's already in conda_packages.txt).
"Write technical note describing galaxy shear fitting experiments Through S15 (DM-1108) and W16 (DM-3561), [~pgee] has conducted a large-scale investigation into galaxy shear fitting. Please summarize the motivation, methodology and results of this study as a [technical note|http://sqr-000.lsst.io/en/master/].",8,DM-5447,datamanagement,write technical note describe galaxy shear fitting experiment s15 dm-1108 w16 dm-3561 ~pgee conduct large scale investigation galaxy shear fitting summarize motivation methodology result study technical note|http://sqr-000.lsst.io en master/,"Write technical note describing galaxy shear fitting experiments Through S15 (DM-1108) and W16 (DM-3561), [~pgee] has conducted a large-scale investigation into galaxy shear fitting. Please summarize the motivation, methodology and results of this study as a [technical note|http://sqr-000.lsst.io/en/master/]."
Familiarization with ngmix codebase Download the ngmix codebase from https://github.com/esheldon/ngmix. Install it and its dependencies in the same environment as the LSST stack. Experiment with using it and understanding how it works,3,DM-5448,datamanagement,familiarization ngmix codebase download ngmix codebase https://github.com/esheldon/ngmix install dependency environment lsst stack experiment understand work,Familiarization with ngmix codebase Download the ngmix codebase from https://github.com/esheldon/ngmix. Install it and its dependencies in the same environment as the LSST stack. Experiment with using it and understanding how it works
inter team discussion (X16) This epic is reserved for inter team discussion and supply/collect input to/from other teams.,6,DM-5451,datamanagement,inter team discussion x16 epic reserve inter team discussion supply collect input team,inter team discussion (X16) This epic is reserved for inter team discussion and supply/collect input to/from other teams.
"Adapt LTD Mason for Single-package doc builds on Travis CI LTD Mason was originally intended to build docs for DM’s Eups-based packages from our Jenkins CI/CD servers. There is tremendous value in consolidating all of DM’s Sphinx-based documentation deployments to use LSST the Docs rather than Read the Docs. This ticket will design and implement adaptations to LTD Mason to build single repo doc projects (Technotes, Design Documents, the Developer Guide, and even generic software projects) from a Travis CI environment. Also includes a template {{.travis.yml}} and associated documentation to allow others to enable travis builds for their documentation.    We name Travis specifically because it is the easiest platform for implementing CI for generic open source projects.",4,DM-5457,datamanagement,adapt ltd mason single package doc build travis ci ltd mason originally intend build doc dm eups base package jenkins ci cd server tremendous value consolidate dm sphinx base documentation deployment use lsst docs read docs ticket design implement adaptation ltd mason build single repo doc project technotes design documents developer guide generic software project travis ci environment include template .travis.yml associate documentation allow enable travis build documentation travis specifically easy platform implement ci generic open source project,"Adapt LTD Mason for Single-package doc builds on Travis CI LTD Mason was originally intended to build docs for DM s Eups-based packages from our Jenkins CI/CD servers. There is tremendous value in consolidating all of DM s Sphinx-based documentation deployments to use LSST the Docs rather than Read the Docs. This ticket will design and implement adaptations to LTD Mason to build single repo doc projects (Technotes, Design Documents, the Developer Guide, and even generic software projects) from a Travis CI environment. Also includes a template {{.travis.yml}} and associated documentation to allow others to enable travis builds for their documentation. We name Travis specifically because it is the easiest platform for implementing CI for generic open source projects."
Update SQR-006 LSST the Docs technote to reflect deployment in DM-5404 This ticket will ensure that [SQR-006|http://sqr-006.lsst.io] reflects the LSST the Docs continuous delivery platform as it is deployed in the DM-5404 epic. (SQR-006 was initially written as a planning/design document).    This story should be closed only once the DM-5404 epic is ready to be closed.,4,DM-5458,datamanagement,update sqr-006 lsst docs technote reflect deployment dm-5404 ticket ensure sqr-006|http://sqr-006.lsst.io reflect lsst docs continuous delivery platform deploy dm-5404 epic sqr-006 initially write planning design document story close dm-5404 epic ready close,Update SQR-006 LSST the Docs technote to reflect deployment in DM-5404 This ticket will ensure that [SQR-006|http://sqr-006.lsst.io] reflects the LSST the Docs continuous delivery platform as it is deployed in the DM-5404 epic. (SQR-006 was initially written as a planning/design document). This story should be closed only once the DM-5404 epic is ready to be closed.
"Add non-linearity correction to ISR task Implement RFC-164    At the moment some preliminary code is on ticket branches, but this need to be redone once the RFC is finished.",6,DM-5462,datamanagement,add non linearity correction isr task implement rfc-164 moment preliminary code ticket branch need redo rfc finish,"Add non-linearity correction to ISR task Implement RFC-164 At the moment some preliminary code is on ticket branches, but this need to be redone once the RFC is finished."
Don't restore the mask in CharacterizeImageTask.characterize CharacterizeImageTask.characterize presently restores the mask from a deep copy for each iteration of the loop to compute PSF. This is unnecessary because repair and detection both clear the relevant mask planes before setting new values.,1,DM-5463,datamanagement,restore mask characterizeimagetask.characterize characterizeimagetask.characterize presently restore mask deep copy iteration loop compute psf unnecessary repair detection clear relevant mask plane set new value,Don't restore the mask in CharacterizeImageTask.characterize CharacterizeImageTask.characterize presently restores the mask from a deep copy for each iteration of the loop to compute PSF. This is unnecessary because repair and detection both clear the relevant mask planes before setting new values.
Develop C++ code for experimenting with Python binding Produce a small C++ codebase that can be used for experimenting with the various technologies we can be used for exposing C++ to Python. It should enable us to experiment with as many of the potential pain points with these technologies as possible,3,DM-5470,datamanagement,develop c++ code experiment python bind produce small c++ codebase experiment technology expose c++ python enable experiment potential pain point technology possible,Develop C++ code for experimenting with Python binding Produce a small C++ codebase that can be used for experimenting with the various technologies we can be used for exposing C++ to Python. It should enable us to experiment with as many of the potential pain points with these technologies as possible
"Update meas_mosaic for compatibility with new single frame processing Following [recent changes to single frame processing|https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581], {{icSrc}} no longer includes celestial coordinates and {{icMatch}} is no longer being written. {{meas_mosaic}} requires this information. Provide a work-around.",3,DM-5472,datamanagement,update meas_mosaic compatibility new single frame processing follow recent change single frame processing|https://community.lsst.org backward incompatible change processccdtask subtasks/581 icsrc long include celestial coordinate icmatch long write meas_mosaic require information provide work,"Update meas_mosaic for compatibility with new single frame processing Following [recent changes to single frame processing|https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581], {{icSrc}} no longer includes celestial coordinates and {{icMatch}} is no longer being written. {{meas_mosaic}} requires this information. Provide a work-around."
"Jenkins/ci_hsc failure: 'base_PixelFlags_flag_clipped' already present in schema Since 15 March, the {{ci_hsc}} build in Jenkins has been failing as follows:    {code}  [2016-03-16T14:23:13.548928Z] Traceback (most recent call last):  [2016-03-16T14:23:13.548956Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/bin/measureCoaddSources.py"", line 3, in <module>  [2016-03-16T14:23:13.548969Z]     MeasureMergedCoaddSourcesTask.parseAndRun()  [2016-03-16T14:23:13.548999Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun  [2016-03-16T14:23:13.549011Z]     resultList = taskRunner.run(parsedCmd)  [2016-03-16T14:23:13.549040Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run  [2016-03-16T14:23:13.549048Z]     if self.precall(parsedCmd):  [2016-03-16T14:23:13.549076Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall  [2016-03-16T14:23:13.549087Z]     task = self.makeTask(parsedCmd=parsedCmd)  [2016-03-16T14:23:13.549115Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 369, in makeTask  [2016-03-16T14:23:13.549132Z]     return self.TaskClass(config=self.config, log=self.log, butler=butler)  [2016-03-16T14:23:13.549160Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/python/lsst/pipe/tasks/multiBand.py"", line 1008, in __init__  [2016-03-16T14:23:13.549179Z]     self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata)  [2016-03-16T14:23:13.549206Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/task.py"", line 226, in makeSubtask  [2016-03-16T14:23:13.549846Z]     subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)  [2016-03-16T14:23:13.549901Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0+1/python/lsst/pex/config/configurableField.py"", line 77, in apply  [2016-03-16T14:23:13.549915Z]     return self.target(*args, config=self.value, **kw)  [2016-03-16T14:23:13.549943Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/sfm.py"", line 248, in __init__  [2016-03-16T14:23:13.549954Z]     self.initializePlugins(schema=self.schema)  [2016-03-16T14:23:13.549985Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins  [2016-03-16T14:23:13.550004Z]     self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds)  [2016-03-16T14:23:13.550032Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 15, in __init__  [2016-03-16T14:23:13.550616Z]     self.cpp = self.factory(config, name, schema, metadata)  [2016-03-16T14:23:13.550647Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 223, in factory  [2016-03-16T14:23:13.550660Z]     return AlgClass(config.makeControl(), name, schema)  [2016-03-16T14:23:13.550688Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseLib.py"", line 3401, in __init__  [2016-03-16T14:23:13.552891Z]     this = _baseLib.new_PixelFlagsAlgorithm(*args)  [2016-03-16T14:23:13.552924Z] lsst.pex.exceptions.wrappers.InvalidParameterError:   [2016-03-16T14:23:13.552967Z]   File ""src/table/Schema.cc"", line 563, in lsst::afw::table::Key<lsst::afw::table::Flag> lsst::afw::table::detail::SchemaImpl::addField(const lsst::afw::table::Field<lsst::afw::table::Flag>&, bool)  [2016-03-16T14:23:13.552986Z]     Field with name 'base_PixelFlags_flag_clipped' already present in schema. {0}  [2016-03-16T14:23:13.553012Z] lsst::pex::exceptions::InvalidParameterError: 'Field with name 'base_PixelFlags_flag_clipped' already present in schema.'  [2016-03-16T14:23:13.553014Z]   [2016-03-16T14:23:13.613484Z] scons: *** [.scons/measure] Error 1  [2016-03-16T14:23:13.617577Z] scons: building terminated because of errors.  {code}    Please fix it.",1,DM-5473,datamanagement,"jenkins ci_hsc failure base_pixelflags_flag_clippe present schema 15 march ci_hsc build jenkins fail follow code 2016 03 16t14:23:13.548928z traceback recent 2016 03 16t14:23:13.548956z file /home build0 lsstsw stack linux64 pipe_tasks/2016_01.0 23 gcf99090 bin measurecoaddsources.py line 2016 03 16t14:23:13.548969z measuremergedcoaddsourcestask.parseandrun 2016 03 16t14:23:13.548999z file /home build0 lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base cmdlinetask.py line 450 parseandrun 2016 03 16t14:23:13.549011z resultlist taskrunner.run(parsedcmd 2016 03 16t14:23:13.549040z file /home build0 lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base cmdlinetask.py line 192 run 2016 03 16t14:23:13.549048z self.precall(parsedcmd 2016 03 16t14:23:13.549076z file /home build0 lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base cmdlinetask.py line 279 precall 2016 03 16t14:23:13.549087z task self.maketask(parsedcmd parsedcmd 2016 03 16t14:23:13.549115z file /home build0 lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base cmdlinetask.py line 369 maketask 2016 03 16t14:23:13.549132z return self taskclass(config self.config log self.log butler butler 2016 03 16t14:23:13.549160z file /home build0 lsstsw stack linux64 pipe_tasks/2016_01.0 23 gcf99090 python lsst pipe task multiband.py line 1008 init 2016 03 16t14:23:13.549179z self.makesubtask(""measurement schema self.schema algmetadata self.algmetadata 2016 03 16t14:23:13.549206z file /home build0 lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base task.py line 226 makesubtask 2016 03 16t14:23:13.549846z subtask configurablefield.apply(name parenttask self keyargs 2016 03 16t14:23:13.549901z file /home build0 lsstsw stack linux64 pex_config/2016_01.0 python lsst pex config configurablefield.py line 77 apply 2016 03 16t14:23:13.549915z return self.target(*args config self.value kw 2016 03 16t14:23:13.549943z file /home build0 lsstsw stack linux64 meas_base/2016_01.0 12 gf26bc28 python lsst meas base sfm.py line 248 init 2016 03 16t14:23:13.549954z self.initializeplugins(schema self.schema 2016 03 16t14:23:13.549985z file /home build0 lsstsw stack linux64 meas_base/2016_01.0 12 gf26bc28 python lsst meas base basemeasurement.py line 298 initializeplugin 2016 03 16t14:23:13.550004z self.plugins[name pluginclass(config metadata self.algmetadata kwd 2016 03 16t14:23:13.550032z file /home build0 lsstsw stack linux64 meas_base/2016_01.0 12 gf26bc28 python lsst meas base wrappers.py line 15 init 2016 03 16t14:23:13.550616z self.cpp self.factory(config schema metadata 2016 03 16t14:23:13.550647z file /home build0 lsstsw stack linux64 meas_base/2016_01.0 12 gf26bc28 python lsst meas base wrappers.py line 223 factory 2016 03 16t14:23:13.550660z return algclass(config.makecontrol schema 2016 03 16t14:23:13.550688z file /home build0 lsstsw stack linux64 meas_base/2016_01.0 12 gf26bc28 python lsst meas base baselib.py line 3401 init 2016 03 16t14:23:13.552891z baselib.new_pixelflagsalgorithm(*args 2016 03 16t14:23:13.552924z lsst.pex.exceptions.wrapper invalidparametererror 2016 03 16t14:23:13.552967z file src table schema.cc line 563 lsst::afw::table::key lsst::afw::table::detail::schemaimpl::addfield(const lsst::afw::table::field bool 2016 03 16t14:23:13.552986z field base_pixelflags_flag_clippe present schema 2016 03 16t14:23:13.553012z lsst::pex::exceptions::invalidparametererror field base_pixelflags_flag_clippe present schema 2016 03 16t14:23:13.553014z 2016 03 16t14:23:13.613484z scon .scons measure error 2016 03 16t14:23:13.617577z scon building terminate error code fix","Jenkins/ci_hsc failure: 'base_PixelFlags_flag_clipped' already present in schema Since 15 March, the {{ci_hsc}} build in Jenkins has been failing as follows: {code} [2016-03-16T14:23:13.548928Z] Traceback (most recent call last): [2016-03-16T14:23:13.548956Z] File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/bin/measureCoaddSources.py"", line 3, in  [2016-03-16T14:23:13.548969Z] MeasureMergedCoaddSourcesTask.parseAndRun() [2016-03-16T14:23:13.548999Z] File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun [2016-03-16T14:23:13.549011Z] resultList = taskRunner.run(parsedCmd) [2016-03-16T14:23:13.549040Z] File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run [2016-03-16T14:23:13.549048Z] if self.precall(parsedCmd): [2016-03-16T14:23:13.549076Z] File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall [2016-03-16T14:23:13.549087Z] task = self.makeTask(parsedCmd=parsedCmd) [2016-03-16T14:23:13.549115Z] File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 369, in makeTask [2016-03-16T14:23:13.549132Z] return self.TaskClass(config=self.config, log=self.log, butler=butler) [2016-03-16T14:23:13.549160Z] File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/python/lsst/pipe/tasks/multiBand.py"", line 1008, in __init__ [2016-03-16T14:23:13.549179Z] self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata) [2016-03-16T14:23:13.549206Z] File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/task.py"", line 226, in makeSubtask [2016-03-16T14:23:13.549846Z] subtask = configurableField.apply(name=name, parentTask=self, **keyArgs) [2016-03-16T14:23:13.549901Z] File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0+1/python/lsst/pex/config/configurableField.py"", line 77, in apply [2016-03-16T14:23:13.549915Z] return self.target(*args, config=self.value, **kw) [2016-03-16T14:23:13.549943Z] File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/sfm.py"", line 248, in __init__ [2016-03-16T14:23:13.549954Z] self.initializePlugins(schema=self.schema) [2016-03-16T14:23:13.549985Z] File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins [2016-03-16T14:23:13.550004Z] self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds) [2016-03-16T14:23:13.550032Z] File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 15, in __init__ [2016-03-16T14:23:13.550616Z] self.cpp = self.factory(config, name, schema, metadata) [2016-03-16T14:23:13.550647Z] File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 223, in factory [2016-03-16T14:23:13.550660Z] return AlgClass(config.makeControl(), name, schema) [2016-03-16T14:23:13.550688Z] File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseLib.py"", line 3401, in __init__ [2016-03-16T14:23:13.552891Z] this = _baseLib.new_PixelFlagsAlgorithm(*args) [2016-03-16T14:23:13.552924Z] lsst.pex.exceptions.wrappers.InvalidParameterError: [2016-03-16T14:23:13.552967Z] File ""src/table/Schema.cc"", line 563, in lsst::afw::table::Key lsst::afw::table::detail::SchemaImpl::addField(const lsst::afw::table::Field&, bool) [2016-03-16T14:23:13.552986Z] Field with name 'base_PixelFlags_flag_clipped' already present in schema. {0} [2016-03-16T14:23:13.553012Z] lsst::pex::exceptions::InvalidParameterError: 'Field with name 'base_PixelFlags_flag_clipped' already present in schema.' [2016-03-16T14:23:13.553014Z] [2016-03-16T14:23:13.613484Z] scons: *** [.scons/measure] Error 1 [2016-03-16T14:23:13.617577Z] scons: building terminated because of errors. {code} Please fix it."
"Bugs in obs_subaru found by PyFlakes I ran pyflakes on the code in obs_subaru and found a few bugs (beyond a few trivial ones that I am fixing as part of DM-5462)    {{ingest.py}} has undefined name {{day0}}    {{ccdTesting.py}} has at least three undefined variables: {{x}}, {{y}} and {{vig}} in the following:  {code}      ngood += pupilImage[y[good], x[good]].sum()    vig[i] = float(ngood)  {code}    {{crosstalkYagi.py}} has many undefined names, starting with {{makeList}}, {{estimateCoeffs}}",1,DM-5474,datamanagement,bug obs_subaru find pyflakes run pyflake code obs_subaru find bug trivial one fix dm-5462 ingest.py undefined day0 ccdtesting.py undefined variable vig following code ngood pupilimage[y[good x[good]].sum vig[i float(ngood code crosstalkyagi.py undefined name start makelist estimatecoeffs,"Bugs in obs_subaru found by PyFlakes I ran pyflakes on the code in obs_subaru and found a few bugs (beyond a few trivial ones that I am fixing as part of DM-5462) {{ingest.py}} has undefined name {{day0}} {{ccdTesting.py}} has at least three undefined variables: {{x}}, {{y}} and {{vig}} in the following: {code} ngood += pupilImage[y[good], x[good]].sum() vig[i] = float(ngood) {code} {{crosstalkYagi.py}} has many undefined names, starting with {{makeList}}, {{estimateCoeffs}}"
"Document investigation of logging, monitoring and metrics technologies and architecture Finish technote SQR-007. Related to DM-4970",4,DM-5475,datamanagement,document investigation log monitoring metric technology architecture finish technote sqr-007 relate dm-4970,"Document investigation of logging, monitoring and metrics technologies and architecture Finish technote SQR-007. Related to DM-4970"
"Revise FlagHandler  {{FlagHandler}} is ""unpolished ... and a bit dangerous to the unwary"" (DM-5247).  It could be improved by leveraging C++11 features, replacing the default constructor with something that defines the (required) general failure flag, and allowing flags to be added individually.    A potential starting point is [here|https://jira.lsstcorp.org/browse/DM-5247?focusedCommentId=45894&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-45894].",4,DM-5476,datamanagement,revise flaghandler flaghandler unpolished bit dangerous unwary dm-5247 improve leverage c++11 feature replace default constructor define require general failure flag allow flag add individually potential starting point here|https://jira.lsstcorp.org browse dm-5247?focusedcommentid=45894&page com.atlassian.jira.plugin.system.issuetabpanel comment tabpanel#comment-45894,"Revise FlagHandler {{FlagHandler}} is ""unpolished ... and a bit dangerous to the unwary"" (DM-5247). It could be improved by leveraging C++11 features, replacing the default constructor with something that defines the (required) general failure flag, and allowing flags to be added individually. A potential starting point is [here|https://jira.lsstcorp.org/browse/DM-5247?focusedCommentId=45894&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-45894]."
"Firefly support for Camera team visualization needs (X16) Attend the weekly meeting with the camera team and UIUC development team, provide support in discussion and API usage. ",4,DM-5477,datamanagement,firefly support camera team visualization need x16 attend weekly meeting camera team uiuc development team provide support discussion api usage,"Firefly support for Camera team visualization needs (X16) Attend the weekly meeting with the camera team and UIUC development team, provide support in discussion and API usage."
Wrote script to print the names of all visits that overlap a patch In order to finish the IDL workflow module for makeCoaddTempExp I needed a program to say which visits overlap a given path.  That's what this script does.,5,DM-5479,datamanagement,wrote script print name visit overlap patch order finish idl workflow module need program visit overlap give path script,Wrote script to print the names of all visits that overlap a patch In order to finish the IDL workflow module for makeCoaddTempExp I needed a program to say which visits overlap a given path. That's what this script does.
Write presentation on verification datasets for AAS Prepared and gave a talk at the NSF booth at the Florida AAS meeting on the progress of the verification datasets effort.,5,DM-5482,datamanagement,write presentation verification dataset aas prepared give talk nsf booth florida aas meeting progress verification dataset effort,Write presentation on verification datasets for AAS Prepared and gave a talk at the NSF booth at the Florida AAS meeting on the progress of the verification datasets effort.
"Work on script to test the astrometric matcher We encouraged astrometric matching problems for the Bulge verification dataset.  Therefore, I wrote a script that tests the matcher by systematically shifting the coordinates of one sets of the data to see if the matcher still works.  It worked well until ~80 arcsec.",5,DM-5483,datamanagement,work script test astrometric matcher encourage astrometric matching problem bulge verification dataset write script test matcher systematically shift coordinate set datum matcher work work ~80 arcsec,"Work on script to test the astrometric matcher We encouraged astrometric matching problems for the Bulge verification dataset. Therefore, I wrote a script that tests the matcher by systematically shifting the coordinates of one sets of the data to see if the matcher still works. It worked well until ~80 arcsec."
SdssMapper.paf has wrong python type for processCcd_config [~npease] reports that {{Sdssmapper.paf}} has the wrong python data type for the dataset {{processCcd_config}}: it is {{lsst.obs.sdss.processCcdSdss.ProcessCcdSdssConfig}} instead of {{lsst.pipe.tasks.processCcd.ProcessCcdConfig}},1,DM-5484,datamanagement,sdssmapper.paf wrong python type processccd_config ~npease report sdssmapper.paf wrong python data type dataset processccd_config lsst.obs.sdss.processccdsdss processccdsdssconfig instead lsst.pipe.tasks.processccd processccdconfig,SdssMapper.paf has wrong python type for processCcd_config [~npease] reports that {{Sdssmapper.paf}} has the wrong python data type for the dataset {{processCcd_config}}: it is {{lsst.obs.sdss.processCcdSdss.ProcessCcdSdssConfig}} instead of {{lsst.pipe.tasks.processCcd.ProcessCcdConfig}}
"Work on plan to test specific algorithmic components of the stack After working on a script to test the astrometric matcher, I decided to put together a plan to run similar tests on our algorithmic code.  The rough plan is here:  https://confluence.lsstcorp.org/display/SQRE/Stack+Testing+Plan",2,DM-5485,datamanagement,work plan test specific algorithmic component stack work script test astrometric matcher decide plan run similar test algorithmic code rough plan https://confluence.lsstcorp.org/display/sqre/stack+testing+plan,"Work on plan to test specific algorithmic components of the stack After working on a script to test the astrometric matcher, I decided to put together a plan to run similar tests on our algorithmic code. The rough plan is here: https://confluence.lsstcorp.org/display/SQRE/Stack+Testing+Plan"
"Work on putting together page of ""tips and tricks for using the stack"" Due to the incomplete state of the stack documentation and tutorials, I decided to write down various ""tips and tricks"" for using the stack as I learn them.  https://confluence.lsstcorp.org/display/SQRE/Tips+and+Tricks+for+using+the+Stack",2,DM-5486,datamanagement,work put page tip trick stack incomplete state stack documentation tutorial decide write tip trick stack learn https://confluence.lsstcorp.org/display/sqre/tips+and+tricks+for+using+the+stack,"Work on putting together page of ""tips and tricks for using the stack"" Due to the incomplete state of the stack documentation and tutorials, I decided to write down various ""tips and tricks"" for using the stack as I learn them. https://confluence.lsstcorp.org/display/SQRE/Tips+and+Tricks+for+using+the+Stack"
"Revise operations concept for Observation Processing System Turn the L1 ConOps document into appropriate sections of LDM-230, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.     (Story points are for KTL drafting and initial contributions)",2,DM-5487,datamanagement,revise operation concept observation processing system turn l1 conops document appropriate section ldm-230 specify automate operation sequence human intervention occur process handle change update story point ktl draft initial contribution,"Revise operations concept for Observation Processing System Turn the L1 ConOps document into appropriate sections of LDM-230, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates. (Story points are for KTL drafting and initial contributions)"
"Field group updates After some work we have realized that the following needs to be done to field groups:    * Tabs group should have a field group smart wrapper component  * field group needs to reinit on id change   * remove mixin, use Higher-Order Components instead  * support a function for a value, this function will return a value or a promise  * hidden fields - init field group with key/value object  * Sub-field groups? study only, unless it is easy to implement.  * maintain an option to keep unmount field value available  * determine if InitValue needs to be passed around  * passing fieldState around too much  * find reason for react warning every time popup is raised  * look at promise code make sure it is working the way we think  * if practical, remove all export default    FieldGroupConnector.  It is the high order component that replaces the mixin.   FieldGroupUtils.js:  (~line 33): The field value would be a function on the file upload case. Therefore the upload does not activate until validation. In the upload case the function would return a promise. However, It could return a value or an object with a value and a valid status. Now the value key of a field can contain a promise or function or primitive. The function can return a primitive, a promise, or an object with primitive and status.    fftools.js lines 102-158 you can see my experimenting with taking out the connector. It works fine and does eliminate one of the warning messages.    ",8,DM-5488,datamanagement,field group update work realize follow need field group tabs group field group smart wrapper component field group need reinit change remove mixin use high order components instead support function value function return value promise hide field init field group key value object sub field group study easy implement maintain option unmount field value available determine initvalue need pass pass fieldstate find reason react warning time popup raise look promise code sure work way think practical remove export default fieldgroupconnector high order component replace mixin fieldgrouputils.js 33 field value function file upload case upload activate validation upload case function return promise return value object value valid status value key field contain promise function primitive function return primitive promise object primitive status fftools.js line 102 158 experiment take connector work fine eliminate warning message,"Field group updates After some work we have realized that the following needs to be done to field groups: * Tabs group should have a field group smart wrapper component * field group needs to reinit on id change * remove mixin, use Higher-Order Components instead * support a function for a value, this function will return a value or a promise * hidden fields - init field group with key/value object * Sub-field groups? study only, unless it is easy to implement. * maintain an option to keep unmount field value available * determine if InitValue needs to be passed around * passing fieldState around too much * find reason for react warning every time popup is raised * look at promise code make sure it is working the way we think * if practical, remove all export default FieldGroupConnector. It is the high order component that replaces the mixin. FieldGroupUtils.js: (~line 33): The field value would be a function on the file upload case. Therefore the upload does not activate until validation. In the upload case the function would return a promise. However, It could return a value or an object with a value and a valid status. Now the value key of a field can contain a promise or function or primitive. The function can return a primitive, a promise, or an object with primitive and status. fftools.js lines 102-158 you can see my experimenting with taking out the connector. It works fine and does eliminate one of the warning messages."
improvement of the north/east arrow on image make the compass sticky when scroll the image,1,DM-5489,datamanagement,improvement north east arrow image compass sticky scroll image,improvement of the north/east arrow on image make the compass sticky when scroll the image
"Develop operations concept for Batch Processing System Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the batch processing environment, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)",3,DM-5490,datamanagement,develop operation concept batch processing system develop conops document include appropriate section ldm-230 describe batch processing environment specify automate operation sequence human intervention occur process handle change update story point ktl draft initial contribution,"Develop operations concept for Batch Processing System Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the batch processing environment, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates. (Story points are for KTL drafting and initial contributions)"
"Develop operations concept for Data Backbone Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Backbone that contains, manages, and provides access to the Science Data Archive, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)",3,DM-5491,datamanagement,develop operation concept data backbone develop conops document include appropriate section ldm-230 describe data backbone contain manage provide access science data archive specify automate operation sequence human intervention occur process handle change update story point ktl draft initial contribution,"Develop operations concept for Data Backbone Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Backbone that contains, manages, and provides access to the Science Data Archive, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates. (Story points are for KTL drafting and initial contributions)"
"Develop operations concept for Data Access Processing System Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Access Processing System that manages L3 computing in and interfaces to the Data Access Center, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)",3,DM-5492,datamanagement,develop operation concept data access processing system develop conops document include appropriate section ldm-230 describe data access processing system manage l3 compute interface data access center specify automate operation sequence human intervention occur process handle change update story point ktl draft initial contribution,"Develop operations concept for Data Access Processing System Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Access Processing System that manages L3 computing in and interfaces to the Data Access Center, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates. (Story points are for KTL drafting and initial contributions)"
"Develop functional breakdown for Observation Processing System Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Observation Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",3,DM-5493,datamanagement,develop functional breakdown observation processing system write section incorporate ldm-148 describe functional breakdown observation processing system include major element overall function input output control interface component description function perform story point ktl draft initial contribution,"Develop functional breakdown for Observation Processing System Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Observation Processing System, including, for each major element: * overall function * inputs, outputs, and control interfaces * components used * descriptions of functions to be performed (Story points are for KTL drafting and initial contributions)"
"Develop functional breakdown for Batch Processing System Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Batch Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",2,DM-5494,datamanagement,develop functional breakdown batch processing system write section incorporate ldm-148 describe functional breakdown batch processing system include major element overall function input output control interface component description function perform story point ktl draft initial contribution,"Develop functional breakdown for Batch Processing System Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Batch Processing System, including, for each major element: * overall function * inputs, outputs, and control interfaces * components used * descriptions of functions to be performed (Story points are for KTL drafting and initial contributions)"
"Develop functional breakdown for Data Backbone Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Backbone, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",2,DM-5495,datamanagement,develop functional breakdown data backbone write section incorporate ldm-148 describe functional breakdown data backbone include major element overall function input output control interface component description function perform story point ktl draft initial contribution,"Develop functional breakdown for Data Backbone Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Backbone, including, for each major element: * overall function * inputs, outputs, and control interfaces * components used * descriptions of functions to be performed (Story points are for KTL drafting and initial contributions)"
"Develop functional breakdown for Data Access Center Processing System Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Access Center Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",2,DM-5496,datamanagement,develop functional breakdown data access center processing system write section incorporate ldm-148 describe functional breakdown data access center processing system include major element overall function input output control interface component description function perform story point ktl draft initial contribution,"Develop functional breakdown for Data Access Center Processing System Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Access Center Processing System, including, for each major element: * overall function * inputs, outputs, and control interfaces * components used * descriptions of functions to be performed (Story points are for KTL drafting and initial contributions)"
Coordinate completion of operations concepts Coordinate the creation of a new version of LDM-230 incorporating DPS-WG-generated operations concepts.,2,DM-5498,datamanagement,coordinate completion operation concept coordinate creation new version ldm-230 incorporate dps wg generate operation concept,Coordinate completion of operations concepts Coordinate the creation of a new version of LDM-230 incorporating DPS-WG-generated operations concepts.
Coordinate completion of functional breakdowns Coordinate the creation of a new version of LDM-148 incorporating DPS-WG-generated functional breakdowns.,2,DM-5499,datamanagement,coordinate completion functional breakdown coordinate creation new version ldm-148 incorporate dps wg generate functional breakdown,Coordinate completion of functional breakdowns Coordinate the creation of a new version of LDM-148 incorporating DPS-WG-generated functional breakdowns.
"Collect usage of header metadata Collect a comprehensive set of exposure oriented metadata used by science code.  This should also include metadata that is not currently needed but that could be utilized in the future.  In practice, I suspect this will involve looking for all calls to PropertySet.get since that is how FITS header metadata is currently passed around.",5,DM-5502,datamanagement,collect usage header metadata collect comprehensive set exposure orient metadata science code include metadata currently need utilize future practice suspect involve look call propertyset.get fits header metadata currently pass,"Collect usage of header metadata Collect a comprehensive set of exposure oriented metadata used by science code. This should also include metadata that is not currently needed but that could be utilized in the future. In practice, I suspect this will involve looking for all calls to PropertySet.get since that is how FITS header metadata is currently passed around."
DAX & DB Docs (ABH) * Add memman documentation (in LDM-135)  * Refresh XRDSSI documentation (in LDM-135),6,DM-5506,datamanagement,dax db docs abh add memman documentation ldm-135 refresh xrdssi documentation ldm-135,DAX & DB Docs (ABH) * Add memman documentation (in LDM-135) * Refresh XRDSSI documentation (in LDM-135)
alert production database next steps (April) Place-holder for additional alert production database work after investigate design task completes.  We should split this into smaller stories for a total of 18 points this cycle.,7,DM-5509,datamanagement,alert production database step april place holder additional alert production database work investigate design task complete split small story total 18 point cycle,alert production database next steps (April) Place-holder for additional alert production database work after investigate design task completes. We should split this into smaller stories for a total of 18 points this cycle.
Weekly and monthly releases   Some manual process at the rate of 1 SP / month is still involved in the releases until the automating publishing process is complete. ,6,DM-5523,datamanagement,weekly monthly release manual process rate sp month involve release automate publishing process complete,Weekly and monthly releases Some manual process at the rate of 1 SP / month is still involved in the releases until the automating publishing process is complete.
"Add documentation to BinnedWcs DM-5282 ported functionality from HSC to work in ""super-pixels"" which are the result of binning in wcs. This functionality was introduced in binnedWcs.(cc/h). This functionality needs proper doxygen documentation added.",1,DM-5526,datamanagement,add documentation binnedwcs dm-5282 port functionality hsc work super pixel result bin wcs functionality introduce binnedwcs.(cc functionality need proper doxygen documentation add,"Add documentation to BinnedWcs DM-5282 ported functionality from HSC to work in ""super-pixels"" which are the result of binning in wcs. This functionality was introduced in binnedWcs.(cc/h). This functionality needs proper doxygen documentation added."
"Change star selectors to return stars instead of PSF candidates Implement RFC-154:  - Make star selectors tasks, but continue to use and prefer a registry  - Add an abstract base class for star selectors with the following methods:    - {{selectStars}} abstract method that takes a catalog of sources and returns a {{lsst.pipe.base.Struct}} containing a catalog of stars    - {{run}} concrete method that takes a catalog of sources and an optional name of a flag field, calls {{selectStars}} to select stars, then sets the flag field (if given) for stars    - {{makePsfCandidates}} make a list of psf candidates from a catalog of stars (does no selection, other than skipping stars that cannot be made into candidates, and logging the rejects)  ",4,DM-5532,datamanagement,change star selector return star instead psf candidate implement rfc-154 star selector task continue use prefer registry add abstract base class star selector follow method selectstars abstract method take catalog source return lsst.pipe.base struct contain catalog star run concrete method take catalog source optional flag field call selectstars select star set flag field give star makepsfcandidates list psf candidate catalog star selection skip star candidate log reject,"Change star selectors to return stars instead of PSF candidates Implement RFC-154: - Make star selectors tasks, but continue to use and prefer a registry - Add an abstract base class for star selectors with the following methods: - {{selectStars}} abstract method that takes a catalog of sources and returns a {{lsst.pipe.base.Struct}} containing a catalog of stars - {{run}} concrete method that takes a catalog of sources and an optional name of a flag field, calls {{selectStars}} to select stars, then sets the flag field (if given) for stars - {{makePsfCandidates}} make a list of psf candidates from a catalog of stars (does no selection, other than skipping stars that cannot be made into candidates, and logging the rejects)"
"AFW rgb.py has undefined variable that breaks a test in some situations The {{rgb.py}} test is failing for me with current AFW master:  {code}  tests/rgb.py  .E............  ======================================================================  ERROR: testMakeRGBResize (__main__.RgbTestCase)  Test the function that does it all, including rescaling  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/rgb.py"", line 313, in testMakeRGBResize      with Tempfile(fileName, remove=True):  NameError: global name 'Tempfile' is not defined    ----------------------------------------------------------------------  Ran 16 tests in 7.296s    FAILED (errors=1)  {code}    {{Tempfile}} is definitely only used in line 313. It was introduced with commit c9864f49.    I'm not entirely sure how this is not picked up by Jenkins as the test will run if matplotlib and scipy are installed and Jenkins does have those.    ",1,DM-5542,datamanagement,afw rgb.py undefine variable break test situation rgb.py test fail current afw master code test rgb.py .e error testmakergbresize main__.rgbtestcase test function include rescale traceback recent file test rgb.py line 313 testmakergbresize tempfile(filename remove true nameerror global tempfile define ran 16 test 7.296s failed errors=1 code tempfile definitely line 313 introduce commit c9864f49 entirely sure pick jenkins test run matplotlib scipy instal jenkins,"AFW rgb.py has undefined variable that breaks a test in some situations The {{rgb.py}} test is failing for me with current AFW master: {code} tests/rgb.py .E............ ====================================================================== ERROR: testMakeRGBResize (__main__.RgbTestCase) Test the function that does it all, including rescaling ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/rgb.py"", line 313, in testMakeRGBResize with Tempfile(fileName, remove=True): NameError: global name 'Tempfile' is not defined ---------------------------------------------------------------------- Ran 16 tests in 7.296s FAILED (errors=1) {code} {{Tempfile}} is definitely only used in line 313. It was introduced with commit c9864f49. I'm not entirely sure how this is not picked up by Jenkins as the test will run if matplotlib and scipy are installed and Jenkins does have those."
"Add renderer option to js table TablePanel and BasicTable now accept optional renderers.  For each column, you can set a custom renderer for the header, cell, or both.  Also, created several commonly used renderer for images, links, and input field.",2,DM-5552,datamanagement,add renderer option js table tablepanel basictable accept optional renderer column set custom renderer header cell create commonly renderer image link input field,"Add renderer option to js table TablePanel and BasicTable now accept optional renderers. For each column, you can set a custom renderer for the header, cell, or both. Also, created several commonly used renderer for images, links, and input field."
Z-scale stretch for image display The z-scale stretch in current system is different from the one in OPS,8,DM-5553,datamanagement,scale stretch image display scale stretch current system different ops,Z-scale stretch for image display The z-scale stretch in current system is different from the one in OPS
"Assist in document investigation of logging, monitoring and metrics technologies and architecture Assist with tech note SQR-007 and document investigation of logging, monitoring and metrics technologies and architecture.",2,DM-5554,datamanagement,assist document investigation log monitoring metric technology architecture assist tech note sqr-007 document investigation log monitoring metric technology architecture,"Assist in document investigation of logging, monitoring and metrics technologies and architecture Assist with tech note SQR-007 and document investigation of logging, monitoring and metrics technologies and architecture."
Present Supertask design to DMLT Present the Supertask design to the November 2015 DMLT in-person meeting.    Covers preparation of a presentation and related discussions preceding and immediately following the meeting.,6,DM-5559,datamanagement,present supertask design dmlt present supertask design november 2015 dmlt person meeting cover preparation presentation related discussion precede immediately follow meeting,Present Supertask design to DMLT Present the Supertask design to the November 2015 DMLT in-person meeting. Covers preparation of a presentation and related discussions preceding and immediately following the meeting.
"Participate in October 2015 OCS-subsystems teleconference Prepare for, attend, and follow up on the OCS-subsystems teleconference on October 8, 2015.",2,DM-5560,datamanagement,participate october 2015 ocs subsystem teleconference prepare attend follow ocs subsystem teleconference october 2015,"Participate in October 2015 OCS-subsystems teleconference Prepare for, attend, and follow up on the OCS-subsystems teleconference on October 8, 2015."
"Participate in November 2015 OCS-subsystems teleconference (LSE-70, LSE-209) Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015.  This story covers work related to LSE-70 and LSE-209; LSE-74 work was also done under a separate epic.",4,DM-5562,datamanagement,participate november 2015 ocs subsystem teleconference lse-70 lse-209 prepare attend follow ocs subsystem teleconference november 11 2015 story cover work relate lse-70 lse-209 lse-74 work separate epic,"Participate in November 2015 OCS-subsystems teleconference (LSE-70, LSE-209) Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015. This story covers work related to LSE-70 and LSE-209; LSE-74 work was also done under a separate epic."
"Participate in November 2015 OCS-subsystems teleconference (LSE-74) Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015.  This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic.",1,DM-5563,datamanagement,participate november 2015 ocs subsystem teleconference lse-74 prepare attend follow ocs subsystem teleconference november 11 2015 story cover work relate lse-74 lse-70 lse-209 work separate epic,"Participate in November 2015 OCS-subsystems teleconference (LSE-74) Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015. This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic."
"Participate in December 2015 OCS-subsystems teleconference (LSE-70, LSE-209) Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-70 and LSE-209; LSE-74 work was also done under a separate epic.",2,DM-5564,datamanagement,participate december 2015 ocs subsystem teleconference lse-70 lse-209 prepare attend follow ocs subsystem teleconference december 2015 story cover work relate lse-70 lse-209 lse-74 work separate epic,"Participate in December 2015 OCS-subsystems teleconference (LSE-70, LSE-209) Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-70 and LSE-209; LSE-74 work was also done under a separate epic."
"Participate in December 2015 OCS-subsystems teleconference (LSE-74) Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic.",1,DM-5565,datamanagement,participate december 2015 ocs subsystem teleconference lse-74 prepare attend follow ocs subsystem teleconference december 2015 story cover work relate lse-74 lse-70 lse-209 work separate epic,"Participate in December 2015 OCS-subsystems teleconference (LSE-74) Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic."
"Review of LSE-70 and LSE-209 drafts, September 2015 Arrange, prepare for, and attend a joint call with the Camera team to review the end-of-summer-2015 drafts of LSE-70 and LSE-209 from the OCS group.",3,DM-5566,datamanagement,review lse-70 lse-209 draft september 2015 arrange prepare attend joint camera team review end summer-2015 draft lse-70 lse-209 ocs group,"Review of LSE-70 and LSE-209 drafts, September 2015 Arrange, prepare for, and attend a joint call with the Camera team to review the end-of-summer-2015 drafts of LSE-70 and LSE-209 from the OCS group."
CCB review of LCR-567 (LSE-70) and LCR-568 (LSE-209) Review the LSE-70 and LSE-209 drafts submitted with change requests LCR-567 and LCR-568 in January 2016.,2,DM-5567,datamanagement,ccb review lcr-567 lse-70 lcr-568 lse-209 review lse-70 lse-209 draft submit change request lcr-567 lcr-568 january 2016,CCB review of LCR-567 (LSE-70) and LCR-568 (LSE-209) Review the LSE-70 and LSE-209 drafts submitted with change requests LCR-567 and LCR-568 in January 2016.
"CCB review of LCR-603 (LSE-74) Review LCR-603, ""LSE-74 document revision""",2,DM-5568,datamanagement,ccb review lcr-603 lse-74 review lcr-603 lse-74 document revision,"CCB review of LCR-603 (LSE-74) Review LCR-603, ""LSE-74 document revision"""
"LSE-70, LSE-209 refinements X16 There are open LCRs for cleanups to the versions of LSE-70 and LSE-209 approved by the CCB in February 2016.  An initial teleconference will be held on 30 March 2016 with the OCS group to discuss these.",4,DM-5569,datamanagement,lse-70 lse-209 refinement x16 open lcr cleanup version lse-70 lse-209 approve ccb february 2016 initial teleconference hold 30 march 2016 ocs group discuss,"LSE-70, LSE-209 refinements X16 There are open LCRs for cleanups to the versions of LSE-70 and LSE-209 approved by the CCB in February 2016. An initial teleconference will be held on 30 March 2016 with the OCS group to discuss these."
"making PSF candidates should be simpler The code to make PSF candidates is too complicated and repeated in too many places (even after DM-5532). Every time lsst.meas.algorithms.makePsfCandidate is called (except in a few tests) it is called as follows:  {code}              cand = measAlg.makePsfCandidate(source, mi)              if cand.getWidth() == 0:                  cand.setBorderWidth(borderWidth)                  cand.setWidth(kernelSize + 2*borderWidth)                  cand.setHeight(kernelSize + 2*borderWidth)                im = cand.getMaskedImage().getImage()              max = afwMath.makeStatistics(im, afwMath.MAX).getValue()              if not numpy.isfinite(max):                  continue  {code}    This should to be centralized somewhere. I suggest adding this code to {{meas.algorithms.makePsfCandidate}} itself (which could delegate some work to a private function, if desired).",2,DM-5578,datamanagement,make psf candidate simple code psf candidate complicated repeat place dm-5532 time lsst.meas.algorithms.makepsfcandidate call test call follow code cand measalg.makepsfcandidate(source mi cand.getwidth cand.setborderwidth(borderwidth cand.setwidth(kernelsize 2*borderwidth cand.setheight(kernelsize 2*borderwidth cand.getmaskedimage().getimage max afwmath.makestatistics(im afwmath max).getvalue numpy.isfinite(max continue code centralize suggest add code meas.algorithms.makepsfcandidate delegate work private function desire,"making PSF candidates should be simpler The code to make PSF candidates is too complicated and repeated in too many places (even after DM-5532). Every time lsst.meas.algorithms.makePsfCandidate is called (except in a few tests) it is called as follows: {code} cand = measAlg.makePsfCandidate(source, mi) if cand.getWidth() == 0: cand.setBorderWidth(borderWidth) cand.setWidth(kernelSize + 2*borderWidth) cand.setHeight(kernelSize + 2*borderWidth) im = cand.getMaskedImage().getImage() max = afwMath.makeStatistics(im, afwMath.MAX).getValue() if not numpy.isfinite(max): continue {code} This should to be centralized somewhere. I suggest adding this code to {{meas.algorithms.makePsfCandidate}} itself (which could delegate some work to a private function, if desired)."
"Add Error and Working feedback to FITS visualizer * Add working message when plot is loading, (downloading..., plotting..., etc)  * Add error message when plot fails  * for multi-viewer remove and failed plot cells  * work out if image select panel should become visible again.  * Any thing else the is plotting feedback related",6,DM-5579,datamanagement,add error working feedback fits visualizer add work message plot load download plot etc add error message plot fail multi viewer remove fail plot cell work image select panel visible thing plot feedback relate,"Add Error and Working feedback to FITS visualizer * Add working message when plot is loading, (downloading..., plotting..., etc) * Add error message when plot fails * for multi-viewer remove and failed plot cells * work out if image select panel should become visible again. * Any thing else the is plotting feedback related"
Docgen draft from EA content for LSE-140 Create a docgen from the LSE-140 content in Enterprise Architect.,2,DM-5580,datamanagement,docgen draft ea content lse-140 create docgen lse-140 content enterprise architect,Docgen draft from EA content for LSE-140 Create a docgen from the LSE-140 content in Enterprise Architect.
SQuaRE Communication and Publication Platforms Document and Presentation [SQR-011|http://sqr-011.lsst.io] documents the various communication and publishing platforms that SQuaRE operates on behalf of DM. This ticket will complete v1 of the document (DM-4721 created a time-boxed first draft) and also include work to present the document to LSST management.,4,DM-5581,datamanagement,square communication publication platforms document presentation sqr-011|http://sqr-011.lsst.io document communication publishing platform square operate behalf dm ticket complete v1 document dm-4721 create time box draft include work present document lsst management,SQuaRE Communication and Publication Platforms Document and Presentation [SQR-011|http://sqr-011.lsst.io] documents the various communication and publishing platforms that SQuaRE operates on behalf of DM. This ticket will complete v1 of the document (DM-4721 created a time-boxed first draft) and also include work to present the document to LSST management.
Support LCR-385 Support getting LCR-385 against LSE-78 through the CCB.,3,DM-5582,datamanagement,support lcr-385 support get lcr-385 lse-78 ccb,Support LCR-385 Support getting LCR-385 against LSE-78 through the CCB.
Create a reusable upload file component This  component will upload and validate the file as part of the input's validation process.  It will return a token generated by the server which will resolve to the uploaded file if the upload success.   ,4,DM-5584,datamanagement,create reusable upload file component component upload validate file input validation process return token generate server resolve uploaded file upload success,Create a reusable upload file component This component will upload and validate the file as part of the input's validation process. It will return a token generated by the server which will resolve to the uploaded file if the upload success.
SQuaRE Communication and Publication Platforms Document and Presentation - Clone This is a clone of DM-5581 tracking [~frossie]'s SPs,5,DM-5585,datamanagement,square communication publication platforms document presentation clone clone dm-5581 track ~frossie sp,SQuaRE Communication and Publication Platforms Document and Presentation - Clone This is a clone of DM-5581 tracking [~frossie]'s SPs
"Fix obs_decam butler level There is a bug in {{obs_decam/policy/DecamMapper.paf}}, causing some butler features for the ""visit"" level or above working incorrectly. The {{hdu}} key is irrelevant for the visit level or above, but wasn't included in the policy file.    Because of this bug, the {{DemoTask}} in {{ctrl_pool}} (ctrlPoolDemo.py) runs incorrectly with DECam data. It incorrectly treats dataRef with different {{hdu}}s as they are from different visits, hence reads each ccd image multiple times (61 times for one visit with 61 hdu). Instead, each ccd image should be read once.        Besides fixing the policy file, I also added an optional test that only runs if {{testdata_decam}} is set up. The part with level=""visit"" in the test fails without the ticket changes in the policy.    (p.s. The raw data file in {{testdata_decam}} is modified and has only 2 hdus.) ",3,DM-5586,datamanagement,"fix obs_decam butler level bug obs_decam policy decammapper.paf cause butler feature visit level work incorrectly hdu key irrelevant visit level include policy file bug demotask ctrl_pool ctrlpooldemo.py run incorrectly decam datum incorrectly treat dataref different hdu}}s different visit read ccd image multiple time 61 time visit 61 hdu instead ccd image read fix policy file add optional test run testdata_decam set level=""visit test fail ticket change policy p.s raw data file testdata_decam modify hdus","Fix obs_decam butler level There is a bug in {{obs_decam/policy/DecamMapper.paf}}, causing some butler features for the ""visit"" level or above working incorrectly. The {{hdu}} key is irrelevant for the visit level or above, but wasn't included in the policy file. Because of this bug, the {{DemoTask}} in {{ctrl_pool}} (ctrlPoolDemo.py) runs incorrectly with DECam data. It incorrectly treats dataRef with different {{hdu}}s as they are from different visits, hence reads each ccd image multiple times (61 times for one visit with 61 hdu). Instead, each ccd image should be read once. Besides fixing the policy file, I also added an optional test that only runs if {{testdata_decam}} is set up. The part with level=""visit"" in the test fails without the ticket changes in the policy. (p.s. The raw data file in {{testdata_decam}} is modified and has only 2 hdus.)"
"Add lmfit package to the stack The current implementation of the new {{DipoleFitTask}} for {{ip_diffim}} uses {{lmfit}} to perform parameter estimation (least-squares minimization). {{lmfit}} is essentially an API on top of {{scipy}}'s optimizer, adding functionality such as parameter boxing (constraints) and improved estimates of parameter uncertainties. It would be nice to include this small, pure-python package in the stack rather than investigating and re-implementing the optimization using {{scipy}} or {{minuit2}} (which are the two optimizers that I know of that are in the stack already).",4,DM-5588,datamanagement,add lmfit package stack current implementation new dipolefittask ip_diffim use lmfit perform parameter estimation square minimization lmfit essentially api scipy optimizer add functionality parameter boxing constraint improve estimate parameter uncertainty nice include small pure python package stack investigate implement optimization scipy minuit2 optimizer know stack,"Add lmfit package to the stack The current implementation of the new {{DipoleFitTask}} for {{ip_diffim}} uses {{lmfit}} to perform parameter estimation (least-squares minimization). {{lmfit}} is essentially an API on top of {{scipy}}'s optimizer, adding functionality such as parameter boxing (constraints) and improved estimates of parameter uncertainties. It would be nice to include this small, pure-python package in the stack rather than investigating and re-implementing the optimization using {{scipy}} or {{minuit2}} (which are the two optimizers that I know of that are in the stack already)."
"Fix afw build issues with recent clang {{afw}} fails to build with recent versions of clang:    {code}  include/lsst/afw/image/MaskedImage.h:553:65: error: '_loc' is a protected member of 'lsst::afw::image::MaskedImage<unsigned short, unsigned short,        float>::MaskedImageLocatorBase<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<unsigned short,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >,        boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<unsigned short,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >,        boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >, Reference>'                                       const_VarianceLocator(iter._loc.template get<2>())  {code}  and issues with statistics.i so far, more errors may turn up as these are cleared.    These problems are apparent with {{Apple LLVM version 7.3.0 (clang-703.0.29)}} (as shipped with the latest release of XCode, hence this now becoming an issue) and {{clang version 3.8.0 (branches/release_38 262722)}} (a recent release from LLVM; note that Apple uses its own versioning scheme). {{clang version 3.7.1 (tags/RELEASE_371/final)}} is not affected.",1,DM-5590,datamanagement,fix afw build issue recent clang afw fail build recent version clang code include lsst afw image maskedimage.h:553:65 error loc protect member lsst::afw::image::maskedimage::maskedimagelocatorbase boost::mpl::range_c boost::gil::memory_based_2d_locator boost::mpl::range_c boost::gil::memory_based_2d_locator boost::mpl::range_c reference const_variancelocator(iter._loc.template get<2 code issue statistics.i far error turn clear problem apparent apple llvm version 7.3.0 clang-703.0.29 ship late release xcode issue clang version 3.8.0 branch release_38 262722 recent release llvm note apple use versioning scheme clang version 3.7.1 tag release_371 final affect,"Fix afw build issues with recent clang {{afw}} fails to build with recent versions of clang: {code} include/lsst/afw/image/MaskedImage.h:553:65: error: '_loc' is a protected member of 'lsst::afw::image::MaskedImage::MaskedImageLocatorBase, boost::mpl::range_c > > *> >, boost::gil::memory_based_2d_locator, boost::mpl::range_c > > *> >, boost::gil::memory_based_2d_locator, boost::mpl::range_c > > *> >, Reference>' const_VarianceLocator(iter._loc.template get<2>()) {code} and issues with statistics.i so far, more errors may turn up as these are cleared. These problems are apparent with {{Apple LLVM version 7.3.0 (clang-703.0.29)}} (as shipped with the latest release of XCode, hence this now becoming an issue) and {{clang version 3.8.0 (branches/release_38 262722)}} (a recent release from LLVM; note that Apple uses its own versioning scheme). {{clang version 3.7.1 (tags/RELEASE_371/final)}} is not affected."
"fix issue where butler repository search returns list for single item Backwards compatible behavior is that when butler returns a single item, it is NOT in a list. A recent change (when the Repository class was added) broke this behavior.     Change it back so that if an operation in repository would return a list with a  single item, it pulls it from the list.    Note this is only related to the case where a repository's parentJoin field is set to 'outer' and since no one is using this yet (they should not be, anyway) then the point is moot.     ",1,DM-5593,datamanagement,fix issue butler repository search return list single item backwards compatible behavior butler return single item list recent change repository class add break behavior change operation repository return list single item pull list note relate case repository parentjoin field set outer point moot,"fix issue where butler repository search returns list for single item Backwards compatible behavior is that when butler returns a single item, it is NOT in a list. A recent change (when the Repository class was added) broke this behavior. Change it back so that if an operation in repository would return a list with a single item, it pulls it from the list. Note this is only related to the case where a repository's parentJoin field is set to 'outer' and since no one is using this yet (they should not be, anyway) then the point is moot."
"Fix qserv service timeout issue After Qserv services have been running over ~couple of days, new queries fail and can also lead to a crash. Investigate and implement a solution.",5,DM-5594,datamanagement,fix qserv service timeout issue qserv service run ~couple day new query fail lead crash investigate implement solution,"Fix qserv service timeout issue After Qserv services have been running over ~couple of days, new queries fail and can also lead to a crash. Investigate and implement a solution."
"daf_persistence build failure on OSX I see the following build failure in {{daf_persistence}} on OSX 10.11:  {code}  c++ -o python/lsst/daf/persistence/_persistenceLib.so -bundle -F/ -undefined suppress -flat_namespace -headerpad_max_install_names python/lsst/daf/persistence/persistenceLib_wrap.os -Llib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/mariadbclient/10.1.11-2-gd04d8b7/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_policy/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_logging/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/daf_base/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/utils/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_exceptions/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/base/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/boost/1.59.lsst5/lib -L/tmp/ssd/swinbank/shared_stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/config -ldaf_persistence -lboost_serialization -lmysqlclient_r -lpex_policy -lpex_logging -lboost_filesystem -lboost_system -ldaf_base -lutils -lpex_exceptions -lbase -lboost_regex -lpthread -ldl -lpython2.7  ld: file not found: libz.1.dylib for architecture x86_64  clang: error: linker command failed with exit code 1 (use -v to see invocation)  scons: *** [python/lsst/daf/persistence/_persistenceLib.so] Error 1  scons: building terminated because of errors.  {code}    This happens with the current master ({{3484020}} at time of writing), but also with a recent weekly ({{3878625}}). ",1,DM-5595,datamanagement,daf_persistence build failure osx follow build failure daf_persistence osx 10.11 code c++ python lsst daf persistence/_persistencelib.so -bundle -f/ -undefine suppress -flat_namespace python lsst daf persistence persistencelib_wrap.os -llib -l private tmp ssd swinbank shared_stack darwinx86 mariadbclient/10.1.11 gd04d8b7 lib -l private tmp ssd swinbank shared_stack darwinx86 pex_policy/2016_01.0 lib -l private tmp ssd swinbank shared_stack darwinx86 pex_logging/2016_01.0 lib -l private tmp ssd swinbank shared_stack darwinx86 daf_base/2016_01.0 lib -l private tmp ssd swinbank shared_stack darwinx86 utils/2016_01.0 lib -l private tmp ssd swinbank shared_stack darwinx86 pex_exceptions/2016_01.0 lib -l private tmp ssd swinbank shared_stack darwinx86 base/2016_01.0 lib -l private tmp ssd swinbank shared_stack darwinx86 boost/1.59.lsst5 lib -l tmp ssd swinbank shared_stack darwinx86 miniconda2/3.19.0.lsst4 lib python2.7 config -ldaf_persistence -lboost_serialization -lmysqlclient_r -lpex_policy -lpex_logge -lboost_filesystem -lboost_system -ldaf_base -lutil -lbase -lboost_regex -lpthread -ldl ld file find libz.1.dylib architecture clang error linker command fail exit code use -v invocation scon python lsst daf persistence/_persistencelib.so error scon building terminate error code happen current master 3484020 time writing recent weekly 3878625,"daf_persistence build failure on OSX I see the following build failure in {{daf_persistence}} on OSX 10.11: {code} c++ -o python/lsst/daf/persistence/_persistenceLib.so -bundle -F/ -undefined suppress -flat_namespace -headerpad_max_install_names python/lsst/daf/persistence/persistenceLib_wrap.os -Llib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/mariadbclient/10.1.11-2-gd04d8b7/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_policy/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_logging/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/daf_base/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/utils/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_exceptions/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/base/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/boost/1.59.lsst5/lib -L/tmp/ssd/swinbank/shared_stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/config -ldaf_persistence -lboost_serialization -lmysqlclient_r -lpex_policy -lpex_logging -lboost_filesystem -lboost_system -ldaf_base -lutils -lpex_exceptions -lbase -lboost_regex -lpthread -ldl -lpython2.7 ld: file not found: libz.1.dylib for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation) scons: *** [python/lsst/daf/persistence/_persistenceLib.so] Error 1 scons: building terminated because of errors. {code} This happens with the current master ({{3484020}} at time of writing), but also with a recent weekly ({{3878625}})."
Remove obsolete install scripts from ~/src/qserv/admin/tools/ Internet-free install scripts are unused and should be removed with related documentation.,1,DM-5604,datamanagement,remove obsolete install script ~/src qserv admin tools/ internet free install script unused remove related documentation,Remove obsolete install scripts from ~/src/qserv/admin/tools/ Internet-free install scripts are unused and should be removed with related documentation.
"runQueries.py fails on IN2P3 cluster Launching runQueries.py produces some errors:  {code:bash}  fjammes@ccosvms0070:~/src/qserv/admin/tools/docker/deployment/in2p3 (tickets/DM-5402 *=)$ ./run-test-queries.sh  +--------------------+--------------------+  | ra                 | decl               |  +--------------------+--------------------+  | 29.308806347275485 | -86.30884046118973 |  +--------------------+--------------------+    real    1m20.725s  user    0m0.004s  sys     0m0.012s  Output directory: /afs/in2p3.fr/home/f/fjammes/runQueries_out  Exception in thread Thread-12:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-23:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (1105, '(proxy) all backends are down')    Exception in thread Thread-16:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-21:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (1105, '(proxy) all backends are down')    Exception in thread Thread-17:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-19:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")    Exception in thread Thread-18:  Traceback (most recent call last):    File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner      self.run()    File ""/usr/lib64/python2.7/threading.py"", line 764, in run      self.__target(*self.__args, **self.__kwargs)    File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries      db='LSST')    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect      return Connection(*args, **kwargs)    File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__      super(Connection, self).__init__(*args, **kwargs2)  OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"")  {code}",4,DM-5605,datamanagement,runqueries.py fail in2p3 cluster launching runqueries.py produce error code bash fjammes@ccosvms0070:~/src qserv admin tool docker deployment in2p3 ticket dm-5402 ./run test queries.sh --------------------+--------------------+ ra decl --------------------+--------------------+ 29.308806347275485 --------------------+--------------------+ real 1m20.725s user 0m0.004s sys 0m0.012s output directory in2p3.fr home fjamme runqueries_out exception thread thread-12 traceback recent file /usr lib64 python2.7 threading.py line 811 bootstrap_inner self.run file /usr lib64 python2.7 threading.py line 764 run self.__target(*self.__args self.__kwargs file /afs in2p3.fr home fjamme src qserv admin tool docker deployment in2p3 runqueries.py line 185 runqueries db='lsst file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb/__init__.py line 81 connect return connection(*arg kwargs file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb connections.py line 187 init super(connection self).__init__(*arg kwargs2 operationalerror 2013 lose connection mysql server read authorization packet system error exception thread thread-23 traceback recent file /usr lib64 python2.7 threading.py line 811 bootstrap_inner self.run file /usr lib64 python2.7 threading.py line 764 run self.__target(*self.__args self.__kwargs file /afs in2p3.fr home fjamme src qserv admin tool docker deployment in2p3 runqueries.py line 185 runqueries db='lsst file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb/__init__.py line 81 connect return connection(*arg kwargs file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb connections.py line 187 init super(connection self).__init__(*arg kwargs2 operationalerror 1105 proxy backend exception thread thread-16 traceback recent file /usr lib64 python2.7 threading.py line 811 bootstrap_inner self.run file /usr lib64 python2.7 threading.py line 764 run self.__target(*self.__args self.__kwargs file /afs in2p3.fr home fjamme src qserv admin tool docker deployment in2p3 runqueries.py line 185 runqueries db='lsst file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb/__init__.py line 81 connect return connection(*arg kwargs file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb connections.py line 187 init super(connection self).__init__(*arg kwargs2 operationalerror 2013 lose connection mysql server read authorization packet system error exception thread thread-21 traceback recent file /usr lib64 python2.7 threading.py line 811 bootstrap_inner self.run file /usr lib64 python2.7 threading.py line 764 run self.__target(*self.__args self.__kwargs file /afs in2p3.fr home fjamme src qserv admin tool docker deployment in2p3 runqueries.py line 185 runqueries db='lsst file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb/__init__.py line 81 connect return connection(*arg kwargs file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb connections.py line 187 init super(connection self).__init__(*arg kwargs2 operationalerror 1105 proxy backend exception thread thread-17 traceback recent file /usr lib64 python2.7 threading.py line 811 bootstrap_inner self.run file /usr lib64 python2.7 threading.py line 764 run self.__target(*self.__args self.__kwargs file /afs in2p3.fr home fjamme src qserv admin tool docker deployment in2p3 runqueries.py line 185 runqueries db='lsst file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb/__init__.py line 81 connect return connection(*arg kwargs file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb connections.py line 187 init super(connection self).__init__(*arg kwargs2 operationalerror 2013 lose connection mysql server read authorization packet system error exception thread thread-19 traceback recent file /usr lib64 python2.7 threading.py line 811 bootstrap_inner self.run file /usr lib64 python2.7 threading.py line 764 run self.__target(*self.__args self.__kwargs file /afs in2p3.fr home fjamme src qserv admin tool docker deployment in2p3 runqueries.py line 185 runqueries db='lsst file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb/__init__.py line 81 connect return connection(*arg kwargs file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb connections.py line 187 init super(connection self).__init__(*arg kwargs2 operationalerror 2013 lose connection mysql server read authorization packet system error exception thread thread-18 traceback recent file /usr lib64 python2.7 threading.py line 811 bootstrap_inner self.run file /usr lib64 python2.7 threading.py line 764 run self.__target(*self.__args self.__kwargs file /afs in2p3.fr home fjamme src qserv admin tool docker deployment in2p3 runqueries.py line 185 runqueries db='lsst file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb/__init__.py line 81 connect return connection(*arg kwargs file /qserv stack linux64 mysqlpython/1.2.3.lsst1 lib python mysql_python-1.2.3 py2.7 linux x86_64.egg mysqldb connections.py line 187 init super(connection self).__init__(*arg kwargs2 operationalerror 2013 lose connection mysql server read authorization packet system error code,"runQueries.py fails on IN2P3 cluster Launching runQueries.py produces some errors: {code:bash} fjammes@ccosvms0070:~/src/qserv/admin/tools/docker/deployment/in2p3 (tickets/DM-5402 *=)$ ./run-test-queries.sh +--------------------+--------------------+ | ra | decl | +--------------------+--------------------+ | 29.308806347275485 | -86.30884046118973 | +--------------------+--------------------+ real 1m20.725s user 0m0.004s sys 0m0.012s Output directory: /afs/in2p3.fr/home/f/fjammes/runQueries_out Exception in thread Thread-12: Traceback (most recent call last): File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner self.run() File ""/usr/lib64/python2.7/threading.py"", line 764, in run self.__target(*self.__args, **self.__kwargs) File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries db='LSST') File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect return Connection(*args, **kwargs) File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__ super(Connection, self).__init__(*args, **kwargs2) OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"") Exception in thread Thread-23: Traceback (most recent call last): File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner self.run() File ""/usr/lib64/python2.7/threading.py"", line 764, in run self.__target(*self.__args, **self.__kwargs) File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries db='LSST') File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect return Connection(*args, **kwargs) File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__ super(Connection, self).__init__(*args, **kwargs2) OperationalError: (1105, '(proxy) all backends are down') Exception in thread Thread-16: Traceback (most recent call last): File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner self.run() File ""/usr/lib64/python2.7/threading.py"", line 764, in run self.__target(*self.__args, **self.__kwargs) File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries db='LSST') File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect return Connection(*args, **kwargs) File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__ super(Connection, self).__init__(*args, **kwargs2) OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"") Exception in thread Thread-21: Traceback (most recent call last): File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner self.run() File ""/usr/lib64/python2.7/threading.py"", line 764, in run self.__target(*self.__args, **self.__kwargs) File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries db='LSST') File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect return Connection(*args, **kwargs) File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__ super(Connection, self).__init__(*args, **kwargs2) OperationalError: (1105, '(proxy) all backends are down') Exception in thread Thread-17: Traceback (most recent call last): File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner self.run() File ""/usr/lib64/python2.7/threading.py"", line 764, in run self.__target(*self.__args, **self.__kwargs) File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries db='LSST') File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect return Connection(*args, **kwargs) File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__ super(Connection, self).__init__(*args, **kwargs2) OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"") Exception in thread Thread-19: Traceback (most recent call last): File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner self.run() File ""/usr/lib64/python2.7/threading.py"", line 764, in run self.__target(*self.__args, **self.__kwargs) File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries db='LSST') File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect return Connection(*args, **kwargs) File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__ super(Connection, self).__init__(*args, **kwargs2) OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"") Exception in thread Thread-18: Traceback (most recent call last): File ""/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner self.run() File ""/usr/lib64/python2.7/threading.py"", line 764, in run self.__target(*self.__args, **self.__kwargs) File ""/afs/in2p3.fr/home/f/fjammes/src/qserv/admin/tools/docker/deployment/in2p3/runQueries.py"", line 185, in runQueries db='LSST') File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/__init__.py"", line 81, in Connect return Connection(*args, **kwargs) File ""/qserv/stack/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 187, in __init__ super(Connection, self).__init__(*args, **kwargs2) OperationalError: (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 0"") {code}"
"check & correct comparison operators in daf_persistence and daf_butlerUtils per comments in DM-5593, an incorrect comparison operator was found, that used {{is}} instead of {{==}} in a string comparison (e.g. {{var is 'left'}} which is incorrect, it should be {{var == 'left'}}.  This needs to be corrected in {{Repository}} (see DM-5593 for details), and the rest of daf_persistence and daf_butlerUtils should be checked for correct use of is vs. ==.",1,DM-5607,datamanagement,check correct comparison operator daf_persistence daf_butlerutil comment dm-5593 incorrect comparison operator find instead string comparison e.g. var leave incorrect var leave need correct repository dm-5593 detail rest daf_persistence daf_butlerutil check correct use vs.,"check & correct comparison operators in daf_persistence and daf_butlerUtils per comments in DM-5593, an incorrect comparison operator was found, that used {{is}} instead of {{==}} in a string comparison (e.g. {{var is 'left'}} which is incorrect, it should be {{var == 'left'}}. This needs to be corrected in {{Repository}} (see DM-5593 for details), and the rest of daf_persistence and daf_butlerUtils should be checked for correct use of is vs. ==."
"Investigate clang issues regarding friendship and protected members  In DM-5590, we worked around a problem in which clang 3.8 refused to access protected members of a cousin class given a friend declaration in the base. To our best understanding at time of writing, the code is valid: it seems possible that this is a bug in clang.    Investigate what went wrong, produce a minimal test case, and (if appropriate) report this as an upstream bug.",3,DM-5609,datamanagement,investigate clang issue friendship protect member dm-5590 work problem clang 3.8 refuse access protect member cousin class give friend declaration base good understanding time writing code valid possible bug clang investigate go wrong produce minimal test case appropriate report upstream bug,"Investigate clang issues regarding friendship and protected members In DM-5590, we worked around a problem in which clang 3.8 refused to access protected members of a cousin class given a friend declaration in the base. To our best understanding at time of writing, the code is valid: it seems possible that this is a bug in clang. Investigate what went wrong, produce a minimal test case, and (if appropriate) report this as an upstream bug."
"x16 Joint Coordination Council Coordination with CC-IN2P3.    Don Petravick, Jason Alt  ",7,DM-5611,datamanagement,x16 joint coordination council coordination cc in2p3 don petravick jason alt,"x16 Joint Coordination Council Coordination with CC-IN2P3. Don Petravick, Jason Alt"
"Add data products and config in obs_decam for multi-band processing Add necessary data products and default config in order to run forcedPhotCcd, coaddDriverTask, and multiBandDriverTask with DECam data. ",3,DM-5633,datamanagement,add datum product config obs_decam multi band processing add necessary data product default config order run forcedphotccd coadddrivertask multibanddrivertask decam datum,"Add data products and config in obs_decam for multi-band processing Add necessary data products and default config in order to run forcedPhotCcd, coaddDriverTask, and multiBandDriverTask with DECam data."
Python version checking in newinstall.sh is not quite right There is a recent report on community where {{newinstall.sh}} reports that the python version is too old despite the user having a modern Anaconda python in their path.  In commit e6fc9ed2 the code was changed to check {{$PYTHON}} for version compatibility but that is not correct as the python that will be used for the actual build is the python in their path. {{$PYTHON}} is defined purely as the python to use for EUPS installation.    In the reported error {{$PYTHON}} was not set and their {{/usr/bin/python}} was too old. Confusingly the error message reporting the version problem actually reported the version information for the python in the path and not the {{$PYTHON}} python. The simple fix is to revert e6fc9ed2.    I already made significant comments on this topic in the original https://github.com/lsst/lsst/pull/19 but I really do have to insist on either reverting that PR or at least fixing the error messages to use a consistent python (I'd argue that this is still wrong but at least consistent). The current situation is at best confusing and at worst pointless and wrong.    The version test only makes sense if we are testing that the default python in the path is the correct version to build the stack. {{$PYTHON}} was originally designed to allow a different python to be used to build EUPS. Even that is no longer an issue as EUPS can work with Python >= 2.6 now.,1,DM-5636,datamanagement,python version check newinstall.sh right recent report community newinstall.sh report python version old despite user have modern anaconda python path commit e6fc9ed2 code change check python version compatibility correct python actual build python path python define purely python use eups installation report error python set /usr bin python old confusingly error message report version problem actually report version information python path python python simple fix revert e6fc9ed2 significant comment topic original https://github.com/lsst/lsst/pull/19 insist revert pr fix error message use consistent python argue wrong consistent current situation well confusing bad pointless wrong version test make sense test default python path correct version build stack python originally design allow different python build eups long issue eups work python 2.6,Python version checking in newinstall.sh is not quite right There is a recent report on community where {{newinstall.sh}} reports that the python version is too old despite the user having a modern Anaconda python in their path. In commit e6fc9ed2 the code was changed to check {{$PYTHON}} for version compatibility but that is not correct as the python that will be used for the actual build is the python in their path. {{$PYTHON}} is defined purely as the python to use for EUPS installation. In the reported error {{$PYTHON}} was not set and their {{/usr/bin/python}} was too old. Confusingly the error message reporting the version problem actually reported the version information for the python in the path and not the {{$PYTHON}} python. The simple fix is to revert e6fc9ed2. I already made significant comments on this topic in the original https://github.com/lsst/lsst/pull/19 but I really do have to insist on either reverting that PR or at least fixing the error messages to use a consistent python (I'd argue that this is still wrong but at least consistent). The current situation is at best confusing and at worst pointless and wrong. The version test only makes sense if we are testing that the default python in the path is the correct version to build the stack. {{$PYTHON}} was originally designed to allow a different python to be used to build EUPS. Even that is no longer an issue as EUPS can work with Python >= 2.6 now.
Build a tool to automatically run autopep8 on LSST Stack Develop a lsst-autopep8 command in [sqre-codekit|https://github.com/lsst-sqre/sqre-codekit] that can run [autopep8|https://github.com/hhatto/autopep8] in an automated fashion across all of the LSST Stack repositories according to the PEP 8 exceptions determined in RFC-162.,1,DM-5640,datamanagement,build tool automatically run autopep8 lsst stack develop lsst autopep8 command sqre codekit|https://github.com lsst sqre sqre codekit run autopep8|https://github.com hhatto autopep8 automate fashion lsst stack repository accord pep exception determine rfc-162,Build a tool to automatically run autopep8 on LSST Stack Develop a lsst-autopep8 command in [sqre-codekit|https://github.com/lsst-sqre/sqre-codekit] that can run [autopep8|https://github.com/hhatto/autopep8] in an automated fashion across all of the LSST Stack repositories according to the PEP 8 exceptions determined in RFC-162.
"finish up afw.table to astropy.table view support At an LSST/AstroPy summit hack session, we've put together a functional system for viewing afw.table objects as astropy.table objects on branch u/jbosch/astropy-tables of afw and https://github.com/astropy/astropy/pull/4740.    Before merging, we should add support for ""object"" columns for subclasses to hold e.g. Footprints in SourceCatalog, and add some documentation.  We may also want to add a convenience method to return an astropy.table.Table directly.",1,DM-5641,datamanagement,finish afw.table astropy.table view support lsst astropy summit hack session functional system view afw.table object astropy.table object branch jbosch astropy table afw https://github.com/astropy/astropy/pull/4740 merge add support object column subclass hold e.g. footprints sourcecatalog add documentation want add convenience method return astropy.table table directly,"finish up afw.table to astropy.table view support At an LSST/AstroPy summit hack session, we've put together a functional system for viewing afw.table objects as astropy.table objects on branch u/jbosch/astropy-tables of afw and https://github.com/astropy/astropy/pull/4740. Before merging, we should add support for ""object"" columns for subclasses to hold e.g. Footprints in SourceCatalog, and add some documentation. We may also want to add a convenience method to return an astropy.table.Table directly."
"use AstroPy-compliant strings for units in afw.table With DM-5641, we'll soon be able to get astropy.table views into afw.table objects.  That will be a bit more useful if astropy can understand the unit strings we give it, and since we currently don't use those strings as anything more than textual information for humans, we might as well standardize on the terms they've already selected.",4,DM-5642,datamanagement,use astropy compliant string unit afw.table dm-5641 soon able astropy.table view afw.table object bit useful astropy understand unit string currently use string textual information human standardize term select,"use AstroPy-compliant strings for units in afw.table With DM-5641, we'll soon be able to get astropy.table views into afw.table objects. That will be a bit more useful if astropy can understand the unit strings we give it, and since we currently don't use those strings as anything more than textual information for humans, we might as well standardize on the terms they've already selected."
"add method to convert Property[Set,List] to nested dict In interfacing with AstroPy it'd be useful to easily convert PropertySet and PropertyList to nested dict and OrderedDict (respectively), converting elements with multiple values to lists in the process.",2,DM-5643,datamanagement,add method convert property[set list nest dict interface astropy useful easily convert propertyset propertylist nest dict ordereddict respectively convert element multiple value list process,"add method to convert Property[Set,List] to nested dict In interfacing with AstroPy it'd be useful to easily convert PropertySet and PropertyList to nested dict and OrderedDict (respectively), converting elements with multiple values to lists in the process."
"Add fine-grained authorization to ltd-keeper users The initial MVP of ltd-keeper had all-or-nothing authentication; any user was effectively an admin user. It would be useful have fine grained roles that each API user could have (for example, one API user might be able to add a build, but not create an edition or product or add another user). The phases of this ticket at:    1. Design a set of roles that cover current functionality  2. Add these roles to the User DB model and user creation API  3. Authorize users against these roles in specific API calls",2,DM-5645,datamanagement,add fine grain authorization ltd keeper user initial mvp ltd keeper authentication user effectively admin user useful fine grain role api user example api user able add build create edition product add user phase ticket design set role cover current functionality add role user db model user creation api authorize user role specific api call,"Add fine-grained authorization to ltd-keeper users The initial MVP of ltd-keeper had all-or-nothing authentication; any user was effectively an admin user. It would be useful have fine grained roles that each API user could have (for example, one API user might be able to add a build, but not create an edition or product or add another user). The phases of this ticket at: 1. Design a set of roles that cover current functionality 2. Add these roles to the User DB model and user creation API 3. Authorize users against these roles in specific API calls"
"visit AP team and work on processing DECam data March 13-17, 2016. Work on various topics about processing DECam data:  - Improve documentations on processing raw DECam data, especially about the steps of ingesting calibration data  - Identify future work on improving processing raw data. Updates about Instrumental Signature Removal tasks.  - Learn how to run difference imaging pipeline with DECam data  - Try jointcal (Simultaneous Astrometry meas_simastrom package from IN2P3) with DECam data and identity necessary code changes for doing jointcal with DECam data  - Use the preliminary jointcal astrometry results to examine DECam data’s distortion  - Also more general discussions on data processing",8,DM-5649,datamanagement,visit ap team work process decam datum march 13 17 2016 work topic process decam datum improve documentation process raw decam datum especially step ingest calibration datum identify future work improve process raw datum update instrumental signature removal task learn run difference imaging pipeline decam datum try jointcal simultaneous astrometry meas_simastrom package in2p3 decam datum identity necessary code change jointcal decam datum use preliminary jointcal astrometry result examine decam datum distortion general discussion datum processing,"visit AP team and work on processing DECam data March 13-17, 2016. Work on various topics about processing DECam data: - Improve documentations on processing raw DECam data, especially about the steps of ingesting calibration data - Identify future work on improving processing raw data. Updates about Instrumental Signature Removal tasks. - Learn how to run difference imaging pipeline with DECam data - Try jointcal (Simultaneous Astrometry meas_simastrom package from IN2P3) with DECam data and identity necessary code changes for doing jointcal with DECam data - Use the preliminary jointcal astrometry results to examine DECam data s distortion - Also more general discussions on data processing"
SUIT vision document Writing down SUIT vision that the group has discussed and shaped in last year off and on.   SUIT will use it as guidance for system design.,4,DM-5650,datamanagement,suit vision document write suit vision group discuss shape year suit use guidance system design,SUIT vision document Writing down SUIT vision that the group has discussed and shaped in last year off and on. SUIT will use it as guidance for system design.
Implement RFC-167 Implement RFC-167 for adding esutil to the stack.  This will be done in the same way as proposed to add scipy.,2,DM-5652,datamanagement,implement rfc-167 implement rfc-167 add esutil stack way propose add scipy,Implement RFC-167 Implement RFC-167 for adding esutil to the stack. This will be done in the same way as proposed to add scipy.
"Reduce code duplication in StarSelectors Both {{ObjectSizeStarSelector}} and {{SecondMomentStarSelector}} have logic to transform measured moments from pixel coordinates to TAN_PIXELS in order to remove optical distortion.  That's generically useful for any star selector that works on measured moments, and we shouldn't have to repeat it everywhere it is used.",2,DM-5655,datamanagement,reduce code duplication starselectors objectsizestarselector secondmomentstarselector logic transform measure moment pixel coordinate tan_pixel order remove optical distortion generically useful star selector work measured moment repeat,"Reduce code duplication in StarSelectors Both {{ObjectSizeStarSelector}} and {{SecondMomentStarSelector}} have logic to transform measured moments from pixel coordinates to TAN_PIXELS in order to remove optical distortion. That's generically useful for any star selector that works on measured moments, and we shouldn't have to repeat it everywhere it is used."
Improve Large Test Scale query script This script is currently located in:   admin/tools/docker/deployment/in2p3/runQueries.py     Here's some improvments:    - use lsst/db instead of mysqlpython  - externalize queries and other parameters in a config file  - add an option to make script stop after a few queries (in order to have deterministic query results for Large Scale integration tests)  - any other minor improvments...,8,DM-5657,datamanagement,improve large test scale query script script currently locate admin tool docker deployment in2p3 runqueries.py improvment use lsst db instead mysqlpython externalize query parameter config file add option script stop query order deterministic query result large scale integration test minor improvment,Improve Large Test Scale query script This script is currently located in: admin/tools/docker/deployment/in2p3/runQueries.py Here's some improvments: - use lsst/db instead of mysqlpython - externalize queries and other parameters in a config file - add an option to make script stop after a few queries (in order to have deterministic query results for Large Scale integration tests) - any other minor improvments...
"Update tables of packages that depend on scipy Now that the {{scipy}} package has been added (DM-5446), the table files of other packages to be fixed as soon as possible so that we have an idea of what is silently depending on {{scipy}}. These include {{afw}}, {{ip_diffim}}, {{meas_modelfit}}, {{mops_daymops}}, {{pipe_tasks}}, {{shapelet}} and {{sims_photUtils}}. Many of these are setupOptional that we should consider making mandatory. Some will be setupRequired.",2,DM-5658,datamanagement,update table package depend scipy scipy package add dm-5446 table file package fix soon possible idea silently depend scipy include afw ip_diffim meas_modelfit mops_daymops pipe_tasks shapelet sims_photutil setupoptional consider make mandatory setuprequire,"Update tables of packages that depend on scipy Now that the {{scipy}} package has been added (DM-5446), the table files of other packages to be fixed as soon as possible so that we have an idea of what is silently depending on {{scipy}}. These include {{afw}}, {{ip_diffim}}, {{meas_modelfit}}, {{mops_daymops}}, {{pipe_tasks}}, {{shapelet}} and {{sims_photUtils}}. Many of these are setupOptional that we should consider making mandatory. Some will be setupRequired."
multiple dialog are not working well together When several dialogs are up together.  The most recently click one should be one top. When table are in the dialogs such a fits header view. The scroll bars will go over other dialogs. This needs some though and work.  Another thing- when a message dialog is show because of a dialog error. It should center on the dialog.  Update- I don't think I will do the error centering now.  I am going to leave that and see if it is a real problem.,3,DM-5659,datamanagement,multiple dialog work dialog recently click table dialog fit header view scroll bar dialog need work thing- message dialog dialog error center dialog update- think error center go leave real problem,multiple dialog are not working well together When several dialogs are up together. The most recently click one should be one top. When table are in the dialogs such a fits header view. The scroll bars will go over other dialogs. This needs some though and work. Another thing- when a message dialog is show because of a dialog error. It should center on the dialog. Update- I don't think I will do the error centering now. I am going to leave that and see if it is a real problem.
"Add motivated model fits to validate_drp  photometric and astrometric scatter/repeatability analysis and plots Implement well-motivated theoretical fits to the astrometric and photometric performance measurements based on derivations from LSST Overview paper.  http://arxiv.org/pdf/0805.2366v4.pdf    Photometric errors described by  Eq. 5  sigma_rand^2 = (0.039 - gamma) * x + gamma * x^2  [mag^2]  where x = 10^(0.4*(m-m_5))    Eq. 4  sigma_1^2 = sigma_sys^2 + sigma_rand^2    Astrometric Errors   error = C * theta / SNR    Based on helpful comments from [~zivezic]    {quote}  I think eq. 5 from the overview paper (with gamma = 0.039 and m5 = 24.35; the former I assumed and the latter I got from the value of your  analytic fit that gives err=0.2 mag) would be a much better fit than the adopted function for mag < 21 (and it is derived from first principles).  Actually, if you fit for the systematic term (eq. 4) and gamma and m5, it would be a nice check whether there is any “weird” behavior in  analyzed data (and you get the limiting depth, m5, even if you don’t go all the way to the faint end).     Similarly, for the astrometric random errors, we’d expect        error = C * theta / SNR,    where theta is the seeing (or a fit parameter), SNR is the photometric SNR (i.e. 1/err in mag), and C ~ 1 (empirically, and 0.6 for the idealized maximum likelihood solution and gaussian seeing).   {quote}",5,DM-5660,datamanagement,add motivated model fit validate_drp photometric astrometric scatter repeatability analysis plot implement motivate theoretical fit astrometric photometric performance measurement base derivation lsst overview paper http://arxiv.org/pdf/0805.2366v4.pdf photometric error describe eq sigma_rand^2 0.039 gamma gamma x^2 mag^2 10^(0.4*(m m_5 eq sigma_1 sigma_sys^2 sigma_rand^2 astrometric errors error theta snr base helpful comment ~zivezic quote think eq overview paper gamma 0.039 m5 24.35 assume get value analytic fit give err=0.2 mag well fit adopt function mag 21 derive principle actually fit systematic term eq gamma m5 nice check weird behavior analyze datum limit depth m5 don way faint end similarly astrometric random error expect error theta snr theta seeing fit parameter snr photometric snr i.e. err mag empirically 0.6 idealize maximum likelihood solution gaussian seeing quote,"Add motivated model fits to validate_drp photometric and astrometric scatter/repeatability analysis and plots Implement well-motivated theoretical fits to the astrometric and photometric performance measurements based on derivations from LSST Overview paper. http://arxiv.org/pdf/0805.2366v4.pdf Photometric errors described by Eq. 5 sigma_rand^2 = (0.039 - gamma) * x + gamma * x^2 [mag^2] where x = 10^(0.4*(m-m_5)) Eq. 4 sigma_1^2 = sigma_sys^2 + sigma_rand^2 Astrometric Errors error = C * theta / SNR Based on helpful comments from [~zivezic] {quote} I think eq. 5 from the overview paper (with gamma = 0.039 and m5 = 24.35; the former I assumed and the latter I got from the value of your analytic fit that gives err=0.2 mag) would be a much better fit than the adopted function for mag < 21 (and it is derived from first principles). Actually, if you fit for the systematic term (eq. 4) and gamma and m5, it would be a nice check whether there is any weird behavior in analyzed data (and you get the limiting depth, m5, even if you don t go all the way to the faint end). Similarly, for the astrometric random errors, we d expect error = C * theta / SNR, where theta is the seeing (or a fit parameter), SNR is the photometric SNR (i.e. 1/err in mag), and C ~ 1 (empirically, and 0.6 for the idealized maximum likelihood solution and gaussian seeing). {quote}"
"Config override fixes needed due to new star selector As of DM-5532 a few config files need updating to not refer to star selector config fields as registries (not ones run by our normal CI, which is how I missed this).",2,DM-5663,datamanagement,config override fix need new star selector dm-5532 config file need update refer star selector config field registry one run normal ci miss,"Config override fixes needed due to new star selector As of DM-5532 a few config files need updating to not refer to star selector config fields as registries (not ones run by our normal CI, which is how I missed this)."
"Organize HSC docs ""hackathon"" Liase with SQuaRE to determine the most effective way to transfer HSC docs to LSST. Organize a hackathon session for DRP developers at which we get this done. Bring doughnuts.",1,DM-5665,datamanagement,organize hsc doc hackathon liase square determine effective way transfer hsc doc lsst organize hackathon session drp developer bring doughnut,"Organize HSC docs ""hackathon"" Liase with SQuaRE to determine the most effective way to transfer HSC docs to LSST. Organize a hackathon session for DRP developers at which we get this done. Bring doughnuts."
Take part in HSC docs hackathon Participate in HSC docs transfer hackathon.,2,DM-5666,datamanagement,hsc doc hackathon participate hsc docs transfer hackathon,Take part in HSC docs hackathon Participate in HSC docs transfer hackathon.
Take part in HSC docs hackathon Participate in HSC docs transfer hackathon.  ,2,DM-5667,datamanagement,hsc doc hackathon participate hsc docs transfer hackathon,Take part in HSC docs hackathon Participate in HSC docs transfer hackathon.
Take part in HSC docs hacakthon Participate in HSC docs transfer hackathon.,2,DM-5669,datamanagement,hsc doc hacakthon participate hsc docs transfer hackathon,Take part in HSC docs hacakthon Participate in HSC docs transfer hackathon.
"Cannot enable shapeHSM because RegistryField fails validation When running ci_hsc after setting-up the meas_extensions_shapeHSM, meas_extensions_photometryKron and dependencies using setup -v -r . in the respective cloned folders, I get  {code}  Cannot enable shapeHSM (RegistryField 'calibrate.detectAndMeasure.measurement.plugins' failed validation: Unknown key 'ext_shapeHSM_HsmMoments' in Registry/ConfigChoiceField  For more information read the Field definition at:    File ""/home/vish/code/lsst/lsstsw/stack/Linux64/pex_config/2016_01.0+3/python/lsst/pex/config/registry.py"", line 179, in __init__      ConfigChoiceField.__init__(self, doc, types, default, optional, multi)  And the Config definition at:    File ""/home/vish/code/lsst/lsstsw/stack/Linux64/meas_base/2016_01.0-13-g779ee14/python/lsst/meas/base/sfm.py"", line 109, in <module>      class SingleFrameMeasurementConfig(BaseMeasurementConfig):  ): disabling HSM shape measurements  {code}  Find out why this is happening and find a fix  ",1,DM-5675,datamanagement,enable shapehsm registryfield fail validation run ci_hsc set meas_extensions_shapehsm meas_extensions_photometrykron dependency setup -r respective clone folder code enable shapehsm registryfield calibrate.detectandmeasure.measurement.plugin fail validation unknown key ext_shapehsm_hsmmoment registry configchoicefield information read field definition file /home vish code lsst lsstsw stack linux64 pex_config/2016_01.0 python lsst pex config registry.py line 179 init configchoicefield.__init__(self doc type default optional multi config definition file /home vish code lsst lsstsw stack linux64 meas_base/2016_01.0 13 g779ee14 python lsst meas base sfm.py line 109 class singleframemeasurementconfig(basemeasurementconfig disable hsm shape measurement code find happen find fix,"Cannot enable shapeHSM because RegistryField fails validation When running ci_hsc after setting-up the meas_extensions_shapeHSM, meas_extensions_photometryKron and dependencies using setup -v -r . in the respective cloned folders, I get {code} Cannot enable shapeHSM (RegistryField 'calibrate.detectAndMeasure.measurement.plugins' failed validation: Unknown key 'ext_shapeHSM_HsmMoments' in Registry/ConfigChoiceField For more information read the Field definition at: File ""/home/vish/code/lsst/lsstsw/stack/Linux64/pex_config/2016_01.0+3/python/lsst/pex/config/registry.py"", line 179, in __init__ ConfigChoiceField.__init__(self, doc, types, default, optional, multi) And the Config definition at: File ""/home/vish/code/lsst/lsstsw/stack/Linux64/meas_base/2016_01.0-13-g779ee14/python/lsst/meas/base/sfm.py"", line 109, in  class SingleFrameMeasurementConfig(BaseMeasurementConfig): ): disabling HSM shape measurements {code} Find out why this is happening and find a fix"
"Provide single-visit processing capability as required by HSC In DM-3368, we provided a means of running multiple processCcd tasks across an exposure, but without performing global calibration etc as provided by HSC's ProcessExposureTask.    Please augment this with whatever additional capability is required to enable HSC data release processing.",2,DM-5681,datamanagement,provide single visit processing capability require hsc dm-3368 provide means run multiple processccd task exposure perform global calibration etc provide hsc processexposuretask augment additional capability require enable hsc datum release processing,"Provide single-visit processing capability as required by HSC In DM-3368, we provided a means of running multiple processCcd tasks across an exposure, but without performing global calibration etc as provided by HSC's ProcessExposureTask. Please augment this with whatever additional capability is required to enable HSC data release processing."
"processCcd.py is failing on some CFHT u band images processCcd.py is failing on some u band CFHT data, as reported by [~boutigny] on c.l.o: https://community.lsst.org/t/testing-dm-4692-the-new-processccdtask/507/24    See that posting for sample data to reproduce the problem.",8,DM-5685,datamanagement,processccd.py fail cfht band image processccd.py fail band cfht datum report ~boutigny c.l.o https://community.lsst.org/t/testing-dm-4692-the-new-processccdtask/507/24 post sample datum reproduce problem,"processCcd.py is failing on some CFHT u band images processCcd.py is failing on some u band CFHT data, as reported by [~boutigny] on c.l.o: https://community.lsst.org/t/testing-dm-4692-the-new-processccdtask/507/24 See that posting for sample data to reproduce the problem."
"Accommodate pixel padding when unpersisting reference catalog matches The reference object loader in {{meas_algorithm}}'s *loadReferenceObjects.py* grows the bbox by the config parameter pixelMargin:  doc = ""Padding to add to 4 all edges of the bounding box (pixels)"" . This is set to 50 by default but is not reflected by the radius parameter set in the metadata, so some matches may reside outside the circle searched within this radius. This increase needs to be reflected in the radius set in the metadata fed into {{joinMatchListWithCatalog()}}.  ",2,DM-5686,datamanagement,accommodate pixel padding unpersiste reference catalog match reference object loader meas_algorithm loadreferenceobjects.py grow bbox config parameter pixelmargin doc pad add edge bounding box pixel set 50 default reflect radius parameter set metadata match reside outside circle search radius increase need reflect radius set metadata feed joinmatchlistwithcatalog,"Accommodate pixel padding when unpersisting reference catalog matches The reference object loader in {{meas_algorithm}}'s *loadReferenceObjects.py* grows the bbox by the config parameter pixelMargin: doc = ""Padding to add to 4 all edges of the bounding box (pixels)"" . This is set to 50 by default but is not reflected by the radius parameter set in the metadata, so some matches may reside outside the circle searched within this radius. This increase needs to be reflected in the radius set in the metadata fed into {{joinMatchListWithCatalog()}}."
"Table performance on Firefly Table seems to perform poorly on Firefox. Firefox gets into the complete refresh state only when the table is visible with charts only, fits view only or fits view and chart it does not happen    Helpful article: http://benchling.engineering/performance-engineering-with-react/    changelog:  - added react performance tools, React.addons.Perf  - fix some performance issues:    - skip render of selection boxes when not needed.    - skip rendering of xyplot options when not needed.    - skip wasted render called for table cell and headers.    - will do a more in depth investigation in another ticket.  - refactor table code and it's state.    - move all table related states into table_space.    - create sub reducers for each data domain    - rename and move functions to better describe what it's doing   - added 'title' to table.  - show mask while loading    ",6,DM-5688,datamanagement,table performance firefly table perform poorly firefox firefox get complete refresh state table visible chart fit view fit view chart happen helpful article http://benchling.engineering/performance-engineering-with-react/ changelog add react performance tool react.addon perf fix performance issue skip render selection box need skip rendering xyplot option need skip waste render call table cell header depth investigation ticket refactor table code state table relate state table_space create sub reducer data domain rename function well describe add title table mask load,"Table performance on Firefly Table seems to perform poorly on Firefox. Firefox gets into the complete refresh state only when the table is visible with charts only, fits view only or fits view and chart it does not happen Helpful article: http://benchling.engineering/performance-engineering-with-react/ changelog: - added react performance tools, React.addons.Perf - fix some performance issues: - skip render of selection boxes when not needed. - skip rendering of xyplot options when not needed. - skip wasted render called for table cell and headers. - will do a more in depth investigation in another ticket. - refactor table code and it's state. - move all table related states into table_space. - create sub reducers for each data domain - rename and move functions to better describe what it's doing - added 'title' to table. - show mask while loading"
Table needs to fire another action when data completely loaded When the data for a table is completely loaded fire another action such as TABLE_NEW_LOADED_DONE. This way the xyplots and the image overlays know to go fetch the data.    4/22/2026 from the pull request:  added new action TABLE_NEW_LOADED to table; fired when table is completely loaded.  added table error handling.  fix active table not updating after an active tab is removed.,2,DM-5689,datamanagement,table need fire action datum completely load datum table completely load fire action table_new_loaded_done way xyplot image overlay know fetch datum 4/22/2026 pull request add new action table_new_loaded table fire table completely load add table error handling fix active table update active tab remove,Table needs to fire another action when data completely loaded When the data for a table is completely loaded fire another action such as TABLE_NEW_LOADED_DONE. This way the xyplots and the image overlays know to go fetch the data. 4/22/2026 from the pull request: added new action TABLE_NEW_LOADED to table; fired when table is completely loaded. added table error handling. fix active table not updating after an active tab is removed.
"Connect CatSim to StarFast simulation tool CatSim can provide a fully realistic simulated catalog, which StarFast could use as an input for simulations. This ticket includes writing the code to connect to the CatSim database and updating the internal catalog format in StarFast to be compatible with CatSim.",4,DM-5692,datamanagement,connect catsim starfast simulation tool catsim provide fully realistic simulated catalog starfast use input simulation ticket include write code connect catsim database update internal catalog format starfast compatible catsim,"Connect CatSim to StarFast simulation tool CatSim can provide a fully realistic simulated catalog, which StarFast could use as an input for simulations. This ticket includes writing the code to connect to the CatSim database and updating the internal catalog format in StarFast to be compatible with CatSim."
"Write StarFast interface to ProcessCCD The simulated images generated by StarFast need to be able to be run through the LSST stack, to test and make use of the existing measurement, fitting, stacking, and image differencing capabilities.   This includes writing or updating a simulations obs package, and determining and supplying the required metadata.",6,DM-5693,datamanagement,write starfast interface processccd simulate image generate starfast need able run lsst stack test use exist measurement fitting stacking image differencing capability include write update simulation obs package determine supply require metadata,"Write StarFast interface to ProcessCCD The simulated images generated by StarFast need to be able to be run through the LSST stack, to test and make use of the existing measurement, fitting, stacking, and image differencing capabilities. This includes writing or updating a simulations obs package, and determining and supplying the required metadata."
Run StarFast simulated images through diffim Determine the metadata and dependencies needed to fully process two images simulated with StarFast through diffim. ,2,DM-5694,datamanagement,run starfast simulate image diffim determine metadata dependency need fully process image simulate starfast diffim,Run StarFast simulated images through diffim Determine the metadata and dependencies needed to fully process two images simulated with StarFast through diffim.
"Implement simple 1D DCR correction on simulated data Nate Lust wrote a simple DCR correction recipe that runs in 1D in an ipython notebook. This ticket is to re-write the notebook in python modules that can be run on StarFast simulated images prior to image differencing. For this ticket, the simulated images will be 2D, but DCR will be purely along the x or y pixel grid, allowing columns or rows of pixels to be treated separately in 1D.",8,DM-5695,datamanagement,implement simple 1d dcr correction simulate datum nate lust write simple dcr correction recipe run 1d ipython notebook ticket write notebook python module run starfast simulate image prior image differencing ticket simulate image 2d dcr purely pixel grid allow column row pixel treat separately 1d.,"Implement simple 1D DCR correction on simulated data Nate Lust wrote a simple DCR correction recipe that runs in 1D in an ipython notebook. This ticket is to re-write the notebook in python modules that can be run on StarFast simulated images prior to image differencing. For this ticket, the simulated images will be 2D, but DCR will be purely along the x or y pixel grid, allowing columns or rows of pixels to be treated separately in 1D."
Add support for blank image We need to add blank image support.,2,DM-5696,datamanagement,add support blank image need add blank image support,Add support for blank image We need to add blank image support.
"Extend simple DCR correction to 2D In DM-5695 a simple DCR correction was applied to simulated images in the case that the effect was purely along the pixel grid and could be reduced to 1D. This ticket extends that work to the general 2D case.  Possible approaches include resampling the ""science"" image to match the ""template"", or including neighboring pixels and computing their covariance. Ideally, multiple approaches will be implemented and tested.",6,DM-5697,datamanagement,extend simple dcr correction 2d dm-5695 simple dcr correction apply simulate image case effect purely pixel grid reduce 1d. ticket extend work general 2d case possible approach include resample science image match template include neighboring pixel compute covariance ideally multiple approach implement test,"Extend simple DCR correction to 2D In DM-5695 a simple DCR correction was applied to simulated images in the case that the effect was purely along the pixel grid and could be reduced to 1D. This ticket extends that work to the general 2D case. Possible approaches include resampling the ""science"" image to match the ""template"", or including neighboring pixels and computing their covariance. Ideally, multiple approaches will be implemented and tested."
"Add astrometric errors to StarFast One concern with the proposed DCR correction is that it might fail in the presence of source position errors. This ticket is to add the capability to simulate a variety of types of position errors, such as atmospheric turbulence or an inaccurate WCS, to test the DCR implementation.",4,DM-5698,datamanagement,add astrometric error starfast concern propose dcr correction fail presence source position error ticket add capability simulate variety type position error atmospheric turbulence inaccurate wcs test dcr implementation,"Add astrometric errors to StarFast One concern with the proposed DCR correction is that it might fail in the presence of source position errors. This ticket is to add the capability to simulate a variety of types of position errors, such as atmospheric turbulence or an inaccurate WCS, to test the DCR implementation."
"Run many sky simulations through DCR correction to find edge cases Once a complete DCR correction prototype is finished, we will want to run many different sky simulations from StarFast with different densities of sources, noise properties, airmasses, and astrometric errors to find the limitations and edge cases where it fails. There are likely to be several thousand simulations needed which will take an as-yet undefined number of CPU hours, but this ticket is for the work in setting up and analyzing the results from the run.",4,DM-5699,datamanagement,run sky simulation dcr correction find edge case complete dcr correction prototype finish want run different sky simulation starfast different density source noise property airmasse astrometric error find limitation edge case fail likely thousand simulation need undefined number cpu hour ticket work set analyze result run,"Run many sky simulations through DCR correction to find edge cases Once a complete DCR correction prototype is finished, we will want to run many different sky simulations from StarFast with different densities of sources, noise properties, airmasses, and astrometric errors to find the limitations and edge cases where it fails. There are likely to be several thousand simulations needed which will take an as-yet undefined number of CPU hours, but this ticket is for the work in setting up and analyzing the results from the run."
Put ImageSelectPanel into dropdown Currently the image select panel is in a dialog.  It also needs to be able to work in a dropdown.,4,DM-5700,datamanagement,imageselectpanel dropdown currently image select panel dialog need able work dropdown,Put ImageSelectPanel into dropdown Currently the image select panel is in a dialog. It also needs to be able to work in a dropdown.
"Create toy composite (AST/GWCS) model with supported components To help us evaluate WCS options, we need to create a relatively complicated composite model in AST and GWCS, using a few models currently available within the existing packages. A minimal composite model to test these things would include:     * FITS linear transform   * ccd distortion   * optical model   * FITS TAN WCS    The middle steps do not need to be realistic models, just something that we can use to compare AST's and GWCS's respective interfaces and capabilities for creating the composite model, and test for differences in their results. We can then use this model to evaluate performance when run on different numbers of pixels.",4,DM-5701,datamanagement,create toy composite ast gwcs model support component help evaluate wcs option need create relatively complicated composite model ast gwcs model currently available exist package minimal composite model test thing include fits linear transform ccd distortion optical model fit tan wcs middle step need realistic model use compare ast gwcs respective interface capability create composite model test difference result use model evaluate performance run different number pixel,"Create toy composite (AST/GWCS) model with supported components To help us evaluate WCS options, we need to create a relatively complicated composite model in AST and GWCS, using a few models currently available within the existing packages. A minimal composite model to test these things would include: * FITS linear transform * ccd distortion * optical model * FITS TAN WCS The middle steps do not need to be realistic models, just something that we can use to compare AST's and GWCS's respective interfaces and capabilities for creating the composite model, and test for differences in their results. We can then use this model to evaluate performance when run on different numbers of pixels."
"Create a new model in AST/GWCS to represent a complex distortion Using lessons learned from DM-5701, create a more complex distortion model that cannot be represented from the basic models in GWCS or AST. A good example for this might be a rapidly varying sinusoidal tree-ring-like function that is not well represented by the standard polynomial basis functions. This will test our ability to extend each framework with new models that have not yet been decided on.    Once completed, we could plug this back into the composite model in DM-5701.",8,DM-5702,datamanagement,create new model ast gwcs represent complex distortion lesson learn dm-5701 create complex distortion model represent basic model gwcs ast good example rapidly vary sinusoidal tree ring like function represent standard polynomial basis function test ability extend framework new model decide complete plug composite model dm-5701,"Create a new model in AST/GWCS to represent a complex distortion Using lessons learned from DM-5701, create a more complex distortion model that cannot be represented from the basic models in GWCS or AST. A good example for this might be a rapidly varying sinusoidal tree-ring-like function that is not well represented by the standard polynomial basis functions. This will test our ability to extend each framework with new models that have not yet been decided on. Once completed, we could plug this back into the composite model in DM-5701."
"Evaluate performance of AST/GWCS over a range of numbers of pixels Once we have a composite distortion model from DM-5701, evaluate the performance of AST and GWCS over a range of numbers of pixels, likely from ~100 through full-CCD (4k^2).    As part of this process, we will try to determine whether there is a way to efficiently warp images/postage stamps using python-only models in GWCS and whether bottlenecks could be worked around via optimizations in cython.",8,DM-5703,datamanagement,evaluate performance ast gwcs range number pixel composite distortion model dm-5701 evaluate performance ast gwcs range number pixel likely ~100 ccd 4k^2 process try determine way efficiently warp image postage stamp python model gwcs bottleneck work optimization cython,"Evaluate performance of AST/GWCS over a range of numbers of pixels Once we have a composite distortion model from DM-5701, evaluate the performance of AST and GWCS over a range of numbers of pixels, likely from ~100 through full-CCD (4k^2). As part of this process, we will try to determine whether there is a way to efficiently warp images/postage stamps using python-only models in GWCS and whether bottlenecks could be worked around via optimizations in cython."
"Produce document describing DRP parallelization use cases At various times in the past few months I've promised [~gpdf], [~petravick], [~kooper], and probably a few others a document describing the parallelization needs for DRP in greater detail.  My understanding of the plans for the eventual DRP probably good enough to do this well now, and is unlikely to improve further in the near future (as that will require algorithmic research).    This needs to be prioritized with my other responsibility for documents that describe the DRP system in other ways, most of which are oriented towards scientists and science pipelines developers.  The document on this ticket is essentially the description that would matter the most for the process control middleware team.",6,DM-5715,datamanagement,produce document describe drp parallelization use case time past month promise ~gpdf ~petravick ~kooper probably document describe parallelization need drp great detail understanding plan eventual drp probably good unlikely improve near future require algorithmic research need prioritize responsibility document describe drp system way orient scientist science pipeline developer document ticket essentially description matter process control middleware team,"Produce document describing DRP parallelization use cases At various times in the past few months I've promised [~gpdf], [~petravick], [~kooper], and probably a few others a document describing the parallelization needs for DRP in greater detail. My understanding of the plans for the eventual DRP probably good enough to do this well now, and is unlikely to improve further in the near future (as that will require algorithmic research). This needs to be prioritized with my other responsibility for documents that describe the DRP system in other ways, most of which are oriented towards scientists and science pipelines developers. The document on this ticket is essentially the description that would matter the most for the process control middleware team."
UI Consistency There is a need to go though the entire ui and document inconsistencies with the old UI.,4,DM-5716,datamanagement,ui consistency need entire ui document inconsistency old ui,UI Consistency There is a need to go though the entire ui and document inconsistencies with the old UI.
JIRA fixes This tracks SPs spent on JIRA requests. ,2,DM-5720,datamanagement,jira fix track sp spend jira request,JIRA fixes This tracks SPs spent on JIRA requests.
Add table client-side sorting Convert gwt's client-side sorting to javascript.,4,DM-5722,datamanagement,add table client sort convert gwt client sorting javascript,Add table client-side sorting Convert gwt's client-side sorting to javascript.
make sure table can be resized properly Test table to make sure it can be resized under a variety of layout.,2,DM-5723,datamanagement,sure table resize properly test table sure resize variety layout,make sure table can be resized properly Test table to make sure it can be resized under a variety of layout.
"attend the weekly meeting with UIUC camera team While Tatiana is the assignee of this ticket, Xiuqin and Gregory participate this weekly telecon semi-regularly to lend support. ",2,DM-5725,datamanagement,attend weekly meeting uiuc camera team tatiana assignee ticket xiuqin gregory participate weekly telecon semi regularly lend support,"attend the weekly meeting with UIUC camera team While Tatiana is the assignee of this ticket, Xiuqin and Gregory participate this weekly telecon semi-regularly to lend support."
attend the weekly meeting with UIUC camera team (May 2016) Tatiana will attend the weekly meeting. Xiuqin and Gregory also attends when needed. ,2,DM-5726,datamanagement,attend weekly meeting uiuc camera team 2016 tatiana attend weekly meeting xiuqin gregory attend need,attend the weekly meeting with UIUC camera team (May 2016) Tatiana will attend the weekly meeting. Xiuqin and Gregory also attends when needed.
"Create django project and initial dashboard app This ticket captures the steps to create the django project for SQUASH, its configuration and the dashboard app http://sqr-009.lsst.io/en/latest/    The planned tasks are:        - Implement the ``Dataset``, ``Visit`` and ``Ccd`` tables in the django ORM layer, as a minimum set      of tables for the dashboard app      - Prototype home page and dashboard pages      ",5,DM-5728,datamanagement,create django project initial dashboard app ticket capture step create django project squash configuration dashboard app http://sqr-009.lsst.io/en/latest/ plan task implement dataset visit ccd table django orm layer minimum set table dashboard app prototype home page dashboard page,"Create django project and initial dashboard app This ticket captures the steps to create the django project for SQUASH, its configuration and the dashboard app http://sqr-009.lsst.io/en/latest/ The planned tasks are: - Implement the ``Dataset``, ``Visit`` and ``Ccd`` tables in the django ORM layer, as a minimum set of tables for the dashboard app - Prototype home page and dashboard pages"
"Config.loadFromStream suppresses NameError Within a config override file being executed via {{Config.load}} or {{Config.loadFromStream}}, using a variable that hasn't been defined results in a {{NameError}} exception, but this is silently suppressed and the user has no idea the following overrides have not been executed.",1,DM-5729,datamanagement,config.loadfromstream suppress nameerror config override file execute config.load config.loadfromstream variable define result nameerror exception silently suppress user idea follow override execute,"Config.loadFromStream suppresses NameError Within a config override file being executed via {{Config.load}} or {{Config.loadFromStream}}, using a variable that hasn't been defined results in a {{NameError}} exception, but this is silently suppressed and the user has no idea the following overrides have not been executed."
"Fix the issues in the server side and the client side introduced by FitsHeaderViewer 's work *  The testing data ""table_data.tbl"" in the testing tree was accidentally moved.  It should be added back so that IpactTableTest.java can run.    * The request in JsontableUtil was mistakenly moved out from the tableModel by the the line   * {code}  * if (request != null && request.getMeta().keySet().size()>1) {              tableModel.put(""request"", toJsonTableRequest(request));  }  {code}.  The meta can be null but the request is not null, the request should be put into the TableModel.     ",1,DM-5734,datamanagement,"fix issue server client introduce fitsheaderviewer work testing datum table_data.tbl testing tree accidentally move add ipacttabletest.java run request jsontableutil mistakenly move tablemodel line code request null request.getmeta().keyset().size()>1 tablemodel.put(""requ tojsontablerequest(requ code meta null request null request tablemodel","Fix the issues in the server side and the client side introduced by FitsHeaderViewer 's work * The testing data ""table_data.tbl"" in the testing tree was accidentally moved. It should be added back so that IpactTableTest.java can run. * The request in JsontableUtil was mistakenly moved out from the tableModel by the the line * {code} * if (request != null && request.getMeta().keySet().size()>1) { tableModel.put(""request"", toJsonTableRequest(request)); } {code}. The meta can be null but the request is not null, the request should be put into the TableModel."
"Move Camera creation out of CameraMapper base class With the new cameraGeom, it's considered desirable that each camera be able to define the serialization format for its static camera data.  Despite this, it's still the base CameraMapper that does loads it (at least for most cameras), going through a circuitous chain of policy files, obs_* package paths, and Python code.    It'd be vastly simpler for each mapper to simply build the Camera object and assign it to {{self.camera}} in its own {{\_\_init\_\_}} method (most would simply delegate all the work to {{afw.cameraGeom.makeCameraFromPath}}).  We could then remove all the camera entries from the PAF files and make it much easier to follow the logic.    Eventually, I think we need to be storing at least some components of the camera definition in the data repository (or something like a calibration repository associated with it), and that would require giving the mapper access to a partially-constructed butler when its time to build the camera.  But we can save that for another day.  ",2,DM-5738,datamanagement,camera creation cameramapper base class new camerageom consider desirable camera able define serialization format static camera datum despite base cameramapper load camera go circuitous chain policy file obs package path python code vastly simple mapper simply build camera object assign self.camera \_\_init\_\ method simply delegate work afw.camerageom.makecamerafrompath remove camera entry paf file easy follow logic eventually think need store component camera definition data repository like calibration repository associate require give mapper access partially construct butler time build camera save day,"Move Camera creation out of CameraMapper base class With the new cameraGeom, it's considered desirable that each camera be able to define the serialization format for its static camera data. Despite this, it's still the base CameraMapper that does loads it (at least for most cameras), going through a circuitous chain of policy files, obs_* package paths, and Python code. It'd be vastly simpler for each mapper to simply build the Camera object and assign it to {{self.camera}} in its own {{\_\_init\_\_}} method (most would simply delegate all the work to {{afw.cameraGeom.makeCameraFromPath}}). We could then remove all the camera entries from the PAF files and make it much easier to follow the logic. Eventually, I think we need to be storing at least some components of the camera definition in the data repository (or something like a calibration repository associated with it), and that would require giving the mapper access to a partially-constructed butler when its time to build the camera. But we can save that for another day."
"--clobber-config modifies input rerun Using the {{--clobber-config}} option in a child butler repository can cause changes in the parent repository, as we try to rename files to back them up in the parent repository.    This is a critical bug because it can cause pipeline outputs to be unexpectedly modified.    It should be easy to fix, as it's just a matter of checking whether the files to be renamed backed up are in the output repository.    This was originally reported as https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1341  ",1,DM-5739,datamanagement,--clobber config modify input rerun --clobber config option child butler repository cause change parent repository try rename file parent repository critical bug cause pipeline output unexpectedly modify easy fix matter check file rename back output repository originally report https://hsc-jira.astro.princeton.edu/jira/browse/hsc-1341,"--clobber-config modifies input rerun Using the {{--clobber-config}} option in a child butler repository can cause changes in the parent repository, as we try to rename files to back them up in the parent repository. This is a critical bug because it can cause pipeline outputs to be unexpectedly modified. It should be easy to fix, as it's just a matter of checking whether the files to be renamed backed up are in the output repository. This was originally reported as https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1341"
"Create and deploy common Ansible roles for ELK Create roles and deploy to Ansible Galaxy.    These are common roles for cloud-init (and any other future cloud dependencies), java (openjdk-jdk) and an editors role.",3,DM-5740,datamanagement,create deploy common ansible role elk create role deploy ansible galaxy common role cloud init future cloud dependency java openjdk jdk editor role,"Create and deploy common Ansible roles for ELK Create roles and deploy to Ansible Galaxy. These are common roles for cloud-init (and any other future cloud dependencies), java (openjdk-jdk) and an editors role."
Create and deploy Elasticsearch and Kibana Ansible roles Create roles and deploy to Ansible Galaxy.,3,DM-5741,datamanagement,create deploy elasticsearch kibana ansible role create role deploy ansible galaxy,Create and deploy Elasticsearch and Kibana Ansible roles Create roles and deploy to Ansible Galaxy.
"Create and deploy Logstash, Fluentd and Riemann Ansible roles Create roles and deploy to Ansible Galaxy.    Create a role to combine all the individual projects together.",3,DM-5743,datamanagement,create deploy logstash fluentd riemann ansible role create role deploy ansible galaxy create role combine individual project,"Create and deploy Logstash, Fluentd and Riemann Ansible roles Create roles and deploy to Ansible Galaxy. Create a role to combine all the individual projects together."
Create packer automation for ELK Build packer automation to create machine images to use for the ELK system.,6,DM-5744,datamanagement,create packer automation elk build packer automation create machine image use elk system,Create packer automation for ELK Build packer automation to create machine images to use for the ELK system.
"Implement ingestion code for the QA results The initial database model was implemented in DM-5728 and outputs of the QA analysis code are being produced by the work described in http://dmtn-008.lsst.io/en/latest/    In this ticket we plan to implement and API for listing and creating jobs, metrics and measurements so that a job or the QA analysis code can register this information in the dashboard app.",4,DM-5745,datamanagement,implement ingestion code qa result initial database model implement dm-5728 output qa analysis code produce work describe http://dmtn-008.lsst.io/en/latest/ ticket plan implement api list create job metric measurement job qa analysis code register information dashboard app,"Implement ingestion code for the QA results The initial database model was implemented in DM-5728 and outputs of the QA analysis code are being produced by the work described in http://dmtn-008.lsst.io/en/latest/ In this ticket we plan to implement and API for listing and creating jobs, metrics and measurements so that a job or the QA analysis code can register this information in the dashboard app."
Build parallel DCR simulator using GalSim The result of DM-4899 was a simulation tool called StarFast that can quickly make simulated images with realistic Differential Chromatic Refraction. This ticket is to build an equivalent simulator using GalSim to check the accuracy of results and benchmark speed and memory usage. ,6,DM-5746,datamanagement,build parallel dcr simulator galsim result dm-4899 simulation tool call starfast quickly simulated image realistic differential chromatic refraction ticket build equivalent simulator galsim check accuracy result benchmark speed memory usage,Build parallel DCR simulator using GalSim The result of DM-4899 was a simulation tool called StarFast that can quickly make simulated images with realistic Differential Chromatic Refraction. This ticket is to build an equivalent simulator using GalSim to check the accuracy of results and benchmark speed and memory usage.
SQUASH dashboard prototype design SQUASH dashboard prototype design is described here    http://sqr-009.lsst.io/en/latest/,8,DM-5747,datamanagement,squash dashboard prototype design squash dashboard prototype design describe http://sqr-009.lsst.io/en/latest/,SQUASH dashboard prototype design SQUASH dashboard prototype design is described here http://sqr-009.lsst.io/en/latest/
"Upgrade mpi4py to latest upstream [mpi4py|https://bitbucket.org/mpi4py/] version 2.0 was released in October 2015 with a number of changes. We should upgrade. When upgrading, we should check whether it contains a proper fix for DM-5409 and, if not, file a bug report upstream.    This issue should not be addressed until we have proper test coverage on code which uses mpi4py (DM-3845).",1,DM-5748,datamanagement,upgrade mpi4py late upstream mpi4py|https://bitbucket.org mpi4py/ version 2.0 release october 2015 number change upgrade upgrade check contain proper fix dm-5409 file bug report upstream issue address proper test coverage code use mpi4py dm-3845,"Upgrade mpi4py to latest upstream [mpi4py|https://bitbucket.org/mpi4py/] version 2.0 was released in October 2015 with a number of changes. We should upgrade. When upgrading, we should check whether it contains a proper fix for DM-5409 and, if not, file a bug report upstream. This issue should not be addressed until we have proper test coverage on code which uses mpi4py (DM-3845)."
"XCode 7.3 can not link indirect dependencies that use @rpath With XCode 7.3 on OS X we have difficulties resolving indirect dependencies when those dependencies are referenced using {{@rpath}}. This can be seen with Qserv:  {code}  Linking shared object build/libqserv_common.dylib  ld: file not found: @rpath/libboost_system.dylib for architecture x86_64  clang: error: linker command failed with exit code 1 (use -v to see invocation)  {code}  where {{libboost_system}} is being loaded via {{libboost_thread}}:  {code}  $ otool -L $BOOST_DIR/lib/libboost_thread.dylib  /Users/timj/work/lsstsw/stack/DarwinX86/boost/1.59.lsst5+fbf04ba888/lib/libboost_thread.dylib:  	@rpath/libboost_thread.dylib (compatibility version 0.0.0, current version 0.0.0)  	@rpath/libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0)  	/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 120.1.0)  	/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1)  {code}    This problem is also found when doing a {{conda}} build of the stack because in {{conda}} all shared libraries are modified on creation to reference other libraries via the {{@rpath}} mechanism.    This bug has been reported to Apple as [rdr://25313838|http://www.openradar.me/25313838] and a [Chromium bug report|https://bugs.chromium.org/p/chromium/issues/detail?id=597459] indicates that the fix is to simply ensure that {{-L}} directives include a trailing slash.  ",4,DM-5753,datamanagement,xcode 7.3 link indirect dependency use @rpath xcode 7.3 os difficulty resolve indirect dependency dependency reference @rpath see qserv code link share object build ld file find @rpath architecture clang error linker command fail exit code use -v invocation code libboost_system load libboost_thread code otool -l boost_dir lib /users timj work lsstsw stack darwinx86 boost/1.59.lsst5+fbf04ba888 lib libboost_thread.dylib @rpath compatibility version 0.0.0 current version 0.0.0 @rpath compatibility version 0.0.0 current version 0.0.0 lib libc++.1.dylib compatibility version 1.0.0 current version 120.1.0 /usr lib libsystem b.dylib compatibility version 1.0.0 current version 1226.10.1 code problem find conda build stack conda share library modify creation reference library @rpath mechanism bug report apple rdr://25313838|http://www.openradar.me/25313838 chromium bug report|https://bugs.chromium.org chromium issue detail?id=597459 indicate fix simply ensure -l directive include trail slash,"XCode 7.3 can not link indirect dependencies that use @rpath With XCode 7.3 on OS X we have difficulties resolving indirect dependencies when those dependencies are referenced using {{@rpath}}. This can be seen with Qserv: {code} Linking shared object build/libqserv_common.dylib ld: file not found: @rpath/libboost_system.dylib for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation) {code} where {{libboost_system}} is being loaded via {{libboost_thread}}: {code} $ otool -L $BOOST_DIR/lib/libboost_thread.dylib /Users/timj/work/lsstsw/stack/DarwinX86/boost/1.59.lsst5+fbf04ba888/lib/libboost_thread.dylib: @rpath/libboost_thread.dylib (compatibility version 0.0.0, current version 0.0.0) @rpath/libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0) /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 120.1.0) /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1) {code} This problem is also found when doing a {{conda}} build of the stack because in {{conda}} all shared libraries are modified on creation to reference other libraries via the {{@rpath}} mechanism. This bug has been reported to Apple as [rdr://25313838|http://www.openradar.me/25313838] and a [Chromium bug report|https://bugs.chromium.org/p/chromium/issues/detail?id=597459] indicates that the fix is to simply ensure that {{-L}} directives include a trailing slash."
Update Scons to v2.5.0 Scons 2.5.0 came out over the weekend. There were many fixes to the dependency determination code. The next version of Scons is intended to be 3.0 which will be the first version to support Python 3. Since we fully intend to switch to Python 3.0 in the summer it is prudent for us to ensuer that 2.5.0 works fine before switching to 3.0.0 so that we do not get confused as to why there is breakage in jumping straight to 3.0.0.,2,DM-5756,datamanagement,update scon v2.5.0 scon 2.5.0 come weekend fix dependency determination code version scons intend 3.0 version support python fully intend switch python 3.0 summer prudent ensuer 2.5.0 work fine switch 3.0.0 confuse breakage jump straight 3.0.0,Update Scons to v2.5.0 Scons 2.5.0 came out over the weekend. There were many fixes to the dependency determination code. The next version of Scons is intended to be 3.0 which will be the first version to support Python 3. Since we fully intend to switch to Python 3.0 in the summer it is prudent for us to ensuer that 2.5.0 works fine before switching to 3.0.0 so that we do not get confused as to why there is breakage in jumping straight to 3.0.0.
"FitsHeader's resize and sorting DM-4494 has merged to the dev.  However, there are still two issues remained:  * Resize the popup with tabs does not work  * Sorting is depending on the BasicTable's sorting",1,DM-5757,datamanagement,fitsheader resize sort dm-4494 merge dev issue remain resize popup tab work sorting depend basictable sorting,"FitsHeader's resize and sorting DM-4494 has merged to the dev. However, there are still two issues remained: * Resize the popup with tabs does not work * Sorting is depending on the BasicTable's sorting"
TabPanel needs a way to keep it state between renders The TabPanel and CollapsiblePanel loses its state when it is re-rendered.  It is going to have to have a way keeps it state. Therefore it needs an option to take an ID and keep it state in the store.      Use case- tabs of tables then the image plot goes to expanded mode.  The table tabs gets reset to the first one.    ,4,DM-5759,datamanagement,tabpanel need way state render tabpanel collapsiblepanel lose state render go way keep state need option id state store use case- tab table image plot go expand mode table tab get reset,TabPanel needs a way to keep it state between renders The TabPanel and CollapsiblePanel loses its state when it is re-rendered. It is going to have to have a way keeps it state. Therefore it needs an option to take an ID and keep it state in the store. Use case- tabs of tables then the image plot goes to expanded mode. The table tabs gets reset to the first one.
XYPlot needs to be expandable Make XYPlot expandable,2,DM-5760,datamanagement,xyplot need expandable xyplot expandable,XYPlot needs to be expandable Make XYPlot expandable
"XYPlot: Optimize decimated plot aspect ratio Currently decimation process assumes aspect ration 1. For decimated plots, the displayed area size (or user supplied value) needs to be used as an aspect ratio to approximate square bins.  - When aspect ratio changes, decimation process needs to be redone.  - To avoid server calls on resize, disallow flexible aspect ratio for decimated data.   ",6,DM-5762,datamanagement,xyplot optimize decimate plot aspect ratio currently decimation process assume aspect ration decimated plot display area size user supply value need aspect ratio approximate square bin aspect ratio change decimation process need redo avoid server call resize disallow flexible aspect ratio decimated datum,"XYPlot: Optimize decimated plot aspect ratio Currently decimation process assumes aspect ration 1. For decimated plots, the displayed area size (or user supplied value) needs to be used as an aspect ratio to approximate square bins. - When aspect ratio changes, decimation process needs to be redone. - To avoid server calls on resize, disallow flexible aspect ratio for decimated data."
XYPlot: decimation options User needs to be able to control number of bins and bin size.,3,DM-5763,datamanagement,xyplot decimation option user need able control number bin bin size,XYPlot: decimation options User needs to be able to control number of bins and bin size.
"XYPlot: separate density plot from scatter plot At the moment we display data as scatter plot, when the number of points does not exceed decimation limit, and as density plot when the number of points does exceed this limit.    Scatter plot and density plot should be separate charts. These are the reasons:  - User should be able to create density plot with any number of points  - Chart type and display might be different for density plot in the future  - Scatter plot look should not change when the number of points exceeds decimation limit  - Scatter plot should support errors in the future",5,DM-5764,datamanagement,xyplot separate density plot scatter plot moment display datum scatter plot number point exceed decimation limit density plot number point exceed limit scatter plot density plot separate chart reason user able create density plot number point chart type display different density plot future scatter plot look change number point exceed decimation limit scatter plot support error future,"XYPlot: separate density plot from scatter plot At the moment we display data as scatter plot, when the number of points does not exceed decimation limit, and as density plot when the number of points does exceed this limit. Scatter plot and density plot should be separate charts. These are the reasons: - User should be able to create density plot with any number of points - Chart type and display might be different for density plot in the future - Scatter plot look should not change when the number of points exceeds decimation limit - Scatter plot should support errors in the future"
"Remove unneeded imports in SConstruct There's an outstanding pull request from an external contributor (Miguel de Val-Borro) [here|https://github.com/lsst/sconsUtils/pull/9] that makes some minor improvements to sconsUtils by cleaning up the imports. Somebody should review and (if appropriate) merge it. (Or, at least, reply to our community!)",1,DM-5765,datamanagement,remove unneeded import sconstruct outstanding pull request external contributor miguel de val borro here|https://github.com lsst sconsutils pull/9 make minor improvement sconsutil clean import somebody review appropriate merge reply community,"Remove unneeded imports in SConstruct There's an outstanding pull request from an external contributor (Miguel de Val-Borro) [here|https://github.com/lsst/sconsUtils/pull/9] that makes some minor improvements to sconsUtils by cleaning up the imports. Somebody should review and (if appropriate) merge it. (Or, at least, reply to our community!)"
"Implement spatial exposure selection task Once DM-3472 lands, it will be possible to write an image selection task that uses the SQLite 3 database produced by {{IndexExposureTask}} (from [daf_ingest|https://github.com/lsst/daf_ingest]) to search for exposures overlapping a region (in particular, the spatial extent of a coadd patch). The {{_rtree_search}} method in {{test_index_exposure.py}} (also from daf_ingest) has an example of how to perform spatial queries quickly.    I was originally scheduled to do something in this space, but Paul mentioned that he had plans to refactor the image selection tasks already, and is much more familiar with the pipeline side of things than I am. Therefore, I'm handing off the implementation of the pipeline task mentioned in DM-3472 to him.",6,DM-5766,datamanagement,implement spatial exposure selection task dm-3472 land possible write image selection task use sqlite database produce indexexposuretask daf_ingest|https://github.com lsst daf_ingest search exposure overlap region particular spatial extent coadd patch rtree_search method test_index_exposure.py daf_ingest example perform spatial query quickly originally schedule space paul mention plan refactor image selection task familiar pipeline thing hand implementation pipeline task mention dm-3472,"Implement spatial exposure selection task Once DM-3472 lands, it will be possible to write an image selection task that uses the SQLite 3 database produced by {{IndexExposureTask}} (from [daf_ingest|https://github.com/lsst/daf_ingest]) to search for exposures overlapping a region (in particular, the spatial extent of a coadd patch). The {{_rtree_search}} method in {{test_index_exposure.py}} (also from daf_ingest) has an example of how to perform spatial queries quickly. I was originally scheduled to do something in this space, but Paul mentioned that he had plans to refactor the image selection tasks already, and is much more familiar with the pipeline side of things than I am. Therefore, I'm handing off the implementation of the pipeline task mentioned in DM-3472 to him."
"Create custom basic coaddition code Create script to do the following:  * Takes a list of DECam exposure numbers  * for each CCD, loads the corresponding calexps  * creates a naive pixel-by-pixel coadd of the underlying images  * Possibly either ANDs or ORs the masks (though perhaps not necessary)  * Either sums the expusure time info from the headers, or averages them, depending on whether the images were normalised to exposure times or not  * write the corresponding images out as coadded fits",1,DM-5767,datamanagement,create custom basic coaddition code create script following take list decam exposure number ccd load correspond calexps create naive pixel pixel coadd underlie image possibly and or mask necessary sum expusure time info header average depend image normalise exposure time write correspond image coadde fit,"Create custom basic coaddition code Create script to do the following: * Takes a list of DECam exposure numbers * for each CCD, loads the corresponding calexps * creates a naive pixel-by-pixel coadd of the underlying images * Possibly either ANDs or ORs the masks (though perhaps not necessary) * Either sums the expusure time info from the headers, or averages them, depending on whether the images were normalised to exposure times or not * write the corresponding images out as coadded fits"
"Coadd CPB exposures Identify sets of DECam exposures from the CBP run and feed them to the coaddition script created in DM-5767.    This will need to be redone each time a reprocessing is done as the script will run on calexps. I will do it once now, and then again after DM-5465 is completed to satisfactory levels.",1,DM-5768,datamanagement,coadd cpb exposure identify set decam exposure cbp run feed coaddition script create dm-5767 need redo time reprocessing script run calexps dm-5465 complete satisfactory level,"Coadd CPB exposures Identify sets of DECam exposures from the CBP run and feed them to the coaddition script created in DM-5767. This will need to be redone each time a reprocessing is done as the script will run on calexps. I will do it once now, and then again after DM-5465 is completed to satisfactory levels."
"Write spot visualisation snippets Write some snippets to aide in the processing and visualisation of the CBP data/analysis.    Essentially, write some helper functions that you can throw sections of images at to help look at the shape of the CBP spots, as ds9 isn't great ideal this.    Some nice features would be:    A function that takes a list of images or arrays, and plots them side-by-side, which provides some intelligent options for the stretches, and optionally stretches each image as is best for it, or ties them all to be the same. This would be as 2D colour plots.    A function that takes part of an image and displays it as a colour-graded surface.    A function that takes part of an image and displays it as a 3D bar-chart (as in ROOT, but without using ROOT because there is already enough evil in the world)",2,DM-5769,datamanagement,write spot visualisation snippet write snippet aide processing visualisation cbp datum analysis essentially write helper function throw section image help look shape cbp spot ds9 great ideal nice feature function take list image array plot provide intelligent option stretch optionally stretch image good tie 2d colour plot function take image display colour grade surface function take image display 3d bar chart root root evil world,"Write spot visualisation snippets Write some snippets to aide in the processing and visualisation of the CBP data/analysis. Essentially, write some helper functions that you can throw sections of images at to help look at the shape of the CBP spots, as ds9 isn't great ideal this. Some nice features would be: A function that takes a list of images or arrays, and plots them side-by-side, which provides some intelligent options for the stretches, and optionally stretches each image as is best for it, or ties them all to be the same. This would be as 2D colour plots. A function that takes part of an image and displays it as a colour-graded surface. A function that takes part of an image and displays it as a 3D bar-chart (as in ROOT, but without using ROOT because there is already enough evil in the world)"
"Investigate image processing for feature enhancement Whilst looking at an individual spot from the CBP on DECam I noticed a weird feature, and upon further investigation, several more, though these were very hard to see.    This ticket is to investigate what image processing techniques will make these hard-to-see features pop out so that they can be examined more closely.",2,DM-5770,datamanagement,investigate image processing feature enhancement whilst look individual spot cbp decam notice weird feature investigation hard ticket investigate image processing technique hard feature pop examine closely,"Investigate image processing for feature enhancement Whilst looking at an individual spot from the CBP on DECam I noticed a weird feature, and upon further investigation, several more, though these were very hard to see. This ticket is to investigate what image processing techniques will make these hard-to-see features pop out so that they can be examined more closely."
"Update config files DM-46921 and DM-5348 changed ProcessCcd to the point where past config files are no longer valid as stuff has moved a lot (see https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581)    This ticket is to go through past configs and create a new config file to reproduce the reductions done, or at least make something sensible come out the end of processCcd",2,DM-5771,datamanagement,update config file dm-46921 dm-5348 change processccd point past config file long valid stuff move lot https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581 ticket past config create new config file reproduce reduction sensible come end processccd,"Update config files DM-46921 and DM-5348 changed ProcessCcd to the point where past config files are no longer valid as stuff has moved a lot (see https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581) This ticket is to go through past configs and create a new config file to reproduce the reductions done, or at least make something sensible come out the end of processCcd"
"Firefly API plan and decision We need a plan for  all the Firefly APIs development in the new React/Redux based JS framework, including JS API and Python API.    - Backward compatibility  - Syntax format for JS API  - Syntax format for Python API  - Schedule     - convert the existing API first     - list of new ones to be added, when    ",2,DM-5773,datamanagement,firefly api plan decision need plan firefly api development new react redux base js framework include js api python api backward compatibility syntax format js api syntax format python api schedule convert exist api list new one add,"Firefly API plan and decision We need a plan for all the Firefly APIs development in the new React/Redux based JS framework, including JS API and Python API. - Backward compatibility - Syntax format for JS API - Syntax format for Python API - Schedule - convert the existing API first - list of new ones to be added, when"
"Change the TabPanel.jsx and TabPanel.css's properties to allow its children can be resizable When an outside container is resizable (using css properties: resize: 'both', overflow: 'auto'...), in order for the child inside the container to be resizable, the child has to specify its height and width properties using percentage format (height: 90%, width:100%).   When the TabPanel is used, the table is put on TabPanel.  The table needs to access the size information of the outside container, ie,, the grandparent's width and height. The TabPanel has to pass the height and width to its child component.  Without specifying the height and width in the TabPanel, by default, the auto is used.  When the width (height) is auto, it allows to use the child's width (height).  However, the child replies on the parent to provide such information.  When this circular relations occur, the default size of the child is used.  That is why the table component forever has 75px when it was put in the TabPanel.  To be able to resize with the outside (root) contains all the ancestors of the component have to specify the width and height explicitly. ",1,DM-5775,datamanagement,change tabpanel.jsx tabpanel.css property allow child resizable outside container resizable css property resize overflow auto order child inside container resizable child specify height width property percentage format height 90 width:100 tabpanel table tabpanel table need access size information outside container ie grandparent width height tabpanel pass height width child component specify height width tabpanel default auto width height auto allow use child width height child reply parent provide information circular relation occur default size child table component forever 75px tabpanel able resize outside root contain ancestor component specify width height explicitly,"Change the TabPanel.jsx and TabPanel.css's properties to allow its children can be resizable When an outside container is resizable (using css properties: resize: 'both', overflow: 'auto'...), in order for the child inside the container to be resizable, the child has to specify its height and width properties using percentage format (height: 90%, width:100%). When the TabPanel is used, the table is put on TabPanel. The table needs to access the size information of the outside container, ie,, the grandparent's width and height. The TabPanel has to pass the height and width to its child component. Without specifying the height and width in the TabPanel, by default, the auto is used. When the width (height) is auto, it allows to use the child's width (height). However, the child replies on the parent to provide such information. When this circular relations occur, the default size of the child is used. That is why the table component forever has 75px when it was put in the TabPanel. To be able to resize with the outside (root) contains all the ancestors of the component have to specify the width and height explicitly."
"Document Configurable concept The Configurable concept (a callable that takes a config as an argument) is a fairly important one in pex_config as the guts behind RegistryField and ConfigurableField, and it's mentioned several times in pex_config's documentation, but it doesn't seem to be directly documented itself.",1,DM-5778,datamanagement,document configurable concept configurable concept callable take config argument fairly important pex_config gut registryfield configurablefield mention time pex_config documentation directly document,"Document Configurable concept The Configurable concept (a callable that takes a config as an argument) is a fairly important one in pex_config as the guts behind RegistryField and ConfigurableField, and it's mentioned several times in pex_config's documentation, but it doesn't seem to be directly documented itself."
Assist schandra with ts_wep Luigi implementation Assist [~schandra] with an initial implementation of his workflow using Luigi.,1,DM-5780,datamanagement,assist schandra ts_wep luigi implementation assist ~schandra initial implementation workflow luigi,Assist schandra with ts_wep Luigi implementation Assist [~schandra] with an initial implementation of his workflow using Luigi.
"Include obs_cfht, obs_decam in lsst-dev shared stack The shared stack on {{lsst-dev}} provided in DM-5435 does not contain the {{obs_cfht}} or {{obs_decam}} camera packages. Please add them.",1,DM-5782,datamanagement,include obs_cfht obs_decam lsst dev share stack share stack lsst dev provide dm-5435 contain obs_cfht obs_decam camera package add,"Include obs_cfht, obs_decam in lsst-dev shared stack The shared stack on {{lsst-dev}} provided in DM-5435 does not contain the {{obs_cfht}} or {{obs_decam}} camera packages. Please add them."
"Port region serializer and data structures from GWT The region serializer in: firefly/src/firefly/java/edu/caltech/ipac/util  * RegionFactory.java    Region container data structures files in : firefly/src/firefly/java/edu/caltech/ipac/util/dd    * ContainsOptions.java  * Global.java  * RegionFileElement.java  * RegParseException.java  * Region.java  * RegionAnnulus.java  * RegionBox.java  * RegionBoxAnnulus.java  * RegionCsys.java  * RegionDimension.java  * RegionEllipse.java  * RegionEllipseAnnulus.java  * RegionFont.java  * RegionLines.java  * RegionOptions.java  * RegionPoint.java  * RegionText.java  * RegionValue.java      Note - do not port CoordException, there are other ways to do this.",8,DM-5784,datamanagement,port region serializer datum structure gwt region serializer firefly src firefly java edu caltech ipac util regionfactory.java region container datum structure file firefly src firefly java edu caltech ipac util dd containsoptions.java global.java regionfileelement.java regparseexception.java region.java regionannulus.java regionbox.java regionboxannulus.java regioncsys.java regiondimension.java regionellipse.java regionellipseannulus.java regionfont.java regionlines.java regionoptions.java regionpoint.java regionvalue.java note port coordexception way,"Port region serializer and data structures from GWT The region serializer in: firefly/src/firefly/java/edu/caltech/ipac/util * RegionFactory.java Region container data structures files in : firefly/src/firefly/java/edu/caltech/ipac/util/dd * ContainsOptions.java * Global.java * RegionFileElement.java * RegParseException.java * Region.java * RegionAnnulus.java * RegionBox.java * RegionBoxAnnulus.java * RegionCsys.java * RegionDimension.java * RegionEllipse.java * RegionEllipseAnnulus.java * RegionFont.java * RegionLines.java * RegionOptions.java * RegionPoint.java * RegionText.java * RegionValue.java Note - do not port CoordException, there are other ways to do this."
Add support for SGE Jean Coupon has requested support for SGE in ctrl_pool.,1,DM-5787,datamanagement,add support sge jean coupon request support sge ctrl_pool,Add support for SGE Jean Coupon has requested support for SGE in ctrl_pool.
"Why is doSelectUnresolved an argument? The {{run}} method in the {{PhotoCalTask}} has an argument that selects whether to use the extendedness parameter to select objects for photometric calibration.  This is a good idea, but it should be configurable, I think. ",1,DM-5791,datamanagement,doselectunresolve argument run method photocaltask argument select use extendedness parameter select object photometric calibration good idea configurable think,"Why is doSelectUnresolved an argument? The {{run}} method in the {{PhotoCalTask}} has an argument that selects whether to use the extendedness parameter to select objects for photometric calibration. This is a good idea, but it should be configurable, I think."
Support artifacts  For now this is the artifacts for WISE images.   We should look at the possibilities to generalize this. ,8,DM-5792,datamanagement,support artifact artifact wise image look possibility generalize,Support artifacts For now this is the artifacts for WISE images. We should look at the possibilities to generalize this.
"Add Python properties for getters and setters in afw::geom and shapelet I'm adding properties via Swig %extend in much of afw::geom right now, because:   - I think we've all agreed this is something we want, even if we haven't agreed how much effort we want to put into it.   - I'm getting annoyed writing lots of parentheses for these getters and setters on DM-5197.   - I can get this done in a couple of hours on a weekend, so I don't need a T/CAM to give me permission to spend my own time on it :)  ",1,DM-5795,datamanagement,add python property getter setter afw::geom shapelet add property swig extend afw::geom right think agree want agree effort want get annoyed writing lot parenthesis getter setter dm-5197 couple hour weekend need cam permission spend time,"Add Python properties for getters and setters in afw::geom and shapelet I'm adding properties via Swig %extend in much of afw::geom right now, because: - I think we've all agreed this is something we want, even if we haven't agreed how much effort we want to put into it. - I'm getting annoyed writing lots of parentheses for these getters and setters on DM-5197. - I can get this done in a couple of hours on a weekend, so I don't need a T/CAM to give me permission to spend my own time on it :)"
"Using 'CONSTANT' for background subtraction fails Running processCcd (on a DECam file) with the following in the config file:    {code}  config.charImage.repair.cosmicray.background.algorithm='AKIMA_SPLINE'  config.charImage.background.algorithm='CONSTANT'  config.charImage.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT'  config.charImage.detectAndMeasure.detection.background.algorithm='CONSTANT'  config.calibrate.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT'  config.calibrate.detectAndMeasure.detection.background.algorithm='CONSTANT'  {code}    fails, and throws the following:    {code}  Traceback (most recent call last):    File ""/home/mfisherlevine/lsst/pipe_tasks/bin/processCcd.py"", line 25, in <module>      ProcessCcdTask.parseAndRun()    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 199, in run      resultList = mapFunc(self, targetList)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 324, in __call__      result = task.run(dataRef, **kwargs)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/processCcd.py"", line 170, in run      doUnpersist = False,    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 298, in run      background = background,    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 356, in characterize      image -= estBg.getImageF()    File ""/home/mfisherlevine/lsst/afw/python/lsst/afw/math/mathLib.py"", line 5788, in getImageF      return _mathLib.Background_getImageF(self, *args)  lsst.pex.exceptions.wrappers.InvalidParameterError:     File ""src/math/Interpolate.cc"", line 61, in std::pair<std::vector<double>, std::vector<double> > lsst::afw::math::{anonymous}::recenter(const std::vector<double>&, const std::vector<double>&)      You must provide at least 1 point {0}    File ""src/math/BackgroundMI.cc"", line 196, in void lsst::afw::math::BackgroundMI::_setGridColumns(lsst::afw::math::Interpolate::Style, lsst::afw::math::UndersampleStyle, int, const std::vector<int>&) const      setting _gridcolumns {1}  lsst::pex::exceptions::InvalidParameterError: 'You must provide at least 1 point {0}; setting _gridcolumns {1}  {code}",2,DM-5797,datamanagement,constant background subtraction fail running processccd decam file following config file code config.charimage.repair.cosmicray.background.algorithm='akima_spline config.charimage.background.algorithm='constant config.charimage.detectandmeasure.detection.templocalbackground.algorithm='constant config.charimage.detectandmeasure.detection.background.algorithm='constant config.calibrate.detectandmeasure.detection.templocalbackground.algorithm='constant config.calibrate.detectandmeasure.detection.background.algorithm='constant code fail throw following code traceback recent file /home mfisherlevine lsst pipe_tasks bin processccd.py line 25 processccdtask.parseandrun file lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base cmdlinetask.py line 450 parseandrun resultlist taskrunner.run(parsedcmd file lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base cmdlinetask.py line 199 run resultlist mapfunc(self targetlist file lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base cmdlinetask.py line 324 result task.run(dataref kwargs file lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home mfisherlevine lsst pipe_tasks python lsst pipe task processccd.py line 170 run dounpersist false file lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home mfisherlevine lsst pipe_tasks python lsst pipe task characterizeimage.py line 298 run background background file lsstsw stack linux64 pipe_base/2016_01.0 g7751869 python lsst pipe base timer.py line 118 wrapper res func(self args keyargs file /home mfisherlevine lsst pipe_tasks python lsst pipe task characterizeimage.py line 356 characterize image estbg.getimagef file /home mfisherlevine lsst afw python lsst afw math mathlib.py line 5788 getimagef return mathlib background_getimagef(self args lsst.pex.exceptions.wrapper invalidparametererror file src math interpolate.cc line 61 std::pair std::vector lsst::afw::math::{anonymous}::recenter(const std::vector const std::vector provide point file src math backgroundmi.cc line 196 void lsst::afw::math::backgroundmi::_setgridcolumns(lsst::afw::math::interpolate::style lsst::afw::math::undersamplestyle int const std::vector const set gridcolumn lsst::pex::exceptions::invalidparametererror provide point set gridcolumn code,"Using 'CONSTANT' for background subtraction fails Running processCcd (on a DECam file) with the following in the config file: {code} config.charImage.repair.cosmicray.background.algorithm='AKIMA_SPLINE' config.charImage.background.algorithm='CONSTANT' config.charImage.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT' config.charImage.detectAndMeasure.detection.background.algorithm='CONSTANT' config.calibrate.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT' config.calibrate.detectAndMeasure.detection.background.algorithm='CONSTANT' {code} fails, and throws the following: {code} Traceback (most recent call last): File ""/home/mfisherlevine/lsst/pipe_tasks/bin/processCcd.py"", line 25, in  ProcessCcdTask.parseAndRun() File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun resultList = taskRunner.run(parsedCmd) File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 199, in run resultList = mapFunc(self, targetList) File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 324, in __call__ result = task.run(dataRef, **kwargs) File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/processCcd.py"", line 170, in run doUnpersist = False, File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 298, in run background = background, File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper res = func(self, *args, **keyArgs) File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 356, in characterize image -= estBg.getImageF() File ""/home/mfisherlevine/lsst/afw/python/lsst/afw/math/mathLib.py"", line 5788, in getImageF return _mathLib.Background_getImageF(self, *args) lsst.pex.exceptions.wrappers.InvalidParameterError: File ""src/math/Interpolate.cc"", line 61, in std::pair, std::vector > lsst::afw::math::{anonymous}::recenter(const std::vector&, const std::vector&) You must provide at least 1 point {0} File ""src/math/BackgroundMI.cc"", line 196, in void lsst::afw::math::BackgroundMI::_setGridColumns(lsst::afw::math::Interpolate::Style, lsst::afw::math::UndersampleStyle, int, const std::vector&) const setting _gridcolumns {1} lsst::pex::exceptions::InvalidParameterError: 'You must provide at least 1 point {0}; setting _gridcolumns {1} {code}"
"Pass butler to ref loader The design of the indexed reference catalogs requires a butler to be sent to the loader.  This requires passing the butler down through the chain of subtasks from the parent command line task.  In this case, I believe only calibrateTask constructs sub-tasks that requires a reference catalog.    This will also require moving the loader and indexer to meas_astrom, otherwise it will introduce a circular dependency.",4,DM-5798,datamanagement,pass butler ref loader design indexed reference catalog require butler send loader require pass butler chain subtask parent command line task case believe calibratetask constructs sub task require reference catalog require move loader indexer meas_astrom introduce circular dependency,"Pass butler to ref loader The design of the indexed reference catalogs requires a butler to be sent to the loader. This requires passing the butler down through the chain of subtasks from the parent command line task. In this case, I believe only calibrateTask constructs sub-tasks that requires a reference catalog. This will also require moving the loader and indexer to meas_astrom, otherwise it will introduce a circular dependency."
"Asinh stretch algorithm corerction in DM-2634, the Asinh stretch algorithm  was implemented, but the behavior was not quite right. We need to figure out the issue and make it right. One possibility is that the understanding the relationship  of zero point  and black point, maximum point and white point. ",8,DM-5799,datamanagement,asinh stretch algorithm corerction dm-2634 asinh stretch algorithm implement behavior right need figure issue right possibility understand relationship zero point black point maximum point white point,"Asinh stretch algorithm corerction in DM-2634, the Asinh stretch algorithm was implemented, but the behavior was not quite right. We need to figure out the issue and make it right. One possibility is that the understanding the relationship of zero point and black point, maximum point and white point."
TabPanel:  Tab titles need to shrink to accommodate a large number of tabs. Should convert GWT's logic over to TabPanel.  - shrink title as needed.  - show full title on mouse over,4,DM-5800,datamanagement,tabpanel tab title need shrink accommodate large number tab convert gwt logic tabpanel shrink title need title mouse,TabPanel: Tab titles need to shrink to accommodate a large number of tabs. Should convert GWT's logic over to TabPanel. - shrink title as needed. - show full title on mouse over
"Cmake in mariadbclient finds wrong libz When building mariadbclient, cmake identifies libz from a separate python installation than the one setup to run the stack. I have an anaconda installation on the disk, and a miniconda installation set up specifically for the lsst stack. During the building process CMake for some reason finds the alternate libz associated with that python installation.",1,DM-5802,datamanagement,cmake mariadbclient find wrong libz build mariadbclient cmake identify libz separate python installation setup run stack anaconda installation disk miniconda installation set specifically lsst stack building process cmake reason find alternate libz associate python installation,"Cmake in mariadbclient finds wrong libz When building mariadbclient, cmake identifies libz from a separate python installation than the one setup to run the stack. I have an anaconda installation on the disk, and a miniconda installation set up specifically for the lsst stack. During the building process CMake for some reason finds the alternate libz associated with that python installation."
fetchUrl is not handling post requests correctly. Parameters are not sent to the server when requests are posted via fetchUrl.,2,DM-5803,datamanagement,fetchurl handle post request correctly parameter send server request post fetchurl,fetchUrl is not handling post requests correctly. Parameters are not sent to the server when requests are posted via fetchUrl.
Use aperture-corrected aperture flux in validate_drp Shift from PsfFlux flux/magnitude to aperture-corrected aperture-based mag/flux measurements for calculating photometric repeatibility.,1,DM-5804,datamanagement,use aperture correct aperture flux validate_drp shift psfflux flux magnitude aperture correct aperture base mag flux measurement calculate photometric repeatibility,Use aperture-corrected aperture flux in validate_drp Shift from PsfFlux flux/magnitude to aperture-corrected aperture-based mag/flux measurements for calculating photometric repeatibility.
"Improve star/galaxy separation for validate_drp Improve the star/galaxy separation for validate_drp.    Many of the LSST SRD KPMs are defined for bright, isolated stars.  There is clear evidence that galaxies are being included in current runs (they have significantly higher photometric scatter at the same mag|SNR).  Improved star/galaxy separation will help generate better numbers    Stretch goal:  Include additional informative plots about how well the `extendedness` value in the catalogs is successfully separating stars and galaxies.",1,DM-5805,datamanagement,improve star galaxy separation validate_drp improve star galaxy separation validate_drp lsst srd kpm define bright isolated star clear evidence galaxy include current run significantly high photometric scatter mag|snr improved star galaxy separation help generate well number stretch goal include additional informative plot extendedness value catalog successfully separate star galaxy,"Improve star/galaxy separation for validate_drp Improve the star/galaxy separation for validate_drp. Many of the LSST SRD KPMs are defined for bright, isolated stars. There is clear evidence that galaxies are being included in current runs (they have significantly higher photometric scatter at the same mag|SNR). Improved star/galaxy separation will help generate better numbers Stretch goal: Include additional informative plots about how well the `extendedness` value in the catalogs is successfully separating stars and galaxies."
Add a paging bar to ImageMetaDataToolbarView Add a paging bar similar to the one for table to the ImageMetaDataToolbarView.  This pages images instead of rows.,4,DM-5806,datamanagement,add page bar imagemetadatatoolbarview add page bar similar table imagemetadatatoolbarview page image instead row,Add a paging bar to ImageMetaDataToolbarView Add a paging bar similar to the one for table to the ImageMetaDataToolbarView. This pages images instead of rows.
Ensure that variance plane in calexps is unchanged HSC⟷LSST Per discussion in HSC telecon 2016-04-19.,1,DM-5808,datamanagement,ensure variance plane calexps unchanged hsc lsst discussion hsc telecon 2016 04 19,Ensure that variance plane in calexps is unchanged HSC LSST Per discussion in HSC telecon 2016-04-19.
"Update imageDifferenceTask to cast template ids and use ObjectSizeStarSelector A couple recent changes to the stack break imageDifferenceTask.     Requires updates to only a few lines.     While I'm updating it to reflect the star selector API, I'm also changing the default star selector from SecondMoment to ObjectSizeStarSelector (which I learned today is what the stack has been using by default for a while). ",1,DM-5810,datamanagement,update imagedifferencetask cast template ids use objectsizestarselector couple recent change stack break imagedifferencetask require update line update reflect star selector api change default star selector secondmoment objectsizestarselector learn today stack default,"Update imageDifferenceTask to cast template ids and use ObjectSizeStarSelector A couple recent changes to the stack break imageDifferenceTask. Requires updates to only a few lines. While I'm updating it to reflect the star selector API, I'm also changing the default star selector from SecondMoment to ObjectSizeStarSelector (which I learned today is what the stack has been using by default for a while)."
"Incorporate Price suggestions to make `validate_drp` faster Increase the loading and processing speed of {{validate_drp}} following suggestions by [~price]    1. Don't read in footprints  Pass {{flags=lsst.afw.table.SOURCE_IO_NO_FOOTPRINTS}} to {{butler.get}}    2. Work on speed of calculation of RMS and other expensive quantities.  Current suggestions:  a. {{calcRmsDistances}}  b. {{multiMatch}}  c. {{matchVisitComputeDistance}}  d. Consider boolean indexing  {code}     objById = {record.get(self.objectKey): record for record in self.reference}  to:     objById = dict(zip(self.reference[self.objectKey], self.reference))  {code}    Note that while this ticket will involve work to reduce the memory footprint of the processing, it will not cover work to re-architect things to enable efficient processing beyond the memory on one node.",2,DM-5819,datamanagement,incorporate price suggestion validate_drp fast increase loading processing speed validate_drp follow suggestion ~price read footprint pass flag lsst.afw.table source_io_no_footprint butler.get work speed calculation rms expensive quantity current suggestion a. calcrmsdistance b. multimatch c. matchvisitcomputedistance d. consider boolean indexing code objbyid record.get(self.objectkey record record self.reference objbyid dict(zip(self.reference[self.objectkey self.reference code note ticket involve work reduce memory footprint processing cover work architect thing enable efficient processing memory node,"Incorporate Price suggestions to make `validate_drp` faster Increase the loading and processing speed of {{validate_drp}} following suggestions by [~price] 1. Don't read in footprints Pass {{flags=lsst.afw.table.SOURCE_IO_NO_FOOTPRINTS}} to {{butler.get}} 2. Work on speed of calculation of RMS and other expensive quantities. Current suggestions: a. {{calcRmsDistances}} b. {{multiMatch}} c. {{matchVisitComputeDistance}} d. Consider boolean indexing {code} objById = {record.get(self.objectKey): record for record in self.reference} to: objById = dict(zip(self.reference[self.objectKey], self.reference)) {code} Note that while this ticket will involve work to reduce the memory footprint of the processing, it will not cover work to re-architect things to enable efficient processing beyond the memory on one node."
"3 color and FITS header clean up There are some issues with three color when not using all three bands (i.e. using on green and blue):  * Mouse readout is not labeled correctly  * FITS head popup does not come up    Other FITS header popup issues:  * If file size is too big then the text is wrapping  * On safari, the resizable indicator in on every cell  ",6,DM-5820,datamanagement,color fit header clean issue color band i.e. green blue mouse readout label correctly fits head popup come fit header popup issue file size big text wrap safari resizable indicator cell,"3 color and FITS header clean up There are some issues with three color when not using all three bands (i.e. using on green and blue): * Mouse readout is not labeled correctly * FITS head popup does not come up Other FITS header popup issues: * If file size is too big then the text is wrapping * On safari, the resizable indicator in on every cell"
"Intermittent fault building ci_hsc through Jenkins Occasionally (see e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-7/10437//console] and [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/9594//console]) the {{ci_hsc}} job in Jenkins fails, reporting:  {code}  RuntimeError: dictionary changed size during iteration  {code}  The fault seems to be intermittent. Please fix it.",3,DM-5821,datamanagement,intermittent fault building ci_hsc jenkins occasionally e.g. here|https://ci.lsst.codes job stack os matrix label centos-7/10437//console here|https://ci.lsst.code job stack os matrix label centos-6/9594//console ci_hsc job jenkins fail report code runtimeerror dictionary change size iteration code fault intermittent fix,"Intermittent fault building ci_hsc through Jenkins Occasionally (see e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-7/10437//console] and [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/9594//console]) the {{ci_hsc}} job in Jenkins fails, reporting: {code} RuntimeError: dictionary changed size during iteration {code} The fault seems to be intermittent. Please fix it."
"Afw fails unit test for convolve depending on compiler optimisation level On OSX 10.11.4 with Apple LLVM version 7.3.0 (clang-703.0.29) afw fails {{test/convolve.py}} with the following error when either {{-O0}} or {{-O1}} is enabled but works fine for {{-O2}} and {{-O3}}.    {code:bash}  tests/convolve.py    .....FF/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py:283: RuntimeWarning: invalid value encountered in isnan    nan0 = np.isnan(filledArr0)  /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:113: RuntimeWarning: invalid value encountered in isinf    nx.logical_and(nx.isinf(x), ~nx.signbit(x), y)  /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:176: RuntimeWarning: invalid value encountered in isinf    nx.logical_and(nx.isinf(x), nx.signbit(x), y)  F.F...  ======================================================================  FAIL: testSpatiallyVaryingAnalyticConvolve (__main__.ConvolveTestCase)  Test in-place convolution with a spatially varying AnalyticKernel  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 437, in testSpatiallyVaryingAnalyticConvolve      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel using brute force) wrote to edge pixels: image planes differ: maxDiff=1.09176e+38 at position (73, 18); value=-1.09176e+38 vs. 2825.0; NaNs differ    ======================================================================  FAIL: testSpatiallyVaryingDeltaFunctionLinearCombination (__main__.ConvolveTestCase)  Test convolution with a spatially varying LinearCombinationKernel of delta function basis kernels.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 556, in testSpatiallyVaryingDeltaFunctionLinearCombination      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of delta function kernels using brute force) wrote to edge pixels: image planes differ: maxDiff=9.06659e+36 at position (75, 29); value=9.06659e+36 vs. 2865.0    ======================================================================  FAIL: testSpatiallyVaryingGaussianLinerCombination (__main__.ConvolveTestCase)  Test convolution with a spatially varying LinearCombinationKernel of two Gaussian basis kernels.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 523, in testSpatiallyVaryingGaussianLinerCombination      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel with 3 basis kernels convolved using brute force) wrote to edge pixels: image planes differ: maxDiff=1.22472e+38 at position (74, 3); value=-1.22472e+38 vs. 2878.0; NaNs differ    ======================================================================  FAIL: testTicket873 (__main__.ConvolveTestCase)  Demonstrate ticket 873: convolution of a MaskedImage with a spatially varying  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 623, in testTicket873      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of basis kernels with low covariance, using brute force) wrote to edge pixels: image planes differ: maxDiff=3.19374e+38 at position (1, 46); value=3.19374e+38 vs. 2774.0    ----------------------------------------------------------------------  Ran 13 tests in 43.252s    FAILED (failures=4)  The following tests failed:  /Users/pschella/Development/lsst/code/afw/tests/.tests/convolve.py.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  {code}",2,DM-5822,datamanagement,"afw fail unit test convolve depend compiler optimisation level osx 10.11.4 apple llvm version 7.3.0 clang-703.0.29 afw fail test convolve.py follow error -o0 -o1 enable work fine -o2 -o3 code bash test convolve.py ff users pschella development lsst code afw python lsst afw image testutils.py:283 runtimewarning invalid value encounter isnan nan0 np.isnan(filledarr0 /users pschella development lsst lsstsw miniconda lib python2.7 site package numpy lib ufunclike.py:113 runtimewarning invalid value encounter isinf nx.logical_and(nx.isinf(x ~nx.signbit(x /users pschella development lsst lsstsw miniconda lib python2.7 site package numpy lib ufunclike.py:176 runtimewarning invalid value encounter isinf nx.logical_and(nx.isinf(x nx.signbit(x f.f fail testspatiallyvaryinganalyticconvolve main__.convolvetestcase test place convolution spatially vary analytickernel traceback recent file test convolve.py line 437 testspatiallyvaryinganalyticconvolve rtol rtol file test convolve.py line 290 runstdt self.runbasicconvolveedgetest(kernel kerneldescr file test convolve.py line 317 runbasicconvolveedget dovariance true rtol=0 atol=0 msg msg file /user pschella development lsst code afw python lsst afw image testutils.py line 201 assertmaskedimagesnearlyequal testcase.fail(""%s msg .join(errstrlist assertionerror basicconvolve(maskedimage kernel spatially vary gaussian analytic kernel brute force write edge pixel image plane differ maxdiff=1.09176e+38 position 73 18 value=-1.09176e+38 vs. 2825.0 nans differ fail testspatiallyvaryingdeltafunctionlinearcombination main__.convolvetestcase test convolution spatially vary linearcombinationkernel delta function basis kernel traceback recent file test convolve.py line 556 testspatiallyvaryingdeltafunctionlinearcombination rtol rtol file test convolve.py line 290 runstdt self.runbasicconvolveedgetest(kernel kerneldescr file test convolve.py line 317 runbasicconvolveedget dovariance true rtol=0 atol=0 msg msg file /user pschella development lsst code afw python lsst afw image testutils.py line 201 assertmaskedimagesnearlyequal testcase.fail(""%s msg .join(errstrlist assertionerror basicconvolve(maskedimage kernel spatially vary linearcombinationkernel delta function kernel brute force write edge pixel image plane differ maxdiff=9.06659e+36 position 75 29 value=9.06659e+36 vs. 2865.0 fail testspatiallyvaryinggaussianlinercombination main__.convolvetestcase test convolution spatially vary linearcombinationkernel gaussian basis kernel traceback recent file test convolve.py line 523 testspatiallyvaryinggaussianlinercombination rtol rtol file test convolve.py line 290 runstdt self.runbasicconvolveedgetest(kernel kerneldescr file test convolve.py line 317 runbasicconvolveedget dovariance true rtol=0 atol=0 msg msg file /user pschella development lsst code afw python lsst afw image testutils.py line 201 assertmaskedimagesnearlyequal testcase.fail(""%s msg .join(errstrlist assertionerror basicconvolve(maskedimage kernel spatially vary gaussian analytic kernel basis kernel convolve brute force write edge pixel image plane differ maxdiff=1.22472e+38 position 74 value=-1.22472e+38 vs. 2878.0 nans differ fail testticket873 main__.convolvetestcase demonstrate ticket 873 convolution maskedimage spatially vary traceback recent file test convolve.py line 623 testticket873 rtol rtol file test convolve.py line 290 runstdt self.runbasicconvolveedgetest(kernel kerneldescr file test convolve.py line 317 runbasicconvolveedget dovariance true rtol=0 atol=0 msg msg file /user pschella development lsst code afw python lsst afw image testutils.py line 201 assertmaskedimagesnearlyequal testcase.fail(""%s msg .join(errstrlist assertionerror basicconvolve(maskedimage kernel spatially vary linearcombinationkernel basis kernel low covariance brute force write edge pixel image plane differ maxdiff=3.19374e+38 position 46 value=3.19374e+38 vs. 2774.0 ran 13 test 43.252s fail failures=4 follow test fail /users pschella development lsst code afw tests/.test convolve.py.faile test fail scon checkteststatus error scon building terminate error code","Afw fails unit test for convolve depending on compiler optimisation level On OSX 10.11.4 with Apple LLVM version 7.3.0 (clang-703.0.29) afw fails {{test/convolve.py}} with the following error when either {{-O0}} or {{-O1}} is enabled but works fine for {{-O2}} and {{-O3}}. {code:bash} tests/convolve.py .....FF/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py:283: RuntimeWarning: invalid value encountered in isnan nan0 = np.isnan(filledArr0) /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:113: RuntimeWarning: invalid value encountered in isinf nx.logical_and(nx.isinf(x), ~nx.signbit(x), y) /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:176: RuntimeWarning: invalid value encountered in isinf nx.logical_and(nx.isinf(x), nx.signbit(x), y) F.F... ====================================================================== FAIL: testSpatiallyVaryingAnalyticConvolve (__main__.ConvolveTestCase) Test in-place convolution with a spatially varying AnalyticKernel ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/convolve.py"", line 437, in testSpatiallyVaryingAnalyticConvolve rtol = rtol) File ""tests/convolve.py"", line 290, in runStdTest self.runBasicConvolveEdgeTest(kernel, kernelDescr) File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest doVariance = True, rtol=0, atol=0, msg=msg) File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList))) AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel using brute force) wrote to edge pixels: image planes differ: maxDiff=1.09176e+38 at position (73, 18); value=-1.09176e+38 vs. 2825.0; NaNs differ ====================================================================== FAIL: testSpatiallyVaryingDeltaFunctionLinearCombination (__main__.ConvolveTestCase) Test convolution with a spatially varying LinearCombinationKernel of delta function basis kernels. ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/convolve.py"", line 556, in testSpatiallyVaryingDeltaFunctionLinearCombination rtol = rtol) File ""tests/convolve.py"", line 290, in runStdTest self.runBasicConvolveEdgeTest(kernel, kernelDescr) File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest doVariance = True, rtol=0, atol=0, msg=msg) File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList))) AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of delta function kernels using brute force) wrote to edge pixels: image planes differ: maxDiff=9.06659e+36 at position (75, 29); value=9.06659e+36 vs. 2865.0 ====================================================================== FAIL: testSpatiallyVaryingGaussianLinerCombination (__main__.ConvolveTestCase) Test convolution with a spatially varying LinearCombinationKernel of two Gaussian basis kernels. ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/convolve.py"", line 523, in testSpatiallyVaryingGaussianLinerCombination rtol = rtol) File ""tests/convolve.py"", line 290, in runStdTest self.runBasicConvolveEdgeTest(kernel, kernelDescr) File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest doVariance = True, rtol=0, atol=0, msg=msg) File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList))) AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel with 3 basis kernels convolved using brute force) wrote to edge pixels: image planes differ: maxDiff=1.22472e+38 at position (74, 3); value=-1.22472e+38 vs. 2878.0; NaNs differ ====================================================================== FAIL: testTicket873 (__main__.ConvolveTestCase) Demonstrate ticket 873: convolution of a MaskedImage with a spatially varying ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/convolve.py"", line 623, in testTicket873 rtol = rtol) File ""tests/convolve.py"", line 290, in runStdTest self.runBasicConvolveEdgeTest(kernel, kernelDescr) File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest doVariance = True, rtol=0, atol=0, msg=msg) File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList))) AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of basis kernels with low covariance, using brute force) wrote to edge pixels: image planes differ: maxDiff=3.19374e+38 at position (1, 46); value=3.19374e+38 vs. 2774.0 ---------------------------------------------------------------------- Ran 13 tests in 43.252s FAILED (failures=4) The following tests failed: /Users/pschella/Development/lsst/code/afw/tests/.tests/convolve.py.failed 1 tests failed scons: *** [checkTestStatus] Error 1 scons: building terminated because of errors. {code}"
"ECL_B1950 coordinate was not defined correctly The CoordSys.js defined ECL_B1950 incorrectly.  When I was testing WebGrid, the grid lines for  Ecliptic B1950 were not right.  Looked further, it was caused by wrong equinox value in its definition.",1,DM-5823,datamanagement,ecl_b1950 coordinate define correctly coordsys.js define ecl_b1950 incorrectly test webgrid grid line ecliptic b1950 right look cause wrong equinox value definition,"ECL_B1950 coordinate was not defined correctly The CoordSys.js defined ECL_B1950 incorrectly. When I was testing WebGrid, the grid lines for Ecliptic B1950 were not right. Looked further, it was caused by wrong equinox value in its definition."
Add CI tests for obs_lsstSim I propose:    1. Create {{testdata_lsstSim}}.  This will be based on 12 images from the current Twinkles Run 1 (or pre-Run 1):  2 epochs each of 6 filters.  (/)    2. Add an optional test method to {{obs_lsstSim}} that runs if {{testdata_lsstSim}} has been declared.  This is the way these tests are set up for the other {{obs_*}} packages.    3. Add {{testdata_lsstSim}} dependency to {{lsst_ci}} (which already depends on {{obs_lsstSim}}).    This will then be run every time a full standard default Jenkins build is processed.,2,DM-5825,datamanagement,add ci test obs_lsstsim propose create testdata_lsstsim base 12 image current twinkles run pre run ): epoch filter add optional test method obs_lsstsim run testdata_lsstsim declare way test set obs package add testdata_lsstsim dependency lsst_ci depend obs_lsstsim run time standard default jenkins build process,Add CI tests for obs_lsstSim I propose: 1. Create {{testdata_lsstSim}}. This will be based on 12 images from the current Twinkles Run 1 (or pre-Run 1): 2 epochs each of 6 filters. (/) 2. Add an optional test method to {{obs_lsstSim}} that runs if {{testdata_lsstSim}} has been declared. This is the way these tests are set up for the other {{obs_*}} packages. 3. Add {{testdata_lsstSim}} dependency to {{lsst_ci}} (which already depends on {{obs_lsstSim}}). This will then be run every time a full standard default Jenkins build is processed.
Create outline of Level 3 ConOps Create an outline of the sections of the Level 3 ConOps document,2,DM-5829,datamanagement,create outline level conops create outline section level conops document,Create outline of Level 3 ConOps Create an outline of the sections of the Level 3 ConOps document
"Level 3 requirements flowdown Document the flowdown of Level 3-related requirements from SRD, LSR, OSS, and DMSR.",3,DM-5830,datamanagement,level requirement flowdown document flowdown level relate requirement srd lsr oss dmsr,"Level 3 requirements flowdown Document the flowdown of Level 3-related requirements from SRD, LSR, OSS, and DMSR."
"SUIT requirement flowdown go through the original requirement of SUIT,  put them in the categories:  done, Tier1, Tier2    ",6,DM-5831,datamanagement,suit requirement flowdown original requirement suit category tier1 tier2,"SUIT requirement flowdown go through the original requirement of SUIT, put them in the categories: done, Tier1, Tier2"
"LSE-140 post-CCB implementation Following CCB approval of LSE-140, perform minor document work required for full implementation (application of standard cover page, change log, etc.).",2,DM-5832,datamanagement,lse-140 post ccb implementation follow ccb approval lse-140 perform minor document work require implementation application standard cover page change log etc,"LSE-140 post-CCB implementation Following CCB approval of LSE-140, perform minor document work required for full implementation (application of standard cover page, change log, etc.)."
SUIT design diagramming Prepare initial set of SysML diagrams of the SUIT's relationship to other system components.,4,DM-5833,datamanagement,suit design diagram prepare initial set sysml diagram suit relationship system component,SUIT design diagramming Prepare initial set of SysML diagrams of the SUIT's relationship to other system components.
"Prepare a draft of the SUIT deployment timeline Prepare a draft schedule, with some detail for 2016-2017, for deployments of the SUIT into (test) production, including the datasets that will be served.",2,DM-5835,datamanagement,prepare draft suit deployment timeline prepare draft schedule detail 2016 2017 deployment suit test production include dataset serve,"Prepare a draft of the SUIT deployment timeline Prepare a draft schedule, with some detail for 2016-2017, for deployments of the SUIT into (test) production, including the datasets that will be served."
access to NCSA Nebular to setup servers for SUIT deployment Get three hosts in NCSA nebular system to deploy the current Firefly application. The goal is workout the possible issues and identify the software needed to be installed for the hosts. Clarify which team is responsible to install what third-party software packages.,4,DM-5836,datamanagement,access ncsa nebular setup server suit deployment host ncsa nebular system deploy current firefly application goal workout possible issue identify software need instal host clarify team responsible install party software package,access to NCSA Nebular to setup servers for SUIT deployment Get three hosts in NCSA nebular system to deploy the current Firefly application. The goal is workout the possible issues and identify the software needed to be installed for the hosts. Clarify which team is responsible to install what third-party software packages.
"Document pipe_drivers Please provide a minimal level of documentation for {{pipe_drivers}}, to include:    * A {{doc}} directory with the usual content so that docstrings get generated by Doxygen;  * A package overview;  * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{""""""!}} where necessary).",2,DM-5837,datamanagement,document pipe_driver provide minimal level documentation pipe_driver include doc directory usual content docstring generate doxygen package overview docstring appropriate parse doxygen ie start necessary,"Document pipe_drivers Please provide a minimal level of documentation for {{pipe_drivers}}, to include: * A {{doc}} directory with the usual content so that docstrings get generated by Doxygen; * A package overview; * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{""""""!}} where necessary)."
horizon console interface broken It appears that at some point in the last few months the horizon console interface has stopped working.  I am still able to access the console log output via the API/CLI.,1,DM-5839,datamanagement,horizon console interface break appear point month horizon console interface stop work able access console log output api cli,horizon console interface broken It appears that at some point in the last few months the horizon console interface has stopped working. I am still able to access the console log output via the API/CLI.
instance limit low vs available cores The LSST project is currently at 81/100 instances but there are over 200 cores unused.  Is it possible to increase the instance limit or are we being encouraged to use large instance flavors?,1,DM-5840,datamanagement,instance limit low vs available core lsst project currently 81/100 instance 200 core unused possible increase instance limit encourage use large instance flavor,instance limit low vs available cores The LSST project is currently at 81/100 instances but there are over 200 cores unused. Is it possible to increase the instance limit or are we being encouraged to use large instance flavors?
"unable to list nebula lsst project users Currently, [with some difficulty] it is possible to discover the {{user_id}} that created an instance (might be possible for other resources as well) but it is not possible to map this back to a username / person.  This can make it difficult to 'self police' instances.    The administrative API endpoints are not publicly accessible and I doubt any end user has the appropriate permission. ",1,DM-5841,datamanagement,unable list nebula lsst project user currently difficulty possible discover user_id create instance possible resource possible map username person difficult self police instance administrative api endpoint publicly accessible doubt end user appropriate permission,"unable to list nebula lsst project users Currently, [with some difficulty] it is possible to discover the {{user_id}} that created an instance (might be possible for other resources as well) but it is not possible to map this back to a username / person. This can make it difficult to 'self police' instances. The administrative API endpoints are not publicly accessible and I doubt any end user has the appropriate permission."
"ci_hsc fails with ""too many open files"" For example, with thanks to [~wmwood-vasey]:    {code}                ci_hsc: master-g78db638f21 .....................................................................................ERROR (207 sec).  *** error building product ci_hsc.  *** exit code = 2  *** log is in /Users/wmwv/lsstsw/build/ci_hsc/_build.log  *** last few lines:  :::::  [2016-04-25T19:25:59.824660Z]     jobs.run(postfunc = jobs_postfunc)  :::::  [2016-04-25T19:25:59.824699Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/Job.py"", line 113:  :::::  [2016-04-25T19:25:59.824709Z]     postfunc()  :::::  [2016-04-25T19:25:59.824752Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/Script/Main.py"", line 1294:  :::::  [2016-04-25T19:25:59.824767Z]     SCons.SConsign.write()  :::::  [2016-04-25T19:25:59.824808Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/SConsign.py"", line 109:  :::::  [2016-04-25T19:25:59.824816Z]     None  :::::  [2016-04-25T19:25:59.824869Z]   File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/dblite.py"", line 116:  :::::  [2016-04-25T19:25:59.824878Z]     None  :::::  [2016-04-25T19:25:59.935601Z] Exception IOError: (24, 'Too many open files', '.sconsign.tmp') in <bound method dblite.__del__ of <SCons.dblite.dblite object at 0x10dfe9c50>> ignored  {code}    Possibly only happens on OSX?",2,DM-5845,datamanagement,ci_hsc fail open file example thank ~wmwood vasey code ci_hsc master g78db638f21 error 207 sec error building product ci_hsc exit code log /users wmwv lsstsw build ci_hsc/_build.log line 2016 04 25t19:25:59.824660z jobs.run(postfunc jobs_postfunc 2016 04 25t19:25:59.824699z file /users wmwv lsstsw stack darwinx86 scons/2.3.5 lib scon scons job.py line 113 2016 04 25t19:25:59.824709z postfunc 2016 04 25t19:25:59.824752z file /users wmwv lsstsw stack darwinx86 scons/2.3.5 lib scon scons script main.py line 1294 2016 04 25t19:25:59.824767z scons.sconsign.write 2016 04 25t19:25:59.824808z file /users wmwv lsstsw stack darwinx86 scons/2.3.5 lib scon scons sconsign.py line 109 2016 04 25t19:25:59.824816z 2016 04 25t19:25:59.824869z file /users wmwv lsstsw stack darwinx86 scons/2.3.5 lib scon scons dblite.py line 116 2016 04 25t19:25:59.824878z 2016 04 25t19:25:59.935601z exception ioerror 24 open file .sconsign.tmp ignore code possibly happen osx,"ci_hsc fails with ""too many open files"" For example, with thanks to [~wmwood-vasey]: {code} ci_hsc: master-g78db638f21 .....................................................................................ERROR (207 sec). *** error building product ci_hsc. *** exit code = 2 *** log is in /Users/wmwv/lsstsw/build/ci_hsc/_build.log *** last few lines: ::::: [2016-04-25T19:25:59.824660Z] jobs.run(postfunc = jobs_postfunc) ::::: [2016-04-25T19:25:59.824699Z] File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/Job.py"", line 113: ::::: [2016-04-25T19:25:59.824709Z] postfunc() ::::: [2016-04-25T19:25:59.824752Z] File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/Script/Main.py"", line 1294: ::::: [2016-04-25T19:25:59.824767Z] SCons.SConsign.write() ::::: [2016-04-25T19:25:59.824808Z] File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/SConsign.py"", line 109: ::::: [2016-04-25T19:25:59.824816Z] None ::::: [2016-04-25T19:25:59.824869Z] File ""/Users/wmwv/lsstsw/stack/DarwinX86/scons/2.3.5/lib/scons/SCons/dblite.py"", line 116: ::::: [2016-04-25T19:25:59.824878Z] None ::::: [2016-04-25T19:25:59.935601Z] Exception IOError: (24, 'Too many open files', '.sconsign.tmp') in > ignored {code} Possibly only happens on OSX?"
"libxml build issue with mpich on OS X On OS X with Xcode installed {{mpich}} fails to build because it can not locate the libxml include files:    {code}  CC       topology-xml-libxml.lo   topology-xml-libxml.c:17:10: fatal error: 'libxml/parser.h' file not found   #include <libxml/parser.h>            ^   1 error generated.  {code}  with {{pkg-config}} 0.29.1 installed. The problem is that {{configure}} determines that {{libxml-2.0}} is available and is installed into {{/usr}} with a CFLAGS of {{-I/usr/include/libxml2}}. {{configure}} does not itself test whether those parameters are reasonable. With Xcode there are no files installed into {{/usr/include}} and {{clang}} knows to look in specific SDK locations. When {{mpich}} builds it assumes that {{libxml2}} can be found but fails to find it.    Strangely, {{pkg-config}} v0.28 does not seem to be able to find {{libxml-2.0}} so there is no issue.    One solution is to install the Command Line Tools but it might be more portable to attempt to disable {{libxml2}}.  ",2,DM-5847,datamanagement,libxml build issue mpich os os xcode instal mpich fail build locate libxml include file code cc topology-xml-libxml.lo topology xml libxml.c:17:10 fatal error libxml parser.h file find include error generate code pkg config 0.29.1 instal problem configure determine libxml-2.0 available instal /usr cflags -i usr include libxml2 configure test parameter reasonable xcode file instal /usr include clang know look specific sdk location mpich build assume libxml2 find fail find strangely pkg config v0.28 able find libxml-2.0 issue solution install command line tools portable attempt disable libxml2,"libxml build issue with mpich on OS X On OS X with Xcode installed {{mpich}} fails to build because it can not locate the libxml include files: {code} CC topology-xml-libxml.lo topology-xml-libxml.c:17:10: fatal error: 'libxml/parser.h' file not found #include  ^ 1 error generated. {code} with {{pkg-config}} 0.29.1 installed. The problem is that {{configure}} determines that {{libxml-2.0}} is available and is installed into {{/usr}} with a CFLAGS of {{-I/usr/include/libxml2}}. {{configure}} does not itself test whether those parameters are reasonable. With Xcode there are no files installed into {{/usr/include}} and {{clang}} knows to look in specific SDK locations. When {{mpich}} builds it assumes that {{libxml2}} can be found but fails to find it. Strangely, {{pkg-config}} v0.28 does not seem to be able to find {{libxml-2.0}} so there is no issue. One solution is to install the Command Line Tools but it might be more portable to attempt to disable {{libxml2}}."
"Investigate Jupyter internals, interactive widgets In preparation for linking Jupyter notebooks with Firefly and other SUIT components, read Jupyter documentation. Learn how to build a sample widget or interactive dashboard in the Jupyter framework",2,DM-5848,datamanagement,investigate jupyter internal interactive widget preparation link jupyter notebook firefly suit component read jupyter documentation learn build sample widget interactive dashboard jupyter framework,"Investigate Jupyter internals, interactive widgets In preparation for linking Jupyter notebooks with Firefly and other SUIT components, read Jupyter documentation. Learn how to build a sample widget or interactive dashboard in the Jupyter framework"
"Investigate Ginga and Glueviz visualization tools Ginga and Glue (glueviz) are community visualization tools in Python. Become familiar with the capabilities of both, thinking from the point of view of using Firefly for the display but using Python for many other things.",2,DM-5849,datamanagement,investigate ginga glueviz visualization tool ginga glue glueviz community visualization tool python familiar capability think point view firefly display python thing,"Investigate Ginga and Glueviz visualization tools Ginga and Glue (glueviz) are community visualization tools in Python. Become familiar with the capabilities of both, thinking from the point of view of using Firefly for the display but using Python for many other things."
"Java array index out of bound error in VisSeverCommand.java The class FileFluxCmdJson in VisServerCommand.java is calling   {code}              String[] res = VisServerOps.getFileFlux(fahAry, pt);  {code}    However, when the mouse is outside the image, the VisServerOps.getFileFlux(fahAry, pt) returns:  {code}  new String[]{PlotState.NO_CONTEXT}  {code}  It is fine for a single band.  However, for 2 or 3 bands, the for loop below caused the index out of bound error because res is an array of length=1 and the expected res is an array of length=no of bands.  {code}    JSONObject obj= new JSONObject();              obj.put(""JSON"", true);              obj.put(""success"", true);                int cnt=0;              JSONObject data= new JSONObject();              for(Band b : state.getBands()) {                  data.put(b.toString(), res[cnt++]);              }              data.put(""success"", true);  {code}    Thus,  res\[cnt++\] caused array index out of bound error.     To fix this issue, the for loop is changed as below:  {code}                        int cnt=0;              JSONObject data= new JSONObject();              Band[] bands = state.getBands();              for (int i=0; i<res.length; i++){                  data.put(bands[i].toString(), res[i]);              }              data.put(""success"", true);                JSONArray wrapperAry= new JSONArray();              obj.put(""data"", data);              wrapperAry.add(obj);  {code}    When the mouse is outside the image, the res returns a new String\[\]\{PlotState.NO_CONTEXT\}, it is added to the JSONObject only once.  ",1,DM-5854,datamanagement,"java array index bind error vissevercommand.java class filefluxcmdjson visservercommand.java call code string res visserverops.getfileflux(fahary pt code mouse outside image visserverops.getfileflux(fahary pt return code new string[]{plotstate no_context code fine single band band loop cause index bind error res array length=1 expect res array length band code jsonobject obj= new jsonobject obj.put(""json true obj.put(""success true int cnt=0 jsonobject data= new jsonobject for(band state.getband data.put(b.tostring data.put(""success true code res\[cnt++\ cause array index bind error fix issue loop change code int cnt=0 jsonobject data= new jsonobject band band state.getbands int i=0 res.length i++ data.put(bands[i].tostring res[i data.put(""success true jsonarray wrapperary= new jsonarray obj.put(""data datum wrapperary.add(obj code mouse outside image res return new string\[\]\{plotstate no_context\ add jsonobject","Java array index out of bound error in VisSeverCommand.java The class FileFluxCmdJson in VisServerCommand.java is calling {code} String[] res = VisServerOps.getFileFlux(fahAry, pt); {code} However, when the mouse is outside the image, the VisServerOps.getFileFlux(fahAry, pt) returns: {code} new String[]{PlotState.NO_CONTEXT} {code} It is fine for a single band. However, for 2 or 3 bands, the for loop below caused the index out of bound error because res is an array of length=1 and the expected res is an array of length=no of bands. {code} JSONObject obj= new JSONObject(); obj.put(""JSON"", true); obj.put(""success"", true); int cnt=0; JSONObject data= new JSONObject(); for(Band b : state.getBands()) { data.put(b.toString(), res[cnt++]); } data.put(""success"", true); {code} Thus, res\[cnt++\] caused array index out of bound error. To fix this issue, the for loop is changed as below: {code} int cnt=0; JSONObject data= new JSONObject(); Band[] bands = state.getBands(); for (int i=0; i<res.length; i++){ data.put(bands[i].toString(), res[i]); } data.put(""success"", true); JSONArray wrapperAry= new JSONArray(); obj.put(""data"", data); wrapperAry.add(obj); {code} When the mouse is outside the image, the res returns a new String\[\]\{PlotState.NO_CONTEXT\}, it is added to the JSONObject only once."
"Make DipoleFitPlugin mask-safe The DipoleFitPlugin does not correctly handle bad pixels and other masks/flags. Make it so it does so, and make tests to ensure it does so.",6,DM-5857,datamanagement,dipolefitplugin mask safe dipolefitplugin correctly handle bad pixel mask flag test ensure,"Make DipoleFitPlugin mask-safe The DipoleFitPlugin does not correctly handle bad pixels and other masks/flags. Make it so it does so, and make tests to ensure it does so."
Table: Add keyboard navigation - Added arrow up/down to move between rows.  - Added page up/down to move between pages.    - Fixed table loading mask not showing  - Fixed PagingBar rendering more than it should  - Fixed annoying StandardView missing unique key warning,2,DM-5859,datamanagement,table add keyboard navigation add arrow row add page page fix table loading mask show fix pagingbar render fix annoying standardview miss unique key warning,Table: Add keyboard navigation - Added arrow up/down to move between rows. - Added page up/down to move between pages. - Fixed table loading mask not showing - Fixed PagingBar rendering more than it should - Fixed annoying StandardView missing unique key warning
"Minor tweaks to Cython and pybind11 tech notes I'll be making some superficial changes to the text of DMTN-13 and DMTN-14 for grammar, while updating links to the python-cpp-challenge repo (which has just moved from my private GitHub to lsst-dm).",1,DM-5863,datamanagement,minor tweak cython pybind11 tech note make superficial change text dmtn-13 dmtn-14 grammar update link python cpp challenge repo move private github lsst dm,"Minor tweaks to Cython and pybind11 tech notes I'll be making some superficial changes to the text of DMTN-13 and DMTN-14 for grammar, while updating links to the python-cpp-challenge repo (which has just moved from my private GitHub to lsst-dm)."
"Literature research on image subtraction algorithms We need to get a good understanding of where the image subtraction implementation in the stack currently stands. This first requires an up-to-date assessment of the literature, including Becker et al. (2012), and ZOGY (2016). Also, the ""preconvolution"" step.",8,DM-5868,datamanagement,literature research image subtraction algorithm need good understanding image subtraction implementation stack currently stand require date assessment literature include becker et al 2012 zogy 2016 preconvolution step,"Literature research on image subtraction algorithms We need to get a good understanding of where the image subtraction implementation in the stack currently stands. This first requires an up-to-date assessment of the literature, including Becker et al. (2012), and ZOGY (2016). Also, the ""preconvolution"" step."
"Assessment of current state-of-the-stack diffim implementation The existing diffim implementation in the stack defaults to the (2000) version of the Alard/Lupton algorithm. Other recent improvements such as ""pre-convolution"", delta-function basis, model selection via BIC, others, seem to be implemented but are not turned on. We need a good understanding of the existing implementation so we can assess how straightforward it is to implement the ZOGY algorithm in real-space in the stack.",6,DM-5869,datamanagement,assessment current state stack diffim implementation exist diffim implementation stack default 2000 version alard lupton algorithm recent improvement pre convolution delta function basis model selection bic implement turn need good understanding exist implementation assess straightforward implement zogy algorithm real space stack,"Assessment of current state-of-the-stack diffim implementation The existing diffim implementation in the stack defaults to the (2000) version of the Alard/Lupton algorithm. Other recent improvements such as ""pre-convolution"", delta-function basis, model selection via BIC, others, seem to be implemented but are not turned on. We need a good understanding of the existing implementation so we can assess how straightforward it is to implement the ZOGY algorithm in real-space in the stack."
Update testdata_subaru to support calib changes Merging DM-5124 broke obs_subaru because the test data in testdata_subaru wasn't updated.  Fix it.,1,DM-5870,datamanagement,update testdata_subaru support calib change merging dm-5124 break obs_subaru test datum testdata_subaru update fix,Update testdata_subaru to support calib changes Merging DM-5124 broke obs_subaru because the test data in testdata_subaru wasn't updated. Fix it.
"Incorporate ""Bickerton algorithm"" for detecting & masking satellite trails In [HSC-1272|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1272], [~bick] proposed an algorithm for detecting and masking satellite trails. This has undergone some review on HSC, but has never been incorporated into an HSC software or data release (and hence is not part of the ""HSC port"").    However: the algorithm is certainly relevant to LSST. Please convert it to work with the LSST stack.",5,DM-5872,datamanagement,incorporate bickerton algorithm detect mask satellite trail hsc-1272|https://hsc jira.astro.princeton.edu jira browse hsc-1272 ~bick propose algorithm detect mask satellite trail undergo review hsc incorporate hsc software datum release hsc port algorithm certainly relevant lsst convert work lsst stack,"Incorporate ""Bickerton algorithm"" for detecting & masking satellite trails In [HSC-1272|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1272], [~bick] proposed an algorithm for detecting and masking satellite trails. This has undergone some review on HSC, but has never been incorporated into an HSC software or data release (and hence is not part of the ""HSC port""). However: the algorithm is certainly relevant to LSST. Please convert it to work with the LSST stack."
Propose text for alternate galaxy models in DPDD Write a paragraph or two describing alternatives to the constrained bulge+disk model currently in the DPDD.,1,DM-5875,datamanagement,propose text alternate galaxy model dpdd write paragraph describe alternative constrained bulge+disk model currently dpdd,Propose text for alternate galaxy models in DPDD Write a paragraph or two describing alternatives to the constrained bulge+disk model currently in the DPDD.
"Use Afterburners to clean up aperture correction logic This issue has several components; I'm combining them into a single issue because they need to be done atomically:   - Rewrite the base_ClassificationExtendedness SingleFramePlugin/ForcedPlugin as an AfterburnerPlugin (and remove the old versions).   - Move the ""applyApCorr"" subtask out of SingleFrameMeasurementTask and ForcedMeasurementTask, making it instead a subclass of their parent Tasks.   - Add afterburner subtask stages to ProcessCcdTask (within DetectAndMeasureTask) and the multiband tasks wherever measurement is currently being run.  The afterburner tasks should be run after aperture corrections are measured and/or applied.    After these changes, throughout the stack, whenever a MeasurementTask is run, we also run ApplyApCorrTask and AfterburnerTask (in that order), while possibly running MeasureApCorrTask immediately after the MeasurementTask.    This may or may not enable significant cleanups in DetectAndMeasureTask (I haven't looked closely).  If so, they should be done on this issue.    Given all the moving parts, it's important to check that the actual behavior of the pipeline (in the aperture correction and extendedness values) does not change, so it might be useful to start by creating some reference outputs to compare against.",6,DM-5877,datamanagement,use afterburners clean aperture correction logic issue component combine single issue need atomically rewrite base_classificationextendedness singleframeplugin forcedplugin afterburnerplugin remove old version applyapcorr subtask singleframemeasurementtask forcedmeasurementtask make instead subclass parent tasks add afterburner subtask stage processccdtask multiband task measurement currently run afterburner task run aperture correction measure and/or apply change stack measurementtask run run applyapcorrtask afterburnertask order possibly run measureapcorrtask immediately measurementtask enable significant cleanup look closely issue give move part important check actual behavior pipeline aperture correction extendedness value change useful start create reference output compare,"Use Afterburners to clean up aperture correction logic This issue has several components; I'm combining them into a single issue because they need to be done atomically: - Rewrite the base_ClassificationExtendedness SingleFramePlugin/ForcedPlugin as an AfterburnerPlugin (and remove the old versions). - Move the ""applyApCorr"" subtask out of SingleFrameMeasurementTask and ForcedMeasurementTask, making it instead a subclass of their parent Tasks. - Add afterburner subtask stages to ProcessCcdTask (within DetectAndMeasureTask) and the multiband tasks wherever measurement is currently being run. The afterburner tasks should be run after aperture corrections are measured and/or applied. After these changes, throughout the stack, whenever a MeasurementTask is run, we also run ApplyApCorrTask and AfterburnerTask (in that order), while possibly running MeasureApCorrTask immediately after the MeasurementTask. This may or may not enable significant cleanups in DetectAndMeasureTask (I haven't looked closely). If so, they should be done on this issue. Given all the moving parts, it's important to check that the actual behavior of the pipeline (in the aperture correction and extendedness values) does not change, so it might be useful to start by creating some reference outputs to compare against."
"Add chi plots to validate_drp output to compare nominal error Make histograms of Deltas / nominal error.    where ""nominal"" error is that reported by the pipeline.    Are they distributed with a sigma=1?",2,DM-5878,datamanagement,add chi plot validate_drp output compare nominal error histogram deltas nominal error nominal error report pipeline distribute sigma=1,"Add chi plots to validate_drp output to compare nominal error Make histograms of Deltas / nominal error. where ""nominal"" error is that reported by the pipeline. Are they distributed with a sigma=1?"
Create MySQL account for monitoring A MySQL account needs to be created in configuration procedure and on existing data on IN2P3 cluster in order to enable ELK access. MySQL password/secret has to be shared accross all containers.,4,DM-5884,datamanagement,create mysql account monitor mysql account need create configuration procedure exist datum in2p3 cluster order enable elk access mysql password secret share accross container,Create MySQL account for monitoring A MySQL account needs to be created in configuration procedure and on existing data on IN2P3 cluster in order to enable ELK access. MySQL password/secret has to be shared accross all containers.
"Create a JSON file for monitoring stack Create a JSON or YAML file with:    - Qserv version  - libraries/deps version  - other idea welcome    This will interesting ""GROUP BY"" in monitoring tool (performance for each Qserv or xrootd version for example)",3,DM-5885,datamanagement,create json file monitor stack create json yaml file qserv version library dep version idea welcome interesting group monitor tool performance qserv xrootd version example,"Create a JSON file for monitoring stack Create a JSON or YAML file with: - Qserv version - libraries/deps version - other idea welcome This will interesting ""GROUP BY"" in monitoring tool (performance for each Qserv or xrootd version for example)"
"Suppress gcc warnings about ""unused local typedefs"" We should add {{\-Wno\-unused\-local\-typedefs}} to our gcc options.  This cleans up the build significantly, because there's a flood of warnings of this type coming from boost.  If we suppress those, it might become possible to notice warnings that we care about.",1,DM-5889,datamanagement,suppress gcc warning unused local typedef add \-wno\-unused\-local\-typedef gcc option clean build significantly flood warning type come boost suppress possible notice warning care,"Suppress gcc warnings about ""unused local typedefs"" We should add {{\-Wno\-unused\-local\-typedefs}} to our gcc options. This cleans up the build significantly, because there's a flood of warnings of this type coming from boost. If we suppress those, it might become possible to notice warnings that we care about."
"LSST the Docs Fastly should redirect /en/latest/ to / Previously we deployed documentation on Read the Docs. By default, Read the Docs would show the master version of documentation on ""/en/latest/"". Many links with that endpoint may already exist. We should configure Fastly to redirect such paths to ""/"".",1,DM-5893,datamanagement,lsst docs fastly redirect latest/ previously deploy documentation read docs default read docs master version documentation /en latest/ link endpoint exist configure fastly redirect path,"LSST the Docs Fastly should redirect /en/latest/ to / Previously we deployed documentation on Read the Docs. By default, Read the Docs would show the master version of documentation on ""/en/latest/"". Many links with that endpoint may already exist. We should configure Fastly to redirect such paths to ""/""."
LSST the Docs Fastly Courtesy Redirects for directory paths Currently if a user browses {{example.lsst.io/v/main/some-directory}} instead of {{example.lsst.io/v/main/some-directory}} they will receive an error.    We should develop a scheme where Fastly can detect that such a path is a directory and redirect to the directory's index.html page.,4,DM-5894,datamanagement,lsst docs fastly courtesy redirects directory path currently user browse example.lsst.io/v/main/some-directory instead example.lsst.io/v/main/some-directory receive error develop scheme fastly detect path directory redirect directory index.html page,LSST the Docs Fastly Courtesy Redirects for directory paths Currently if a user browses {{example.lsst.io/v/main/some-directory}} instead of {{example.lsst.io/v/main/some-directory}} they will receive an error. We should develop a scheme where Fastly can detect that such a path is a directory and redirect to the directory's index.html page.
"Robustify coadd In running the processing from the Twinkles data challenge in DESC we found that it was very easy to use the wrong skymap when making a coadd.  Since the coadd code doesn't even make a cursory check that the coordinate system it is using is the same as that of the coaddTempExps, it is very possible to mess this up.    Adding a check that the coadd WCS is that of each input tempCoaddExp is would solve this.",2,DM-5897,datamanagement,robustify coadd run processing twinkles datum challenge desc find easy use wrong skymap make coadd coadd code cursory check coordinate system coaddtempexps possible mess add check coadd wcs input tempcoaddexp solve,"Robustify coadd In running the processing from the Twinkles data challenge in DESC we found that it was very easy to use the wrong skymap when making a coadd. Since the coadd code doesn't even make a cursory check that the coordinate system it is using is the same as that of the coaddTempExps, it is very possible to mess this up. Adding a check that the coadd WCS is that of each input tempCoaddExp is would solve this."
Python EUPS package can use $PYTHON The {{python}} eups package has a script that checks that the python being used is version 2.7. This script can optionally check {{$PYTHON}} rather than the python in the path but I am confused as to what that test is going to do for us. The problem is that {{sconsUtils}} uses {{python}} and most of the shebangs use {{/bin/env python}} (although shebang rewriting on all platforms could help with that). I think the check script should have the {{$PYTHON}} support removed due to excessive confusion.    It would also help if the check script worked with python 3 so that the wrong python could be caught.,1,DM-5898,datamanagement,python eups package use python python eup package script check python version 2.7 script optionally check python python path confuse test go problem sconsutils use python shebang use /bin env python shebang rewrite platform help think check script python support remove excessive confusion help check script work python wrong python catch,Python EUPS package can use $PYTHON The {{python}} eups package has a script that checks that the python being used is version 2.7. This script can optionally check {{$PYTHON}} rather than the python in the path but I am confused as to what that test is going to do for us. The problem is that {{sconsUtils}} uses {{python}} and most of the shebangs use {{/bin/env python}} (although shebang rewriting on all platforms could help with that). I think the check script should have the {{$PYTHON}} support removed due to excessive confusion. It would also help if the check script worked with python 3 so that the wrong python could be caught.
Create psutil EUPS package Add the python {{psutil}} package to the stack as {{python_psutil}}.,1,DM-5900,datamanagement,create psutil eups package add python psutil package stack python_psutil,Create psutil EUPS package Add the python {{psutil}} package to the stack as {{python_psutil}}.
LTD Keeper: More Robust Edition purges LTD Keeper needs to purge Fastly when an Edition is rebuilt. Currently the surrogate-key for the build is also used to cover editions. This means that the key needed to purge an edition is the same as that for an build. Hence purging an edition means that the system needs to purge the surrogate key of the previous build.    We're seeing situations where the surrogate key that Keeper is purging is not the one that needs to be purged. A more robust configuration would be for each edition to have a stable surrogate-key that can be unambiguously purged.    This ticket covers the following work:    # Diagnose the issue.  # Enable Alembic migrations for Flask (Flask-Migrate)  # Add a surrogate-key column to the Edition model  # Change the S3 copy rebuild code to change the surrogate-key header  # Change the rebuild code to purge based on the edition's surrogate-key.,3,DM-5901,datamanagement,ltd keeper robust edition purge ltd keeper need purge fastly edition rebuild currently surrogate key build cover edition mean key need purge edition build purge edition mean system need purge surrogate key previous build see situation surrogate key keeper purge need purge robust configuration edition stable surrogate key unambiguously purge ticket cover following work diagnose issue enable alembic migration flask flask migrate add surrogate key column edition model change s3 copy rebuild code change surrogate key header change rebuild code purge base edition surrogate key,LTD Keeper: More Robust Edition purges LTD Keeper needs to purge Fastly when an Edition is rebuilt. Currently the surrogate-key for the build is also used to cover editions. This means that the key needed to purge an edition is the same as that for an build. Hence purging an edition means that the system needs to purge the surrogate key of the previous build. We're seeing situations where the surrogate key that Keeper is purging is not the one that needs to be purged. A more robust configuration would be for each edition to have a stable surrogate-key that can be unambiguously purged. This ticket covers the following work: # Diagnose the issue. # Enable Alembic migrations for Flask (Flask-Migrate) # Add a surrogate-key column to the Edition model # Change the S3 copy rebuild code to change the surrogate-key header # Change the rebuild code to purge based on the edition's surrogate-key.
"Finish technical note on galaxy shear experiments In the review of DM-5447 we decided it made sense for [~jbosch] to take over finishing the technote, in particular providing an introduction and concluion with more context.",4,DM-5903,datamanagement,finish technical note galaxy shear experiment review dm-5447 decide sense ~jbosch finish technote particular provide introduction concluion context,"Finish technical note on galaxy shear experiments In the review of DM-5447 we decided it made sense for [~jbosch] to take over finishing the technote, in particular providing an introduction and concluion with more context."
"Create focus script In DM-3368, we stripped out the focus calculation since it's not camera-generic, and the scatter/gather isn't necessary for general processing.  We need to reinstate the focus calculation in its own scatter/gather script.",2,DM-5904,datamanagement,create focus script dm-3368 strip focus calculation camera generic scatter gather necessary general processing need reinstate focus calculation scatter gather script,"Create focus script In DM-3368, we stripped out the focus calculation since it's not camera-generic, and the scatter/gather isn't necessary for general processing. We need to reinstate the focus calculation in its own scatter/gather script."
"Refactor DipoleFitPlugin classification into separate Classification plugin Currently the new DipoleFitPlugin runs measurement and then classification from a single measurement method. The classification should be moved out to a separate plugin. This will require more information be stored in the measRecord, in order to do the classification separately. Given that complication, evaluate whether this is even worthwhile.",6,DM-5905,datamanagement,refactor dipolefitplugin classification separate classification plugin currently new dipolefitplugin run measurement classification single measurement method classification move separate plugin require information store measrecord order classification separately give complication evaluate worthwhile,"Refactor DipoleFitPlugin classification into separate Classification plugin Currently the new DipoleFitPlugin runs measurement and then classification from a single measurement method. The classification should be moved out to a separate plugin. This will require more information be stored in the measRecord, in order to do the classification separately. Given that complication, evaluate whether this is even worthwhile."
Remove qmeta::QueryId and use global qserv::QueryId Remove the qmeta::QueryId and use the typedef of QueryId in global/intTypes.h instead. Also try to verify that QueryId is used instead of uint64_t where applicable.,6,DM-5906,datamanagement,remove qmeta::queryid use global qserv::queryid remove qmeta::queryid use typedef queryid global inttypes.h instead try verify queryid instead uint64_t applicable,Remove qmeta::QueryId and use global qserv::QueryId Remove the qmeta::QueryId and use the typedef of QueryId in global/intTypes.h instead. Also try to verify that QueryId is used instead of uint64_t where applicable.
"After the first result set is returned, have the thread leave the pool. When large results are returned from the worker to the czar, the thread should leave the thread pool and the Task should indicate to the scheduler that it is done.   ",6,DM-5909,datamanagement,result set return thread leave pool large result return worker czar thread leave thread pool task indicate scheduler,"After the first result set is returned, have the thread leave the pool. When large results are returned from the worker to the czar, the thread should leave the thread pool and the Task should indicate to the scheduler that it is done."
"Fix circular references in Mapper objects Whilst running tests with pytest and the new file descriptor leak checker it became clear that Mapper objects were not freeing their resources when they were deleted. In particular, the registry objects remained and the associated sqlite database files were opened. This led to pytest running out of file descriptors when large test suites were being executed.    The problem turns out to be the dynamically created map functions. These are created as functions (not bound methods) attached to an instance. Since they are not bound methods the instance object (self) has to be passed in to closure. This leads to self containing a reference to a function that contains a reference to self and this prevents the Mapper from ever being garbage collected (leading to all the resources being retained).    A short term fix is pass the mappers into the closures using {{weakref}}.    Eventually it would be nice to consistently make the {{map_}} items bound methods rather than attaching them as functions but that is beyond the scope of this ticket.",1,DM-5911,datamanagement,fix circular reference mapper object whilst run test pyt new file descriptor leak checker clear mapper object free resource delete particular registry object remain associate sqlite database file open lead pyt run file descriptor large test suite execute problem turn dynamically create map function create function bind method attach instance bind method instance object self pass closure lead self contain reference function contain reference self prevent mapper garbage collect lead resource retain short term fix pass mapper closure weakref eventually nice consistently map item bind method attach function scope ticket,"Fix circular references in Mapper objects Whilst running tests with pytest and the new file descriptor leak checker it became clear that Mapper objects were not freeing their resources when they were deleted. In particular, the registry objects remained and the associated sqlite database files were opened. This led to pytest running out of file descriptors when large test suites were being executed. The problem turns out to be the dynamically created map functions. These are created as functions (not bound methods) attached to an instance. Since they are not bound methods the instance object (self) has to be passed in to closure. This leads to self containing a reference to a function that contains a reference to self and this prevents the Mapper from ever being garbage collected (leading to all the resources being retained). A short term fix is pass the mappers into the closures using {{weakref}}. Eventually it would be nice to consistently make the {{map_}} items bound methods rather than attaching them as functions but that is beyond the scope of this ticket."
"Add ""everything"" scan Add a low priority ScanScheduler to the worker to handle very slow scans or scans that do not  work well on the other schedulers.",3,DM-5912,datamanagement,add scan add low priority scanscheduler worker handle slow scan scan work scheduler,"Add ""everything"" scan Add a low priority ScanScheduler to the worker to handle very slow scans or scans that do not work well on the other schedulers."
"Document planned implementation of toy model of Lupton(ZOGY) Develop a better understanding of the planned implementation of ZOGY in real space by implementing the kernel correction in k-space and investigating its characteristics when transformed back into real space. Do this either symbolically (if possible) or numerically in an ipython notebook. First in 1-D, then in 2-D, both assuming a constant kernel. Include documentation in the ipython notebook describing the current understanding of how this will be implemented",6,DM-5914,datamanagement,document plan implementation toy model lupton(zogy develop well understanding plan implementation zogy real space implement kernel correction space investigate characteristic transform real space symbolically possible numerically ipython notebook assume constant kernel include documentation ipython notebook describe current understanding implement,"Document planned implementation of toy model of Lupton(ZOGY) Develop a better understanding of the planned implementation of ZOGY in real space by implementing the kernel correction in k-space and investigating its characteristics when transformed back into real space. Do this either symbolically (if possible) or numerically in an ipython notebook. First in 1-D, then in 2-D, both assuming a constant kernel. Include documentation in the ipython notebook describing the current understanding of how this will be implemented"
"Decide how to rework afw:Wcs guts with AST Following the to-be-written recommendation for DM-4157, we plan to rework the guts of afw:Wcs to use AST. We need to decide how afw:Wcs will use AST, whether as a wrapper or as a complete replacement with AST.    The product is a design",8,DM-5915,datamanagement,decide rework afw wcs gut ast follow write recommendation dm-4157 plan rework gut afw wcs use ast need decide afw wcs use ast wrapper complete replacement ast product design,"Decide how to rework afw:Wcs guts with AST Following the to-be-written recommendation for DM-4157, we plan to rework the guts of afw:Wcs to use AST. We need to decide how afw:Wcs will use AST, whether as a wrapper or as a complete replacement with AST. The product is a design"
"What transforms do we currently need? In order to use AST in the stack, we may need to add mappings to it. We also need to be able to describe our transforms at a high level so that we know how to create them.    We need a list of the currently necessary transformations (e.g. from afw:wcs, XYTransform, GTransfo and any other relevant stack packages), and some concrete ideas about the kinds of transforms we may need in the future. These should be described in a high-level mathematical manner, independent of our wcs/transform system.    This can be informed by DMTN-005 and the requirements section of DMTN-010",4,DM-5918,datamanagement,transform currently need order use ast stack need add mapping need able describe transform high level know create need list currently necessary transformation e.g. afw wcs xytransform gtransfo relevant stack package concrete idea kind transform need future describe high level mathematical manner independent wcs transform system inform dmtn-005 requirement section dmtn-010,"What transforms do we currently need? In order to use AST in the stack, we may need to add mappings to it. We also need to be able to describe our transforms at a high level so that we know how to create them. We need a list of the currently necessary transformations (e.g. from afw:wcs, XYTransform, GTransfo and any other relevant stack packages), and some concrete ideas about the kinds of transforms we may need in the future. These should be described in a high-level mathematical manner, independent of our wcs/transform system. This can be informed by DMTN-005 and the requirements section of DMTN-010"
"Describe our composite mappings and transformation endpoints (Frames) To use AST in the stack, we need to be clear what our different transformations (AST:Mappings) and endpoints (AST:Frames) are going to be so we can create the chain of transformations (AST:FrameSets) that will be used throughout the stack. This applies to both images and CameraGeom. We may want to produce similar descriptions for other stack objects.    This ticket is the high-level Frames equivalent to the mathematical Mapping description in DM-5918.    This will help us determine how we can put our current input/output image frames into the new system.",4,DM-5919,datamanagement,describe composite mapping transformation endpoint frames use ast stack need clear different transformation ast mappings endpoint ast frames go create chain transformation ast framesets stack apply image camerageom want produce similar description stack object ticket high level frames equivalent mathematical mapping description dm-5918 help determine current input output image frame new system,"Describe our composite mappings and transformation endpoints (Frames) To use AST in the stack, we need to be clear what our different transformations (AST:Mappings) and endpoints (AST:Frames) are going to be so we can create the chain of transformations (AST:FrameSets) that will be used throughout the stack. This applies to both images and CameraGeom. We may want to produce similar descriptions for other stack objects. This ticket is the high-level Frames equivalent to the mathematical Mapping description in DM-5918. This will help us determine how we can put our current input/output image frames into the new system."
"Create DCR metric using new dipole measurement In order to evaluate DCR correction algorithms we need a metric that defines the severity of DCR in a residual image. This ticket is to run the new dipole measurement task on simulated difference images affected by DCR, and to define a useful metric. The result will be a brief technical note defining the process and the metric, with a few examples.",6,DM-5920,datamanagement,create dcr metric new dipole measurement order evaluate dcr correction algorithm need metric define severity dcr residual image ticket run new dipole measurement task simulate difference image affect dcr define useful metric result brief technical note define process metric example,"Create DCR metric using new dipole measurement In order to evaluate DCR correction algorithms we need a metric that defines the severity of DCR in a residual image. This ticket is to run the new dipole measurement task on simulated difference images affected by DCR, and to define a useful metric. The result will be a brief technical note defining the process and the metric, with a few examples."
"Clarify how to work with ci_hsc's astrometry_net_data ci_hsc's {{README.rst}} contains [a note|https://github.com/lsst/ci_hsc/blob/87b6ecb1cc0157cac8dafb356520f49f971bb1ec/README.rst#reference-catalog] on declaring & setting up the included reference catalogue data.    I believe this was rendered obsolete by DM-5135, which automatically sets up the reference catalogue when ci_hsc itself is set up. Attempting to follow the documentation therefore produces confusing warning messages, and may break things.    Please check if my understanding is correct and, if so, fix the documentation.",1,DM-5921,datamanagement,clarify work ci_hsc astrometry_net_data ci_hsc readme.rst contain note|https://github.com lsst ci_hsc blob/87b6ecb1cc0157cac8dafb356520f49f971bb1ec readme.rst#reference catalog declare set include reference catalogue datum believe render obsolete dm-5135 automatically set reference catalogue ci_hsc set attempt follow documentation produce confusing warning message break thing check understanding correct fix documentation,"Clarify how to work with ci_hsc's astrometry_net_data ci_hsc's {{README.rst}} contains [a note|https://github.com/lsst/ci_hsc/blob/87b6ecb1cc0157cac8dafb356520f49f971bb1ec/README.rst#reference-catalog] on declaring & setting up the included reference catalogue data. I believe this was rendered obsolete by DM-5135, which automatically sets up the reference catalogue when ci_hsc itself is set up. Attempting to follow the documentation therefore produces confusing warning messages, and may break things. Please check if my understanding is correct and, if so, fix the documentation."
"Rework camera geometry to use the replacement for XYTransform As part of overhauling XYTransform we will likely need to replace the way we describe the transformations supported by camera geometry and {{Detector}}. This is likely to include a new way of describing the coordinate frames (e.g. {{PIXEL}}. {{FOCAL_PLANE}} and {{PUPIL}}).    If we adopt AST (as seems likely) then these frames will be AST {{Frames}}, the transforms will be AST {{Mappings}} and the collection described by {{Camera}} and {{Detector}} will be one or more AST {{FrameSets}}.    An RFC for the redesigned API for camera geometry will be required and this ticket is to implement the resulting design.",8,DM-5922,datamanagement,rework camera geometry use replacement xytransform overhaul xytransform likely need replace way describe transformation support camera geometry detector likely include new way describe coordinate frame e.g. pixel focal_plane pupil adopt ast likely frame ast frames transform ast mappings collection describe camera detector ast framesets rfc redesign api camera geometry require ticket implement result design,"Rework camera geometry to use the replacement for XYTransform As part of overhauling XYTransform we will likely need to replace the way we describe the transformations supported by camera geometry and {{Detector}}. This is likely to include a new way of describing the coordinate frames (e.g. {{PIXEL}}. {{FOCAL_PLANE}} and {{PUPIL}}). If we adopt AST (as seems likely) then these frames will be AST {{Frames}}, the transforms will be AST {{Mappings}} and the collection described by {{Camera}} and {{Detector}} will be one or more AST {{FrameSets}}. An RFC for the redesigned API for camera geometry will be required and this ticket is to implement the resulting design."
"Support arbitrary sky rotation angles in StarFast Currently, if a region of sky is simulated in StarFast the stars must always have the same x,y coordinates (before DCR effects). This ticket is to support arbitrary rotations and offsets of the simulated stars to mimic realistic repeated observations of the same field.",2,DM-5923,datamanagement,support arbitrary sky rotation angle starfast currently region sky simulate starfast star coordinate dcr effect ticket support arbitrary rotation offset simulate star mimic realistic repeat observation field,"Support arbitrary sky rotation angles in StarFast Currently, if a region of sky is simulated in StarFast the stars must always have the same x,y coordinates (before DCR effects). This ticket is to support arbitrary rotations and offsets of the simulated stars to mimic realistic repeated observations of the same field."
"Improve overscan correction Overscan correction can be improved.  Specifically, some systems have sharp discontinuities in the bias section.",6,DM-5924,datamanagement,improve overscan correction overscan correction improve specifically system sharp discontinuity bias section,"Improve overscan correction Overscan correction can be improved. Specifically, some systems have sharp discontinuities in the bias section."
Implement fringe correction in ISR There is an initial implementation of fringe correction in the obs_subaru package.  It should be ported and generalized.,6,DM-5925,datamanagement,implement fringe correction isr initial implementation fringe correction obs_subaru package port generalize,Implement fringe correction in ISR There is an initial implementation of fringe correction in the obs_subaru package. It should be ported and generalized.
"networking in strange state for newly created instances When starting a new instance, occasionally something strange seems to happen with the  network setup.  The instance will come up but is inaccessible (icmp, ssh). When this happens, the console log shows that a DHCP address was obtained and cloud-init injected ssh-keys, so it isn't a total network setup failure.    I have seen this happen a few times in the last couple of weeks but I can't reliably reproduce it.  I'm wondering if neutron is logging anything interesting when this happens.    This failure mode happened  again a few minutes ago with 7adffa82-7221-454c-acfe-5f21cdd34ea8.  Which I killed and recreated as instance b6f64981-099b-46e5-a27e-e3694372f447 with the same private IP address.   The new instance is accessible as expected.",1,DM-5926,datamanagement,network strange state newly create instance start new instance occasionally strange happen network setup instance come inaccessible icmp ssh happen console log show dhcp address obtain cloud init inject ssh key total network setup failure see happen time couple week reliably reproduce wonder neutron log interesting happen failure mode happen minute ago 7adffa82 7221 454c acfe-5f21cdd34ea8 kill recreate instance b6f64981 099b-46e5 a27e e3694372f447 private ip address new instance accessible expect,"networking in strange state for newly created instances When starting a new instance, occasionally something strange seems to happen with the network setup. The instance will come up but is inaccessible (icmp, ssh). When this happens, the console log shows that a DHCP address was obtained and cloud-init injected ssh-keys, so it isn't a total network setup failure. I have seen this happen a few times in the last couple of weeks but I can't reliably reproduce it. I'm wondering if neutron is logging anything interesting when this happens. This failure mode happened again a few minutes ago with 7adffa82-7221-454c-acfe-5f21cdd34ea8. Which I killed and recreated as instance b6f64981-099b-46e5-a27e-e3694372f447 with the same private IP address. The new instance is accessible as expected."
"API errors when trying to start up multiple instances I am attempting to start up 20 {{m1.medium}} instances without floating IPs to take available of the new instance cap from DM-5840.  This consistently fails after starting a few instances with an HTTP 403.    {code:java}  Error creating OpenStack server: Expected HTTP response code [201 202] when accessing [POST http://nebula.ncsa.illinois.edu:8774/v2/8c1ba1e0b84d486fbe7a665c30030113/servers], but got 403 instead  {""forbidden"": {""message"": ""Maximum number of ports exceeded"", ""code"": 403}}  {code}    Of the instances that do manage to start, most end up in an error state with.      {code:java}  (openstack) server show 134b69dc-56fc-4249-b92f-e958e561ae3b  +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+  | Field                                | Value                                                                                                                                               |  +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+  | OS-DCF:diskConfig                    | MANUAL                                                                                                                                              |  | OS-EXT-AZ:availability_zone          | nova                                                                                                                                                |  | OS-EXT-STS:power_state               | 0                                                                                                                                                   |  | OS-EXT-STS:task_state                | None                                                                                                                                                |  | OS-EXT-STS:vm_state                  | error                                                                                                                                               |  | OS-SRV-USG:launched_at               | None                                                                                                                                                |  | OS-SRV-USG:terminated_at             | None                                                                                                                                                |  | accessIPv4                           |                                                                                                                                                     |  | accessIPv6                           |                                                                                                                                                     |  | addresses                            |                                                                                                                                                     |  | config_drive                         |                                                                                                                                                     |  | created                              | 2016-05-02T20:29:54Z                                                                                                                                |  | fault                                | {'code': 500, 'message': 'No valid host was found. Exceeded max scheduling attempts 3 for instance 134b69dc-56fc-4249-b92f-e958e561ae3b. Last       |  |                                      | exception: [u\'Traceback (most recent call last):\\n\', u\'  File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 2235, in _do',   |  |                                      | 'created': '2016-05-02T20:29:57Z'}                                                                                                                  |  | flavor                               | m1.medium (3)                                                                                                                                       |  | hostId                               | b383eddb06f7a1cc5929e5fa8b6982cc523f5ac1cbe3c9c40120a700                                                                                            |  | id                                   | 134b69dc-56fc-4249-b92f-e958e561ae3b                                                                                                                |  | image                                | centos-7-slurm-20160422210744 (7364ada7-263e-4fb0-a9f4-219ab19e0be0)                                                                                |  | key_name                             | jhoblitt-slurm                                                                                                                                      |  | name                                 | slurm-slave4                                                                                                                                        |  | os-extended-volumes:volumes_attached | []                                                                                                                                                  |  | project_id                           | 8c1ba1e0b84d486fbe7a665c30030113                                                                                                                    |  | properties                           | slurm_node_type='slave'                                                                                                                             |  | status                               | ERROR                                                                                                                                               |  | updated                              | 2016-05-02T20:29:57Z                                                                                                                                |  | user_id                              | 83bf259d1f0c4f458e03f9002f9b4008                                                                                                                    |  +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+  {code}      ",2,DM-5927,datamanagement,api error try start multiple instance attempt start 20 m1.medium instance float ip available new instance cap dm-5840 consistently fail start instance http 403 code java error create openstack server expect http response code 201 202 access post http://nebula.ncsa.illinois.edu:8774/v2/8c1ba1e0b84d486fbe7a665c30030113/servers get 403 instead forbid message maximum number port exceed code 403 code instance manage start end error state code java openstack server 134b69dc-56fc-4249 b92f e958e561ae3b field value --------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ os dcf diskconfig manual os ext az availability_zone nova os ext sts power_state os ext sts task_state os ext sts vm_state error os srv usg launched_at os srv usg terminated_at accessipv4 accessipv6 address config_drive create 2016 05 02t20:29:54z fault code 500 message valid host find exceed max scheduling attempt instance 134b69dc-56fc-4249 b92f e958e561ae3b exception u\'traceback recent last):\\n\ u\ file /usr lib python2.7 site package nova compute manager.py line 2235 create 2016 05 02t20:29:57z flavor m1.medium hostid b383eddb06f7a1cc5929e5fa8b6982cc523f5ac1cbe3c9c40120a700 134b69dc-56fc-4249 b92f e958e561ae3b image centos-7 slurm-20160422210744 7364ada7 263e-4fb0 a9f4 219ab19e0be0 key_name jhoblitt slurm slurm slave4 os extend volume volumes_attache project_id 8c1ba1e0b84d486fbe7a665c30030113 property slurm_node_type='slave status error update 2016 05 02t20:29:57z user_id 83bf259d1f0c4f458e03f9002f9b4008 --------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ code,"API errors when trying to start up multiple instances I am attempting to start up 20 {{m1.medium}} instances without floating IPs to take available of the new instance cap from DM-5840. This consistently fails after starting a few instances with an HTTP 403. {code:java} Error creating OpenStack server: Expected HTTP response code [201 202] when accessing [POST http://nebula.ncsa.illinois.edu:8774/v2/8c1ba1e0b84d486fbe7a665c30030113/servers], but got 403 instead {""forbidden"": {""message"": ""Maximum number of ports exceeded"", ""code"": 403}} {code} Of the instances that do manage to start, most end up in an error state with. {code:java} (openstack) server show 134b69dc-56fc-4249-b92f-e958e561ae3b +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | None | | OS-EXT-STS:vm_state | error | | OS-SRV-USG:launched_at | None | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | | | config_drive | | | created | 2016-05-02T20:29:54Z | | fault | {'code': 500, 'message': 'No valid host was found. Exceeded max scheduling attempts 3 for instance 134b69dc-56fc-4249-b92f-e958e561ae3b. Last | | | exception: [u\'Traceback (most recent call last):\\n\', u\' File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 2235, in _do', | | | 'created': '2016-05-02T20:29:57Z'} | | flavor | m1.medium (3) | | hostId | b383eddb06f7a1cc5929e5fa8b6982cc523f5ac1cbe3c9c40120a700 | | id | 134b69dc-56fc-4249-b92f-e958e561ae3b | | image | centos-7-slurm-20160422210744 (7364ada7-263e-4fb0-a9f4-219ab19e0be0) | | key_name | jhoblitt-slurm | | name | slurm-slave4 | | os-extended-volumes:volumes_attached | [] | | project_id | 8c1ba1e0b84d486fbe7a665c30030113 | | properties | slurm_node_type='slave' | | status | ERROR | | updated | 2016-05-02T20:29:57Z | | user_id | 83bf259d1f0c4f458e03f9002f9b4008 | +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ {code}"
"April 2016 LAAIM work Drafted documentation for Web SSO capabilities: https://confluence.lsstcorp.org/display/LAAIM/Web+SSO  Began testing new NCSA IAM capabilities (group management, user self-registration).  Registered NCSA with InCommon as a sub-org of UIUC to ease future IdP/SP registrations.  Attended local NCSA LSST coordination meetings.",4,DM-5928,datamanagement,april 2016 laaim work drafted documentation web sso capability https://confluence.lsstcorp.org/display/laaim/web+sso begin test new ncsa iam capability group management user self registration registered ncsa incommon sub org uiuc ease future idp sp registration attend local ncsa lsst coordination meeting,"April 2016 LAAIM work Drafted documentation for Web SSO capabilities: https://confluence.lsstcorp.org/display/LAAIM/Web+SSO Began testing new NCSA IAM capabilities (group management, user self-registration). Registered NCSA with InCommon as a sub-org of UIUC to ease future IdP/SP registrations. Attended local NCSA LSST coordination meetings."
"Replace exiting DipoleMeasurementTask with DipoleFitTask The goal of this ticket is to replace the existing DipoleMeasurementTask with the new  DipoleFitTask, subsequent to ticket DM-5413.    TBD: does this include completely removing all remnants of DipoleMeasurementTask code?",6,DM-5930,datamanagement,replace exit dipolemeasurementtask dipolefittask goal ticket replace exist dipolemeasurementtask new dipolefittask subsequent ticket dm-5413 tbd include completely remove remnant dipolemeasurementtask code,"Replace exiting DipoleMeasurementTask with DipoleFitTask The goal of this ticket is to replace the existing DipoleMeasurementTask with the new DipoleFitTask, subsequent to ticket DM-5413. TBD: does this include completely removing all remnants of DipoleMeasurementTask code?"
Test planned implementation of Lupton(ZOGY) algorithm in real space Develop a (1-D?) simple toy model and test the effects of the correction for varying I1 and I2 noise levels and different image PSFs and matching kernel(s). This will be done first in an ipython notebook.    See DM-5914.,6,DM-5931,datamanagement,test plan implementation lupton(zogy algorithm real space develop simple toy model test effect correction vary i1 i2 noise level different image psf match kernel(s ipython notebook dm-5914,Test planned implementation of Lupton(ZOGY) algorithm in real space Develop a (1-D?) simple toy model and test the effects of the correction for varying I1 and I2 noise levels and different image PSFs and matching kernel(s). This will be done first in an ipython notebook. See DM-5914.
Replace jointcal.StarSelector with meas_algorithms.starSelector jointcal has its own custom star selector. This should be removed and replaced with a star selector based on meas_algorithms.starSelector. A good choice might be meas_algorithms.objectSizeStarSelector.,2,DM-5933,datamanagement,replace jointcal starselector meas_algorithms.starselector jointcal custom star selector remove replace star selector base meas_algorithms.starselector good choice meas_algorithms.objectsizestarselector,Replace jointcal.StarSelector with meas_algorithms.starSelector jointcal has its own custom star selector. This should be removed and replaced with a star selector based on meas_algorithms.starSelector. A good choice might be meas_algorithms.objectSizeStarSelector.
Update developer guide with Astropy guidance Once RFC-178 is adopted the developer guide has to be updated to include guidance as to how Astropy can be used in the stack (similar to how Boost is documented).,1,DM-5934,datamanagement,update developer guide astropy guidance rfc-178 adopt developer guide update include guidance astropy stack similar boost document,Update developer guide with Astropy guidance Once RFC-178 is adopted the developer guide has to be updated to include guidance as to how Astropy can be used in the stack (similar to how Boost is documented).
Package Astropy for the stack Once RFC-178 is adopted Astropy needs to be packaged in an EUPS container. Given the complexity of Astropy dependencies the packaging will be done as for {{numpy}} and {{scipy}} by checking that Astropy is available (v1.1 will be the minimum version).,1,DM-5935,datamanagement,package astropy stack rfc-178 adopt astropy need package eups container give complexity astropy dependency packaging numpy scipy check astropy available v1.1 minimum version,Package Astropy for the stack Once RFC-178 is adopted Astropy needs to be packaged in an EUPS container. Given the complexity of Astropy dependencies the packaging will be done as for {{numpy}} and {{scipy}} by checking that Astropy is available (v1.1 will be the minimum version).
"Make afw rgb unit test PEP440 compliant for matplotlib check If a user has a version of matplotlib installed from a git clone, the afw rgb unit test fails at the matplotlib version check. The versioning scheme for this type of install is determined by pep 440. Make the unit test properly handle this type of version comparison.",1,DM-5936,datamanagement,afw rgb unit test pep440 compliant matplotlib check user version matplotlib instal git clone afw rgb unit test fail matplotlib version check versioning scheme type install determine pep 440 unit test properly handle type version comparison,"Make afw rgb unit test PEP440 compliant for matplotlib check If a user has a version of matplotlib installed from a git clone, the afw rgb unit test fails at the matplotlib version check. The versioning scheme for this type of install is determined by pep 440. Make the unit test properly handle this type of version comparison."
April work for middleware Participated in requirements definition,1,DM-5937,datamanagement,april work middleware participate requirement definition,April work for middleware Participated in requirements definition
"Pre-release versions of matplotlib 2.0 break afw unit tests In the afw rgb unit test, testWriteStarsLegacyAPI checks to make sure that a file name with an unknown extension raises a value error. In current version of matplotlib, saving a file with an unknown extension causes this error:  {code}  *** ValueError: Format ""unknown"" is not supported.  Supported formats: eps, jpeg, jpg, pdf, pgf, png, ps, raw, rgba, svg, svgz, tif, tiff.  {code}    In matplotlib 2.0 prerelease the file is saved as a png when an unknown extension is specified. Since the write call success the unit test fails as it is expecting a failure.     If nothing depends on this behavior, the unit test should probably be removed.",1,DM-5939,datamanagement,pre release version matplotlib 2.0 break afw unit test afw rgb unit test testwritestarslegacyapi check sure file unknown extension raise value error current version matplotlib save file unknown extension cause error code valueerror format unknown support support format eps jpeg jpg pdf pgf png ps raw rgba svg svgz tif tiff code matplotlib 2.0 prerelease file save png unknown extension specify write success unit test fail expect failure depend behavior unit test probably remove,"Pre-release versions of matplotlib 2.0 break afw unit tests In the afw rgb unit test, testWriteStarsLegacyAPI checks to make sure that a file name with an unknown extension raises a value error. In current version of matplotlib, saving a file with an unknown extension causes this error: {code} *** ValueError: Format ""unknown"" is not supported. Supported formats: eps, jpeg, jpg, pdf, pgf, png, ps, raw, rgba, svg, svgz, tif, tiff. {code} In matplotlib 2.0 prerelease the file is saved as a png when an unknown extension is specified. Since the write call success the unit test fails as it is expecting a failure. If nothing depends on this behavior, the unit test should probably be removed."
"Create new build based on the converted firefly code. - remove all of the gwt code except for a few remaining files.  - create separate build for the new firefly viewer, leaving the old fftools as it was before the JS conversion.  - repackage files as needed moving forward.",8,DM-5940,datamanagement,create new build base converted firefly code remove gwt code remain file create separate build new firefly viewer leave old fftool js conversion repackage file need move forward,"Create new build based on the converted firefly code. - remove all of the gwt code except for a few remaining files. - create separate build for the new firefly viewer, leaving the old fftools as it was before the JS conversion. - repackage files as needed moving forward."
"Private network not available across all instances I'm setting up an ELK system. Part of that is an Elasticsearch system. When I bring up the system the private network is bisected. I attempted creating a security group, in case that was a problem but it didn't help. Note that the work around is to create security groups or use a firewall and use floating ips. This is far from ideal. I think the right solution is to use the private network.    Example:    First section {{p-es-1}} {{p-es-3}} {{p-es-k}}    {code:bash}  vagrant@es-1:~$ ifconfig  ens3      Link encap:Ethernet  HWaddr fa:16:3e:47:28:a7            inet addr:10.0.42.30  Bcast:10.0.42.255  Mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe47:28a7/64 Scope:Link            UP BROADCAST RUNNING MULTICAST  MTU:1454  Metric:1            RX packets:363265 errors:0 dropped:0 overruns:0 frame:0            TX packets:304215 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            RX bytes:95396177 (95.3 MB)  TX bytes:238466304 (238.4 MB)    lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0            inet6 addr: ::1/128 Scope:Host            UP LOOPBACK RUNNING  MTU:65536  Metric:1            RX packets:850 errors:0 dropped:0 overruns:0 frame:0            TX packets:850 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            RX bytes:138411 (138.4 KB)  TX bytes:138411 (138.4 KB)    vagrant@es-1:~$ ping 10.0.42.32  PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  64 bytes from 10.0.42.32: icmp_seq=1 ttl=64 time=0.284 ms  64 bytes from 10.0.42.32: icmp_seq=2 ttl=64 time=0.266 ms  64 bytes from 10.0.42.32: icmp_seq=3 ttl=64 time=0.265 ms  64 bytes from 10.0.42.32: icmp_seq=4 ttl=64 time=0.302 ms  ^C  --- 10.0.42.32 ping statistics ---  4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.265/0.279/0.302/0.019 ms  vagrant@es-1:~$ ping 10.0.42.34  PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  64 bytes from 10.0.42.34: icmp_seq=1 ttl=64 time=0.333 ms  64 bytes from 10.0.42.34: icmp_seq=2 ttl=64 time=0.325 ms  64 bytes from 10.0.42.34: icmp_seq=3 ttl=64 time=0.322 ms  64 bytes from 10.0.42.34: icmp_seq=4 ttl=64 time=0.319 ms  ^C  --- 10.0.42.34 ping statistics ---  4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.319/0.324/0.333/0.022 ms  vagrant@es-1:~$ ping 10.0.42.31  PING 10.0.42.31 (10.0.42.31) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.31 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3017ms  pipe 3  vagrant@es-1:~$ ping 10.0.42.33  PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.33 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3008ms  pipe 3  vagrant@es-1:~$ ping 10.0.42.35  PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.35 ping statistics ---  5 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3999ms  pipe 3  {code}    Second section {{p-es-2}} {{p-es-4}} {{p-lfr}}    {code:bash}  vagrant@es-2:~$ ifconfig  ens3      Link encap:Ethernet  HWaddr fa:16:3e:6f:30:2c            inet addr:10.0.42.31  Bcast:10.0.42.255  Mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe6f:302c/64 Scope:Link            UP BROADCAST RUNNING MULTICAST  MTU:1454  Metric:1            RX packets:196344 errors:0 dropped:0 overruns:0 frame:0            TX packets:160561 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            RX bytes:47667399 (47.6 MB)  TX bytes:7135345 (7.1 MB)    lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0            inet6 addr: ::1/128 Scope:Host            UP LOOPBACK RUNNING  MTU:65536  Metric:1            RX packets:97268 errors:0 dropped:0 overruns:0 frame:0            TX packets:97268 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            RX bytes:8558096 (8.5 MB)  TX bytes:8558096 (8.5 MB)    vagrant@es-2:~$ ping 10.0.42.33  PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  64 bytes from 10.0.42.33: icmp_seq=1 ttl=64 time=0.311 ms  64 bytes from 10.0.42.33: icmp_seq=2 ttl=64 time=0.309 ms  64 bytes from 10.0.42.33: icmp_seq=3 ttl=64 time=0.300 ms  ^C  --- 10.0.42.33 ping statistics ---  3 packets transmitted, 3 received, 0% packet loss, time 2000ms  rtt min/avg/max/mdev = 0.300/0.306/0.311/0.020 ms  vagrant@es-2:~$ ping 10.0.42.30  PING 10.0.42.30 (10.0.42.30) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  From 10.0.42.31 icmp_seq=4 Destination Host Unreachable  ^C  --- 10.0.42.30 ping statistics ---  5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4014ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.32  PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  From 10.0.42.31 icmp_seq=4 Destination Host Unreachable  From 10.0.42.31 icmp_seq=5 Destination Host Unreachable  ^C  --- 10.0.42.32 ping statistics ---  5 packets transmitted, 0 received, +5 errors, 100% packet loss, time 4023ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.34  PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.34 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3006ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.35  PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  64 bytes from 10.0.42.35: icmp_seq=1 ttl=64 time=0.387 ms  64 bytes from 10.0.42.35: icmp_seq=2 ttl=64 time=0.278 ms  64 bytes from 10.0.42.35: icmp_seq=3 ttl=64 time=0.288 ms  ^C  --- 10.0.42.35 ping statistics ---  3 packets transmitted, 3 received, 0% packet loss, time 1998ms  rtt min/avg/max/mdev = 0.278/0.317/0.387/0.053 ms  {code}    This can be reproduced by sourcing your OpenStack credentials and running this [{{Vagrantfile}}|https://gist.github.com/jmatt/7b6eb6a042c4e63531d40d1a68069f33]. Use {{vagrant ssh p-es-1}} to connect to the {{p-es-1}} instance.  ",2,DM-5941,datamanagement,private network available instance set elk system elasticsearch system bring system private network bisect attempt create security group case problem help note work create security group use firewall use float ip far ideal think right solution use private network example section es-1 es-3 es code bash vagrant@es-1:~$ ifconfig ens3 link encap ethernet hwaddr fa:16:3e:47:28 a7 inet addr:10.0.42.30 bcast:10.0.42.255 mask:255.255.255.0 inet6 addr fe80::f816:3eff fe47:28a7/64 scope link broadcast running multicast mtu:1454 metric:1 rx packets:363265 errors:0 dropped:0 overruns:0 frame:0 tx packets:304215 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 rx bytes:95396177 95.3 mb tx bytes:238466304 238.4 mb lo link encap local loopback inet addr:127.0.0.1 mask:255.0.0.0 inet6 addr 1/128 scope host loopback run mtu:65536 metric:1 rx packets:850 dropped:0 overruns:0 frame:0 tx packets:850 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 rx bytes:138411 138.4 kb tx bytes:138411 138.4 kb vagrant@es-1:~$ ping 10.0.42.32 ping 10.0.42.32 10.0.42.32 56(84 byte datum 64 byte 10.0.42.32 icmp_seq=1 ttl=64 time=0.284 ms 64 byte 10.0.42.32 icmp_seq=2 ttl=64 time=0.266 ms 64 byte 10.0.42.32 icmp_seq=3 ttl=64 time=0.265 ms 64 byte 10.0.42.32 icmp_seq=4 ttl=64 time=0.302 ms ^c 10.0.42.32 ping statistic packet transmit receive packet loss time 2998ms rtt min avg max mdev 0.265/0.279/0.302/0.019 ms vagrant@es-1:~$ ping 10.0.42.34 ping 10.0.42.34 10.0.42.34 56(84 byte datum 64 byte 10.0.42.34 icmp_seq=1 ttl=64 time=0.333 ms 64 byte 10.0.42.34 icmp_seq=2 ttl=64 time=0.325 ms 64 byte 10.0.42.34 icmp_seq=3 ttl=64 time=0.322 ms 64 byte 10.0.42.34 icmp_seq=4 ttl=64 time=0.319 ms ^c 10.0.42.34 ping statistic packet transmit receive packet loss time 2998ms rtt min avg max mdev 0.319/0.324/0.333/0.022 ms vagrant@es-1:~$ ping 10.0.42.31 ping 10.0.42.31 10.0.42.31 56(84 byte datum 10.0.42.30 icmp_seq=1 destination host unreachable 10.0.42.30 icmp_seq=2 destination host unreachable 10.0.42.30 icmp_seq=3 destination host unreachable ^c 10.0.42.31 ping statistic packet transmit receive +3 error 100 packet loss time 3017m pipe vagrant@es-1:~$ ping 10.0.42.33 ping 10.0.42.33 10.0.42.33 56(84 byte datum 10.0.42.30 icmp_seq=1 destination host unreachable 10.0.42.30 icmp_seq=2 destination host unreachable 10.0.42.30 icmp_seq=3 destination host unreachable ^c 10.0.42.33 ping statistic packet transmit receive +3 error 100 packet loss time 3008ms pipe vagrant@es-1:~$ ping 10.0.42.35 ping 10.0.42.35 10.0.42.35 56(84 byte datum 10.0.42.30 icmp_seq=1 destination host unreachable 10.0.42.30 icmp_seq=2 destination host unreachable 10.0.42.30 icmp_seq=3 destination host unreachable ^c 10.0.42.35 ping statistic packet transmit receive +3 error 100 packet loss time 3999ms pipe code second section es-2 es-4 lfr code bash vagrant@es-2:~$ ifconfig ens3 link encap ethernet hwaddr fa:16:3e:6f:30:2c inet bcast:10.0.42.255 mask:255.255.255.0 inet6 addr fe80::f816:3eff fe6f:302c/64 scope link broadcast running multicast mtu:1454 metric:1 rx packets:196344 errors:0 dropped:0 overruns:0 frame:0 tx packets:160561 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 rx bytes:47667399 47.6 mb tx bytes:7135345 7.1 mb lo link encap local loopback inet addr:127.0.0.1 mask:255.0.0.0 inet6 addr 1/128 scope host loopback run mtu:65536 metric:1 rx packets:97268 dropped:0 overruns:0 frame:0 tx packets:97268 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 rx bytes:8558096 8.5 mb tx bytes:8558096 8.5 mb vagrant@es-2:~$ ping 10.0.42.33 ping 10.0.42.33 10.0.42.33 56(84 byte datum 64 byte 10.0.42.33 icmp_seq=1 ttl=64 time=0.311 ms 64 byte 10.0.42.33 icmp_seq=2 ttl=64 time=0.309 ms 64 byte 10.0.42.33 icmp_seq=3 ttl=64 time=0.300 ms ^c 10.0.42.33 ping statistic packet transmit receive packet loss time 2000ms rtt min avg max mdev 0.300/0.306/0.311/0.020 ms vagrant@es-2:~$ ping 10.0.42.30 ping 10.0.42.30 10.0.42.30 56(84 byte datum 10.0.42.31 icmp_seq=1 destination host unreachable 10.0.42.31 icmp_seq=2 destination host unreachable 10.0.42.31 icmp_seq=3 destination host unreachable 10.0.42.31 icmp_seq=4 destination host unreachable ^c 10.0.42.30 ping statistic packet transmit receive +4 error 100 packet loss time 4014ms pipe vagrant@es-2:~$ ping 10.0.42.32 ping 10.0.42.32 10.0.42.32 56(84 byte datum 10.0.42.31 icmp_seq=1 destination host unreachable 10.0.42.31 icmp_seq=2 destination host unreachable 10.0.42.31 icmp_seq=3 destination host unreachable 10.0.42.31 icmp_seq=4 destination host unreachable 10.0.42.31 icmp_seq=5 destination host unreachable ^c 10.0.42.32 ping statistic packet transmit receive +5 error 100 packet loss time 4023ms pipe vagrant@es-2:~$ ping 10.0.42.34 ping 10.0.42.34 10.0.42.34 56(84 byte datum 10.0.42.31 icmp_seq=1 destination host unreachable 10.0.42.31 icmp_seq=2 destination host unreachable 10.0.42.31 icmp_seq=3 destination host unreachable ^c 10.0.42.34 ping statistic packet transmit receive +3 error 100 packet loss time 3006ms pipe vagrant@es-2:~$ ping 10.0.42.35 ping 10.0.42.35 10.0.42.35 56(84 byte datum 64 byte 10.0.42.35 icmp_seq=1 ttl=64 time=0.387 ms 64 byte 10.0.42.35 icmp_seq=2 ttl=64 time=0.278 ms 64 byte 10.0.42.35 icmp_seq=3 ttl=64 time=0.288 ms ^c 10.0.42.35 ping statistic packet transmit receive packet loss time 1998ms rtt min avg max mdev 0.278/0.317/0.387/0.053 ms code reproduce source openstack credential run vagrantfile}}|https://gist.github.com jmatt/7b6eb6a042c4e63531d40d1a68069f33 use vagrant ssh es-1 connect es-1 instance,"Private network not available across all instances I'm setting up an ELK system. Part of that is an Elasticsearch system. When I bring up the system the private network is bisected. I attempted creating a security group, in case that was a problem but it didn't help. Note that the work around is to create security groups or use a firewall and use floating ips. This is far from ideal. I think the right solution is to use the private network. Example: First section {{p-es-1}} {{p-es-3}} {{p-es-k}} {code:bash} vagrant@es-1:~$ ifconfig ens3 Link encap:Ethernet HWaddr fa:16:3e:47:28:a7 inet addr:10.0.42.30 Bcast:10.0.42.255 Mask:255.255.255.0 inet6 addr: fe80::f816:3eff:fe47:28a7/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1454 Metric:1 RX packets:363265 errors:0 dropped:0 overruns:0 frame:0 TX packets:304215 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:95396177 (95.3 MB) TX bytes:238466304 (238.4 MB) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:850 errors:0 dropped:0 overruns:0 frame:0 TX packets:850 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:138411 (138.4 KB) TX bytes:138411 (138.4 KB) vagrant@es-1:~$ ping 10.0.42.32 PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data. 64 bytes from 10.0.42.32: icmp_seq=1 ttl=64 time=0.284 ms 64 bytes from 10.0.42.32: icmp_seq=2 ttl=64 time=0.266 ms 64 bytes from 10.0.42.32: icmp_seq=3 ttl=64 time=0.265 ms 64 bytes from 10.0.42.32: icmp_seq=4 ttl=64 time=0.302 ms ^C --- 10.0.42.32 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 2998ms rtt min/avg/max/mdev = 0.265/0.279/0.302/0.019 ms vagrant@es-1:~$ ping 10.0.42.34 PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data. 64 bytes from 10.0.42.34: icmp_seq=1 ttl=64 time=0.333 ms 64 bytes from 10.0.42.34: icmp_seq=2 ttl=64 time=0.325 ms 64 bytes from 10.0.42.34: icmp_seq=3 ttl=64 time=0.322 ms 64 bytes from 10.0.42.34: icmp_seq=4 ttl=64 time=0.319 ms ^C --- 10.0.42.34 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 2998ms rtt min/avg/max/mdev = 0.319/0.324/0.333/0.022 ms vagrant@es-1:~$ ping 10.0.42.31 PING 10.0.42.31 (10.0.42.31) 56(84) bytes of data. From 10.0.42.30 icmp_seq=1 Destination Host Unreachable From 10.0.42.30 icmp_seq=2 Destination Host Unreachable From 10.0.42.30 icmp_seq=3 Destination Host Unreachable ^C --- 10.0.42.31 ping statistics --- 4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3017ms pipe 3 vagrant@es-1:~$ ping 10.0.42.33 PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data. From 10.0.42.30 icmp_seq=1 Destination Host Unreachable From 10.0.42.30 icmp_seq=2 Destination Host Unreachable From 10.0.42.30 icmp_seq=3 Destination Host Unreachable ^C --- 10.0.42.33 ping statistics --- 4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3008ms pipe 3 vagrant@es-1:~$ ping 10.0.42.35 PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data. From 10.0.42.30 icmp_seq=1 Destination Host Unreachable From 10.0.42.30 icmp_seq=2 Destination Host Unreachable From 10.0.42.30 icmp_seq=3 Destination Host Unreachable ^C --- 10.0.42.35 ping statistics --- 5 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3999ms pipe 3 {code} Second section {{p-es-2}} {{p-es-4}} {{p-lfr}} {code:bash} vagrant@es-2:~$ ifconfig ens3 Link encap:Ethernet HWaddr fa:16:3e:6f:30:2c inet addr:10.0.42.31 Bcast:10.0.42.255 Mask:255.255.255.0 inet6 addr: fe80::f816:3eff:fe6f:302c/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1454 Metric:1 RX packets:196344 errors:0 dropped:0 overruns:0 frame:0 TX packets:160561 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:47667399 (47.6 MB) TX bytes:7135345 (7.1 MB) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:97268 errors:0 dropped:0 overruns:0 frame:0 TX packets:97268 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:8558096 (8.5 MB) TX bytes:8558096 (8.5 MB) vagrant@es-2:~$ ping 10.0.42.33 PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data. 64 bytes from 10.0.42.33: icmp_seq=1 ttl=64 time=0.311 ms 64 bytes from 10.0.42.33: icmp_seq=2 ttl=64 time=0.309 ms 64 bytes from 10.0.42.33: icmp_seq=3 ttl=64 time=0.300 ms ^C --- 10.0.42.33 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2000ms rtt min/avg/max/mdev = 0.300/0.306/0.311/0.020 ms vagrant@es-2:~$ ping 10.0.42.30 PING 10.0.42.30 (10.0.42.30) 56(84) bytes of data. From 10.0.42.31 icmp_seq=1 Destination Host Unreachable From 10.0.42.31 icmp_seq=2 Destination Host Unreachable From 10.0.42.31 icmp_seq=3 Destination Host Unreachable From 10.0.42.31 icmp_seq=4 Destination Host Unreachable ^C --- 10.0.42.30 ping statistics --- 5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4014ms pipe 3 vagrant@es-2:~$ ping 10.0.42.32 PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data. From 10.0.42.31 icmp_seq=1 Destination Host Unreachable From 10.0.42.31 icmp_seq=2 Destination Host Unreachable From 10.0.42.31 icmp_seq=3 Destination Host Unreachable From 10.0.42.31 icmp_seq=4 Destination Host Unreachable From 10.0.42.31 icmp_seq=5 Destination Host Unreachable ^C --- 10.0.42.32 ping statistics --- 5 packets transmitted, 0 received, +5 errors, 100% packet loss, time 4023ms pipe 3 vagrant@es-2:~$ ping 10.0.42.34 PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data. From 10.0.42.31 icmp_seq=1 Destination Host Unreachable From 10.0.42.31 icmp_seq=2 Destination Host Unreachable From 10.0.42.31 icmp_seq=3 Destination Host Unreachable ^C --- 10.0.42.34 ping statistics --- 4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3006ms pipe 3 vagrant@es-2:~$ ping 10.0.42.35 PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data. 64 bytes from 10.0.42.35: icmp_seq=1 ttl=64 time=0.387 ms 64 bytes from 10.0.42.35: icmp_seq=2 ttl=64 time=0.278 ms 64 bytes from 10.0.42.35: icmp_seq=3 ttl=64 time=0.288 ms ^C --- 10.0.42.35 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 1998ms rtt min/avg/max/mdev = 0.278/0.317/0.387/0.053 ms {code} This can be reproduced by sourcing your OpenStack credentials and running this [{{Vagrantfile}}|https://gist.github.com/jmatt/7b6eb6a042c4e63531d40d1a68069f33]. Use {{vagrant ssh p-es-1}} to connect to the {{p-es-1}} instance."
Public elasticsearch configuration using SSL/TLS and basic auth Create a (relatively) secure way to use Elasticsearch from outside of Nebula using Ansible. See:    https://www.elastic.co/blog/playing-http-tricks-nginx    Note that this will not allow another Elasticsearch to join but will allow the usual admin and client queries using basic auth.,1,DM-5942,datamanagement,public elasticsearch configuration ssl tls basic auth create relatively secure way use elasticsearch outside nebula ansible https://www.elastic.co/blog/playing-http-tricks-nginx note allow elasticsearch join allow usual admin client query basic auth,Public elasticsearch configuration using SSL/TLS and basic auth Create a (relatively) secure way to use Elasticsearch from outside of Nebula using Ansible. See: https://www.elastic.co/blog/playing-http-tricks-nginx Note that this will not allow another Elasticsearch to join but will allow the usual admin and client queries using basic auth.
Add Git refs to Jobs Table of QA Dashboard The Level 0 QA DB should know what Stack Git refs correspond to each job. This will enable plots to filter jobs based on development ticket so that a developer can understand how a branch compares to master.    This ticket will add jobs to the schema (http://sqr-009.lsst.io/en/latest/#level-0-qa) and create the necessary migration script.,5,DM-5943,datamanagement,add git ref jobs table qa dashboard level qa db know stack git ref correspond job enable plot filter job base development ticket developer understand branch compare master ticket add job schema http://sqr-009.lsst.io/en/latest/#level-0-qa create necessary migration script,Add Git refs to Jobs Table of QA Dashboard The Level 0 QA DB should know what Stack Git refs correspond to each job. This will enable plots to filter jobs based on development ticket so that a developer can understand how a branch compares to master. This ticket will add jobs to the schema (http://sqr-009.lsst.io/en/latest/#level-0-qa) and create the necessary migration script.
Implement validate_drp plot in Bokeh as proof-of-concept for QA Dashboard This ticket will implement a plot from validate_drp in the QA Dashboard as a proof-of-concept for how existing matplotlib plots can be re-implemented in Bokeh with data from the QA database.    Stretch goals (maybe for a future ticket) will be to overplot the validate_drp output of one job against another’s to understand performance changes.,2,DM-5945,datamanagement,implement validate_drp plot bokeh proof concept qa dashboard ticket implement plot validate_drp qa dashboard proof concept exist matplotlib plot implement bokeh datum qa database stretch goal maybe future ticket overplot validate_drp output job understand performance change,Implement validate_drp plot in Bokeh as proof-of-concept for QA Dashboard This ticket will implement a plot from validate_drp in the QA Dashboard as a proof-of-concept for how existing matplotlib plots can be re-implemented in Bokeh with data from the QA database. Stretch goals (maybe for a future ticket) will be to overplot the validate_drp output of one job against another s to understand performance changes.
"Make obs_subaru PEP8 (pyflakes) compliment Running pyflakes on obs_subaru revels many places where the code is not (LSST specific) PEP8 compliment. Actual coding bugs reviled by pyflakes were fixed in DM-5474, however many of the formatting issues need to be fixed. Once DM-4740 and DM-4668 are done, all remaining code should be brought into coding standard compliance.",3,DM-5951,datamanagement,obs_subaru pep8 pyflake compliment running pyflake obs_subaru revel place code lsst specific pep8 compliment actual code bug revile pyflake fix dm-5474 format issue need fix dm-4740 dm-4668 remain code bring code standard compliance,"Make obs_subaru PEP8 (pyflakes) compliment Running pyflakes on obs_subaru revels many places where the code is not (LSST specific) PEP8 compliment. Actual coding bugs reviled by pyflakes were fixed in DM-5474, however many of the formatting issues need to be fixed. Once DM-4740 and DM-4668 are done, all remaining code should be brought into coding standard compliance."
"Initial draft(s) of the Data Backbone ConOps Initial drafts of the data backbone concept of operations. Versions 0.1, 0.2. Produced document that is ready for friendly, internal review although document is not complete.",8,DM-5952,datamanagement,initial draft(s data backbone conops initial draft data backbone concept operation version 0.1 0.2 produce document ready friendly internal review document complete,"Initial draft(s) of the Data Backbone ConOps Initial drafts of the data backbone concept of operations. Versions 0.1, 0.2. Produced document that is ready for friendly, internal review although document is not complete."
Watch Boot Camp materials Videos from the [DM Boot Camp|https://community.lsst.org/t/dm-boot-camp-announcement/249] cover a lot of topics a newbie like me is interested in.,4,DM-5954,datamanagement,watch boot camp material videos dm boot camp|https://community.lsst.org dm boot camp announcement/249 cover lot topic newbie like interested,Watch Boot Camp materials Videos from the [DM Boot Camp|https://community.lsst.org/t/dm-boot-camp-announcement/249] cover a lot of topics a newbie like me is interested in.
Build LSST Software Stack from the source Create a virtual machine with functioning LSST Software Stack to have an environment where I can see and play with its code.,3,DM-5955,datamanagement,build lsst software stack source create virtual machine function lsst software stack environment play code,Build LSST Software Stack from the source Create a virtual machine with functioning LSST Software Stack to have an environment where I can see and play with its code.
"Networking Unpacking networking equipment, inventory, ordering/procure/unpack cabling, equipment racking, cabling, config creation and management.",6,DM-5961,datamanagement,networking unpacking networking equipment inventory order procure unpack cabling equipment racking cable config creation management,"Networking Unpacking networking equipment, inventory, ordering/procure/unpack cabling, equipment racking, cabling, config creation and management."
Object Storage Installation Discussions and planning for allocating the 1PB storage increase within Nebula between object storage and block storage. Further discussions with LSST DM interested stakeholders about this feature. Policy development for use and monitoring of use of this feature.,5,DM-5964,datamanagement,object storage installation discussions plan allocate 1pb storage increase nebula object storage block storage discussion lsst dm interested stakeholder feature policy development use monitoring use feature,Object Storage Installation Discussions and planning for allocating the 1PB storage increase within Nebula between object storage and block storage. Further discussions with LSST DM interested stakeholders about this feature. Policy development for use and monitoring of use of this feature.
"Remove use of Boost smart pointers in meas extensions Removal of boost smart pointers in DM-5879 missed some meas extensions which are not built as part of {{lsst_distrib}}. Namely: {{meas_extensions_shapeHSM}}, {{meas_extensions_simpleShape}} and {{meas_extensions_photometryKron}}.  Update these too.",1,DM-5966,datamanagement,remove use boost smart pointer meas extension removal boost smart pointer dm-5879 miss mea extension build lsst_distrib meas_extensions_shapehsm meas_extensions_simpleshape meas_extensions_photometrykron update,"Remove use of Boost smart pointers in meas extensions Removal of boost smart pointers in DM-5879 missed some meas extensions which are not built as part of {{lsst_distrib}}. Namely: {{meas_extensions_shapeHSM}}, {{meas_extensions_simpleShape}} and {{meas_extensions_photometryKron}}. Update these too."
"Split secondary index loading from qserv_data_loader.py to separate unit test To isolate development and validation of secondary index loading strategies, encapsulate loading of ""pure"" secondary index data via qserv_data_loader.py, without using entire datasets.",8,DM-5968,datamanagement,split secondary index load qserv_data_loader.py separate unit test isolate development validation secondary index loading strategy encapsulate loading pure secondary index datum qserv_data_loader.py entire dataset,"Split secondary index loading from qserv_data_loader.py to separate unit test To isolate development and validation of secondary index loading strategies, encapsulate loading of ""pure"" secondary index data via qserv_data_loader.py, without using entire datasets."
"Deploy any secondary index loader mods into qserv_data_loader.py Following completion of DM-5968, any modifications to the top-level data loading procedure for the secondary index need to be deployed back to the main qserv_data_loader.py driver.",5,DM-5969,datamanagement,deploy secondary index loader mod qserv_data_loader.py follow completion dm-5968 modification level datum loading procedure secondary index need deploy main qserv_data_loader.py driver,"Deploy any secondary index loader mods into qserv_data_loader.py Following completion of DM-5968, any modifications to the top-level data loading procedure for the secondary index need to be deployed back to the main qserv_data_loader.py driver."
Update developer guide with pytest guidance Now that DM-5561 explains how to migrate to pytest compatibility the developer guide must be updated to state how to use pytest in unittests.,5,DM-5973,datamanagement,update developer guide pyt guidance dm-5561 explain migrate pyt compatibility developer guide update state use pyt unittest,Update developer guide with pytest guidance Now that DM-5561 explains how to migrate to pytest compatibility the developer guide must be updated to state how to use pytest in unittests.
Change SubtractBackgroundConfig.isNanSafe default to True [~price] suggests that the default value for {{SubtractBackgroundConfig.isNanSafe}} be changed from False to True.,1,DM-5976,datamanagement,change subtractbackgroundconfig.isnansafe default true ~price suggest default value subtractbackgroundconfig.isnansafe change false true,Change SubtractBackgroundConfig.isNanSafe default to True [~price] suggests that the default value for {{SubtractBackgroundConfig.isNanSafe}} be changed from False to True.
Create and deploy Beats for Logstash and Elasticsearch. Create and deploy Beats for Logstash and Elasticsearch. These beats are used to transport logs and monitoring data to ELK.    See: https://www.elastic.co/products/beats,3,DM-5977,datamanagement,create deploy beat logstash elasticsearch create deploy beat logstash elasticsearch beat transport log monitor datum elk https://www.elastic.co/products/beat,Create and deploy Beats for Logstash and Elasticsearch. Create and deploy Beats for Logstash and Elasticsearch. These beats are used to transport logs and monitoring data to ELK. See: https://www.elastic.co/products/beats
"Miscellaneous Nebula service items for x16 This story is for miscellaneous Nebula service items that do not have individual LSSTDM JIRA issues (often handled in the RT ticket system)  including account creation requests, reporting on hanging/errant processes for cleanup, response & communiques on security incidents, etc. ",8,DM-5978,datamanagement,miscellaneous nebula service item x16 story miscellaneous nebula service item individual lsstdm jira issue handle rt ticket system include account creation request report hang errant process cleanup response communique security incident etc,"Miscellaneous Nebula service items for x16 This story is for miscellaneous Nebula service items that do not have individual LSSTDM JIRA issues (often handled in the RT ticket system) including account creation requests, reporting on hanging/errant processes for cleanup, response & communiques on security incidents, etc."
" tests in testArgumentParser.py fail Jenkins run-rebuild on nfs (1) {{testOutputs}} fails because paths are compared literally  Jenkins run-rebuild #139 failed with pipe_base  https://ci.lsst.codes/job/run-rebuild/139//console  {code:java}  FAIL: testOutputs (__main__.ArgumentParserTestCase)  Test output directories, specified in different ways  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testArgumentParser.py"", line 497, in testOutputs      self.assertEqual(args.input, DataPath)  AssertionError: '/nfs/home/lsstsw/stack/Linux64/obs_test/2016_01.0-3-gafa6dd0+10/data/input' != '/home/lsstsw/stack/Linux64/obs_test/2016_01.0-3-gafa6dd0+10/data/input'  {code}    Please make the comparison more robust.     (2) File descriptor leaks  Jenkins run-rebuild #138 failed with pipe_base  https://ci.lsst.codes/job/run-rebuild/138//console  {code:java}  FAIL: testFileDescriptorLeaks (lsst.utils.tests.MemoryTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/utils/2016_01.0-2-g97a6e33/python/lsst/utils/tests.py"", line 133, in testFileDescriptorLeaks      self.fail(""Failed to close %d files"" % len(diff))  AssertionError: Failed to close 1 files  {code}    {code:java}  File open: /nfs/home/lsstsw/build/pipe_base/.nfs000000000a20a3f700005679  {code}  This test passes on local disk.  ",2,DM-5979,datamanagement,"test testargumentparser.py fail jenkins run rebuild nfs testoutput fail path compare literally jenkins run rebuild 139 fail pipe_base https://ci.lsst.codes/job/run-rebuild/139//console code java fail testoutput main__.argumentparsertestcase test output directory specify different way traceback recent file test testargumentparser.py line 497 testoutput self.assertequal(args.input datapath assertionerror /nfs home lsstsw stack linux64 obs_test/2016_01.0 gafa6dd0 10 data input /home lsstsw stack linux64 obs_test/2016_01.0 gafa6dd0 10 data input code comparison robust file descriptor leak jenkins run rebuild 138 fail pipe_base https://ci.lsst.codes/job/run-rebuild/138//console code java fail testfiledescriptorleak lsst.utils.test memorytestcase traceback recent file /home lsstsw stack linux64 utils/2016_01.0 g97a6e33 python lsst util tests.py line 133 testfiledescriptorleak self.fail(""faile close file len(diff assertionerror fail close file code code java file open /nfs home lsstsw build pipe_base/.nfs000000000a20a3f700005679 code test pass local disk","tests in testArgumentParser.py fail Jenkins run-rebuild on nfs (1) {{testOutputs}} fails because paths are compared literally Jenkins run-rebuild #139 failed with pipe_base https://ci.lsst.codes/job/run-rebuild/139//console {code:java} FAIL: testOutputs (__main__.ArgumentParserTestCase) Test output directories, specified in different ways ---------------------------------------------------------------------- Traceback (most recent call last): File ""tests/testArgumentParser.py"", line 497, in testOutputs self.assertEqual(args.input, DataPath) AssertionError: '/nfs/home/lsstsw/stack/Linux64/obs_test/2016_01.0-3-gafa6dd0+10/data/input' != '/home/lsstsw/stack/Linux64/obs_test/2016_01.0-3-gafa6dd0+10/data/input' {code} Please make the comparison more robust. (2) File descriptor leaks Jenkins run-rebuild #138 failed with pipe_base https://ci.lsst.codes/job/run-rebuild/138//console {code:java} FAIL: testFileDescriptorLeaks (lsst.utils.tests.MemoryTestCase) ---------------------------------------------------------------------- Traceback (most recent call last): File ""/home/lsstsw/stack/Linux64/utils/2016_01.0-2-g97a6e33/python/lsst/utils/tests.py"", line 133, in testFileDescriptorLeaks self.fail(""Failed to close %d files"" % len(diff)) AssertionError: Failed to close 1 files {code} {code:java} File open: /nfs/home/lsstsw/build/pipe_base/.nfs000000000a20a3f700005679 {code} This test passes on local disk."
Stop cleanly MySQL if configuration step fails Next scripts doesn't stop cleanly MySQL if configuration step fails:    {code:bash}  admin/templates/configuration/tmp/configure/scisql.sh  admin/templates/configuration/tmp/configure/tools/sql-loader.sh  {code},2,DM-5983,datamanagement,stop cleanly mysql configuration step fail script stop cleanly mysql configuration step fail code bash admin template configuration tmp configure scisql.sh admin template configuration tmp configure tool sql loader.sh code,Stop cleanly MySQL if configuration step fails Next scripts doesn't stop cleanly MySQL if configuration step fails: {code:bash} admin/templates/configuration/tmp/configure/scisql.sh admin/templates/configuration/tmp/configure/tools/sql-loader.sh {code}
"Use RO MySQL account in qserv_testdata It seems integration tests datasets are now loaded with Loader. So using MySQL root account is no more required in integration tests, an account with SELECT access on test databases (like qsmaster), should be enough.     Furthermore, all code related to MySQL writes can be removed from   {code:bash}  python/lsst/qserv/tests/sql/cmd.py  python/lsst/qserv/tests/sql/connection.py  {code}",4,DM-5984,datamanagement,use ro mysql account qserv_testdata integration test dataset load loader mysql root account require integration test account select access test database like qsmaster furthermore code relate mysql write remove code bash python lsst qserv test sql cmd.py python lsst qserv test sql connection.py code,"Use RO MySQL account in qserv_testdata It seems integration tests datasets are now loaded with Loader. So using MySQL root account is no more required in integration tests, an account with SELECT access on test databases (like qsmaster), should be enough. Furthermore, all code related to MySQL writes can be removed from {code:bash} python/lsst/qserv/tests/sql/cmd.py python/lsst/qserv/tests/sql/connection.py {code}"
Add unicode support for Qserv password Qserv password must be encoded in ASCII for now in qserv-meta.conf. Unicode passwords should be supported.,6,DM-5985,datamanagement,add unicode support qserv password qserv password encode ascii qserv-meta.conf unicode password support,Add unicode support for Qserv password Qserv password must be encoded in ASCII for now in qserv-meta.conf. Unicode passwords should be supported.
"Use sagas in place of side-effects in chart-related controllers  Replace side-effects with saga and clean-up chart related controllers (TableStats, XYPlot and Histogram).",3,DM-5986,datamanagement,use sagas place effect chart relate controller replace effect saga clean chart related controller tablestats xyplot histogram,"Use sagas in place of side-effects in chart-related controllers Replace side-effects with saga and clean-up chart related controllers (TableStats, XYPlot and Histogram)."
"Support Monocam reduction Monocam is being used on a telescope, and we want to reduce the data obtained.  This is made difficult by the fact that the camera and the telescope are not talking to each other so the usual header keywords are in separate files from the data.",6,DM-5988,datamanagement,support monocam reduction monocam telescope want reduce datum obtain difficult fact camera telescope talk usual header keyword separate file datum,"Support Monocam reduction Monocam is being used on a telescope, and we want to reduce the data obtained. This is made difficult by the fact that the camera and the telescope are not talking to each other so the usual header keywords are in separate files from the data."
A look at the overall performance of the application Investigate the overall performance of the application and improve it where possible.  It is pointed out that triview is especially slow compare to expanded.  Need to investigate.,4,DM-5991,datamanagement,look overall performance application investigate overall performance application improve possible point triview especially slow compare expand need investigate,A look at the overall performance of the application Investigate the overall performance of the application and improve it where possible. It is pointed out that triview is especially slow compare to expanded. Need to investigate.
Documentation Document each component sufficient enough for transfer of knowledge and system recovery as needed.,5,DM-5996,datamanagement,documentation document component sufficient transfer knowledge system recovery need,Documentation Document each component sufficient enough for transfer of knowledge and system recovery as needed.
Security Vetting Review of capability by site security team,3,DM-5998,datamanagement,security vetting review capability site security team,Security Vetting Review of capability by site security team
"Acceptance by Stakeholders Review with stakeholders (target users, release manager, others as necessary) to confirm that capability fulfills original requirements. ",2,DM-5999,datamanagement,acceptance stakeholders review stakeholder target user release manager necessary confirm capability fulfill original requirement,"Acceptance by Stakeholders Review with stakeholders (target users, release manager, others as necessary) to confirm that capability fulfills original requirements."
"Acceptance by Stakeholders Review of services (compute, storage, networking) by LSST project before considering work final.",5,DM-6004,datamanagement,acceptance stakeholders review service compute storage networking lsst project consider work final,"Acceptance by Stakeholders Review of services (compute, storage, networking) by LSST project before considering work final."
"Lazy load related chart data on table data update When new table data received, the related chart data should be updated only for the components on display. Hidden components' data should be lazily updated when a component becomes visible.",3,DM-6022,datamanagement,lazy load relate chart datum table datum update new table datum receive related chart datum update component display hide component datum lazily update component visible,"Lazy load related chart data on table data update When new table data received, the related chart data should be updated only for the components on display. Hidden components' data should be lazily updated when a component becomes visible."
"ingest.py throwing away errors Line 118 of ingest.py has a problem try block which is currently just throwing away errors, which has made for some confusing/frustrating debugging.    This should be changed to either warn or raise, but not silently dispose of errors.",1,DM-6025,datamanagement,ingest.py throw away error line 118 ingest.py problem try block currently throw away error confusing frustrating debugging change warn raise silently dispose error,"ingest.py throwing away errors Line 118 of ingest.py has a problem try block which is currently just throwing away errors, which has made for some confusing/frustrating debugging. This should be changed to either warn or raise, but not silently dispose of errors."
"Make it possible to distinguish TABLE_NEW_LOADED actions triggered by sort It would be beneficial to have in the TABLE_NEW_LOADED payload a  trigger field, which would differentiate actions triggered by sort (where  data do not change, only their order) or filter from other loads. We don't  need to reload table statistics or histogram on sort. But we do need to to  reload them on filter.      created TABLE_SORT action to distinguish sorting from filtering.  sorting should not reload xyplot nor catalog overlay.    Also:  - disable history when in api mode.  - ensure tableMeta.source reflects the file on the server.  - fix TablePanelOptions not resetting columns selection.  - remove 'Fits Data' tab when no images available.  - fix 'Coverage' appearing when it should.",2,DM-6026,datamanagement,possible distinguish table_new_loaded action trigger sort beneficial table_new_loaded payload trigger field differentiate action trigger sort datum change order filter load need reload table statistic histogram sort need reload filter create table_sort action distinguish sort filter sort reload xyplot catalog overlay disable history api mode ensure tablemeta.source reflect file server fix tablepaneloption reset column selection remove fit data tab image available fix coverage appear,"Make it possible to distinguish TABLE_NEW_LOADED actions triggered by sort It would be beneficial to have in the TABLE_NEW_LOADED payload a trigger field, which would differentiate actions triggered by sort (where data do not change, only their order) or filter from other loads. We don't need to reload table statistics or histogram on sort. But we do need to to reload them on filter. created TABLE_SORT action to distinguish sorting from filtering. sorting should not reload xyplot nor catalog overlay. Also: - disable history when in api mode. - ensure tableMeta.source reflects the file on the server. - fix TablePanelOptions not resetting columns selection. - remove 'Fits Data' tab when no images available. - fix 'Coverage' appearing when it should."
"Validation is not performed on unchanged fields Currently, validation is performed only if a field has changed. We need to be able to validate all fields on form submit.    The issue is not limited to initial (ex. empty) value being invalid. The invalid message is lost when a field is unmounted/re-mounted.    You can test the following way:  - http://localhost:8080/firefly/;a=layout.showDropDown?view=AnyDataSetSearch  - Open chart settings, enter 1000 into X/Y ratio - the field is shown as invalid  - Switch to histogram and back, the invalid message is gone, the field appears to be valid    Another test case is Example Dialog tab 'X 3', 'X 3'  tab test field initial value 88 is invalid (it should be between 22 and 23), but it appears valid. ",2,DM-6028,datamanagement,validation perform unchanged field currently validation perform field change need able validate field form submit issue limit initial ex value invalid invalid message lose field unmounte mounted test following way http://localhost:8080 firefly/;a layout.showdropdown?view anydatasetsearch open chart setting enter 1000 ratio field show invalid switch histogram invalid message go field appear valid test case example dialog tab tab test field initial value 88 invalid 22 23 appear valid,"Validation is not performed on unchanged fields Currently, validation is performed only if a field has changed. We need to be able to validate all fields on form submit. The issue is not limited to initial (ex. empty) value being invalid. The invalid message is lost when a field is unmounted/re-mounted. You can test the following way: - http://localhost:8080/firefly/;a=layout.showDropDown?view=AnyDataSetSearch - Open chart settings, enter 1000 into X/Y ratio - the field is shown as invalid - Switch to histogram and back, the invalid message is gone, the field appears to be valid Another test case is Example Dialog tab 'X 3', 'X 3' tab test field initial value 88 is invalid (it should be between 22 and 23), but it appears valid."
Error message is not shown The error message is not showing consistently when mouse is over tha exclamation icon.,1,DM-6029,datamanagement,error message show error message show consistently mouse tha exclamation icon,Error message is not shown The error message is not showing consistently when mouse is over tha exclamation icon.
"Create documentation for bright object masks The bright object mask code ported from hsc bought the ability to mask regions, during coaddition, by providing mask files. How to create these files, and where they should be placed in the file system is documented on and HSC ticket (https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1351) but not on the LSST side. The information on how to create and use bright object masks should be put into LSST documentation.",3,DM-6031,datamanagement,create documentation bright object mask bright object mask code port hsc buy ability mask region coaddition provide mask file create file place file system document hsc ticket https://hsc-jira.astro.princeton.edu/jira/browse/hsc-1351 lsst information create use bright object mask lsst documentation,"Create documentation for bright object masks The bright object mask code ported from hsc bought the ability to mask regions, during coaddition, by providing mask files. How to create these files, and where they should be placed in the file system is documented on and HSC ticket (https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1351) but not on the LSST side. The information on how to create and use bright object masks should be put into LSST documentation."
Support sphinxcontrib-bibtex in technotes Allow bibtex-based references in technotes using [sphinxcontrib-bibtex|https://github.com/mcmtroffaes/sphinxcontrib-bibtex].    DMTN-010 will be used as a pilot case.,1,DM-6033,datamanagement,support sphinxcontrib bibtex technote allow bibtex base reference technote sphinxcontrib bibtex|https://github.com mcmtroffaes sphinxcontrib bibtex dmtn-010 pilot case,Support sphinxcontrib-bibtex in technotes Allow bibtex-based references in technotes using [sphinxcontrib-bibtex|https://github.com/mcmtroffaes/sphinxcontrib-bibtex]. DMTN-010 will be used as a pilot case.
"Produce and ingest master calibs for USNO monocam data. Use the construct*.py scripts added to pipe_drivers to produce temporally relevant master biases, darks, flats (and fringe frames?) for the recent USNO observing with monocam.    A small amount of hacking will be required due to the fact that the current ingestion model assumes that each CCD frame has a USNO counterpart which tells about the telescope pointing etc, but the bias frames do not have these.    Once the master calibs are produced, get them ingested.",2,DM-6036,datamanagement,produce ingest master calibs usno monocam datum use construct*.py script add pipe_driver produce temporally relevant master bias dark flat fringe frame recent usno observe monocam small hacking require fact current ingestion model assume ccd frame usno counterpart tell telescope point etc bias frame master calib produce ingest,"Produce and ingest master calibs for USNO monocam data. Use the construct*.py scripts added to pipe_drivers to produce temporally relevant master biases, darks, flats (and fringe frames?) for the recent USNO observing with monocam. A small amount of hacking will be required due to the fact that the current ingestion model assumes that each CCD frame has a USNO counterpart which tells about the telescope pointing etc, but the bias frames do not have these. Once the master calibs are produced, get them ingested."
"Reduce sky data from USNO monocam run Using the master calibs produced in DM-6036, push all the monocam data through processCcd.    Others will run sanity checks on the output (initial astrometry & photometry). From there I believe people will look at using the data to test jointcal & sim_astrom etc, but this ticket just related to the initial reduction.    As more data comes in from the 2nd telescope, a little further hacking may be necessary to keep everything running. Some of this will likely be hacky or need one-off solutions/header modification, hence the higher-than-normal number of story points assigned to what one might expect to be an hour-long job.",3,DM-6037,datamanagement,reduce sky datum usno monocam run master calib produce dm-6036 push monocam datum processccd run sanity check output initial astrometry photometry believe people look datum test jointcal sim_astrom etc ticket relate initial reduction datum come 2nd telescope little hacking necessary run likely hacky need solution header modification high normal number story point assign expect hour long job,"Reduce sky data from USNO monocam run Using the master calibs produced in DM-6036, push all the monocam data through processCcd. Others will run sanity checks on the output (initial astrometry & photometry). From there I believe people will look at using the data to test jointcal & sim_astrom etc, but this ticket just related to the initial reduction. As more data comes in from the 2nd telescope, a little further hacking may be necessary to keep everything running. Some of this will likely be hacky or need one-off solutions/header modification, hence the higher-than-normal number of story points assigned to what one might expect to be an hour-long job."
"Monocam bias structure analysis Estimates of the noise in the bias frames coming from the USNO monocam run around ~20e- RMS. This is higher than with the same readout configuration in the lab, and could be due to several things.    This ticket is to take a look at a few bias frames and investigate the structure of the noise. If it is periodic and at a constant phase then using master biases will significantly improve the SNR in the calexps produced, but if it is not, then whether or not bias frames should be used at all should be considered.",1,DM-6038,datamanagement,monocam bias structure analysis estimate noise bias frame come usno monocam run rms high readout configuration lab thing ticket look bias frame investigate structure noise periodic constant phase master bias significantly improve snr calexps produce bias frame consider,"Monocam bias structure analysis Estimates of the noise in the bias frames coming from the USNO monocam run around ~20e- RMS. This is higher than with the same readout configuration in the lab, and could be due to several things. This ticket is to take a look at a few bias frames and investigate the structure of the noise. If it is periodic and at a constant phase then using master biases will significantly improve the SNR in the calexps produced, but if it is not, then whether or not bias frames should be used at all should be considered."
"Download temporally relevant raw calibs for CTIO DECam data Go to the NOAO portal and download a sufficient number of darks, biases and flats (in each band) from around the time of the CTIO trip to produce master calibs.",1,DM-6039,datamanagement,download temporally relevant raw calibs ctio decam datum noao portal download sufficient number dark bias flat band time ctio trip produce master calib,"Download temporally relevant raw calibs for CTIO DECam data Go to the NOAO portal and download a sufficient number of darks, biases and flats (in each band) from around the time of the CTIO trip to produce master calibs."
"Create and ingest master calibs for DECam CBP reduction Having collected data in DM-6039, push all this through the master calib creation scripts and ingest master calibs into registry.    This is a necessary but not-necessarily-sufficient ingredient for making progress on DM-5465.",2,DM-6040,datamanagement,create ingest master calibs decam cbp reduction having collect datum dm-6039 push master calib creation script ing master calib registry necessary necessarily sufficient ingredient make progress dm-5465,"Create and ingest master calibs for DECam CBP reduction Having collected data in DM-6039, push all this through the master calib creation scripts and ingest master calibs into registry. This is a necessary but not-necessarily-sufficient ingredient for making progress on DM-5465."
"Functional use cases for tools Work with Vandana Desai (IRSA) to tabulate science use cases for tools. Then transform the science use cases to functional use cases (""this is how we want the tool/interface to behave"").    This work is  to identify common functional and scientific use cases between PTF/ZTF and LSST to inform LSST on how the SUIT web portal might be organized for user interaction with LSST data. ",1,DM-6041,datamanagement,functional use case tool work vandana desai irsa tabulate science use case tool transform science use case functional use case want tool interface behave work identify common functional scientific use case ptf ztf lsst inform lsst suit web portal organize user interaction lsst data,"Functional use cases for tools Work with Vandana Desai (IRSA) to tabulate science use cases for tools. Then transform the science use cases to functional use cases (""this is how we want the tool/interface to behave""). This work is to identify common functional and scientific use cases between PTF/ZTF and LSST to inform LSST on how the SUIT web portal might be organized for user interaction with LSST data."
"Add viewer launching API Add ability to launch the viewer and load images, xyplots, and tables from the api. We use to call this firefly.getExternalViewer()  Also, we have this concept of 'root path' through out the code. The api use can set a root path so he can use his when we are cross site. Need to implement.  ",6,DM-6042,datamanagement,add viewer launch api add ability launch viewer load image xyplot table api use firefly.getexternalviewer concept root path code api use set root path use cross site need implement,"Add viewer launching API Add ability to launch the viewer and load images, xyplots, and tables from the api. We use to call this firefly.getExternalViewer() Also, we have this concept of 'root path' through out the code. The api use can set a root path so he can use his when we are cross site. Need to implement."
Bundle up more HSC data for validate_drp We would like to include a larger set of HSC data for validation.  I tested this while in Tucson.  My working dir was {{/tigress/pprice/frossie}}.  The raw and processed data should be stuffed into validation_data_hsc,1,DM-6048,datamanagement,bundle hsc datum validate_drp like include large set hsc datum validation test tucson work dir /tigress pprice frossie raw process datum stuff validation_data_hsc,Bundle up more HSC data for validate_drp We would like to include a larger set of HSC data for validation. I tested this while in Tucson. My working dir was {{/tigress/pprice/frossie}}. The raw and processed data should be stuffed into validation_data_hsc
"Table caching optimizations We need to avoid duplicate requests which result from minor differences in TableRequest parameters, which are not used to get data.  For example, loading catalog table, which triggers table statistics, and then getting an XY plot, results in 3 requests, returning identical data.    1. RequestClass=ServerRequest; *tbl_id=tbl_id-1;* UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; SearchMethod=Cone; catalog=wise_allwise_p3as_psd; RequestedDataSet=wise_allwise_p3as_psd; radius=200; use=catalog_overlay; catalogProject=WISE    2. RequestClass=ServerRequest;RequestedDataSet=wise_allwise_p3as_psd; catalog=wise_allwise_p3as_psd; use=catalog_overlay; UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; catalogProject=WISE; radius=200; SearchMethod=Cone    3. RequestClass=ServerRequest; *tbl_id=xyplot-tbl_id-1;* catalog=wise_allwise_p3as_psd; use=catalog_overlay; UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; SearchMethod=Cone; RequestedDataSet=wise_allwise_p3as_psd; catalogProject=WISE; radius=200; *decimate=decimate=ra,dec,10000,1,,,,*    The difference between 1 and 2 is tbl_id parameter. The difference between 2 and 3 is tbl_id and decimate parameters. As well as the order of the parameters. None of which change the catalog search result.    Test Case: Test Searches, Test catalog, AllWISE Source, radius=200",2,DM-6050,datamanagement,"table cache optimization need avoid duplicate request result minor difference tablerequest parameter datum example load catalog table trigger table statistic get xy plot result request return identical datum requestclass serverrequest tbl_id tbl_id-1 usertargetworldpt=10.68479;41.26906;eq_j2000;m31;ned searchmethod cone catalog wise_allwise_p3as_psd requesteddataset wise_allwise_p3as_psd radius=200 use catalog_overlay catalogproject wise requestclass serverrequest;requesteddataset wise_allwise_p3as_psd catalog wise_allwise_p3as_psd use catalog_overlay usertargetworldpt=10.68479;41.26906;eq_j2000;m31;ne catalogproject wise radius=200 searchmethod cone requestclass serverrequest tbl_id xyplot tbl_id-1 catalog wise_allwise_p3as_psd use catalog_overlay usertargetworldpt=10.68479;41.26906;eq_j2000;m31;ne searchmethod cone requesteddataset wise_allwise_p3as_psd catalogproject wise radius=200 decimate decimate ra dec,10000,1 difference tbl_id parameter difference tbl_id decimate parameter order parameter change catalog search result test case test searches test catalog allwise source radius=200","Table caching optimizations We need to avoid duplicate requests which result from minor differences in TableRequest parameters, which are not used to get data. For example, loading catalog table, which triggers table statistics, and then getting an XY plot, results in 3 requests, returning identical data. 1. RequestClass=ServerRequest; *tbl_id=tbl_id-1;* UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; SearchMethod=Cone; catalog=wise_allwise_p3as_psd; RequestedDataSet=wise_allwise_p3as_psd; radius=200; use=catalog_overlay; catalogProject=WISE 2. RequestClass=ServerRequest;RequestedDataSet=wise_allwise_p3as_psd; catalog=wise_allwise_p3as_psd; use=catalog_overlay; UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; catalogProject=WISE; radius=200; SearchMethod=Cone 3. RequestClass=ServerRequest; *tbl_id=xyplot-tbl_id-1;* catalog=wise_allwise_p3as_psd; use=catalog_overlay; UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; SearchMethod=Cone; RequestedDataSet=wise_allwise_p3as_psd; catalogProject=WISE; radius=200; *decimate=decimate=ra,dec,10000,1,,,,* The difference between 1 and 2 is tbl_id parameter. The difference between 2 and 3 is tbl_id and decimate parameters. As well as the order of the parameters. None of which change the catalog search result. Test Case: Test Searches, Test catalog, AllWISE Source, radius=200"
"Add extendedness vs. star selector test to single-visit validation in ci_hsc ci_hsc has a test that verifies that extendedness as measured on coadds broadly agrees with the star selection done for PSF estimation on individual frames.  This tests a bunch of stuff, including aperture corrections on the coadds and propagation of flags from visits to coadds.    It doesn't test that aperture correction vs. extendedness logic is correct in processCcd.py, but just copying this test to the appropriate validation function in ci_hsc should do the trick.  This is currently broken, but should be fixed in DM-5877.",2,DM-6051,datamanagement,add extendedness vs. star selector test single visit validation ci_hsc ci_hsc test verify extendedness measure coadd broadly agree star selection psf estimation individual frame test bunch stuff include aperture correction coadd propagation flag visit coadd test aperture correction vs. extendedness logic correct processccd.py copy test appropriate validation function ci_hsc trick currently break fix dm-5877,"Add extendedness vs. star selector test to single-visit validation in ci_hsc ci_hsc has a test that verifies that extendedness as measured on coadds broadly agrees with the star selection done for PSF estimation on individual frames. This tests a bunch of stuff, including aperture corrections on the coadds and propagation of flags from visits to coadds. It doesn't test that aperture correction vs. extendedness logic is correct in processCcd.py, but just copying this test to the appropriate validation function in ci_hsc should do the trick. This is currently broken, but should be fixed in DM-5877."
"Improve password management for Qserv MySQL accounts Qserv passwords management for MySQL account (i.e. root, monitor, qsmaster) should be improved. See wmgr password management to have a good example. Furthermore qsmaster use currently empty password, this must be fixed.",8,DM-6052,datamanagement,improve password management qserv mysql account qserv password management mysql account i.e. root monitor qsmaster improve wmgr password management good example furthermore qsmaster use currently password fix,"Improve password management for Qserv MySQL accounts Qserv passwords management for MySQL account (i.e. root, monitor, qsmaster) should be improved. See wmgr password management to have a good example. Furthermore qsmaster use currently empty password, this must be fixed."
"Allow use of other MySQL account thant 'qsmaster' 'qsmaster' value can't be changed in qserv-meta.conf, this must be fixed    {code}  diff --git a/admin/templates/installation/qserv-meta.conf b/admin/templates/installation/qserv-meta.conf  index 81b6ca9..203ebda 100644  --- a/admin/templates/installation/qserv-meta.conf  +++ b/admin/templates/installation/qserv-meta.conf  @@ -103,7 +103,7 @@ user_monitor = monitor   password_monitor = CHANGEMETOO      # Used to access Qserv data and metadata (like indexes)  -user_qserv = qsmaster  +user_qserv = qservdata  {code}    Above change leads to next error in integration tests:    {code:bash}  154 [0x7f1beacf9700] ERROR lsst.qserv.sql.SqlConnection null - connectToDb failed to connect!  154 [0x7f1beacf9700] ERROR lsst.qserv.sql.SqlConnection null - runQuery failed connectToDb: START TRANSACTION  2016-05-10 14:55:09,684 - root - CRITICAL - Exception occured: Error from mysql: (-999) Error connecting to mysql with config:[host=127.0.0.1, port=13306, user=qsmaster, password=XXXXXX, db=qservCssData, socket=]  Traceback (most recent call last):    File ""/home/dev/src/qserv/bin/qserv-data-loader.py"", line 274, in <module>      loader = Loader()    File ""/home/dev/src/qserv/bin/qserv-data-loader.py"", line 225, in __init__      css_inst = css.CssAccess.createFromConfig(config, """")  CssError: Error from mysql: (-999) Error connecting to mysql with config:[host=127.0.0.1, port=13306, user=qsmaster, password=XXXXXX, db=qservCssData, socket=]  2016-05-10 14:55:09,810 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : qserv-data-loader.py -v --config=/qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/common.cfg --host=127.0.0.1 --port=5012 --secret=/home/dev/qserv-run/git/etc/wmgr.secret --delete-tables --config=/qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/Object.cfg --css-remove --skip-partition --chunks-dir=/home/dev/qserv-run/git/tmp/qservTest_case05/chunks/Object --config=/qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/Object.cfg --empty-chunks=/home/dev/qserv-run/git/var/lib/qserv/empty_qservTest_case05_qserv.txt qservTest_case05_qserv Object /qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/Object.sql /home/dev/qserv-run/git/tmp/qservTest_case05/chunks/Object/Object.txt   ERROR  {code}  ",4,DM-6053,datamanagement,"allow use mysql account thant qsmaster qsmaster value change qserv-meta.conf fix code diff admin template installation qserv meta.conf admin template installation qserv meta.conf index 81b6ca9 203ebda 100644 admin template installation qserv meta.conf admin template installation qserv meta.conf +103,7 @@ user_monitor monitor password_monitor changemetoo access qserv datum metadata like index -user_qserv qsmaster user_qserv qservdata code change lead error integration test code bash 154 0x7f1beacf9700 error lsst.qserv.sql sqlconnection null connecttodb fail connect 154 0x7f1beacf9700 error lsst.qserv.sql sqlconnection null runquery fail connecttodb start transaction 2016 05 10 14:55:09,684 root critical exception occur error mysql -999 error connect mysql config:[host=127.0.0.1 port=13306 user qsmaster password xxxxxx db qservcssdata socket= traceback recent file /home dev src qserv bin qserv data loader.py line 274 loader loader file /home dev src qserv bin qserv data loader.py line 225 init css_inst css cssaccess.createfromconfig(config csserror error mysql -999 error connect mysql config:[host=127.0.0.1 port=13306 user qsmaster password xxxxxx db qservcssdata socket= 2016 05 10 14:55:09,810 lsst.qserv.admin.common critical error code return command qserv-data-loader.py -v --config=/qserv stack linux64 qserv_testdata/2016_01 g7b10791 dataset case05 data common.cfg --port=5012 --secret=/home dev qserv run git etc wmgr.secret --delete table stack linux64 qserv_testdata/2016_01 g7b10791 dataset case05 data object.cfg --css remove --skip partition --chunks dir=/home dev qserv run git tmp qservtest_case05 chunk object stack linux64 qserv_testdata/2016_01 g7b10791 dataset case05 data object.cfg --empty chunks=/home dev qserv run git var lib qserv empty_qservtest_case05_qserv.txt qservtest_case05_qserv object stack linux64 qserv_testdata/2016_01 g7b10791 dataset case05 data object.sql dev qserv run git tmp qservtest_case05 chunk object object.txt error code","Allow use of other MySQL account thant 'qsmaster' 'qsmaster' value can't be changed in qserv-meta.conf, this must be fixed {code} diff --git a/admin/templates/installation/qserv-meta.conf b/admin/templates/installation/qserv-meta.conf index 81b6ca9..203ebda 100644 --- a/admin/templates/installation/qserv-meta.conf +++ b/admin/templates/installation/qserv-meta.conf @@ -103,7 +103,7 @@ user_monitor = monitor password_monitor = CHANGEMETOO # Used to access Qserv data and metadata (like indexes) -user_qserv = qsmaster +user_qserv = qservdata {code} Above change leads to next error in integration tests: {code:bash} 154 [0x7f1beacf9700] ERROR lsst.qserv.sql.SqlConnection null - connectToDb failed to connect! 154 [0x7f1beacf9700] ERROR lsst.qserv.sql.SqlConnection null - runQuery failed connectToDb: START TRANSACTION 2016-05-10 14:55:09,684 - root - CRITICAL - Exception occured: Error from mysql: (-999) Error connecting to mysql with config:[host=127.0.0.1, port=13306, user=qsmaster, password=XXXXXX, db=qservCssData, socket=] Traceback (most recent call last): File ""/home/dev/src/qserv/bin/qserv-data-loader.py"", line 274, in  loader = Loader() File ""/home/dev/src/qserv/bin/qserv-data-loader.py"", line 225, in __init__ css_inst = css.CssAccess.createFromConfig(config, """") CssError: Error from mysql: (-999) Error connecting to mysql with config:[host=127.0.0.1, port=13306, user=qsmaster, password=XXXXXX, db=qservCssData, socket=] 2016-05-10 14:55:09,810 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : qserv-data-loader.py -v --config=/qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/common.cfg --host=127.0.0.1 --port=5012 --secret=/home/dev/qserv-run/git/etc/wmgr.secret --delete-tables --config=/qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/Object.cfg --css-remove --skip-partition --chunks-dir=/home/dev/qserv-run/git/tmp/qservTest_case05/chunks/Object --config=/qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/Object.cfg --empty-chunks=/home/dev/qserv-run/git/var/lib/qserv/empty_qservTest_case05_qserv.txt qservTest_case05_qserv Object /qserv/stack/Linux64/qserv_testdata/2016_01-1-g7b10791+7/datasets/case05/data/Object.sql /home/dev/qserv-run/git/tmp/qservTest_case05/chunks/Object/Object.txt ERROR {code}"
Minor updates in suptertask from following DMTN-002 Some examples in the DMTN-002 seem slightly out of date.    Update supertask documentation and code to catch up with some recent developments in the stack. ,2,DM-6054,datamanagement,minor update suptertask follow dmtn-002 example dmtn-002 slightly date update supertask documentation code catch recent development stack,Minor updates in suptertask from following DMTN-002 Some examples in the DMTN-002 seem slightly out of date. Update supertask documentation and code to catch up with some recent developments in the stack.
Fix how aperture correction is applied [~lauren] committed a fix Jan 15 to how aperture correction is applied that I accidentally lost when refactoring in DM-4692. https://github.com/lsst/pipe_tasks/commit/d904e3d188698b4f57bf3dad1516b0bf201078f5 Restore the fix.    The need for this fix suggests a design flaw in measurement that will be fixed as part of DM-5877,1,DM-6063,datamanagement,fix aperture correction apply ~lauren commit fix jan 15 aperture correction apply accidentally lose refactore dm-4692 https://github.com/lsst/pipe_tasks/commit/d904e3d188698b4f57bf3dad1516b0bf201078f5 restore fix need fix suggest design flaw measurement fix dm-5877,Fix how aperture correction is applied [~lauren] committed a fix Jan 15 to how aperture correction is applied that I accidentally lost when refactoring in DM-4692. https://github.com/lsst/pipe_tasks/commit/d904e3d188698b4f57bf3dad1516b0bf201078f5 Restore the fix. The need for this fix suggests a design flaw in measurement that will be fixed as part of DM-5877
"Pass background to NoiseReplacerTask Implement RFC-180:    `NoiseReplacerTask` wants some statistics about the background that was subtracted from the exposure, but it gets these in a fragile and roundabout fashion: it expects the code that measures the background to put the mean and variance into the exposure's metadata, using special keys. It is difficult to enforce correctness because background is measured several times while processing an exposure.    To solve this, pass the background directly to `NoiseReplacerTask`. This will require passing the background through the various measurement tasks, which will require small changes to code that calls the measurement tasks.    In addition, remove computation of background statistics from the background fitting code.",4,DM-6073,datamanagement,pass background noisereplacertask implement rfc-180 noisereplacertask want statistic background subtract exposure get fragile roundabout fashion expect code measure background mean variance exposure metadata special key difficult enforce correctness background measure time process exposure solve pass background directly noisereplacertask require pass background measurement task require small change code call measurement task addition remove computation background statistic background fit code,"Pass background to NoiseReplacerTask Implement RFC-180: `NoiseReplacerTask` wants some statistics about the background that was subtracted from the exposure, but it gets these in a fragile and roundabout fashion: it expects the code that measures the background to put the mean and variance into the exposure's metadata, using special keys. It is difficult to enforce correctness because background is measured several times while processing an exposure. To solve this, pass the background directly to `NoiseReplacerTask`. This will require passing the background through the various measurement tasks, which will require small changes to code that calls the measurement tasks. In addition, remove computation of background statistics from the background fitting code."
Add RegistryField support to Task.makeSubtask As part of implementing RFC-183 add support for tasks specified in {{lsst.pex.config.RegistryField}} to {{lsst.pipe.base.Task.makeSubtask}}  ,2,DM-6074,datamanagement,add registryfield support task.makesubtask implement rfc-183 add support task specify lsst.pex.config registryfield lsst.pipe.base task.makesubtask,Add RegistryField support to Task.makeSubtask As part of implementing RFC-183 add support for tasks specified in {{lsst.pex.config.RegistryField}} to {{lsst.pipe.base.Task.makeSubtask}}
"Document the need for abstract base tasks for tasks As part of RFC-183 document the fact that variant tasks should have a common abstract base class that defines the API. If we add future tasks that we feel are likely to have variants, then we should create an abstract base class.    Candidates include star selectors, PSF determiners and ISR tasks.    Note that this applies to tasks LSST provides in its stack, not to variants users produce and other obscure one-off code.    Also document the desire that tasks with anticipated many variants, such as star selectors, and PSF determiners should be in registries. This explicitly excludes tasks such as ISR where only one task is likely to be useful for a given set of data.  ",2,DM-6075,datamanagement,document need abstract base task task rfc-183 document fact variant task common abstract base class define api add future task feel likely variant create abstract base class candidate include star selector psf determiner isr task note apply task lsst provide stack variant user produce obscure code document desire task anticipate variant star selector psf determiner registry explicitly exclude task isr task likely useful give set datum,"Document the need for abstract base tasks for tasks As part of RFC-183 document the fact that variant tasks should have a common abstract base class that defines the API. If we add future tasks that we feel are likely to have variants, then we should create an abstract base class. Candidates include star selectors, PSF determiners and ISR tasks. Note that this applies to tasks LSST provides in its stack, not to variants users produce and other obscure one-off code. Also document the desire that tasks with anticipated many variants, such as star selectors, and PSF determiners should be in registries. This explicitly excludes tasks such as ISR where only one task is likely to be useful for a given set of data."
Create a registry for star selectors Create a registry for star selectors and use the registry instead of ConfigurableField in tasks that call a star selector.    Update config overrides in obs_* packages and unit tests accordingly.,3,DM-6076,datamanagement,create registry star selector create registry star selector use registry instead configurablefield task star selector update config override obs package unit test accordingly,Create a registry for star selectors Create a registry for star selectors and use the registry instead of ConfigurableField in tasks that call a star selector. Update config overrides in obs_* packages and unit tests accordingly.
"Change PSF determiners into tasks PSF determiners are already configurables, and some benefit from having a log. Take the logical next step and make them instances of {{lsst.pipe.base.Task}}.",1,DM-6077,datamanagement,change psf determiner task psf determiner configurable benefit have log logical step instance lsst.pipe.base task,"Change PSF determiners into tasks PSF determiners are already configurables, and some benefit from having a log. Take the logical next step and make them instances of {{lsst.pipe.base.Task}}."
"Aperture correction fails to measure a correction for the final plugin in the list and reports misleading errors Since the refactoring of DM-4692, runs of *processCcd.py* detail the following in their logs:    {code:title=With base_PsfFlux and base_GaussianFlux plugins registered}  processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_PsfFlux'; setting to 1.0  processCcd.charImage.detectAndMeasure.measurement: Measuring 65 sources (65 parents, 0 children)   processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 1 flux fields  processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  ...  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 2 flux fields  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr WARNING: Could not find base_GaussianFlux_flux or base_GaussianFlux_fluxSigma in apCorrMap  {code}    {code:title=With base_PsfFlux, base_GaussianFlux, and ext_photometryKron_KronFlux plugins registered}  processCcd.charImage.detectAndMeasure.measureApCorr: Measuring aperture corrections for 2 flux fields  processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_PsfFlux'; setting to 1.0  processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_GaussianFlux'; setting to 1.0  processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 2 flux fields  processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  ...  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 3 flux fields  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr WARNING: Could not find ext_photometryKron_KronFlux_flux or ext_photometryKron_KronFlux_fluxSigma in apCorrMap  {code}    I can confirm that for the latter, running HSC data with the fix on DM-6063, the aperture corrections are being measured and applied for the PsfFlux and GaussianFlux measurements, but NOT for the KronFlux measurements.      Looking at the output from the current ""expected"" values for the {{lsst_dm_stack_demo}} we see that there is an offset in the Psf-Gaussian fluxes, implying the Gaussian fluxes are not being measured (and hence not applied):  !demo_current.png|width=500!    From this I conclude that the aperture corrections are indeed being measured for all but the final entry in the plugin list.  This implies that the report of ""Only 0 sources for calculation of aperture correction for 'xxx_xxFlux'; setting to 1.0"" is incorrect for all but the final plugin measurement.    The demo previously successfully calculated aperture corrections and, after the logic fix of DM-4836, applied them in the correct order:  !demo_previous.png|width=500!    The sources of these issues and fixes for them are the goal of this issue.",2,DM-6078,datamanagement,aperture correction fail measure correction final plugin list report mislead error refactoring dm-4692 run processccd.py detail following log code title base_psfflux base_gaussianflux plugin register processccd.charimage.detectandmeasure.measureapcorr warning source calculation aperture correction base_psfflux set 1.0 processccd.charimage.detectandmeasure.measurement measure 65 source 65 parent child apply aperture correction flux field use naive flux sigma computation processccd.calibrate.detectandmeasure.measurement.applyapcorr apply aperture correction flux field processccd.calibrate.detectandmeasure.measurement.applyapcorr use naive flux sigma computation processccd.calibrate.detectandmeasure.measurement.applyapcorr warning find base_gaussianflux_flux base_gaussianflux_fluxsigma code code title base_psfflux base_gaussianflux ext_photometrykron_kronflux plugin register processccd.charimage.detectandmeasure.measureapcorr measure aperture correction flux field processccd.charimage.detectandmeasure.measureapcorr warning source calculation aperture correction base_psfflux set 1.0 processccd.charimage.detectandmeasure.measureapcorr warning source calculation aperture correction base_gaussianflux set 1.0 processccd.charimage.detectandmeasure.measurement.applyapcorr apply aperture correction flux field use naive flux sigma computation processccd.calibrate.detectandmeasure.measurement.applyapcorr apply aperture correction flux field processccd.calibrate.detectandmeasure.measurement.applyapcorr use naive flux sigma computation processccd.calibrate.detectandmeasure.measurement.applyapcorr warning find ext_photometrykron_kronflux_flux ext_photometrykron_kronflux_fluxsigma code confirm run hsc datum fix dm-6063 aperture correction measure apply psfflux gaussianflux measurement kronflux measurement look output current expect value lsst_dm_stack_demo offset psf gaussian flux imply gaussian flux measure apply demo_current.png|width=500 conclude aperture correction measure final entry plugin list imply report source calculation aperture correction set 1.0 incorrect final plugin measurement demo previously successfully calculate aperture correction logic fix dm-4836 apply correct order demo_previous.png|width=500 source issue fix goal issue,"Aperture correction fails to measure a correction for the final plugin in the list and reports misleading errors Since the refactoring of DM-4692, runs of *processCcd.py* detail the following in their logs: {code:title=With base_PsfFlux and base_GaussianFlux plugins registered} processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_PsfFlux'; setting to 1.0 processCcd.charImage.detectAndMeasure.measurement: Measuring 65 sources (65 parents, 0 children) processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 1 flux fields processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation ... processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 2 flux fields processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation processCcd.calibrate.detectAndMeasure.measurement.applyApCorr WARNING: Could not find base_GaussianFlux_flux or base_GaussianFlux_fluxSigma in apCorrMap {code} {code:title=With base_PsfFlux, base_GaussianFlux, and ext_photometryKron_KronFlux plugins registered} processCcd.charImage.detectAndMeasure.measureApCorr: Measuring aperture corrections for 2 flux fields processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_PsfFlux'; setting to 1.0 processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_GaussianFlux'; setting to 1.0 processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 2 flux fields processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation ... processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 3 flux fields processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation processCcd.calibrate.detectAndMeasure.measurement.applyApCorr WARNING: Could not find ext_photometryKron_KronFlux_flux or ext_photometryKron_KronFlux_fluxSigma in apCorrMap {code} I can confirm that for the latter, running HSC data with the fix on DM-6063, the aperture corrections are being measured and applied for the PsfFlux and GaussianFlux measurements, but NOT for the KronFlux measurements. Looking at the output from the current ""expected"" values for the {{lsst_dm_stack_demo}} we see that there is an offset in the Psf-Gaussian fluxes, implying the Gaussian fluxes are not being measured (and hence not applied): !demo_current.png|width=500! From this I conclude that the aperture corrections are indeed being measured for all but the final entry in the plugin list. This implies that the report of ""Only 0 sources for calculation of aperture correction for 'xxx_xxFlux'; setting to 1.0"" is incorrect for all but the final plugin measurement. The demo previously successfully calculated aperture corrections and, after the logic fix of DM-4836, applied them in the correct order: !demo_previous.png|width=500! The sources of these issues and fixes for them are the goal of this issue."
"description of archive in a box Please put in more detailed description of the ""Archive in a box"" concept",1,DM-6079,datamanagement,description archive box detailed description archive box concept,"description of archive in a box Please put in more detailed description of the ""Archive in a box"" concept"
"Add Sublime Text configuration tips to Developer Documentation [~rowen] and [~Parejkoj] have some good tips about setting up Sublime Text.  [~jsick] suggested that we add these configuration tips to the Developer Documentation.    http://developer.lsst.io/en/latest/#part-tools    Both want to include info about recommended packages, but also the linter configurations to support the DM styles.    I paste in here various helpful parts from the HipChat Software Development room discussion of this.  Both verbatim, and summarized.    1. Install {{Package Control}}    2. Packages:  {{Git}}, {{GitGutter}}, {{SideBarEnhancements}}, {{SublimeLinter}}, {{SublimeLinter-flake8}}, {{SublimeLinter-html-tidy}}, {{SumNumbers}}, {{Gist}}, {{BracketHighlighter}}, {{TrailingSpaces}}, {{Trimmer}}, {{OmniMarkupPreviewer}}, {{ReStructuredTextImproved}}, {{MarkDown Editing}}, {{Colorsublime}}    3. Themes:  {{Sunburst}} color scheme    * VIM users:  {{Vintageous}}  + Mac OS X configuration:  {{defaults write com.sublimetext.3 ApplePressAndHoldEnabled -bool false}}  so that holding down 'j' moves downward.  Note that {{Vintageous}} is not a complete implementation of {{vim}}, but it at least allows enough basics so that one doesn't go crazy switching back and forth.    link the {{subl}} command to {{/usr/local/bin}}     Quick Tips:  ""option-select (to select blocks) and select something then cmd-D are both extremely useful for modifying lots of things at once.""    ""Similarly, ctrl-shift-up/down arrow.""    ""cmd-click on multiple lines to have multiple synchronized cursors""    Configurations:  1. [~rowen]'s SublimeText Preferences file: https://jira.lsstcorp.org/secure/attachment/27846/Preferences.sublime-settings  2. Configuration {{flake8}} so that it works in the linting can take a bit of work if {{flake8}} isn't in your default path.  See SublimeLinter.sublime-settings attachment for [~rowen]'s configuration: https://jira.lsstcorp.org/secure/attachment/27845/SublimeLinter.sublime-settings    The above are useful, but we'll need someone to detail the linter stuff more.",1,DM-6082,datamanagement,add sublime text configuration tip developer documentation ~rowen ~parejkoj good tip set sublime text ~jsick suggest add configuration tip developer documentation http://developer.lsst.io/en/latest/#part-tool want include info recommend package linter configuration support dm style paste helpful part hipchat software development room discussion verbatim summarize install package control package git gitgutter sidebarenhancements sublimelinter sublimelinter flake8 sublimelinter html tidy sumnumbers gist brackethighlighter trailingspace trimmer omnimarkuppreviewer restructuredtextimprove markdown editing colorsublime theme sunburst color scheme vim user vintageous mac os configuration default write com.sublimetext.3 applepressandholdenable -bool false hold move downward note vintageous complete implementation vim allow basic crazy switch forth link subl command /usr local bin quick tips option select select block select cmd extremely useful modify lot thing similarly ctrl shift arrow cmd click multiple line multiple synchronize cursor configuration ~rowen sublimetext preferences file https://jira.lsstcorp.org/secure/attachment/27846/preferences.sublime-setting configuration flake8 work linting bit work flake8 default path sublimelinter.sublime setting attachment ~rowen configuration https://jira.lsstcorp.org/secure/attachment/27845/sublimelinter.sublime-setting useful need detail linter stuff,"Add Sublime Text configuration tips to Developer Documentation [~rowen] and [~Parejkoj] have some good tips about setting up Sublime Text. [~jsick] suggested that we add these configuration tips to the Developer Documentation. http://developer.lsst.io/en/latest/#part-tools Both want to include info about recommended packages, but also the linter configurations to support the DM styles. I paste in here various helpful parts from the HipChat Software Development room discussion of this. Both verbatim, and summarized. 1. Install {{Package Control}} 2. Packages: {{Git}}, {{GitGutter}}, {{SideBarEnhancements}}, {{SublimeLinter}}, {{SublimeLinter-flake8}}, {{SublimeLinter-html-tidy}}, {{SumNumbers}}, {{Gist}}, {{BracketHighlighter}}, {{TrailingSpaces}}, {{Trimmer}}, {{OmniMarkupPreviewer}}, {{ReStructuredTextImproved}}, {{MarkDown Editing}}, {{Colorsublime}} 3. Themes: {{Sunburst}} color scheme * VIM users: {{Vintageous}} + Mac OS X configuration: {{defaults write com.sublimetext.3 ApplePressAndHoldEnabled -bool false}} so that holding down 'j' moves downward. Note that {{Vintageous}} is not a complete implementation of {{vim}}, but it at least allows enough basics so that one doesn't go crazy switching back and forth. link the {{subl}} command to {{/usr/local/bin}} Quick Tips: ""option-select (to select blocks) and select something then cmd-D are both extremely useful for modifying lots of things at once."" ""Similarly, ctrl-shift-up/down arrow."" ""cmd-click on multiple lines to have multiple synchronized cursors"" Configurations: 1. [~rowen]'s SublimeText Preferences file: https://jira.lsstcorp.org/secure/attachment/27846/Preferences.sublime-settings 2. Configuration {{flake8}} so that it works in the linting can take a bit of work if {{flake8}} isn't in your default path. See SublimeLinter.sublime-settings attachment for [~rowen]'s configuration: https://jira.lsstcorp.org/secure/attachment/27845/SublimeLinter.sublime-settings The above are useful, but we'll need someone to detail the linter stuff more."
Enable websocket client to pickup channel parameter from url send websocket channel information via url.  keep channel information on browser reload.    This is needed for Firefly Python API and external (when Firefly viewer is invoked trough URL) API.  ,1,DM-6083,datamanagement,enable websocket client pickup channel parameter url send websocket channel information url channel information browser reload need firefly python api external firefly viewer invoke trough url api,Enable websocket client to pickup channel parameter from url send websocket channel information via url. keep channel information on browser reload. This is needed for Firefly Python API and external (when Firefly viewer is invoked trough URL) API.
"JSON Schema for metric data from validate_drp to be ingested by the QA Dashboard app A well-defined JSON schema is needed for {{validate_drp}}’s JSON output so that it can be easily, and consistently ingested into the QA Database. The schema will also make the JSON more self-describing, and potentially useful for other tools to build upon as well.    The schema is being drafted in a thread at https://community.lsst.org/t/json-schema-for-squash/777?u=jsick. Once an informal consensus is reached the schema will be implemented in {{validate_drp}} on this ticket.",8,DM-6086,datamanagement,json schema metric datum validate_drp ingest qa dashboard app define json schema need validate_drp json output easily consistently ingest qa database schema json self describe potentially useful tool build schema draft thread https://community.lsst.org/t/json-schema-for-squash/777?u=jsick informal consensus reach schema implement validate_drp ticket,"JSON Schema for metric data from validate_drp to be ingested by the QA Dashboard app A well-defined JSON schema is needed for {{validate_drp}} s JSON output so that it can be easily, and consistently ingested into the QA Database. The schema will also make the JSON more self-describing, and potentially useful for other tools to build upon as well. The schema is being drafted in a thread at https://community.lsst.org/t/json-schema-for-squash/777?u=jsick. Once an informal consensus is reached the schema will be implemented in {{validate_drp}} on this ticket."
"jenkins job to execute validate_drp and push results to qa dashboard This is the initial jenkins job that ""ties"" all the components together.    It needs to:    * execute validate_drp  * push metadata about the jenkins build to qa dashboard  * push the validate_drp metrics to qa dashboard",7,DM-6087,datamanagement,jenkin job execute validate_drp push result qa dashboard initial jenkin job tie component need execute validate_drp push metadata jenkin build qa dashboard push validate_drp metric qa dashboard,"jenkins job to execute validate_drp and push results to qa dashboard This is the initial jenkins job that ""ties"" all the components together. It needs to: * execute validate_drp * push metadata about the jenkins build to qa dashboard * push the validate_drp metrics to qa dashboard"
Use fixed width integer types from std instead of boost The following fixed width integer types are used in the stack:    * {{boost::int16_t}}  * {{boost::int32_t}}  * {{boost::int64_t}}  * {{boost::int8_t}}  * {{boost::uint16_t}}  * {{boost::uint32_t}}  * {{boost::uint64_t}}  * {{boost::uint8_t}}    This ticket aims to replace them with their equivalents from {{cstdint}}.,1,DM-6089,datamanagement,use fix width integer type std instead boost follow fix width integer type stack boost::int16_t boost::int32_t boost::int64_t boost::int8_t boost::uint16_t boost::uint32_t boost::uint64_t boost::uint8_t ticket aim replace equivalent cstdint,Use fixed width integer types from std instead of boost The following fixed width integer types are used in the stack: * {{boost::int16_t}} * {{boost::int32_t}} * {{boost::int64_t}} * {{boost::int8_t}} * {{boost::uint16_t}} * {{boost::uint32_t}} * {{boost::uint64_t}} * {{boost::uint8_t}} This ticket aims to replace them with their equivalents from {{cstdint}}.
"draw a diagram of DRP data flow Study Jim Bosch's diagrams and descriptions (Parallelization in Data Release Production, Data Release Production Top-Level Overview), consider inputs/outputs of high level pipelines and parallelization of the DRP, draw a diagram to illustrate the data flow. ",4,DM-6098,datamanagement,draw diagram drp datum flow study jim bosch diagram description parallelization data release production data release production level overview consider input output high level pipeline parallelization drp draw diagram illustrate data flow,"draw a diagram of DRP data flow Study Jim Bosch's diagrams and descriptions (Parallelization in Data Release Production, Data Release Production Top-Level Overview), consider inputs/outputs of high level pipelines and parallelization of the DRP, draw a diagram to illustrate the data flow."
"Improve afw.table Astropy view support DM-5641 completed the first version of Astropy view support, but there is still room for improvement:   - Make {{Footprint}} s in {{SourceCatalog}} s available as a {{dtype=object}} column.  Same for {{Psf}} , {{Wcs}} , {{Calib}} in {{ExposureCatalog}}.   - Use Astropy's coordinate classes for Coord fields (may require an RFC to determine how much we want to use Astropy's coordinate classes).  ",4,DM-6099,datamanagement,improve afw.table astropy view support dm-5641 complete version astropy view support room improvement footprint sourcecatalog available dtype object column psf wcs calib exposurecatalog use astropy coordinate class coord field require rfc determine want use astropy coordinate class,"Improve afw.table Astropy view support DM-5641 completed the first version of Astropy view support, but there is still room for improvement: - Make {{Footprint}} s in {{SourceCatalog}} s available as a {{dtype=object}} column. Same for {{Psf}} , {{Wcs}} , {{Calib}} in {{ExposureCatalog}}. - Use Astropy's coordinate classes for Coord fields (may require an RFC to determine how much we want to use Astropy's coordinate classes)."
"afw/tests/rgb.py fails due to .ttf files afw/tests/rgb.py fails for me with the below error. We likely shouldn't be trying to track system resources like fonts, as we don't have any control over them.    {code}  [2016-05-12T19:46:12.528961Z] Failed test output:  [2016-05-12T19:46:12.536029Z] tests/rgb.py  [2016-05-12T19:46:12.536057Z]  [2016-05-12T19:46:12.536070Z] ...s......ss...F.  [2016-05-12T19:46:12.536106Z] ======================================================================  [2016-05-12T19:46:12.536138Z] FAIL: testFileDescriptorLeaks (lsst.utils.tests.MemoryTestCase)  [2016-05-12T19:46:12.536173Z] ----------------------------------------------------------------------  [2016-05-12T19:46:12.536192Z] Traceback (most recent call last):  [2016-05-12T19:46:12.536261Z]   File ""/Users/parejkoj/lsst/lsstsw/stack/DarwinX86/utils/2016_01.0-4-g52f464f/python/lsst/utils/tests.py"", line 134, in testFileDescriptorLeaks  [2016-05-12T19:46:12.536330Z]     self.fail(""Failed to close %d file%s"" % (len(diff), ""s"" if len(diff) != 1 else """"))  [2016-05-12T19:46:12.536352Z] AssertionError: Failed to close 2 files  [2016-05-12T19:46:12.536356Z]  [2016-05-12T19:46:12.536391Z] ----------------------------------------------------------------------  [2016-05-12T19:46:12.536404Z] Ran 17 tests in 3.451s  [2016-05-12T19:46:12.536407Z]  [2016-05-12T19:46:12.536424Z] FAILED (failures=1, skipped=3)  [2016-05-12T19:46:12.536445Z] File open: /Library/Fonts/NISC18030.ttf  [2016-05-12T19:46:12.536479Z] File open: /System/Library/Fonts/Apple Color Emoji.ttf  [2016-05-12T19:46:12.536495Z] The following tests failed:  [2016-05-12T19:46:12.539928Z] /Users/parejkoj/lsst/lsstsw/build/afw/tests/.tests/rgb.py.failed  [2016-05-12T19:46:12.540060Z] 1 tests failed  {code}",1,DM-6100,datamanagement,"afw test rgb.py fail .ttf file afw test rgb.py fail error likely try track system resource like font control code 2016 05 12t19:46:12.528961z fail test output 2016 05 12t19:46:12.536029z test rgb.py 2016 05 12t19:46:12.536057z 2016 05 12t19:46:12.536070z ss f. 2016 05 12t19:46:12.536106z 2016 05 12t19:46:12.536138z fail testfiledescriptorleak lsst.utils.test memorytestcase 2016 05 12t19:46:12.536173z 2016 05 12t19:46:12.536192z traceback recent 2016 05 12t19:46:12.536261z file /users parejkoj lsst lsstsw stack darwinx86 utils/2016_01.0 g52f464f python lsst util tests.py line 134 testfiledescriptorleak 2016 05 12t19:46:12.536330z self.fail(""faile close file%s len(diff len(diff 2016 05 12t19:46:12.536352z assertionerror fail close file 2016 05 12t19:46:12.536356z 2016 05 12t19:46:12.536391z 2016 05 12t19:46:12.536404z run 17 test 3.451s 2016 05 12t19:46:12.536407z 2016 05 12t19:46:12.536424z fail failures=1 skipped=3 2016 05 12t19:46:12.536445z file open /library fonts nisc18030.ttf 2016 05 12t19:46:12.536479z file open /system library fonts apple color emoji.ttf 2016 05 12t19:46:12.536495z follow test fail 2016 05 12t19:46:12.539928z /users parejkoj lsst lsstsw build afw tests/.test rgb.py.faile 2016 05 12t19:46:12.540060z test fail code","afw/tests/rgb.py fails due to .ttf files afw/tests/rgb.py fails for me with the below error. We likely shouldn't be trying to track system resources like fonts, as we don't have any control over them. {code} [2016-05-12T19:46:12.528961Z] Failed test output: [2016-05-12T19:46:12.536029Z] tests/rgb.py [2016-05-12T19:46:12.536057Z] [2016-05-12T19:46:12.536070Z] ...s......ss...F. [2016-05-12T19:46:12.536106Z] ====================================================================== [2016-05-12T19:46:12.536138Z] FAIL: testFileDescriptorLeaks (lsst.utils.tests.MemoryTestCase) [2016-05-12T19:46:12.536173Z] ---------------------------------------------------------------------- [2016-05-12T19:46:12.536192Z] Traceback (most recent call last): [2016-05-12T19:46:12.536261Z] File ""/Users/parejkoj/lsst/lsstsw/stack/DarwinX86/utils/2016_01.0-4-g52f464f/python/lsst/utils/tests.py"", line 134, in testFileDescriptorLeaks [2016-05-12T19:46:12.536330Z] self.fail(""Failed to close %d file%s"" % (len(diff), ""s"" if len(diff) != 1 else """")) [2016-05-12T19:46:12.536352Z] AssertionError: Failed to close 2 files [2016-05-12T19:46:12.536356Z] [2016-05-12T19:46:12.536391Z] ---------------------------------------------------------------------- [2016-05-12T19:46:12.536404Z] Ran 17 tests in 3.451s [2016-05-12T19:46:12.536407Z] [2016-05-12T19:46:12.536424Z] FAILED (failures=1, skipped=3) [2016-05-12T19:46:12.536445Z] File open: /Library/Fonts/NISC18030.ttf [2016-05-12T19:46:12.536479Z] File open: /System/Library/Fonts/Apple Color Emoji.ttf [2016-05-12T19:46:12.536495Z] The following tests failed: [2016-05-12T19:46:12.539928Z] /Users/parejkoj/lsst/lsstsw/build/afw/tests/.tests/rgb.py.failed [2016-05-12T19:46:12.540060Z] 1 tests failed {code}"
"implement basic oauth2 authentication for qa-dashboard Per discussion at the SQRE co-working session on Thursday, we agreed to implement minimal authentication for the MVP version of the qa dashboard as an external reverse proxy, such as https://github.com/bitly/oauth2_proxy.",6,DM-6102,datamanagement,implement basic oauth2 authentication qa dashboard discussion sqre co working session thursday agree implement minimal authentication mvp version qa dashboard external reverse proxy https://github.com/bitly/oauth2_proxy,"implement basic oauth2 authentication for qa-dashboard Per discussion at the SQRE co-working session on Thursday, we agreed to implement minimal authentication for the MVP version of the qa dashboard as an external reverse proxy, such as https://github.com/bitly/oauth2_proxy."
Implement the post_save mechanism to update bokeh sessions when new data is available In tickets/DM-5750 the bokeh python library was integrated in the squash django project. In order to exemplify its use the KPM CI chart is showing only hardcoded values for now.    In this ticket we will implement methods to read the data from the database and the post_save mechanism to update the bokeh session when new data is available.   ,4,DM-6105,datamanagement,implement post_save mechanism update bokeh session new datum available ticket dm-5750 bokeh python library integrate squash django project order exemplify use kpm ci chart show hardcode value ticket implement method read datum database post_save mechanism update bokeh session new datum available,Implement the post_save mechanism to update bokeh sessions when new data is available In tickets/DM-5750 the bokeh python library was integrated in the squash django project. In order to exemplify its use the KPM CI chart is showing only hardcoded values for now. In this ticket we will implement methods to read the data from the database and the post_save mechanism to update the bokeh session when new data is available.
"color map in visualization the four new colormaps introduced in matplotlib last year  http://bids.github.io/colormap/    d3js cmap: http://bl.ocks.org/mbostock/3289530    D3 supports CIELAB (Lab) and CIELCH (HCL) color spaces, which are designed for humans rather than computers. http://bl.ocks.org/mbostock/3014589    ",4,DM-6106,datamanagement,color map visualization new colormap introduce matplotlib year http://bids.github.io/colormap/ d3js cmap http://bl.ocks.org/mbostock/3289530 d3 support cielab lab cielch hcl color space design human computer http://bl.ocks.org/mbostock/3014589,"color map in visualization the four new colormaps introduced in matplotlib last year http://bids.github.io/colormap/ d3js cmap: http://bl.ocks.org/mbostock/3289530 D3 supports CIELAB (Lab) and CIELCH (HCL) color spaces, which are designed for humans rather than computers. http://bl.ocks.org/mbostock/3014589"
"More work on firefly viewer layout control More work needs to be done on the triview layout controlling:    * When there is a table with image meta data is loaded, the images need to show with the meta data tab selected  * When any data is pushed then drop downs needs to close  * when a table a catalog table is loaded and there are no plots then then the tri-view should be up with the coverage tab selected. When there is plots then the coverage tab should not be selected.  * we need a way to remove load a table and then only see the table, same with xy-plots  * catalog and image meta data are determined by looking at the data.  However, we might need this logic in a single function  * when a table is loaded and we cannot determine what type it is then the table and the xyplots only should some up.  * When all data is deleted the default tab should open.  (in IRSAViewer case the is the select image panel)",8,DM-6108,datamanagement,work firefly view layout control work need triview layout control table image meta datum load image need meta datum tab select datum push drop down need close table catalog table load plot tri view coverage tab select plot coverage tab select need way remove load table table xy plot catalog image meta datum determine look datum need logic single function table load determine type table xyplot datum delete default tab open irsaviewer case select image panel,"More work on firefly viewer layout control More work needs to be done on the triview layout controlling: * When there is a table with image meta data is loaded, the images need to show with the meta data tab selected * When any data is pushed then drop downs needs to close * when a table a catalog table is loaded and there are no plots then then the tri-view should be up with the coverage tab selected. When there is plots then the coverage tab should not be selected. * we need a way to remove load a table and then only see the table, same with xy-plots * catalog and image meta data are determined by looking at the data. However, we might need this logic in a single function * when a table is loaded and we cannot determine what type it is then the table and the xyplots only should some up. * When all data is deleted the default tab should open. (in IRSAViewer case the is the select image panel)"
"Browsers should cache editions for a shorter time period than Fastly Currently we set {{Cache-Control: max-age=31536000}} so that Fastly caches uploads from LTD Mason for a year on its POPs. This has the side-effect of also having browsers potentially cache documentation on the client for up to a year. In practice, browsers churn through their cache space more quickly, but I've noticed that Safari has no cap on its cache space, and therefore can hold onto pages for a long time.    The solution is to set a {{Surrogate-Control}} max age to 1 year, and have {{Cache-Control: max-age=0, private, must-revalidate}}. This will be done on LTD Keeper during the copy phase of a build into an edition (since it is reasonable for a client to cache a build forever), but then give us the flexibility to update an edition instantly.    In the future we may want a more nuanced solution where CSS and JavaScript, for example, are cached longer on the browser.",1,DM-6111,datamanagement,browser cache edition short time period fastly currently set cache control max age=31536000 fastly cache upload ltd mason year pop effect have browser potentially cache documentation client year practice browser churn cache space quickly notice safari cap cache space hold page long time solution set surrogate control max age year cache control max age=0 private revalidate ltd keeper copy phase build edition reasonable client cache build forever flexibility update edition instantly future want nuance solution css javascript example cache long browser,"Browsers should cache editions for a shorter time period than Fastly Currently we set {{Cache-Control: max-age=31536000}} so that Fastly caches uploads from LTD Mason for a year on its POPs. This has the side-effect of also having browsers potentially cache documentation on the client for up to a year. In practice, browsers churn through their cache space more quickly, but I've noticed that Safari has no cap on its cache space, and therefore can hold onto pages for a long time. The solution is to set a {{Surrogate-Control}} max age to 1 year, and have {{Cache-Control: max-age=0, private, must-revalidate}}. This will be done on LTD Keeper during the copy phase of a build into an edition (since it is reasonable for a client to cache a build forever), but then give us the flexibility to update an edition instantly. In the future we may want a more nuanced solution where CSS and JavaScript, for example, are cached longer on the browser."
"Provide minimal documentation for meas_extensions_photometryKron Please provide a minimal level of documentation for meas_extensions_photometryKron, to include:  * A doc directory with the usual content so that docstrings get generated by Doxygen;  * A package overview;  * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{""""""!}} where necessary).  ",1,DM-6112,datamanagement,provide minimal documentation meas_extensions_photometrykron provide minimal level documentation meas_extensions_photometrykron include doc directory usual content docstring generate doxygen package overview docstring appropriate parse doxygen ie start necessary,"Provide minimal documentation for meas_extensions_photometryKron Please provide a minimal level of documentation for meas_extensions_photometryKron, to include: * A doc directory with the usual content so that docstrings get generated by Doxygen; * A package overview; * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{""""""!}} where necessary)."
"The color stretch dialog box does not work properly There are a few issues in the Color Stretch dialog box:  # When the asinh or gamma algorithm is selected,  the asinh parameters and gamma parameters are always reset to the default.  The user-entered values can not be kept and used.   # When there are two or more images, when the lower/upper range in one of the image is set, the lower/upper range in all the rest images are set to the same lower/upper range.    # The rangeValues are always reset each time when the Color Stretch dialog is open.  ",5,DM-6118,datamanagement,color stretch dialog box work properly issue color stretch dialog box asinh gamma algorithm select asinh parameter gamma parameter reset default user enter value keep image low upper range image set low upper range rest image set low upper range rangevalue reset time color stretch dialog open,"The color stretch dialog box does not work properly There are a few issues in the Color Stretch dialog box: # When the asinh or gamma algorithm is selected, the asinh parameters and gamma parameters are always reset to the default. The user-entered values can not be kept and used. # When there are two or more images, when the lower/upper range in one of the image is set, the lower/upper range in all the rest images are set to the same lower/upper range. # The rangeValues are always reset each time when the Color Stretch dialog is open."
"Remove old DipoleMeasurementAlgorithm from imageDifference.py Currently the new algorithm is run alongside the old. This ticket will deprecate the old algorithm, making the new one the default. This will be done after the new algorithm is vetted on real data (DM-5412).  It may also be blocked by the pending SFM overhaul so that it can be implemented as a standard registered plugin.",4,DM-6121,datamanagement,remove old dipolemeasurementalgorithm imagedifference.py currently new algorithm run alongside old ticket deprecate old algorithm make new default new algorithm vet real datum dm-5412 block pende sfm overhaul implement standard register plugin,"Remove old DipoleMeasurementAlgorithm from imageDifference.py Currently the new algorithm is run alongside the old. This ticket will deprecate the old algorithm, making the new one the default. This will be done after the new algorithm is vetted on real data (DM-5412). It may also be blocked by the pending SFM overhaul so that it can be implemented as a standard registered plugin."
"Build SFM housing for PSF approximation using ngmix code Build a measurement plugin which allows PSF approximation to be done using ngmix.  After consulting with Erin, it was decided that this would make use of the EM code and would produce as its output some variable number of Gaussians.  These will be turned into MultiShapeletFunction outputs.    A suitable set of configuration options and output failure flags will also be provided.",6,DM-6123,datamanagement,build sfm housing psf approximation ngmix code build measurement plugin allow psf approximation ngmix consult erin decide use em code produce output variable number gaussians turn multishapeletfunction output suitable set configuration option output failure flag provide,"Build SFM housing for PSF approximation using ngmix code Build a measurement plugin which allows PSF approximation to be done using ngmix. After consulting with Erin, it was decided that this would make use of the EM code and would produce as its output some variable number of Gaussians. These will be turned into MultiShapeletFunction outputs. A suitable set of configuration options and output failure flags will also be provided."
Testing ngmix Psf plugin with CModel Test that the ngmix PSF approx plugin works correctly in our measurement framework by testing it with CModel and comparing results with those produced with ShapeletPsfApprox.,6,DM-6124,datamanagement,testing ngmix psf plugin cmodel test ngmix psf approx plugin work correctly measurement framework test cmodel compare result produce shapeletpsfapprox,Testing ngmix Psf plugin with CModel Test that the ngmix PSF approx plugin works correctly in our measurement framework by testing it with CModel and comparing results with those produced with ShapeletPsfApprox.
"Do robustness tests of ngmix PSF approx plugin Run tests on the ngmix PSF approx plugin similar to those which were run on ShapeletPsfApprox.  We will test both for how long the plugin takes to run, and how often it fails.    Note previous report on CModel and SPA was DM-4368",6,DM-6125,datamanagement,robustness test ngmix psf approx plugin run test ngmix psf approx plugin similar run shapeletpsfapprox test long plugin take run fail note previous report cmodel spa dm-4368,"Do robustness tests of ngmix PSF approx plugin Run tests on the ngmix PSF approx plugin similar to those which were run on ShapeletPsfApprox. We will test both for how long the plugin takes to run, and how often it fails. Note previous report on CModel and SPA was DM-4368"
"LSST's version of Astrometry.net doesn't build on Ubuntu 16.04 Reproduced building on Ubuntu 16.04.    https://groups.google.com/forum/#!topic/astrometry/aDCjhfMYhpE    The current version (0.67) does build successfully standalone.    These two patches fix 0.5.0:  https://github.com/dstndstn/astrometry.net/commit/7ded70917d7cf1efa1d3af6d0da8b336ebbf9d92.diff and https://github.com/dstndstn/astrometry.net/commit/7c65b3cefc4f33c59af90c1a40b5f246002cdf28.diff  Though only the first one is needed, I believe the second one is part of the build already.",1,DM-6126,datamanagement,lsst version astrometry.net build ubuntu 16.04 reproduced building ubuntu 16.04 https://groups.google.com/forum/#!topic/astrometry/adcjhfmyhpe current version 0.67 build successfully standalone patch fix https://github.com/dstndstn/astrometry.net/commit/7ded70917d7cf1efa1d3af6d0da8b336ebbf9d92.diff https://github.com/dstndstn/astrometry.net/commit/7c65b3cefc4f33c59af90c1a40b5f246002cdf28.diff need believe second build,"LSST's version of Astrometry.net doesn't build on Ubuntu 16.04 Reproduced building on Ubuntu 16.04. https://groups.google.com/forum/#!topic/astrometry/aDCjhfMYhpE The current version (0.67) does build successfully standalone. These two patches fix 0.5.0: https://github.com/dstndstn/astrometry.net/commit/7ded70917d7cf1efa1d3af6d0da8b336ebbf9d92.diff and https://github.com/dstndstn/astrometry.net/commit/7c65b3cefc4f33c59af90c1a40b5f246002cdf28.diff Though only the first one is needed, I believe the second one is part of the build already."
"ngmix has no license ngmix does not have a license, which means we shouldn't distribute it. Work with Erin Sheldon to see if he is willing to add one.",1,DM-6127,datamanagement,ngmix license ngmix license mean distribute work erin sheldon willing add,"ngmix has no license ngmix does not have a license, which means we shouldn't distribute it. Work with Erin Sheldon to see if he is willing to add one."
"Expanded view not doing fit/fill consistently  Expanded view not doing fit/fill consistently. Sometimes is seems to fit/fill and resize it correctly, other times it stays at the zoom level.  It should always fit/fill and change zoom level with resize when in expanded mode. (unless zoom type is FORCE_STANDARD).",2,DM-6128,datamanagement,expand view fit fill consistently expand view fit fill consistently fit fill resize correctly time stay zoom level fit fill change zoom level resize expand mode zoom type force_standard,"Expanded view not doing fit/fill consistently Expanded view not doing fit/fill consistently. Sometimes is seems to fit/fill and resize it correctly, other times it stays at the zoom level. It should always fit/fill and change zoom level with resize when in expanded mode. (unless zoom type is FORCE_STANDARD)."
"mpi4py does not compile under Yosemite due to hardcoded MACOSX_DEPLOYMENT_TARGET {{mpi4py}} build on Yosemite (Mac OS X 10.10) fails with   {code}  _build.log:[2016-05-17T16:51:55.847161Z] error: $MACOSX_DEPLOYMENT_TARGET mismatch: now ""10.9"" but ""10.10"" during configure  {code}    For details see attached build log.    The {{MACOSX_DEPLOYMENT_TARGET}} is being set in {{ups/eupspkg.cfg.sh}}    {code}  [serenity mpi4py] cat ups/eupspkg.cfg.sh  # If MACOSX_DEPLOYMENT_TARGET is not set, we force it to be at least 10.9  # (Mavericks). This is the earliest version of OS X expected to work with  # release 11 of the LSST stack.  # This works around DM-5409, wherein mpi4py was attempting to use an OS X 10.5  # SDK, based on querying Anaconda, and failing.  export MACOSX_DEPLOYMENT_TARGET=${MACOSX_DEPLOYMENT_TARGET:-10.9}  {code}    What is it that is supposed to be setting {{MACOSX_DEPLOYMENT_TARGET}}?  And why is it not set at the time when {{ups/eupspkg.cfg.sh}} is run, but is set to 10.10 by the time the actually compilation is done?   ",1,DM-6133,datamanagement,mpi4py compile yosemite hardcode macosx_deployment_target mpi4py build yosemite mac os 10.10 fail code build.log:[2016 05 17t16:51:55.847161z error macosx_deployment_target mismatch 10.9 10.10 configure code detail attach build log macosx_deployment_target set ups eupspkg.cfg.sh code serenity mpi4py cat ups eupspkg.cfg.sh macosx_deployment_target set force 10.9 mavericks early version os expect work release 11 lsst stack work dm-5409 mpi4py attempt use os 10.5 sdk base query anaconda fail export macosx_deployment_target=${macosx_deployment_target:-10.9 code suppose set macosx_deployment_target set time ups eupspkg.cfg.sh run set 10.10 time actually compilation,"mpi4py does not compile under Yosemite due to hardcoded MACOSX_DEPLOYMENT_TARGET {{mpi4py}} build on Yosemite (Mac OS X 10.10) fails with {code} _build.log:[2016-05-17T16:51:55.847161Z] error: $MACOSX_DEPLOYMENT_TARGET mismatch: now ""10.9"" but ""10.10"" during configure {code} For details see attached build log. The {{MACOSX_DEPLOYMENT_TARGET}} is being set in {{ups/eupspkg.cfg.sh}} {code} [serenity mpi4py] cat ups/eupspkg.cfg.sh # If MACOSX_DEPLOYMENT_TARGET is not set, we force it to be at least 10.9 # (Mavericks). This is the earliest version of OS X expected to work with # release 11 of the LSST stack. # This works around DM-5409, wherein mpi4py was attempting to use an OS X 10.5 # SDK, based on querying Anaconda, and failing. export MACOSX_DEPLOYMENT_TARGET=${MACOSX_DEPLOYMENT_TARGET:-10.9} {code} What is it that is supposed to be setting {{MACOSX_DEPLOYMENT_TARGET}}? And why is it not set at the time when {{ups/eupspkg.cfg.sh}} is run, but is set to 10.10 by the time the actually compilation is done?"
Fix style of catalog panel   finish up DM-5388 ticket by fixing the UI style of the panel,4,DM-6134,datamanagement,fix style catalog panel finish dm-5388 ticket fix ui style panel,Fix style of catalog panel finish up DM-5388 ticket by fixing the UI style of the panel
Review and connect validation part to the input area field component Catalog panel (DM-5388) needed an input area for polygon input search but it needs a review and connect the validation reducer to it.,6,DM-6136,datamanagement,review connect validation input area field component catalog panel dm-5388 need input area polygon input search need review connect validation reducer,Review and connect validation part to the input area field component Catalog panel (DM-5388) needed an input area for polygon input search but it needs a review and connect the validation reducer to it.
"Change Fields groups to handle other actions better The fields group can be out of sync with actions if they are trying to use store data when that actual value is changes.  This is a classic side-effect issue.  It can be solved with sagas.    Our current example.  The color panels updating from the plot when the activePlotId changes.    More to do:  * field groups need a sega to more effective respond to out side actions  * the dispatchChangeFieldGroup needs better, more documented parameters  * update multiple fields at the same time.  * should we have the field group support reset to init state? probably not, but look into it.  * change init values?  * Check example dialog and see if the large/smaller example is validating correctly.",2,DM-6138,datamanagement,change fields group handle action well field group sync action try use store datum actual value change classic effect issue solve sagas current example color panel update plot activeplotid change field group need sega effective respond action dispatchchangefieldgroup need well document parameter update multiple field time field group support reset init state probably look change init value check example dialog large small example validate correctly,"Change Fields groups to handle other actions better The fields group can be out of sync with actions if they are trying to use store data when that actual value is changes. This is a classic side-effect issue. It can be solved with sagas. Our current example. The color panels updating from the plot when the activePlotId changes. More to do: * field groups need a sega to more effective respond to out side actions * the dispatchChangeFieldGroup needs better, more documented parameters * update multiple fields at the same time. * should we have the field group support reset to init state? probably not, but look into it. * change init values? * Check example dialog and see if the large/smaller example is validating correctly."
Change server side hardcopy code to work better with the non-GWT call The server hard to make a hard copy now takes a StaticDrawInfo object.  We want to use only a region array.  Change the server side to support this.,2,DM-6139,datamanagement,change server hardcopy code work well non gwt server hard hard copy take staticdrawinfo object want use region array change server support,Change server side hardcopy code to work better with the non-GWT call The server hard to make a hard copy now takes a StaticDrawInfo object. We want to use only a region array. Change the server side to support this.
Produce tech note describing detailed project management procedures Write a technical note describing the detailed project management procedures derived by the pmp-wg. Source material is [~jbecla]'s document at https://github.com/lsst/ldm-pmt/.,8,DM-6140,datamanagement,produce tech note describe detailed project management procedure write technical note describe detailed project management procedure derive pmp wg source material ~jbecla document https://github.com/lsst/ldm-pmt/.,Produce tech note describing detailed project management procedures Write a technical note describing the detailed project management procedures derived by the pmp-wg. Source material is [~jbecla]'s document at https://github.com/lsst/ldm-pmt/.
"Drawing layer improvement to handle mouse selection Drawing layers are not handling and sharing the mouse quite  right.  Also the mechanism to determine to is priority for the mouse needs work as well. This is all necessary to make markers work correctly, since every marker is an individual drawing layer.     Also, the draw layer utilities are all in PlotViewUtil.js.  They need to be moved to something closer to the draw layers.",4,DM-6141,datamanagement,draw layer improvement handle mouse selection drawing layer handle share mouse right mechanism determine priority mouse need work necessary marker work correctly marker individual drawing layer draw layer utility plotviewutil.js need move close draw layer,"Drawing layer improvement to handle mouse selection Drawing layers are not handling and sharing the mouse quite right. Also the mechanism to determine to is priority for the mouse needs work as well. This is all necessary to make markers work correctly, since every marker is an individual drawing layer. Also, the draw layer utilities are all in PlotViewUtil.js. They need to be moved to something closer to the draw layers."
Client side Hardcopy support for png with drawing layer overlay Add all hard copy support so we can create a png with all the overlays.,6,DM-6142,datamanagement,client hardcopy support png draw layer overlay add hard copy support create png overlay,Client side Hardcopy support for png with drawing layer overlay Add all hard copy support so we can create a png with all the overlays.
jenkins/qa terraform destroy fails if there is an existing rds final snapshot AWS appears to prevent the overwrite of an existing final rds snapshot.  This scenario may arise when creating/destroying a dev env multiple times.  This can be avoided by disabling the final snapshot when destroying an rds instance.  One way to resolve this would be to add a terraform var to signal this is his is a development env.,2,DM-6145,datamanagement,jenkin qa terraform destroy fail exist rd final snapshot aw appear prevent overwrite exist final rd snapshot scenario arise create destroy dev env multiple time avoid disable final snapshot destroy rds instance way resolve add terraform var signal development env,jenkins/qa terraform destroy fails if there is an existing rds final snapshot AWS appears to prevent the overwrite of an existing final rds snapshot. This scenario may arise when creating/destroying a dev env multiple times. This can be avoided by disabling the final snapshot when destroying an rds instance. One way to resolve this would be to add a terraform var to signal this is his is a development env.
"Evaluate performance of dipole fitting in crowded regions There are legitimate concerns about performance of the new dipole fitting algorithm in crowded fields. This will be evaluated (on real data? if no existing data, then realistic simulated data) and contrasted with other possible alternatives. This is in response to Zejlko's concern and suggestion that DipoleFitTask should constrain only positions using pre-subtraction images, and only fit fluxes using the diffim.",8,DM-6146,datamanagement,evaluate performance dipole fit crowd region legitimate concern performance new dipole fitting algorithm crowded field evaluate real datum exist datum realistic simulated datum contrast possible alternative response zejlko concern suggestion dipolefittask constrain position pre subtraction image fit flux diffim,"Evaluate performance of dipole fitting in crowded regions There are legitimate concerns about performance of the new dipole fitting algorithm in crowded fields. This will be evaluated (on real data? if no existing data, then realistic simulated data) and contrasted with other possible alternatives. This is in response to Zejlko's concern and suggestion that DipoleFitTask should constrain only positions using pre-subtraction images, and only fit fluxes using the diffim."
"Set SUSPECT mask in ISR task and make saturation a double Implement RFC-190 and mask suspect pixels:    Add {{selectLevel}} to {{lsst.afw.cameraGeom.AmpInfoCatalog}}, as a double, and add support for it to {{lsst.ip.isr.IsrTask}}, analogously to masking saturation: iif {{suspectLevel}} is not {{nan}} then set the {{SUSPECT}} flag for pixels above the suspect level.    Also change the type of {{saturation}} in the {{AmpInfoCatalog}} from {{int}} to {{double}}, so that the existing test for {{nan}} actually works",5,DM-6147,datamanagement,set suspect mask isr task saturation double implement rfc-190 mask suspect pixel add selectlevel lsst.afw.camerageom ampinfocatalog double add support lsst.ip.isr isrtask analogously mask saturation iif suspectlevel nan set suspect flag pixel suspect level change type saturation ampinfocatalog int double exist test nan actually work,"Set SUSPECT mask in ISR task and make saturation a double Implement RFC-190 and mask suspect pixels: Add {{selectLevel}} to {{lsst.afw.cameraGeom.AmpInfoCatalog}}, as a double, and add support for it to {{lsst.ip.isr.IsrTask}}, analogously to masking saturation: iif {{suspectLevel}} is not {{nan}} then set the {{SUSPECT}} flag for pixels above the suspect level. Also change the type of {{saturation}} in the {{AmpInfoCatalog}} from {{int}} to {{double}}, so that the existing test for {{nan}} actually works"
Reduce memory utilization in mysql proxy Jon is trying to run tests with large result which kills proxy/czar because it runs out of virtual memory. Would be nice to reduce memory use and find a way not to keep query result in memory.,2,DM-6149,datamanagement,reduce memory utilization mysql proxy jon try run test large result kill proxy czar run virtual memory nice reduce memory use find way query result memory,Reduce memory utilization in mysql proxy Jon is trying to run tests with large result which kills proxy/czar because it runs out of virtual memory. Would be nice to reduce memory use and find a way not to keep query result in memory.
"Failure to fail when fallbackFilterName is None When no {{fallbackFilterName}} is set, we can get a confusing error message when failing to load a calib:  {code}  RuntimeError: Unable to retrieve dark for {'filter': 'U', 'date': '2016-05-12T02:58:56.591', 'ccd': 0, 'basename': '2016-05-12skyflats_02', 'object': 'FLAT', 'visit': 883, 'expTime': 20.0006, 'channel': 16} and no fallback filter specified: Unknown value type for filter: <type 'NoneType'>  {code}  This is unrelated to the calib load failure, and merely reflects the fact that {{fallbackFilterName=None}}.",1,DM-6151,datamanagement,failure fail fallbackfiltername fallbackfiltername set confusing error message fail load calib code runtimeerror unable retrieve dark filter date 2016 05 12t02:58:56.591 ccd basename 2016 05 12skyflats_02 object flat visit 883 exptime 20.0006 channel 16 fallback filter specify unknown value type filter code unrelated calib load failure merely reflect fact fallbackfiltername,"Failure to fail when fallbackFilterName is None When no {{fallbackFilterName}} is set, we can get a confusing error message when failing to load a calib: {code} RuntimeError: Unable to retrieve dark for {'filter': 'U', 'date': '2016-05-12T02:58:56.591', 'ccd': 0, 'basename': '2016-05-12skyflats_02', 'object': 'FLAT', 'visit': 883, 'expTime': 20.0006, 'channel': 16} and no fallback filter specified: Unknown value type for filter:  {code} This is unrelated to the calib load failure, and merely reflects the fact that {{fallbackFilterName=None}}."
Attend SciPi WG meeting Attend the Science Pipelines Working Group meeting in Seattle.,8,DM-6155,datamanagement,attend scipi wg meeting attend science pipelines working group meeting seattle,Attend SciPi WG meeting Attend the Science Pipelines Working Group meeting in Seattle.
Attend SciPi WG phonecon Attend the Science Pipelines working group meeting by video con.,3,DM-6156,datamanagement,attend scipi wg phonecon attend science pipelines work group meeting video con,Attend SciPi WG phonecon Attend the Science Pipelines working group meeting by video con.
Flesh out MOPS work Work with Lynne Jones and Colin Slater to flesh out the high level MOPS design to a point where we can plane the risk associated with each component.,3,DM-6157,datamanagement,flesh mop work work lynne jones colin slater flesh high level mop design point plane risk associate component,Flesh out MOPS work Work with Lynne Jones and Colin Slater to flesh out the high level MOPS design to a point where we can plane the risk associated with each component.
Attend SciPi WG F2F in Tucson Attend the Science Pipelines working group face to face meeting in Tucson.,3,DM-6158,datamanagement,attend scipi wg f2f tucson attend science pipelines work group face face meeting tucson,Attend SciPi WG F2F in Tucson Attend the Science Pipelines working group face to face meeting in Tucson.
Adding an int to the end of CzarConfig causes a segfault error. Adding and int to the private members of CzarConfig causes a segfault when Czar::Czar() calls LOG_CONFIG(logConfig);. gdb shows logConfig is the correct string value but somewhere in log4cxx something is corrupted and causes a segfault.    Adding an int to the end of Czar (the class where CzarConfig is instantiated) does not cause the issue. ,6,DM-6160,datamanagement,add int end czarconfig cause segfault error add int private member czarconfig cause segfault czar::czar call log_config(logconfig gdb show logconfig correct string value log4cxx corrupt cause segfault add int end czar class czarconfig instantiate cause issue,Adding an int to the end of CzarConfig causes a segfault error. Adding and int to the private members of CzarConfig causes a segfault when Czar::Czar() calls LOG_CONFIG(logConfig);. gdb shows logConfig is the correct string value but somewhere in log4cxx something is corrupted and causes a segfault. Adding an int to the end of Czar (the class where CzarConfig is instantiated) does not cause the issue.
"Investigate how the diffim decorrelation correction works for the case of non-uniform PSFs and noise It is not clear whether, or how, the L(ZOGY) post-convolution kernel (PCK; see DM-5914) will work for non-uniform PSFs or noise/variance. This will be investigated using the simple implementation from DM-5914.     Tasks:  1. Determine whether variation in the PCK across the field is significant enough to matter for typical LSST images  2. If it does matter, investigate options for performing interpolation of the PCK across the field, or via calculating the PCK across the field from the spatially-varying matching kernel. ",8,DM-6162,datamanagement,investigate diffim decorrelation correction work case non uniform psf noise clear l(zogy post convolution kernel pck dm-5914 work non uniform psf noise variance investigate simple implementation dm-5914 task determine variation pck field significant matter typical lsst image matter investigate option perform interpolation pck field calculate pck field spatially vary matching kernel,"Investigate how the diffim decorrelation correction works for the case of non-uniform PSFs and noise It is not clear whether, or how, the L(ZOGY) post-convolution kernel (PCK; see DM-5914) will work for non-uniform PSFs or noise/variance. This will be investigated using the simple implementation from DM-5914. Tasks: 1. Determine whether variation in the PCK across the field is significant enough to matter for typical LSST images 2. If it does matter, investigate options for performing interpolation of the PCK across the field, or via calculating the PCK across the field from the spatially-varying matching kernel."
"Time AST and compare to our WCS code Time TAN-SIP for our code and for AST, in order to get a sense of the performance impact of switching to AST for our WCS implementation.",3,DM-6166,datamanagement,time ast compare wcs code time tan sip code ast order sense performance impact switch ast wcs implementation,"Time AST and compare to our WCS code Time TAN-SIP for our code and for AST, in order to get a sense of the performance impact of switching to AST for our WCS implementation."
"Create DMBP project in jira Create a new project in JIRA (DM Baseline Plan), spec provided here: https://confluence.lsstcorp.org/display/DM/ProjMgmtWG%3A+The+New+DLP  I am sure we will fine tune it, but it is a (hopefully good) start.",2,DM-6167,datamanagement,create dmbp project jira create new project jira dm baseline plan spec provide https://confluence.lsstcorp.org/display/dm/projmgmtwg%3a+the+new+dlp sure fine tune hopefully good start,"Create DMBP project in jira Create a new project in JIRA (DM Baseline Plan), spec provided here: https://confluence.lsstcorp.org/display/DM/ProjMgmtWG%3A+The+New+DLP I am sure we will fine tune it, but it is a (hopefully good) start."
"Increase memory locked amount in container In order to lock memory, the memory locking limit within the container for the qserv worker needs to be raised. My understanding is the container uses whatever is the host setting so the limit has to be set for the container user and whatever the user is inside the container. The particular limits is:    memorylocked 64 kbytes    notice that by default it's 64K. That needs to be raised to say 75% of the real machine size. I wouldn't make it unlimited as a memlock mistake may crash the whole machine. The limits are specified in ""/etc/security/limits.conf"". You will know that you are successul when you ssh into the container as the qserv worker user and the ""limit"" command tell you have can lock lots of memory.    We would also set the CAP_IPC_LOCK privilege but setting the soft/hard limit above should be good enough. So, let's start with that. ",2,DM-6177,datamanagement,increase memory lock container order lock memory memory lock limit container qserv worker need raise understanding container use host set limit set container user user inside container particular limit memorylocke 64 kbyte notice default 64k. need raise 75 real machine size unlimited memlock mistake crash machine limit specify /etc security limits.conf know successul ssh container qserv worker user limit command tell lock lot memory set cap_ipc_lock privilege set soft hard limit good let start,"Increase memory locked amount in container In order to lock memory, the memory locking limit within the container for the qserv worker needs to be raised. My understanding is the container uses whatever is the host setting so the limit has to be set for the container user and whatever the user is inside the container. The particular limits is: memorylocked 64 kbytes notice that by default it's 64K. That needs to be raised to say 75% of the real machine size. I wouldn't make it unlimited as a memlock mistake may crash the whole machine. The limits are specified in ""/etc/security/limits.conf"". You will know that you are successul when you ssh into the container as the qserv worker user and the ""limit"" command tell you have can lock lots of memory. We would also set the CAP_IPC_LOCK privilege but setting the soft/hard limit above should be good enough. So, let's start with that."
"Add eups version for Qserv for stack package version For stack packaged Qserv version, version needs to be retrieved and added to monitor.yaml using next command:    dev@clrinfopc04:~/src/qserv$ eups list qserv -s -V  LOCAL:/home/dev/src/qserv    Indeed, pkgautorversion doesn't work in this case, I.e. with no git repos",2,DM-6178,datamanagement,add eup version qserv stack package version stack package qserv version version need retrieve add monitor.yaml command dev@clrinfopc04:~/src qserv$ eup list qserv -s -v local:/home dev src qserv pkgautorversion work case i.e. git repos,"Add eups version for Qserv for stack package version For stack packaged Qserv version, version needs to be retrieved and added to monitor.yaml using next command: dev@clrinfopc04:~/src/qserv$ eups list qserv -s -V LOCAL:/home/dev/src/qserv Indeed, pkgautorversion doesn't work in this case, I.e. with no git repos"
"reST roles for JIRA References Add {{:jira:`DM-1234`}}-type roles to documenteer so that JIRA tickets, epics and RFCs can be referenced easily from all of our Sphinx-based projects.",2,DM-6181,datamanagement,rest role jira references add jira:`dm-1234`}}-type role documenteer jira ticket epic rfc reference easily sphinx base project,"reST roles for JIRA References Add {{:jira:`DM-1234`}}-type roles to documenteer so that JIRA tickets, epics and RFCs can be referenced easily from all of our Sphinx-based projects."
sourceSelector needs a schema in ImageDifferenceTask imageDifference.py crashes with a vague error on initializing the sourceSelector task. The problem turns out to be that sourceSelector needs a schema passed in.,1,DM-6183,datamanagement,sourceselector need schema imagedifferencetask imagedifference.py crash vague error initialize sourceselector task problem turn sourceselector need schema pass,sourceSelector needs a schema in ImageDifferenceTask imageDifference.py crashes with a vague error on initializing the sourceSelector task. The problem turns out to be that sourceSelector needs a schema passed in.
Update LSE-75 DM-TCS ICD Submit an LCR to update LSE-75 to reflect current thinking on telemetry feedback from DM to the TCS.,5,DM-6192,datamanagement,update lse-75 dm tcs icd submit lcr update lse-75 reflect current thinking telemetry feedback dm tcs,Update LSE-75 DM-TCS ICD Submit an LCR to update LSE-75 to reflect current thinking on telemetry feedback from DM to the TCS.
Update LSE-72 DM-OCS ICD Submit an LCR to update LSE-72 to reflect changes discovered by work at NCSA to support early integration tests.,5,DM-6193,datamanagement,update lse-72 dm ocs icd submit lcr update lse-72 reflect change discover work ncsa support early integration test,Update LSE-72 DM-OCS ICD Submit an LCR to update LSE-72 to reflect changes discovered by work at NCSA to support early integration tests.
"Update LSE-68 DM-Camera DAQ ICD Submit an LCR to update LSE-68 to reflect understandings developed between Mike Huffer and NCSA about the interface, including the image deletion policy for the camera data buffer.",5,DM-6194,datamanagement,update lse-68 dm camera daq icd submit lcr update lse-68 reflect understanding develop mike huffer ncsa interface include image deletion policy camera datum buffer,"Update LSE-68 DM-Camera DAQ ICD Submit an LCR to update LSE-68 to reflect understandings developed between Mike Huffer and NCSA about the interface, including the image deletion policy for the camera data buffer."
Provide input to Commissioning Plan Provide input based on understanding of the DM interfaces to the Commissioning Plan being developed by Chuck Claver.,5,DM-6195,datamanagement,provide input commissioning plan provide input base understanding dm interface commissioning plan develop chuck claver,Provide input to Commissioning Plan Provide input based on understanding of the DM interfaces to the Commissioning Plan being developed by Chuck Claver.
Update LSE-76 Summit ICD Submit an LCR to update LSE-76 based on Summit rack and power needs obtained from Ron Lambert.,2,DM-6197,datamanagement,update lse-76 summit icd submit lcr update lse-76 base summit rack power need obtain ron lambert,Update LSE-76 Summit ICD Submit an LCR to update LSE-76 based on Summit rack and power needs obtained from Ron Lambert.
Resource load F16 part II Resource load for second half of F16    (SP estimate from first half),6,DM-6201,datamanagement,resource load f16 ii resource load second half f16 sp estimate half,Resource load F16 part II Resource load for second half of F16 (SP estimate from first half)
Build/CI/Deploy improvements requested by the Architecture Team   Build/CI/Deploy improvements requested by the Architecture Team prioritised by request from the DM System Architect.     They cover predominantly support for the Python3 support. ,4,DM-6205,datamanagement,build ci deploy improvement request architecture team build ci deploy improvement request architecture team prioritise request dm system architect cover predominantly support python3 support,Build/CI/Deploy improvements requested by the Architecture Team Build/CI/Deploy improvements requested by the Architecture Team prioritised by request from the DM System Architect. They cover predominantly support for the Python3 support.
"CI Improvements: Jenkins 2 upgrade etc   This epic covers a timeboxed maintainance of the Jenkins-based CI system, including the Jenkins 2 upgrade as well as the required updates to the Jenkins-puppet module. It also may include work done as part of DM-6204 brought over to the apps CI service. ",8,DM-6206,datamanagement,ci improvements jenkins upgrade etc epic cover timeboxed maintainance jenkins base ci system include jenkins upgrade require update jenkins puppet module include work dm-6204 bring app ci service,"CI Improvements: Jenkins 2 upgrade etc This epic covers a timeboxed maintainance of the Jenkins-based CI system, including the Jenkins 2 upgrade as well as the required updates to the Jenkins-puppet module. It also may include work done as part of DM-6204 brought over to the apps CI service."
CI/Build/Deploy improvements for Sims This is a timeboxed effort to prioritise support requests from the Sims group,4,DM-6207,datamanagement,ci build deploy improvement sims timeboxed effort prioritise support request sims group,CI/Build/Deploy improvements for Sims This is a timeboxed effort to prioritise support requests from the Sims group
SQuaRE services disaster recovery This is a timeboxed effort to test and improve backups and disaster recovery for SQuaRE services. It is unlikely to be sufficient in itself. ,8,DM-6208,datamanagement,square service disaster recovery timeboxed effort test improve backup disaster recovery square service unlikely sufficient,SQuaRE services disaster recovery This is a timeboxed effort to test and improve backups and disaster recovery for SQuaRE services. It is unlikely to be sufficient in itself.
"Ad-hoc developer requests This is a bucket epic for ad-hoc developer requests that cannot be postponed till the next planning cycle. In the event that it is underutilised for this purpose, it will be assigned to technical debt DM-5850",8,DM-6209,datamanagement,ad hoc developer request bucket epic ad hoc developer request postpone till planning cycle event underutilise purpose assign technical debt dm-5850,"Ad-hoc developer requests This is a bucket epic for ad-hoc developer requests that cannot be postponed till the next planning cycle. In the event that it is underutilised for this purpose, it will be assigned to technical debt DM-5850"
The grid labels are not placed in the right position when the coordinate is Ecliptic coordianates The algorithm to calculate the label position does not work well for the Ecliptic coordinate system.  The algorithm needs to be modified to work for all the coordinates.,2,DM-6239,datamanagement,grid label place right position coordinate ecliptic coordianate algorithm calculate label position work ecliptic coordinate system algorithm need modify work coordinate,The grid labels are not placed in the right position when the coordinate is Ecliptic coordianates The algorithm to calculate the label position does not work well for the Ecliptic coordinate system. The algorithm needs to be modified to work for all the coordinates.
Study the impact of having a spatially invariant decorrelation correction factor to A&L The initial implementation of the A&L + noise whitening correction term assumes a single matching kernel and variance value(s) for the image(s) in the correction kernel.  We should assess how well that assumption performs in simulated and real images.  One test would be the variance and covariance in the noise as a function of position in a set of typical images.,8,DM-6243,datamanagement,study impact have spatially invariant decorrelation correction factor a&l initial implementation a&l noise whiten correction term assume single matching kernel variance value(s image(s correction kernel assess assumption perform simulated real image test variance covariance noise function position set typical image,Study the impact of having a spatially invariant decorrelation correction factor to A&L The initial implementation of the A&L + noise whitening correction term assumes a single matching kernel and variance value(s) for the image(s) in the correction kernel. We should assess how well that assumption performs in simulated and real images. One test would be the variance and covariance in the noise as a function of position in a set of typical images.
"Assess performance of the decorrelation correction to A&L Study the performance when using the (currently, spatially invariant) correction term to the base A&L algorithm in terms of runtime, detection threshold, reported measurement noise, and false positive rate for similarly tuned versions of both the base algorithm and that with the correction applied.",6,DM-6244,datamanagement,assess performance decorrelation correction a&l study performance currently spatially invariant correction term base a&l algorithm term runtime detection threshold report measurement noise false positive rate similarly tune version base algorithm correction apply,"Assess performance of the decorrelation correction to A&L Study the performance when using the (currently, spatially invariant) correction term to the base A&L algorithm in terms of runtime, detection threshold, reported measurement noise, and false positive rate for similarly tuned versions of both the base algorithm and that with the correction applied."
"Vertical overscan off by one again In DM-5524 [~price] fixed the vertical overscan by directly editing the amp info catalogs, but didn't mark the camera generating code as bad. In DM-6147 I regenerated the files, reintroducing the problem. The problem seems to be a subtle bug in the camera generating code. Rather than try to fix it, I'll convert the fixed catalogs directly and mark the generating code as broken. [~price] will issue an RFC that suggests a better way to handle generating amp info and once that is dealt with we can come up with a more permanent fix (e.g. delete the generating code or fix it).",1,DM-6246,datamanagement,vertical overscan dm-5524 ~price fix vertical overscan directly edit amp info catalog mark camera generate code bad dm-6147 regenerate file reintroduce problem problem subtle bug camera generate code try fix convert fix catalog directly mark generate code break ~price issue rfc suggest well way handle generate amp info deal come permanent fix e.g. delete generate code fix,"Vertical overscan off by one again In DM-5524 [~price] fixed the vertical overscan by directly editing the amp info catalogs, but didn't mark the camera generating code as bad. In DM-6147 I regenerated the files, reintroducing the problem. The problem seems to be a subtle bug in the camera generating code. Rather than try to fix it, I'll convert the fixed catalogs directly and mark the generating code as broken. [~price] will issue an RFC that suggests a better way to handle generating amp info and once that is dealt with we can come up with a more permanent fix (e.g. delete the generating code or fix it)."
"DRP Outline for LDM-151 Write outline for Data Release Production section of LDM-151, using the DRP Data Flow diagram as the organizing principle.",2,DM-6247,datamanagement,drp outline ldm-151 write outline data release production section ldm-151 drp data flow diagram organize principle,"DRP Outline for LDM-151 Write outline for Data Release Production section of LDM-151, using the DRP Data Flow diagram as the organizing principle."
"DRP Top-Level Diagram and Descriptions, Draft 1 Insert the content from the DRP Data Flow diagram on Confluence into LDM-151, adjusting it to the outline developed on DM-6247.",2,DM-6248,datamanagement,drp level diagram descriptions draft insert content drp data flow diagram confluence ldm-151 adjust outline develop dm-6247,"DRP Top-Level Diagram and Descriptions, Draft 1 Insert the content from the DRP Data Flow diagram on Confluence into LDM-151, adjusting it to the outline developed on DM-6247."
"Convert DRP Top-Level Diagram to standard conventions DM-6248 adds a large, complex diagram that will need to be cleaned up and converted to use the same conventions and colors as other diagrams in LDM-151.",2,DM-6251,datamanagement,convert drp level diagram standard convention dm-6248 add large complex diagram need clean convert use convention color diagram ldm-151,"Convert DRP Top-Level Diagram to standard conventions DM-6248 adds a large, complex diagram that will need to be cleaned up and converted to use the same conventions and colors as other diagrams in LDM-151."
"Do bakeoff between the two algorithms in simplified case The original matrix inversion technique and the competing technique will likely have different sensitivities.  This should be a comparison of the algorithms, likely based on numbers of dipoles, along with performance (memory and runtime) considerations.    This will be done on 2-D images with DCR along one axis.",6,DM-6252,datamanagement,bakeoff algorithm simplify case original matrix inversion technique compete technique likely different sensitivity comparison algorithm likely base number dipole performance memory runtime consideration image dcr axis,"Do bakeoff between the two algorithms in simplified case The original matrix inversion technique and the competing technique will likely have different sensitivities. This should be a comparison of the algorithms, likely based on numbers of dipoles, along with performance (memory and runtime) considerations. This will be done on 2-D images with DCR along one axis."
Bakeoff between algorithms extended to arbitrary rotation. Redo bakeoff in the case of arbitrary rotation in DCR effect.,6,DM-6253,datamanagement,bakeoff algorithm extend arbitrary rotation redo bakeoff case arbitrary rotation dcr effect,Bakeoff between algorithms extended to arbitrary rotation. Redo bakeoff in the case of arbitrary rotation in DCR effect.
"Develop standard conventions and colors for LDM-151 diagrams We want diagrams in LDM-151 to have consistent notation and colors, and to be produced using the same tool.  Someone needs to look at the diagrams produced so far to gather requirements, decide on and document these conventions, and select the tool we'll use to produce them.",4,DM-6254,datamanagement,develop standard convention color ldm-151 diagram want diagram ldm-151 consistent notation color produce tool need look diagram produce far gather requirement decide document convention select tool use produce,"Develop standard conventions and colors for LDM-151 diagrams We want diagrams in LDM-151 to have consistent notation and colors, and to be produced using the same tool. Someone needs to look at the diagrams produced so far to gather requirements, decide on and document these conventions, and select the tool we'll use to produce them."
Improve detail for for DRP imchar/jointcal in LDM-151 Write more detailed descriptions and possibly draw a rough diagram for the single-frame processing and simultaneous calibration components of Data Release Production.    Does not necessarily involve turning this section into prose.,6,DM-6255,datamanagement,improve detail drp imchar jointcal ldm-151 write detailed description possibly draw rough diagram single frame processing simultaneous calibration component data release production necessarily involve turn section prose,Improve detail for for DRP imchar/jointcal in LDM-151 Write more detailed descriptions and possibly draw a rough diagram for the single-frame processing and simultaneous calibration components of Data Release Production. Does not necessarily involve turning this section into prose.
"Improve detail for DRP object characterization in LDM-151 Includes coadd measurement, multifit, and forced photometry.    Could be faster to write than other sections because we can lift from ""blended-measurement"" document that already exists in LDM-151 repo; could be harder because that document has already exposed a number of unresolved questions that may need to be addressed (by at least getting agreement among pundits on the best-bet approaches) before we can plan.",4,DM-6258,datamanagement,improve detail drp object characterization ldm-151 include coadd measurement multifit force photometry fast write section lift blend measurement document exist ldm-151 repo hard document expose number unresolved question need address get agreement pundit well bet approach plan,"Improve detail for DRP object characterization in LDM-151 Includes coadd measurement, multifit, and forced photometry. Could be faster to write than other sections because we can lift from ""blended-measurement"" document that already exists in LDM-151 repo; could be harder because that document has already exposed a number of unresolved questions that may need to be addressed (by at least getting agreement among pundits on the best-bet approaches) before we can plan."
"Cleanup and standardize DRP imchar/jointcal diagrams DM-6255 will produce some rough, draft-level diagrams that will need cleanup and standardization.",1,DM-6260,datamanagement,cleanup standardize drp imchar jointcal diagram dm-6255 produce rough draft level diagram need cleanup standardization,"Cleanup and standardize DRP imchar/jointcal diagrams DM-6255 will produce some rough, draft-level diagrams that will need cleanup and standardization."
"Cleanup and standardize DRP background matching, coaddition, and diffim diagrams DM-6256 will produce rough diagrams that will require cleanup and standardization.    [~ctslater] has made some suggestions for the current diagram that I'll implement on this issue, so I'm assigning it back to me.  I'll also go ahead and integrate his updated DRP overview diagram (currently on Confluence) into LDM-151 here.  ",1,DM-6261,datamanagement,cleanup standardize drp background matching coaddition diffim diagram dm-6256 produce rough diagram require cleanup standardization ~ctslater suggestion current diagram implement issue assign ahead integrate update drp overview diagram currently confluence ldm-151,"Cleanup and standardize DRP background matching, coaddition, and diffim diagrams DM-6256 will produce rough diagrams that will require cleanup and standardization. [~ctslater] has made some suggestions for the current diagram that I'll implement on this issue, so I'm assigning it back to me. I'll also go ahead and integrate his updated DRP overview diagram (currently on Confluence) into LDM-151 here."
"Cleanup and standardize DRP detection, association, and deblending diagrams DM-6257 will produce rough, draft-level diagrams that will require cleanup and standardization.",1,DM-6262,datamanagement,cleanup standardize drp detection association deblending diagram dm-6257 produce rough draft level diagram require cleanup standardization,"Cleanup and standardize DRP detection, association, and deblending diagrams DM-6257 will produce rough, draft-level diagrams that will require cleanup and standardization."
"Cleanup and standardize DRP object characterization diagrams DM-6258 will produce rough, draft-level diagrams that will require cleanup and standardization.",1,DM-6263,datamanagement,cleanup standardize drp object characterization diagram dm-6258 produce rough draft level diagram require cleanup standardization,"Cleanup and standardize DRP object characterization diagrams DM-6258 will produce rough, draft-level diagrams that will require cleanup and standardization."
"Cleanup and standardize DRP afterburners and level-3 gathering diagrams DM-6259 will produce rough, draft-level diagrams that will require cleanup and standardization.",1,DM-6264,datamanagement,cleanup standardize drp afterburner level-3 gather diagram dm-6259 produce rough draft level diagram require cleanup standardization,"Cleanup and standardize DRP afterburners and level-3 gathering diagrams DM-6259 will produce rough, draft-level diagrams that will require cleanup and standardization."
"Audit DRP LDM-151 for correct handling of chromaticity Correctly handling wavelength-dependent photometric and PSF effects is one of the biggest qualitative differences between the current state-of-the-art and what we have in mind for LSST, and that makes it easy to get wrong.  We need to make sure all steps that produce high-quality fluxes or rely on high-quality PSFs have access to object colors and a reasonable approach to using them.  ",2,DM-6265,datamanagement,audit drp ldm-151 correct handling chromaticity correctly handle wavelength dependent photometric psf effect big qualitative difference current state art mind lsst make easy wrong need sure step produce high quality flux rely high quality psf access object color reasonable approach,"Audit DRP LDM-151 for correct handling of chromaticity Correctly handling wavelength-dependent photometric and PSF effects is one of the biggest qualitative differences between the current state-of-the-art and what we have in mind for LSST, and that makes it easy to get wrong. We need to make sure all steps that produce high-quality fluxes or rely on high-quality PSFs have access to object colors and a reasonable approach to using them."
"Audit DRP LDM-151 for correct handling of crowded fields [~jbosch]'s background is in extragalactic science on high-latitude fields, and he frequently forgets to think about how algorithms will perform in crowded stellar fields.  When the first draft is complete, we should have someone experienced in that area read closely to check that he hasn't made any incorrect algorithmic assumptions as a result.",1,DM-6271,datamanagement,audit drp ldm-151 correct handling crowded field ~jbosch background extragalactic science high latitude field frequently forget think algorithm perform crowded stellar field draft complete experience area read closely check incorrect algorithmic assumption result,"Audit DRP LDM-151 for correct handling of crowded fields [~jbosch]'s background is in extragalactic science on high-latitude fields, and he frequently forgets to think about how algorithms will perform in crowded stellar fields. When the first draft is complete, we should have someone experienced in that area read closely to check that he hasn't made any incorrect algorithmic assumptions as a result."
"Investigate proper precision for afw::image::Image pixel transforms The various pixel based transforms in {{afw/src/image/Image.cc}} were converted from using {{boost::lambda}} to C++11 lambda per DM-6091.    At many places the previous implementation contained implicit casts (through {{boost::ret}}) of intermediate results to {{PixelT}} (e.g. {{float}}).  In particular this affects opperations such as {{result = l + c*r}} where {{l}} is the left hand side image, {{r}} is the right hand side image and {{c}} a {{double}} constant.  When calculated at double precision (e.g. without the casts, which are not needed with C++11 lambdas) the result is slightly different and this causes {{tests/testProcessCcd.py}} to fail on {{self.assertAlmostEqual(psfIyy, 2.17386182921239, places=7)}} which is only equal up to the fifth place.    In order to not break existing behaviour I added explicit casts to {{PixelT}} for intermediate results. But this approach is questionable as the end result will be less accurate then possible. The aim of this ticket is to decide which approach is best:    1. Calculate at full precision and modify the test case.  2. Cast intermediate results to final precision (as it is done now).  3. Do something else?",1,DM-6278,datamanagement,investigate proper precision afw::image::image pixel transform pixel base transform afw src image image.cc convert boost::lambda c++11 lambda dm-6091 place previous implementation contain implicit cast boost::ret intermediate result pixelt e.g. float particular affect opperation result c*r left hand image right hand image double constant calculate double precision e.g. cast need c++11 lambdas result slightly different cause test testprocessccd.py fail self.assertalmostequal(psfiyy 2.17386182921239 places=7 equal fifth place order break exist behaviour add explicit cast pixelt intermediate result approach questionable end result accurate possible aim ticket decide approach good calculate precision modify test case cast intermediate result final precision,"Investigate proper precision for afw::image::Image pixel transforms The various pixel based transforms in {{afw/src/image/Image.cc}} were converted from using {{boost::lambda}} to C++11 lambda per DM-6091. At many places the previous implementation contained implicit casts (through {{boost::ret}}) of intermediate results to {{PixelT}} (e.g. {{float}}). In particular this affects opperations such as {{result = l + c*r}} where {{l}} is the left hand side image, {{r}} is the right hand side image and {{c}} a {{double}} constant. When calculated at double precision (e.g. without the casts, which are not needed with C++11 lambdas) the result is slightly different and this causes {{tests/testProcessCcd.py}} to fail on {{self.assertAlmostEqual(psfIyy, 2.17386182921239, places=7)}} which is only equal up to the fifth place. In order to not break existing behaviour I added explicit casts to {{PixelT}} for intermediate results. But this approach is questionable as the end result will be less accurate then possible. The aim of this ticket is to decide which approach is best: 1. Calculate at full precision and modify the test case. 2. Cast intermediate results to final precision (as it is done now). 3. Do something else?"
Fix possible logic error in pex_policy dictionary Investigate and fix the following warning in {{pex_policy}}.    {code}  src/Dictionary.cc:312:9: warning: logical not is only applied to the left hand side of this comparison [-Wlogical-not-parentheses]      if (!getType() == Policy::POLICY) // should have checked this at a higher level          ^          ~~  src/Dictionary.cc:312:9: note: add parentheses after the '!' to evaluate the comparison first      if (!getType() == Policy::POLICY) // should have checked this at a higher level          ^           (                          )  src/Dictionary.cc:312:9: note: add parentheses around left hand side expression to silence this warning      if (!getType() == Policy::POLICY) // should have checked this at a higher level          ^          (         )  src/Dictionary.cc:312:20: warning: comparison of constant 'POLICY' (5) with expression of type 'bool' is always false [-Wtautological-constant-out-of-range-compare]      if (!getType() == Policy::POLICY) // should have checked this at a higher level  {code},1,DM-6279,datamanagement,fix possible logic error pex_policy dictionary investigate fix follow warning pex_policy code src dictionary.cc:312:9 warn logical apply left hand comparison -wlogical parenthese gettype policy::policy check high level src dictionary.cc:312:9 note add parenthesis evaluate comparison gettype policy::policy check high level src dictionary.cc:312:9 note add parenthesis left hand expression silence warning gettype policy::policy check high level src dictionary.cc:312:20 warning comparison constant policy expression type bool false -wtautological constant range compare gettype policy::policy check high level code,Fix possible logic error in pex_policy dictionary Investigate and fix the following warning in {{pex_policy}}. {code} src/Dictionary.cc:312:9: warning: logical not is only applied to the left hand side of this comparison [-Wlogical-not-parentheses] if (!getType() == Policy::POLICY) // should have checked this at a higher level ^ ~~ src/Dictionary.cc:312:9: note: add parentheses after the '!' to evaluate the comparison first if (!getType() == Policy::POLICY) // should have checked this at a higher level ^ ( ) src/Dictionary.cc:312:9: note: add parentheses around left hand side expression to silence this warning if (!getType() == Policy::POLICY) // should have checked this at a higher level ^ ( ) src/Dictionary.cc:312:20: warning: comparison of constant 'POLICY' (5) with expression of type 'bool' is always false [-Wtautological-constant-out-of-range-compare] if (!getType() == Policy::POLICY) // should have checked this at a higher level {code}
The labels in HMS formate are wrong in WebGrid The labels in HMS format no longer show hh:mm:ss anymore.  The porting introduced the bug.  ,1,DM-6280,datamanagement,label hms formate wrong webgrid label hms format long hh mm ss anymore porting introduce bug,The labels in HMS formate are wrong in WebGrid The labels in HMS format no longer show hh:mm:ss anymore. The porting introduced the bug.
Fix mismatched-tags warnings in meas_modelfit The following warnings are produced in {{meas_modelfit}}. Fix them.    {code}  include/lsst/meas/modelfit/UnitSystem.h:90:1: warning: 'LocalUnitTransform' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct LocalUnitTransform {  ^  include/lsst/meas/modelfit/Model.h:41:1: note: did you mean struct here?  class LocalUnitTransform;  ^~~~~  struct  {code},1,DM-6283,datamanagement,fix mismatch tag warning meas_modelfit follow warning produce meas_modelfit fix code include lsst meas modelfit unitsystem.h:90:1 warning localunittransform define struct previously declare class -wmismatched tag struct localunittransform include lsst meas modelfit model.h:41:1 note mean struct class localunittransform ^~~~~ struct code,Fix mismatched-tags warnings in meas_modelfit The following warnings are produced in {{meas_modelfit}}. Fix them. {code} include/lsst/meas/modelfit/UnitSystem.h:90:1: warning: 'LocalUnitTransform' defined as a struct here but previously declared as a class [-Wmismatched-tags] struct LocalUnitTransform { ^ include/lsst/meas/modelfit/Model.h:41:1: note: did you mean struct here? class LocalUnitTransform; ^~~~~ struct {code}
Remove swig special casing for obsolete boost features In {{utils}} the file {{python/lsst/p_lsstSwig.i}} defines special cases for boost features that are removed as part of DM-5880. This ticket removes the special cases.,1,DM-6284,datamanagement,remove swig special casing obsolete boost feature util file python lsst p_lsstswig.i define special case boost feature remove dm-5880 ticket remove special case,Remove swig special casing for obsolete boost features In {{utils}} the file {{python/lsst/p_lsstSwig.i}} defines special cases for boost features that are removed as part of DM-5880. This ticket removes the special cases.
Chart API: external API and the API for histogram need to support external API and the API for histogram    for now only addXYPlot and showPlot are implemented,4,DM-6285,datamanagement,chart api external api api histogram need support external api api histogram addxyplot showplot implement,Chart API: external API and the API for histogram need to support external API and the API for histogram for now only addXYPlot and showPlot are implemented
"Charts refactoring I'd like to do some cleanup, which would facilitate further development. This includes:  - moving chart related code to a separate package (now it is in visualize)  - converting components created with React.createClass to es6 classes  - reorganize store and controllers to have all charts related things under 'charts'. Now we have 'charts' for charts ui, xyplot for xyplot charts, histogram for histogram charts, and tblstats for table statistics.      Fixed bugs    * missing chart mount action, when a chart is removed and then recreated on the same table    Steps to reproduce: load a table (default scatter plot created), create histogram, delete scatter, create new scatter.       The last scatter did not produce mount action, and the plot was not tracking table changes, like filter.    * undefined shows as a label when no server call is necessary    Steps to reproduce: load table (default scatter created), clear options and choose the same columns , click apply.    ""undefined"" are shown as axis labels",4,DM-6287,datamanagement,chart refactoring like cleanup facilitate development include move chart relate code separate package visualize convert component create react.createclass es6 class reorganize store controller chart relate thing chart chart chart ui xyplot xyplot chart histogram histogram chart tblstat table statistic fix bug miss chart mount action chart remove recreate table step reproduce load table default scatter plot create create histogram delete scatter create new scatter scatter produce mount action plot track table change like filter undefined show label server necessary step reproduce load table default scatter create clear option choose column click apply undefined show axis label,"Charts refactoring I'd like to do some cleanup, which would facilitate further development. This includes: - moving chart related code to a separate package (now it is in visualize) - converting components created with React.createClass to es6 classes - reorganize store and controllers to have all charts related things under 'charts'. Now we have 'charts' for charts ui, xyplot for xyplot charts, histogram for histogram charts, and tblstats for table statistics. Fixed bugs * missing chart mount action, when a chart is removed and then recreated on the same table Steps to reproduce: load a table (default scatter plot created), create histogram, delete scatter, create new scatter. The last scatter did not produce mount action, and the plot was not tracking table changes, like filter. * undefined shows as a label when no server call is necessary Steps to reproduce: load table (default scatter created), clear options and choose the same columns , click apply. ""undefined"" are shown as axis labels"
"Chart options display Make chart options ""in-place"" popup, similar to table options for consistent look. It will also alleviate resizing, because the chart size won't need to change when options are open.",3,DM-6288,datamanagement,chart option display chart option place popup similar table option consistent look alleviate resizing chart size will need change option open,"Chart options display Make chart options ""in-place"" popup, similar to table options for consistent look. It will also alleviate resizing, because the chart size won't need to change when options are open."
Chart options reset and clear Need to support reset and clear for plot and histogram options.    Should be no-brainer after DM-6138 (update multiple fields at the same time),3,DM-6289,datamanagement,chart option reset clear need support reset clear plot histogram option brainer dm-6138 update multiple field time,Chart options reset and clear Need to support reset and clear for plot and histogram options. Should be no-brainer after DM-6138 (update multiple fields at the same time)
"Attend SBAG Meeting Meeting runs Tues 28 to Thurs 30 June; that means we'll likely lose Nate for the whole week, given travel.",8,DM-6290,datamanagement,attend sbag meeting meeting run tues 28 thurs 30 june mean likely lose nate week give travel,"Attend SBAG Meeting Meeting runs Tues 28 to Thurs 30 June; that means we'll likely lose Nate for the whole week, given travel."
"Read materials related to SBAG prep and attend telecon Read up material to prepare for SBAG, and discuss readings with Mario, Lynne, and Zeljko.",4,DM-6291,datamanagement,read material relate sbag prep attend telecon read material prepare sbag discuss reading mario lynne zeljko,"Read materials related to SBAG prep and attend telecon Read up material to prepare for SBAG, and discuss readings with Mario, Lynne, and Zeljko."
"Fix error in cmodel related to computing LinearTransforms When running cmodel in ci_hsc, the cmodel plugin throws the error:  {code}  processCcd.charImage.detectAndMeasure.measurement WARNING: Error in modelfit_CModel.measure on record 775961510756221246:     File ""src/geom/LinearTransform.cc"", line 66, in const lsst::afw::geom::LinearTransform lsst::afw::geom::LinearTransform::invert() const      Could not compute LinearTransform inverse {0}  lsst::afw::geom::SingularTransformException: 'Could not compute LinearTransform inverse'  {code}    This seems to be causing some aperture corrections to fail, as there are no sources to compute the corrections from. Investigate why this error is being thrown. If it is a bug, fix it, if the code is not handling situations it should then make the algorithm more robust.",4,DM-6293,datamanagement,fix error cmodel relate compute lineartransforms run cmodel ci_hsc cmodel plugin throw error code processccd.charimage.detectandmeasure.measurement warning error modelfit_cmodel.measure record 775961510756221246 file src geom lineartransform.cc line 66 const lsst::afw::geom::lineartransform lsst::afw::geom::lineartransform::invert const compute lineartransform inverse lsst::afw::geom::singulartransformexception compute lineartransform inverse code cause aperture correction fail source compute correction investigate error throw bug fix code handle situation algorithm robust,"Fix error in cmodel related to computing LinearTransforms When running cmodel in ci_hsc, the cmodel plugin throws the error: {code} processCcd.charImage.detectAndMeasure.measurement WARNING: Error in modelfit_CModel.measure on record 775961510756221246: File ""src/geom/LinearTransform.cc"", line 66, in const lsst::afw::geom::LinearTransform lsst::afw::geom::LinearTransform::invert() const Could not compute LinearTransform inverse {0} lsst::afw::geom::SingularTransformException: 'Could not compute LinearTransform inverse' {code} This seems to be causing some aperture corrections to fail, as there are no sources to compute the corrections from. Investigate why this error is being thrown. If it is a bug, fix it, if the code is not handling situations it should then make the algorithm more robust."
Add support for pybind11 to build system Add pybind11 as third party package to the stack. Update sconsUtils to support building with pybind11. Use daf_base DateTime to demonstrate that this works.,8,DM-6294,datamanagement,add support pybind11 build system add pybind11 party package stack update sconsutil support build pybind11 use daf_base datetime demonstrate work,Add support for pybind11 to build system Add pybind11 as third party package to the stack. Update sconsUtils to support building with pybind11. Use daf_base DateTime to demonstrate that this works.
"Unit test for coadds in pipe tasks detects too many sources The unit test for pipe tasks creates a dozen stars, to use in coaddition testing. However the results of running the test show over a hundred sources found. Investigate why the extra sources are being detected, and fix to increase the robustness of the test. If this relates to other sections of the codebase (deblender) investigate if it is appropriate to make changes to those components to make them more robust instead of creating a simple hack in the unit test.",5,DM-6295,datamanagement,unit test coadd pipe task detect source unit test pipe task create dozen star use coaddition testing result run test source find investigate extra source detect fix increase robustness test relate section codebase deblender investigate appropriate change component robust instead create simple hack unit test,"Unit test for coadds in pipe tasks detects too many sources The unit test for pipe tasks creates a dozen stars, to use in coaddition testing. However the results of running the test show over a hundred sources found. Investigate why the extra sources are being detected, and fix to increase the robustness of the test. If this relates to other sections of the codebase (deblender) investigate if it is appropriate to make changes to those components to make them more robust instead of creating a simple hack in the unit test."
"Wrap afw::geom with pybind11 The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete.",5,DM-6296,datamanagement,wrap afw::geom pybind11 generate wrapper live parallel swig wrapper ticket cover c++ wrapper python layer continue use old wrapper work stay separate branch merge master dm-6168 complete,"Wrap afw::geom with pybind11 The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete."
"Wrap afw::detection with pybind11 The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete.",5,DM-6297,datamanagement,wrap afw::detection pybind11 generate wrapper live parallel swig wrapper ticket cover c++ wrapper python layer continue use old wrapper work stay separate branch merge master dm-6168 complete,"Wrap afw::detection with pybind11 The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete."
"Wrap afw::math with pybind11 The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete.",5,DM-6298,datamanagement,wrap afw::math pybind11 generate wrapper live parallel swig wrapper ticket cover c++ wrapper python layer continue use old wrapper work stay separate branch merge master dm-6168 complete,"Wrap afw::math with pybind11 The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete."
"Write example meas_base plugin in Python During X16, new functionality was exposed to Python plugins in meas_base. Write a complete pedagogical example. It should go beyond our current pure Python plugins to demonstrate use of:    * FlagHandler;  * SafeCentroidExtractor;  * Other relevant, undocumented functionality.    This should be added to the package level documentation for meas_base, so it appears in some extended version of https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/meas_base.html.",6,DM-6301,datamanagement,write example meas_base plugin python x16 new functionality expose python plugin meas_base write complete pedagogical example current pure python plugin demonstrate use flaghandler safecentroidextractor relevant undocumented functionality add package level documentation meas_base appear extended version https://lsst-web.ncsa.illinois.edu/doxygen/x_masterdoxydoc/meas_base.html,"Write example meas_base plugin in Python During X16, new functionality was exposed to Python plugins in meas_base. Write a complete pedagogical example. It should go beyond our current pure Python plugins to demonstrate use of: * FlagHandler; * SafeCentroidExtractor; * Other relevant, undocumented functionality. This should be added to the package level documentation for meas_base, so it appears in some extended version of https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/meas_base.html."
"Wrap pex_exceptions with pybind11 While wrapping these packages, pay particular attention to exception translation (see Jim's bullet point 3: https://jira.lsstcorp.org/browse/RFC-182?focusedCommentId=48644&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48644).",4,DM-6302,datamanagement,wrap pex_exception pybind11 wrap package pay particular attention exception translation jim bullet point https://jira.lsstcorp.org/browse/rfc-182?focusedcommentid=48644&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48644,"Wrap pex_exceptions with pybind11 While wrapping these packages, pay particular attention to exception translation (see Jim's bullet point 3: https://jira.lsstcorp.org/browse/RFC-182?focusedCommentId=48644&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48644)."
Executable test in utils needs to test an executable In DM-4036 all the test binaries were removed as no longer being needed. This had the unfortunate side effect that the {{testExecutables.py}} test no longer tests anything. This ticket will be used for adding a test file.,1,DM-6306,datamanagement,executable test util need test executable dm-4036 test binary remove long need unfortunate effect testexecutables.py test long test ticket add test file,Executable test in utils needs to test an executable In DM-4036 all the test binaries were removed as no longer being needed. This had the unfortunate side effect that the {{testExecutables.py}} test no longer tests anything. This ticket will be used for adding a test file.
input outlines to LDM-151 for AP Write outlines into the LDM-151 document for the alert production pipelines.,4,DM-6307,datamanagement,input outline ldm-151 ap write outline ldm-151 document alert production pipeline,input outlines to LDM-151 for AP Write outlines into the LDM-151 document for the alert production pipelines.
"Upload JSON from validate_drp to SQuaSH REST API on Jenkins This ticket covers work to build a Python package/script whose role is to take JSON output from validate_drp (DM-6086), shim it into the JSON schema currently expected by the SQuaSH REST API (http://sqr-009.lsst.io), and post the data to the API's {{/jobs}} endpoint.    This tool also adds additional metadata to the ‘Job’ document, including the build ID and versions of packages as run by validate_drp (see DM-5943).",2,DM-6308,datamanagement,upload json validate_drp squash rest api jenkins ticket cover work build python package script role json output validate_drp dm-6086 shim json schema currently expect squash rest api http://sqr-009.lsst.io post datum api /jobs endpoint tool add additional metadata job document include build id version package run validate_drp dm-5943,"Upload JSON from validate_drp to SQuaSH REST API on Jenkins This ticket covers work to build a Python package/script whose role is to take JSON output from validate_drp (DM-6086), shim it into the JSON schema currently expected by the SQuaSH REST API (http://sqr-009.lsst.io), and post the data to the API's {{/jobs}} endpoint. This tool also adds additional metadata to the Job document, including the build ID and versions of packages as run by validate_drp (see DM-5943)."
Update LDM-151 with SDQA Skeleton and Outline 1. Update the LDM-151 draft with the SDQA Skeleton from the DMLT + SciPipelines working group discussions of May 16-20.  Implement as bullet points in a semi-coherent list. (/)    2. Clean up list. (/),1,DM-6309,datamanagement,update ldm-151 sdqa skeleton outline update ldm-151 draft sdqa skeleton dmlt scipipelines work group discussion 16 20 implement bullet point semi coherent list clean list,Update LDM-151 with SDQA Skeleton and Outline 1. Update the LDM-151 draft with the SDQA Skeleton from the DMLT + SciPipelines working group discussions of May 16-20. Implement as bullet points in a semi-coherent list. (/) 2. Clean up list. (/)
Update Scons to v3.0 Scons v3 is the scons version that supports both Python 3 and Python 2.7. This ticket is for updating Scons and ensuring that the Python 2.7 stack still builds.    This work depends on the Scons developers delivering a new Scons by mid July. Whilst work is ongoing it may be necessary to help out with the port if we wish to meet our Python 3 target.,4,DM-6312,datamanagement,update scon v3.0 scons v3 scon version support python python 2.7 ticket update scon ensure python 2.7 stack build work depend scons developer deliver new scons mid july whilst work ongoing necessary help port wish meet python target,Update Scons to v3.0 Scons v3 is the scons version that supports both Python 3 and Python 2.7. This ticket is for updating Scons and ensuring that the Python 2.7 stack still builds. This work depends on the Scons developers delivering a new Scons by mid July. Whilst work is ongoing it may be necessary to help out with the port if we wish to meet our Python 3 target.
Create miniconda3 EUPS package {{newinstall.sh}} currently installs miniconda via EUPS. To replicate that functionality in Python3 we need to create a {{miniconda3}} package. This package should be almost identical to {{miniconda2}}.    Requires that {{lsstsw}} first be updated to support python 3.,1,DM-6313,datamanagement,create miniconda3 eups package newinstall.sh currently install miniconda eups replicate functionality python3 need create miniconda3 package package identical miniconda2 require lsstsw update support python,Create miniconda3 EUPS package {{newinstall.sh}} currently installs miniconda via EUPS. To replicate that functionality in Python3 we need to create a {{miniconda3}} package. This package should be almost identical to {{miniconda2}}. Requires that {{lsstsw}} first be updated to support python 3.
Port lsstsw to Python 3 Get {{lsstsw}} working with Python 3:  * Update the {{deploy}} script to allow a Python 3 python to be installed and modify the version checking code.  * Demonstrate that {{lsstsw}} {{rebuild}} will successfully build and install a third-party non-Scons package.,5,DM-6314,datamanagement,port lsstsw python lsstsw work python update deploy script allow python python instal modify version check code demonstrate lsstsw rebuild successfully build install party non scons package,Port lsstsw to Python 3 Get {{lsstsw}} working with Python 3: * Update the {{deploy}} script to allow a Python 3 python to be installed and modify the version checking code. * Demonstrate that {{lsstsw}} {{rebuild}} will successfully build and install a third-party non-Scons package.
Update newinstall.sh to support Python 3 {{newinstall.sh}} currently insists on installing and checking for python 2.7. This needs to be changed to allow Python 3.    Requires {{sconsUtils}} works with Python 3 as the {{lsst}} EUPS package is installed as part of {{newinstall.sh}}.,1,DM-6316,datamanagement,update newinstall.sh support python newinstall.sh currently insist instal check python 2.7 need change allow python require sconsutils work python lsst eups package instal newinstall.sh,Update newinstall.sh to support Python 3 {{newinstall.sh}} currently insists on installing and checking for python 2.7. This needs to be changed to allow Python 3. Requires {{sconsUtils}} works with Python 3 as the {{lsst}} EUPS package is installed as part of {{newinstall.sh}}.
Update developer guide to include Python 3 Update the developer guide to indicate that Python 3 must be supported and that code must run on Python 2.7 and 3.    This ticket will reference the tech note delivered as part of DM-6315. Writing extensive user documentation on the {{future}} package is beyond the scope of this ticket.,2,DM-6317,datamanagement,update developer guide include python update developer guide indicate python support code run python 2.7 ticket reference tech note deliver dm-6315 write extensive user documentation future package scope ticket,Update developer guide to include Python 3 Update the developer guide to indicate that Python 3 must be supported and that code must run on Python 2.7 and 3. This ticket will reference the tech note delivered as part of DM-6315. Writing extensive user documentation on the {{future}} package is beyond the scope of this ticket.
Port sconsUtils to Python 3 {{sconsUtils}} has to be modified to ensure it works with Python 3. Additionally SWIG calls must be changed to trigger Python 3 mode.,3,DM-6319,datamanagement,port sconsutil python sconsutils modify ensure work python additionally swig call change trigger python mode,Port sconsUtils to Python 3 {{sconsUtils}} has to be modified to ensure it works with Python 3. Additionally SWIG calls must be changed to trigger Python 3 mode.
Port utils to Python 3 Ensure that the {{utils}} package will work with Python 3.,2,DM-6320,datamanagement,port util python ensure util package work python,Port utils to Python 3 Ensure that the {{utils}} package will work with Python 3.
Port base package to Python 3 Ensure that {{base}} works with Python 3.,2,DM-6322,datamanagement,port base package python ensure base work python,Port base package to Python 3 Ensure that {{base}} works with Python 3.
Lead Python 3 migration at All Hands Meeting * Prepare for all hands meeting.  * Present plan to developers.  * Advise developers doing migration.  * Contribute fixes as required.,8,DM-6323,datamanagement,lead python migration hands meeting prepare hand meeting present plan developer advise developer migration contribute fix require,Lead Python 3 migration at All Hands Meeting * Prepare for all hands meeting. * Present plan to developers. * Advise developers doing migration. * Contribute fixes as required.
Replace BOOST_STATIC_ASSERT with static_assert Replace BOOST_STATIC_ASSERT with static_assert from C++11.,1,DM-6325,datamanagement,replace boost_static_assert static_assert replace boost_static_assert static_assert c++11,Replace BOOST_STATIC_ASSERT with static_assert Replace BOOST_STATIC_ASSERT with static_assert from C++11.
"reST roles for mock code references Add mock code reference roles so that authors can add semantics to their writing without attempting to make actual references to API documentation that does not _yet_ exist. Covers all roles in the Python domain, and supports tilde syntax for collapsing the namespace.",1,DM-6326,datamanagement,rest role mock code reference add mock code reference role author add semantic writing attempt actual reference api documentation exist cover role python domain support tilde syntax collapse namespace,"reST roles for mock code references Add mock code reference roles so that authors can add semantics to their writing without attempting to make actual references to API documentation that does not _yet_ exist. Covers all roles in the Python domain, and supports tilde syntax for collapsing the namespace."
"Shifting F16 milestones to S16 Per [~jbecla]'s request, provide Kevin with a list of milestones which we will not address in F16. Reschedule them to S16 in JIRA.",1,DM-6331,datamanagement,shift f16 milestone s16 ~jbecla request provide kevin list milestone address f16 reschedule s16 jira,"Shifting F16 milestones to S16 Per [~jbecla]'s request, provide Kevin with a list of milestones which we will not address in F16. Reschedule them to S16 in JIRA."
"Update LDM-151 introduction to reflect new structure Some proposals made on DM-6247 to change the structure of LDM-151 (add Algorithmic Components section, move overview into production-specific sections, add notation section to introduction) were accepted at the live meeting on 6/27.  This issue rewrites the introduction accordingly.",1,DM-6343,datamanagement,update ldm-151 introduction reflect new structure proposal dm-6247 change structure ldm-151 add algorithmic components section overview production specific section add notation section introduction accept live meeting 6/27 issue rewrite introduction accordingly,"Update LDM-151 introduction to reflect new structure Some proposals made on DM-6247 to change the structure of LDM-151 (add Algorithmic Components section, move overview into production-specific sections, add notation section to introduction) were accepted at the live meeting on 6/27. This issue rewrites the introduction accordingly."
Firefly Python API scope and decision Python API to use Firefly visualization components and other functions.   This story is to come up with a good plan for the rest of the development work to related to Python API. ,2,DM-6345,datamanagement,firefly python api scope decision python api use firefly visualization component function story come good plan rest development work relate python api,Firefly Python API scope and decision Python API to use Firefly visualization components and other functions. This story is to come up with a good plan for the rest of the development work to related to Python API.
User installation and operation instructions for conda  Create documentation for the Stack conda binaries created in DM-5415 as part of the Science Pipelines documentation,3,DM-6346,datamanagement,user installation operation instruction conda create documentation stack conda binary create dm-5415 science pipelines documentation,User installation and operation instructions for conda Create documentation for the Stack conda binaries created in DM-5415 as part of the Science Pipelines documentation
"Add FlagDecorator to support FlagHandler in Python DM-4009 added the C++ and swig changes needed to allow the FlagHandler to be used from Python.  During review, Nate suggested that a decorator class could be used to improve the use of this code in Python.  This ticket will be to review Nate's decorator and confirm that it is the correct model for Python-only plugins.    We will also modify the unit test in DM-4009 and the EmPsfApprox plugin in DM-6123 to use the decorator.",2,DM-6347,datamanagement,add flagdecorator support flaghandler python dm-4009 add c++ swig change need allow flaghandler python review nate suggest decorator class improve use code python ticket review nate decorator confirm correct model python plugin modify unit test dm-4009 empsfapprox plugin dm-6123 use decorator,"Add FlagDecorator to support FlagHandler in Python DM-4009 added the C++ and swig changes needed to allow the FlagHandler to be used from Python. During review, Nate suggested that a decorator class could be used to improve the use of this code in Python. This ticket will be to review Nate's decorator and confirm that it is the correct model for Python-only plugins. We will also modify the unit test in DM-4009 and the EmPsfApprox plugin in DM-6123 to use the decorator."
"Generate camera description at build time Camera geometry used to be defined using PAF (policy) files, which are now deprecated. As part of the transition to the refactored camera geometry scheme, scripts were introduced to convert from the PAF files to the new camera geometry configuration scheme which uses FITS files and a python file to describe the camera. These scripts are still part of the obs_* packages, and some people rely on them for making changes to the camera description. On the other hand, the generated FITS files and python file are also first-class members of the obs_* packages. This means that we have two sources of the same information, which is dangerous.    For obs_lsstSim, obs_decam, obs_cfht and obs_sdss, we want these scripts to be the primary source of information.  This means we should delete the generated files, and create them at build time.  We should also standardise the name of the script used to generate these.",3,DM-6350,datamanagement,generate camera description build time camera geometry define paf policy file deprecate transition refactored camera geometry scheme script introduce convert paf file new camera geometry configuration scheme use fits file python file describe camera script obs package people rely make change camera description hand generate fits file python file class member obs package mean source information dangerous obs_lsstsim obs_decam obs_cfht obs_sdss want script primary source information mean delete generate file create build time standardise script generate,"Generate camera description at build time Camera geometry used to be defined using PAF (policy) files, which are now deprecated. As part of the transition to the refactored camera geometry scheme, scripts were introduced to convert from the PAF files to the new camera geometry configuration scheme which uses FITS files and a python file to describe the camera. These scripts are still part of the obs_* packages, and some people rely on them for making changes to the camera description. On the other hand, the generated FITS files and python file are also first-class members of the obs_* packages. This means that we have two sources of the same information, which is dangerous. For obs_lsstSim, obs_decam, obs_cfht and obs_sdss, we want these scripts to be the primary source of information. This means we should delete the generated files, and create them at build time. We should also standardise the name of the script used to generate these."
"Use the HTM based reference catalogs in tests In order to move A.net out of meas_astrom to make it a true dependency, we need to replace its use in tests.",4,DM-6352,datamanagement,use htm base reference catalog test order a.net meas_astrom true dependency need replace use test,"Use the HTM based reference catalogs in tests In order to move A.net out of meas_astrom to make it a true dependency, we need to replace its use in tests."
Add linearity correction to obs_decam Add linearity correction from DM-5462 to obs_decam using the standard linearity tables.  ,4,DM-6356,datamanagement,add linearity correction obs_decam add linearity correction dm-5462 obs_decam standard linearity table,Add linearity correction to obs_decam Add linearity correction from DM-5462 to obs_decam using the standard linearity tables.
"Update ""Using Boost"" section in DM Developer Guide to prefer standard library by default Implement RFC-185 by updating the ""Using Boost"" section in DM Developer Guide to prefer standard library by default.",1,DM-6360,datamanagement,update boost section dm developer guide prefer standard library default implement rfc-185 update boost section dm developer guide prefer standard library default,"Update ""Using Boost"" section in DM Developer Guide to prefer standard library by default Implement RFC-185 by updating the ""Using Boost"" section in DM Developer Guide to prefer standard library by default."
Adjust version check of EUPS python package to allow v3 To enable Python 3 support of the stack the EUPS {{python}} stub package needs to allow Python 3.    ,1,DM-6368,datamanagement,adjust version check eups python package allow v3 enable python support stack eups python stub package need allow python,Adjust version check of EUPS python package to allow v3 To enable Python 3 support of the stack the EUPS {{python}} stub package needs to allow Python 3.
Test DIA simulation script with Postgres I will be useful to compare MySQL and Postgres performance for use in L1. After DM-6918 is complete (means works with MySQL) verify that it can also run against Postgres. ,5,DM-6369,datamanagement,test dia simulation script postgres useful compare mysql postgres performance use l1 dm-6918 complete mean work mysql verify run postgres,Test DIA simulation script with Postgres I will be useful to compare MySQL and Postgres performance for use in L1. After DM-6918 is complete (means works with MySQL) verify that it can also run against Postgres.
Improve skeleton for LDM-151 Algorithmic Components For all subsections in  Algorithms Components owned by [~jbosch]:   - Provide enough bullet points to capture scope.   - Add bullet points for subtly difficult aspects of components.   - Add extra level subsubsubsection level for Measurement.   - Create matrix of measurement algorithms and contexts.  ,1,DM-6373,datamanagement,improve skeleton ldm-151 algorithmic components subsection algorithms components own ~jbosch provide bullet point capture scope add bullet point subtly difficult aspect component add extra level subsubsubsection level measurement create matrix measurement algorithm context,Improve skeleton for LDM-151 Algorithmic Components For all subsections in Algorithms Components owned by [~jbosch]: - Provide enough bullet points to capture scope. - Add bullet points for subtly difficult aspects of components. - Add extra level subsubsubsection level for Measurement. - Create matrix of measurement algorithms and contexts.
Implement DAX containers Implement containerized DAX services,4,DM-6376,datamanagement,implement dax container implement containerize dax service,Implement DAX containers Implement containerized DAX services
Persist output of simple DCR correction DM-5695 will create transfer matrices stored as numpy arrays. This ticket extends that work to determine a useful format and write functionality to persist those arrays.,2,DM-6378,datamanagement,persist output simple dcr correction dm-5695 create transfer matrix store numpy array ticket extend work determine useful format write functionality persist array,Persist output of simple DCR correction DM-5695 will create transfer matrices stored as numpy arrays. This ticket extends that work to determine a useful format and write functionality to persist those arrays.
"Investigate CAOM Investigate observation model interfaces and storage, and applicability of CAOM",4,DM-6379,datamanagement,investigate caom investigate observation model interface storage applicability caom,"Investigate CAOM Investigate observation model interfaces and storage, and applicability of CAOM"
ADQL support in dbserv Work on understanding coordinate systems in ADQL and implement the ADQL->qserv rewriter,6,DM-6381,datamanagement,adql support dbserv work understand coordinate system adql implement adql->qserv rewriter,ADQL support in dbserv Work on understanding coordinate systems in ADQL and implement the ADQL->qserv rewriter
Generate template DCR images DM-5695 will create transfer matrices that can be used to create template images of a field at arbitrary airmass. This ticket is to write the code to generate and persist those template images.,4,DM-6382,datamanagement,generate template dcr image dm-5695 create transfer matrix create template image field arbitrary airmass ticket write code generate persist template image,Generate template DCR images DM-5695 will create transfer matrices that can be used to create template images of a field at arbitrary airmass. This ticket is to write the code to generate and persist those template images.
"Use template DCR images for image differencing DM-6382 creates template images of a field at arbitrary airmasses, which can be used to match the template airmass to the science image precisely to mitigate Differential Chromatic Refraction in image differencing. This ticket is to determine the best method to supply the new templates to image differencing, which may be simply to create a new exposure and ingest/process the template as though it were a real observation.",2,DM-6383,datamanagement,use template dcr image image differencing dm-6382 create template image field arbitrary airmasse match template airmass science image precisely mitigate differential chromatic refraction image differencing ticket determine good method supply new template image differencing simply create new exposure ingest process template real observation,"Use template DCR images for image differencing DM-6382 creates template images of a field at arbitrary airmasses, which can be used to match the template airmass to the science image precisely to mitigate Differential Chromatic Refraction in image differencing. This ticket is to determine the best method to supply the new templates to image differencing, which may be simply to create a new exposure and ingest/process the template as though it were a real observation."
Create CLI tool to add mac users. Create a CLI script to add and delete Mac OS X users. Somehow this is a many step process on Mac OS X.,1,DM-6385,datamanagement,create cli tool add mac user create cli script add delete mac os user step process mac os x.,Create CLI tool to add mac users. Create a CLI script to add and delete Mac OS X users. Somehow this is a many step process on Mac OS X.
Deploy Conda repository to S3 Deploy conda binaries to s3 using their static website feature. http://conda.lsst.codes/stack/current.,4,DM-6387,datamanagement,deploy conda repository s3 deploy conda binary s3 static website feature http://conda.lsst.codes/stack/current,Deploy Conda repository to S3 Deploy conda binaries to s3 using their static website feature. http://conda.lsst.codes/stack/current.
Create Ansible automation to run the conda build Create an Ansible deploy to automate Conda binary builds. Target Mac OS X and CentOS5.,5,DM-6388,datamanagement,create ansible automation run conda build create ansible deploy automate conda binary build target mac os centos5,Create Ansible automation to run the conda build Create an Ansible deploy to automate Conda binary builds. Target Mac OS X and CentOS5.
Create CentOS5 Conda binaries Create CentOS5 conda binary builds using docker then push them to the S3 static website.,2,DM-6389,datamanagement,create centos5 conda binarie create centos5 conda binary build docker push s3 static website,Create CentOS5 Conda binaries Create CentOS5 conda binary builds using docker then push them to the S3 static website.
Text on variability characterization for LDM-151 Expand the variability characterization algorithmic section of LDM-151.,1,DM-6392,datamanagement,text variability characterization ldm-151 expand variability characterization algorithmic section ldm-151,Text on variability characterization for LDM-151 Expand the variability characterization algorithmic section of LDM-151.
Data Backbone conops iteration 1: create raw draft (internal) Write a raw draft of the concept of operations for data backbone services. In this iteration the document is developed in Google docs following the ConOps template.,3,DM-6395,datamanagement,data backbone conop iteration create raw draft internal write raw draft concept operation datum backbone service iteration document develop google doc follow conops template,Data Backbone conops iteration 1: create raw draft (internal) Write a raw draft of the concept of operations for data backbone services. In this iteration the document is developed in Google docs following the ConOps template.
"Data Backbone conops iteration 2: group review to produce first draft Review raw draft of concept of operations for the data backbone services to work through underdeveloped areas, clear up uncertainties, and make readable.",2,DM-6396,datamanagement,data backbone conop iteration group review produce draft review raw draft concept operation data backbone service work underdeveloped area clear uncertainty readable,"Data Backbone conops iteration 2: group review to produce first draft Review raw draft of concept of operations for the data backbone services to work through underdeveloped areas, clear up uncertainties, and make readable."
"Data Backbone conops iteration 3: larger review to produce second draft Review first draft of data backbone services conops within Data Processing Architecture working group, bringing in relevant experts.    Input from review is incorporated into a second draft.",6,DM-6397,datamanagement,data backbone conop iteration large review produce second draft review draft datum backbone service conop data processing architecture working group bring relevant expert input review incorporate second draft,"Data Backbone conops iteration 3: larger review to produce second draft Review first draft of data backbone services conops within Data Processing Architecture working group, bringing in relevant experts. Input from review is incorporated into a second draft."
"Data Backbone conops formatting: convert second draft to reStructuredText When the data backbone services conops is in a solid state, convert the Google doc to reStructuredText following DM's documentation versioning process.",2,DM-6398,datamanagement,data backbone conop format convert second draft restructuredtext data backbone service conop solid state convert google doc restructuredtext follow dm documentation versioning process,"Data Backbone conops formatting: convert second draft to reStructuredText When the data backbone services conops is in a solid state, convert the Google doc to reStructuredText following DM's documentation versioning process."
"DM replanning: participation in PM working group Participation in Project Management working group for DM replanning.    Deliverable: Deliverables to PM working group.  Staff: Santanu Chaudhuri, Don Petravick, Margaret Johnson  Effort: 3 days  Planned Start: 6/1/2016  Planned End: 6/30/2016",6,DM-6403,datamanagement,dm replanne participation pm working group participation project management working group dm replanne deliverable deliverable pm working group staff santanu chaudhuri don petravick margaret johnson effort day planned start 6/1/2016 planned end 6/30/2016,"DM replanning: participation in PM working group Participation in Project Management working group for DM replanning. Deliverable: Deliverables to PM working group. Staff: Santanu Chaudhuri, Don Petravick, Margaret Johnson Effort: 3 days Planned Start: 6/1/2016 Planned End: 6/30/2016"
"Python 3 migration work Migrate existing packages in the LSST stack to support Python 3 compatibility. Some work will occur during the All-Hands meeting.    Deliverable: Migration of existing packages for Python 3 compatibility  Staff: Steve Petrowicz, Jim Parsons, Matias Carrasco-Kind, Mikolaj Kowalik, Hsin-Fang Chiang  Effort: 1 days  Planned Start: 8/1/2016  Planned End: 8/31/2016",2,DM-6422,datamanagement,python migration work migrate exist package lsst stack support python compatibility work occur hands meeting deliverable migration exist package python compatibility staff steve petrowicz jim parsons matias carrasco kind mikolaj kowalik hsin fang chiang effort day planned start 8/1/2016 planned end 8/31/2016,"Python 3 migration work Migrate existing packages in the LSST stack to support Python 3 compatibility. Some work will occur during the All-Hands meeting. Deliverable: Migration of existing packages for Python 3 compatibility Staff: Steve Petrowicz, Jim Parsons, Matias Carrasco-Kind, Mikolaj Kowalik, Hsin-Fang Chiang Effort: 1 days Planned Start: 8/1/2016 Planned End: 8/31/2016"
Analyze existing implementation of Supertask Look through the code base of current prototype of Supertask and Activator to understand better limits of existing design of data processing task and how they are being addressed.,6,DM-6425,datamanagement,analyze exist implementation supertask look code base current prototype supertask activator understand well limit exist design datum processing task address,Analyze existing implementation of Supertask Look through the code base of current prototype of Supertask and Activator to understand better limits of existing design of data processing task and how they are being addressed.
Understand how EUPS works Read Developer's Guide tutorial and official documentation of EUPS to understand how it works to manage Stack's packages easily.,2,DM-6426,datamanagement,understand eups work read developer guide tutorial official documentation eups understand work manage stack package easily,Understand how EUPS works Read Developer's Guide tutorial and official documentation of EUPS to understand how it works to manage Stack's packages easily.
Data Backbone: produce abstract API to ingest data into L1 archive  Implement an abstract API to ingest data into the L1 archive in the Data Backbone.    Deliverable: abstract API in github  Staff: Steve Pietrowicz  Effort: 4 days  Planned Start: 6/1/2016  Planned End: 6/30/2016,8,DM-6427,datamanagement,data backbone produce abstract api ingest datum l1 archive implement abstract api ingest datum l1 archive data backbone deliverable abstract api github staff steve pietrowicz effort day planned start 6/1/2016 planned end 6/30/2016,Data Backbone: produce abstract API to ingest data into L1 archive Implement an abstract API to ingest data into the L1 archive in the Data Backbone. Deliverable: abstract API in github Staff: Steve Pietrowicz Effort: 4 days Planned Start: 6/1/2016 Planned End: 6/30/2016
Produce evaluation report WAN data interchange evaluation report.,5,DM-6433,datamanagement,produce evaluation report wan data interchange evaluation report,Produce evaluation report WAN data interchange evaluation report.
Convert GWT projection and Coorindate Conversion routine to JavaScript convert Booth's projection code and Judy Bennet's coordinate conversion routines to pure javascript  ,8,DM-6437,datamanagement,convert gwt projection coorindate conversion routine javascript convert booth projection code judy bennet coordinate conversion routine pure javascript,Convert GWT projection and Coorindate Conversion routine to JavaScript convert Booth's projection code and Judy Bennet's coordinate conversion routines to pure javascript
Remove GWT from build After the code is tested remove the GWT from the build. Should check with [~roby] to make sure the boolean to enable GWT has been removed from VisUtil.js and WebPlot.js,4,DM-6439,datamanagement,remove gwt build code test remove gwt build check ~roby sure boolean enable gwt remove visutil.js webplot.js,Remove GWT from build After the code is tested remove the GWT from the build. Should check with [~roby] to make sure the boolean to enable GWT has been removed from VisUtil.js and WebPlot.js
May 2016 LAAIM work Gave input on IAM design for FY16 Integration Environment.  Discussed IAM replication requirements with stakeholders.  Attended local NCSA LSST coordination meeting.,4,DM-6440,datamanagement,2016 laaim work give input iam design fy16 integration environment discuss iam replication requirement stakeholder attend local ncsa lsst coordination meeting,May 2016 LAAIM work Gave input on IAM design for FY16 Integration Environment. Discussed IAM replication requirements with stakeholders. Attended local NCSA LSST coordination meeting.
Create Ansible automation to run the conda build Complete Ansible implementation started in DM-6388.,4,DM-6441,datamanagement,create ansible automation run conda build complete ansible implementation start dm-6388,Create Ansible automation to run the conda build Complete Ansible implementation started in DM-6388.
Verification CoDR preparation Just capturing FE's SPs  towards this. ,6,DM-6445,datamanagement,verification codr preparation capture fe sp,Verification CoDR preparation Just capturing FE's SPs towards this.
"Remove boost dependencies where possible In X16/DM-5580, we removed Boost from a number of packages. However, we may not have rigorously updated their dependency lists to indicate where Boost is no longer required. Please do so.",1,DM-6446,datamanagement,remove boost dependency possible x16 dm-5580 remove boost number package rigorously update dependency list indicate boost long require,"Remove boost dependencies where possible In X16/DM-5580, we removed Boost from a number of packages. However, we may not have rigorously updated their dependency lists to indicate where Boost is no longer required. Please do so."
"create description of features in storage APIs The APIs for the storage brokers we're looking into are similar, but don't have a 1-1 correspondence.  Write up the features offered by the APIs, and see where there is overlap.",4,DM-6456,datamanagement,create description feature storage api api storage broker look similar correspondence write feature offer api overlap,"create description of features in storage APIs The APIs for the storage brokers we're looking into are similar, but don't have a 1-1 correspondence. Write up the features offered by the APIs, and see where there is overlap."
"Please provide how-to-reproduce instructions for LSST/HSC comparison epics For all the stories describing comparisons between HSC and LSST results (notably DM-5301 and DM-5827), please provide instructions describing the steps to reproduce the comparison. In particular, include:    * A list of any tweaks that had to be applied to the code;  * Non-default configuration options;  * Exactly which comparisons were made.",2,DM-6463,datamanagement,provide reproduce instruction lsst hsc comparison epic story describe comparison hsc lsst result notably dm-5301 dm-5827 provide instruction describe step reproduce comparison particular include list tweak apply code non default configuration option exactly comparison,"Please provide how-to-reproduce instructions for LSST/HSC comparison epics For all the stories describing comparisons between HSC and LSST results (notably DM-5301 and DM-5827), please provide instructions describing the steps to reproduce the comparison. In particular, include: * A list of any tweaks that had to be applied to the code; * Non-default configuration options; * Exactly which comparisons were made."
"Compare CModel results from LSST and HSC Demonstrate that the HSC and LSST stacks produce consistent results for CModel measurement. Account for (and fix, where relevant) differences.",4,DM-6464,datamanagement,compare cmodel result lsst hsc demonstrate hsc lsst stack produce consistent result cmodel measurement account fix relevant difference,"Compare CModel results from LSST and HSC Demonstrate that the HSC and LSST stacks produce consistent results for CModel measurement. Account for (and fix, where relevant) differences."
"Compare Kron results from LSST and HSC Demonstrate that the HSC and LSST stacks produce consistent results for Kron measurement. Account for (and fix, if relevant) any differences.",4,DM-6465,datamanagement,compare kron result lsst hsc demonstrate hsc lsst stack produce consistent result kron measurement account fix relevant difference,"Compare Kron results from LSST and HSC Demonstrate that the HSC and LSST stacks produce consistent results for Kron measurement. Account for (and fix, if relevant) any differences."
"Compare meas_mosaic-ed HSC and LSST coadds The previous comparison of coadds on HSC and LSST was performed without an operable LSST-based meas_mosaic. When one becomes available, demonstrate that mosaicking is consistent between HSC and LSST; describe, account for, and (where possible) correct differences.",5,DM-6466,datamanagement,compare meas_mosaic ed hsc lsst coadd previous comparison coadd hsc lsst perform operable lsst base meas_mosaic available demonstrate mosaicking consistent hsc lsst describe account possible correct difference,"Compare meas_mosaic-ed HSC and LSST coadds The previous comparison of coadds on HSC and LSST was performed without an operable LSST-based meas_mosaic. When one becomes available, demonstrate that mosaicking is consistent between HSC and LSST; describe, account for, and (where possible) correct differences."
"Account for noise replacement differences between LSST and HSC In DM-5827, [~rearmstr] wrote:  {quote}  In most of these plots you can see some scatter at relatively bright magnitudes... These are likely getting different pixel values when we replace objects with noise which is causing these changes.  {quote}  Check that this is the case, and, if so, explain why the noise replacement is different.",5,DM-6467,datamanagement,account noise replacement difference lsst hsc dm-5827 ~rearmstr write quote plot scatter relatively bright magnitude likely get different pixel value replace object noise cause change quote check case explain noise replacement different,"Account for noise replacement differences between LSST and HSC In DM-5827, [~rearmstr] wrote: {quote} In most of these plots you can see some scatter at relatively bright magnitudes... These are likely getting different pixel values when we replace objects with noise which is causing these changes. {quote} Check that this is the case, and, if so, explain why the noise replacement is different."
"convert jenkins-ebs-snapshot job to use credentials for aws keys At present, this job is being templated by puppet to inject the keys in plain text which are they converted by jenkins to stored secrets if/when the job is edited and resaved via the jenkins UI.  This means that the credentials may be leaked.",1,DM-6470,datamanagement,convert jenkins ebs snapshot job use credential aw key present job template puppet inject key plain text convert jenkin store secret job edit resave jenkins ui mean credential leak,"convert jenkins-ebs-snapshot job to use credentials for aws keys At present, this job is being templated by puppet to inject the keys in plain text which are they converted by jenkins to stored secrets if/when the job is edited and resaved via the jenkins UI. This means that the credentials may be leaked."
"Conda eups packages don't work if eups is already configured When [~tjenness] attempted to use the Conda repository he ran into a problem installing and using the packages because he already had an active EUPS_DIR and EUPS_PATH.    When the eups package is installed and linked, it should warn users when these environment variables are set.    I'm open to another solution but would prefer it doesn't change the eups package behavior. Changing behavior goes against the best practices for Conda and more generally packaging.",1,DM-6471,datamanagement,conda eup package work eup configure ~tjenness attempt use conda repository run problem instal package active eups_dir eups_path eup package instal link warn user environment variable set open solution prefer change eup package behavior change behavior go good practice conda generally package,"Conda eups packages don't work if eups is already configured When [~tjenness] attempted to use the Conda repository he ran into a problem installing and using the packages because he already had an active EUPS_DIR and EUPS_PATH. When the eups package is installed and linked, it should warn users when these environment variables are set. I'm open to another solution but would prefer it doesn't change the eups package behavior. Changing behavior goes against the best practices for Conda and more generally packaging."
"Possible image related issues in firefly viewer Image Meta Data tab  * images cannot be remove, but in expanded mode, it can.  * selecting image no longer highlight table.  the reverse works fine.  * visualize/saga/ImageMetaDataWatcher.js:272 returns -1.  * when a non-meta table is selected, images are shown, but not the toolbar.  * after table is removed, images are still there.    image external api does not mix well with firefly viewer.  * firefly viewer uses 'triViewImages’ viewer_id while api has no viewer_id.  as a result, images loaded by api will be lost once table or other searched data are returned.    catalog overlay are drawn outside of the images.    more issues:    - It's possible to select distance tool and then area selection. First drag would define area selection, all the following line. A click would be defining a 0 length line, even if point selection is enabled.  ",8,DM-6473,datamanagement,possible image relate issue firefly view image meta data tab image remove expand mode select image long highlight table reverse work fine visualize saga imagemetadatawatcher.js:272 return -1 non meta table select image show toolbar table remove image image external api mix firefly viewer firefly viewer use triviewimage viewer_id api viewer_id result image load api lose table search datum return catalog overlay draw outside image issue possible select distance tool area selection drag define area selection follow line click define length line point selection enable,"Possible image related issues in firefly viewer Image Meta Data tab * images cannot be remove, but in expanded mode, it can. * selecting image no longer highlight table. the reverse works fine. * visualize/saga/ImageMetaDataWatcher.js:272 returns -1. * when a non-meta table is selected, images are shown, but not the toolbar. * after table is removed, images are still there. image external api does not mix well with firefly viewer. * firefly viewer uses 'triViewImages viewer_id while api has no viewer_id. as a result, images loaded by api will be lost once table or other searched data are returned. catalog overlay are drawn outside of the images. more issues: - It's possible to select distance tool and then area selection. First drag would define area selection, all the following line. A click would be defining a 0 length line, even if point selection is enabled."
"Restore star selector registry Restore the registry for star selectors that was lost in DM-5532, now that tasks in registries can be used as subtasks.    Also use the registry where appropriate.",2,DM-6474,datamanagement,restore star selector registry restore registry star selector lose dm-5532 task registry subtask use registry appropriate,"Restore star selector registry Restore the registry for star selectors that was lost in DM-5532, now that tasks in registries can be used as subtasks. Also use the registry where appropriate."
Install conda psutil instead of LSST's version [~tjenness] requests that conda-lsst uses Conda's psutil. Currently we use our own version.,1,DM-6475,datamanagement,install conda psutil instead lsst version ~tjenness request conda lsst use conda psutil currently use version,Install conda psutil instead of LSST's version [~tjenness] requests that conda-lsst uses Conda's psutil. Currently we use our own version.
Report and work around conda repository change This needs to be worked around by either using a different version of conda-build or addressing the changes to the conda recipe structure. I also want to comment and/or create an issue on conda-build so they know that such changes are affecting users.    See:    https://github.com/conda/conda-build/issues/1003  https://github.com/conda/conda-build/pull/1004    https://github.com/conda/conda-build/commit/b4ec0e0659d8f376042d4fc391616bf235996cf5    [Mario fixed it|https://github.com/mjuric/conda-lsst/commit/6a552b6f9cada2530681cfdc4a9f67add261ff99] but that fix will be broken as soon as conda-build #1004 is merged.,1,DM-6476,datamanagement,report work conda repository change need work different version conda build address change conda recipe structure want comment and/or create issue conda build know change affect user https://github.com/conda/conda-build/issues/1003 https://github.com/conda/conda-build/pull/1004 https://github.com/conda/conda-build/commit/b4ec0e0659d8f376042d4fc391616bf235996cf5 mario fix it|https://github.com mjuric conda lsst commit/6a552b6f9cada2530681cfdc4a9f67add261ff99 fix break soon conda build 1004 merge,Report and work around conda repository change This needs to be worked around by either using a different version of conda-build or addressing the changes to the conda recipe structure. I also want to comment and/or create an issue on conda-build so they know that such changes are affecting users. See: https://github.com/conda/conda-build/issues/1003 https://github.com/conda/conda-build/pull/1004 https://github.com/conda/conda-build/commit/b4ec0e0659d8f376042d4fc391616bf235996cf5 [Mario fixed it|https://github.com/mjuric/conda-lsst/commit/6a552b6f9cada2530681cfdc4a9f67add261ff99] but that fix will be broken as soon as conda-build #1004 is merged.
"Form validation regression issues Recent changes in 'dev' made some of the components stop working.  We need to click through firefly viewer to identify the problems and fix it.  Below are a few that I've spotted.    Data Sets menu:  Form fail validation when they should not.  filters and upload file should be nullable.    Catalogs Classic menu:  form fail validation without any visual indications.  valid parameter is false when passed into validUpdate in CompleteButton.    xyPlot(Scatter Plot) options:  Beside X and Y, everything else should be optional(nullable).  Even after entering a valid value into all of the fields, 'OK' still fail validation similar to above where 'valid' parameter is false when passed into CompleteButton.    There may be more, please do a quick search to make sure all usage of CompleteButton is good.   ",4,DM-6487,datamanagement,form validation regression issue recent change dev component stop work need click firefly viewer identify problem fix spot data sets menu form fail validation filter upload file nullable catalogs classic menu form fail validation visual indication valid parameter false pass validupdate completebutton xyplot(scatter plot option optional(nullable enter valid value field ok fail validation similar valid parameter false pass completebutton quick search sure usage completebutton good,"Form validation regression issues Recent changes in 'dev' made some of the components stop working. We need to click through firefly viewer to identify the problems and fix it. Below are a few that I've spotted. Data Sets menu: Form fail validation when they should not. filters and upload file should be nullable. Catalogs Classic menu: form fail validation without any visual indications. valid parameter is false when passed into validUpdate in CompleteButton. xyPlot(Scatter Plot) options: Beside X and Y, everything else should be optional(nullable). Even after entering a valid value into all of the fields, 'OK' still fail validation similar to above where 'valid' parameter is false when passed into CompleteButton. There may be more, please do a quick search to make sure all usage of CompleteButton is good."
"Investigate calibration zeropoint offset between HSC vs. LSST processCcd.py runs As reported in DM-4730, while the scatter between single frame processing measurements of the same dataset on the HSC vs. LSST stacks is quite good (rms = 0.009 mag between Gaussian fluxes, for example, in the figure shown on that ticket), there is a clear offset (0.0166 mag in the figure shown) in the zeropoint between the two stacks (it is systematic, i.e. no trend with magnitude).  The cause may well be due to slight differences in the reference stars selected for calibration.  We also speculated about differences in slot definitions used in the calibrations steps (e.g. for aperture corrections, psfex, etc...), so I have rerun visit 1322 through both stacks having forced all apertures used in calibration to be the same, namely a circular aperture of 12 pixels measured using the sinc algorithm (as opposed to ""naive"").  I have attached the *processCcd.py* config files for the two runs so my settings can be reproduced.    Also of note, I am using a {{meas_algorithms}} branch on the HSC stack with the following commit:    {code}  commit 173ad0b32ed4f4ab074f1a942d2d3f758e189917  Author: Lauren MacArthur <lauren@astro.princeton.edu>  Date:   Wed Jan 13 16:35:59 2016 -0500        Hack to allow flux.aperture to be used in apCorr            Since it does not seem possible to access the nth element of a      schema element that is an array in the context of setting a config      override, this allows for flux.aperture to be set as      calibrate.measureApCorr.reference and it sets it to index 4 (which      corresponds to a radius of 12 pixels) in the __init__.  This was      selected to match the current LSST default.    diff --git a/python/lsst/meas/algorithms/measureApCorr.py b/python/lsst/meas/algorithms/measureApCorr.py  index 9f6c599..f1fa99d 100644  --- a/python/lsst/meas/algorithms/measureApCorr.py  +++ b/python/lsst/meas/algorithms/measureApCorr.py  @@ -81,6 +81,9 @@ class MeasureApCorrTask(lsst.pipe.base.Task):       def __init__(self, schema, **kwds):           lsst.pipe.base.Task.__init__(self, **kwds)           self.reference = KeyTuple(self.config.reference, schema)  +        if self.config.reference == 'flux.aperture':  +            print ""NOTE: setting aperture correction flux to flux.aperture[4] ==> radius = 12 pixels""  +            self.reference.flux = self.reference.flux[4]           self.toCorrect = {}  {code}    I attach some of the figures comparing the PSF fluxes from these runs which compare the output of the two stacks having matched the two src catalogs.  There are two sets: 1) having adjusted the flux for each source to the zeropoint calculated in the calibration and stored as *FLUXMAG0* 2) having adjusted the flux for all sources to a common zeropoint (zp=33.0, chosen to roughly match the calibrated zp).  Note that my figures do include aperture corrections (in DM-5301, many of the plots show fluxes pre-aperture correction).  I have also included plots that directly compare the aperture corrections applied (difference in mag units).  Finally, I also include plots comparing the 12 pixel circular aperture mags (i.e. to which no apCorr is added).    Clearly, the zeropoint determined in the calibration of the two stacks differs between the two stacks and, in particular, there seem to be some very problematic CCDs where the differences are particularly significant (~0.05 mag, and not always in the same direction).  Please investigate the source of this discrepancy.",8,DM-6490,datamanagement,"investigate calibration zeropoint offset hsc vs. lsst processccd.py run report dm-4730 scatter single frame processing measurement dataset hsc vs. lsst stack good rm 0.009 mag gaussian flux example figure show ticket clear offset 0.0166 mag figure show zeropoint stack systematic i.e. trend magnitude cause slight difference reference star select calibration speculate difference slot definition calibration step e.g. aperture correction psfex etc ... rerun visit 1322 stack having force aperture calibration circular aperture 12 pixel measure sinc algorithm oppose naive attach processccd.py config file run setting reproduce note meas_algorithms branch hsc stack following commit code commit 173ad0b32ed4f4ab074f1a942d2d3f758e189917 author lauren macarthur date jan 13 16:35:59 2016 -0500 hack allow flux.aperture apcorr possible access nth element schema element array context set config override allow flux.aperture set calibrate.measureapcorr.reference set index correspond radius 12 pixel init select match current lsst default diff python lsst meas algorithm measureapcorr.py python lsst meas algorithm measureapcorr.py index 9f6c599 100644 python lsst meas algorithm measureapcorr.py python lsst meas algorithm measureapcorr.py +81,9 class measureapcorrtask(lsst.pipe.base task def init__(self schema kwd lsst.pipe.base task.__init__(self kwd self.reference keytuple(self.config.reference schema self.config.reference flux.aperture print note set aperture correction flux flux.aperture[4 radius 12 pixel self.reference.flux self.reference.flux[4 self.tocorrect code attach figure compare psf flux run compare output stack having match src catalog set having adjust flux source zeropoint calculate calibration store fluxmag0 having adjust flux source common zeropoint zp=33.0 choose roughly match calibrate zp note figure include aperture correction dm-5301 plot flux pre aperture correction include plot directly compare aperture correction apply difference mag unit finally include plot compare 12 pixel circular aperture mag i.e. apcorr add clearly zeropoint determine calibration stack differ stack particular problematic ccd difference particularly significant ~0.05 mag direction investigate source discrepancy","Investigate calibration zeropoint offset between HSC vs. LSST processCcd.py runs As reported in DM-4730, while the scatter between single frame processing measurements of the same dataset on the HSC vs. LSST stacks is quite good (rms = 0.009 mag between Gaussian fluxes, for example, in the figure shown on that ticket), there is a clear offset (0.0166 mag in the figure shown) in the zeropoint between the two stacks (it is systematic, i.e. no trend with magnitude). The cause may well be due to slight differences in the reference stars selected for calibration. We also speculated about differences in slot definitions used in the calibrations steps (e.g. for aperture corrections, psfex, etc...), so I have rerun visit 1322 through both stacks having forced all apertures used in calibration to be the same, namely a circular aperture of 12 pixels measured using the sinc algorithm (as opposed to ""naive""). I have attached the *processCcd.py* config files for the two runs so my settings can be reproduced. Also of note, I am using a {{meas_algorithms}} branch on the HSC stack with the following commit: {code} commit 173ad0b32ed4f4ab074f1a942d2d3f758e189917 Author: Lauren MacArthur  Date: Wed Jan 13 16:35:59 2016 -0500 Hack to allow flux.aperture to be used in apCorr Since it does not seem possible to access the nth element of a schema element that is an array in the context of setting a config override, this allows for flux.aperture to be set as calibrate.measureApCorr.reference and it sets it to index 4 (which corresponds to a radius of 12 pixels) in the __init__. This was selected to match the current LSST default. diff --git a/python/lsst/meas/algorithms/measureApCorr.py b/python/lsst/meas/algorithms/measureApCorr.py index 9f6c599..f1fa99d 100644 --- a/python/lsst/meas/algorithms/measureApCorr.py +++ b/python/lsst/meas/algorithms/measureApCorr.py @@ -81,6 +81,9 @@ class MeasureApCorrTask(lsst.pipe.base.Task): def __init__(self, schema, **kwds): lsst.pipe.base.Task.__init__(self, **kwds) self.reference = KeyTuple(self.config.reference, schema) + if self.config.reference == 'flux.aperture': + print ""NOTE: setting aperture correction flux to flux.aperture[4] ==> radius = 12 pixels"" + self.reference.flux = self.reference.flux[4] self.toCorrect = {} {code} I attach some of the figures comparing the PSF fluxes from these runs which compare the output of the two stacks having matched the two src catalogs. There are two sets: 1) having adjusted the flux for each source to the zeropoint calculated in the calibration and stored as *FLUXMAG0* 2) having adjusted the flux for all sources to a common zeropoint (zp=33.0, chosen to roughly match the calibrated zp). Note that my figures do include aperture corrections (in DM-5301, many of the plots show fluxes pre-aperture correction). I have also included plots that directly compare the aperture corrections applied (difference in mag units). Finally, I also include plots comparing the 12 pixel circular aperture mags (i.e. to which no apCorr is added). Clearly, the zeropoint determined in the calibration of the two stacks differs between the two stacks and, in particular, there seem to be some very problematic CCDs where the differences are particularly significant (~0.05 mag, and not always in the same direction). Please investigate the source of this discrepancy."
"Investigate offset in baseline zeropoint between LSST vs. HSC stack reductions for some HSC visits DM-6490 reports on an offset between the calibration zeropoints between HSC vs. LSST *processCcd.py* runs.  Here we report another, additional, offset seen in certain HSC visits.  It is not seen in the figures shown in DM-6490 for visit 1322.  However, here attach the same figures for visit 19696, run with identical setups/configs for both stacks as in DM-6490, where we see an additional offset in the ""common ZP"" figures (i.e. all fluxes have been scaled the same zp=33.0 for comparison).    A best guess at present is that the calibration frames are different between the HSC and LSST stacks for the timeframe of this visit; e.g. were the inputs ingested exactly the same for both sets?  Did the bug in regards to flagging on the flats noted in DM-5124:  {quote}  I found a difference in the codes doing the statistics: the HSC code uses a hard-coded mask ignore list of DETECTED only, while the LSST code uses a configurable mask ignore list that defaults to DETECTED,BAD (and the default isn't overridden). This produces a large difference on CCDs with bad amps (e.g., ccd=9).  There's a smaller difference on ccd=49 because the number of BAD pixels is smaller. Also note that the scaling of one CCD (like ccd=9) can affect others because we force the normalisations to correspond to that which we get from solving the system of M exposures of N CCDs.  {quote}  have a greater impact on these calibs?    Please investigate the cause of this offset.",8,DM-6491,datamanagement,investigate offset baseline zeropoint lsst vs. hsc stack reduction hsc visit dm-6490 report offset calibration zeropoint hsc vs. lsst processccd.py run report additional offset see certain hsc visit see figure show dm-6490 visit 1322 attach figure visit 19696 run identical setup config stack dm-6490 additional offset common zp figure i.e. flux scale zp=33.0 comparison good guess present calibration frame different hsc lsst stack timeframe visit e.g. input ingest exactly set bug regard flag flat note dm-5124 quote find difference code statistic hsc code use hard code mask ignore list detected lsst code use configurable mask ignore list default detected bad default overridden produce large difference ccd bad amp e.g. ccd=9 small difference ccd=49 number bad pixel small note scaling ccd like ccd=9 affect force normalisation correspond solve system exposure ccds quote great impact calib investigate cause offset,"Investigate offset in baseline zeropoint between LSST vs. HSC stack reductions for some HSC visits DM-6490 reports on an offset between the calibration zeropoints between HSC vs. LSST *processCcd.py* runs. Here we report another, additional, offset seen in certain HSC visits. It is not seen in the figures shown in DM-6490 for visit 1322. However, here attach the same figures for visit 19696, run with identical setups/configs for both stacks as in DM-6490, where we see an additional offset in the ""common ZP"" figures (i.e. all fluxes have been scaled the same zp=33.0 for comparison). A best guess at present is that the calibration frames are different between the HSC and LSST stacks for the timeframe of this visit; e.g. were the inputs ingested exactly the same for both sets? Did the bug in regards to flagging on the flats noted in DM-5124: {quote} I found a difference in the codes doing the statistics: the HSC code uses a hard-coded mask ignore list of DETECTED only, while the LSST code uses a configurable mask ignore list that defaults to DETECTED,BAD (and the default isn't overridden). This produces a large difference on CCDs with bad amps (e.g., ccd=9). There's a smaller difference on ccd=49 because the number of BAD pixels is smaller. Also note that the scaling of one CCD (like ccd=9) can affect others because we force the normalisations to correspond to that which we get from solving the system of M exposures of N CCDs. {quote} have a greater impact on these calibs? Please investigate the cause of this offset."
"Better error messages from the camera mapper when a template cannot be formatted The CameraMapper produces a very unhelpful traceback if it cannot format a template string with the provided data ID dict. For example:  {code}  Traceback (most recent call last):    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_tasks/12.0.rc1-3-gb785bf9/bin/processCcd.py"", line 25, in <module>      ProcessCcdTask.parseAndRun()    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun      parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 479, in parse_args      self._processDataIds(namespace)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 577, in _processDataIds      dataIdContainer.makeDataRefList(namespace)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 126, in makeDataRefList      dataRef=dr)]    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 935, in dataExists      return butler.datasetExists(datasetType = datasetType, dataId = dataRef.dataId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/butler.py"", line 288, in datasetExists      locations = self.repository.map(datasetType, dataId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 392, in map      return self.doParents(Repository.doMap, *args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 325, in doParents      res = func(parent, *args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 405, in doMap      loc = self._mapper.map(*args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/mapper.py"", line 169, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 284, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/mapping.py"", line 123, in map      path = mapper._mapActualToPath(self.template, actualId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 732, in _mapActualToPath      return template % self._transformId(actualId)  TypeError: %d format: a number is required, not NoneType  {code}    It is unclear what string was being formatted with what data, making the problem difficult to diagnose and correct.    I suggest changing line 732 of CameraMapper.py from:  {code}  return template % self._transformId(actualId)  {code}  to something like the following:  {code}  try:      transformedId = self._transformId(actualId)      return template % transformedId  except Exception as e:      raise RuntimeError(""Failed to format %r with data %r: %s"" % (template, transformedId, e))  {code}    Here are the last few lines of the same traceback after applying this change:  {code}      path = mapper._mapActualToPath(self.template, actualId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 735, in _mapActualToPath      raise RuntimeError(""Failed to format %r with data %r: %s"" % (template, transformedId, e))  RuntimeError: Failed to format '%(date)s/%(filter)s/decam%(visit)07d.fits.fz[%(hdu)d]' with data {'date': '2013-02-10', 'ccdnum': 10, 'hdu': None, 'visit': 176837, 'filter': 'z'}: %d format: a number is required, not NoneType  {code}    A bit wordy, but it is much easier to figure out what went wrong.    I have stumbled across this problem twice in the last few weeks, so I consider this change fairly important. The first time it was caused by a defective format string in a paf file. This time I'm not yet sure what is causing it, but at least I have something to go on.",1,DM-6494,datamanagement,"well error message camera mapper template format cameramapper produce unhelpful traceback format template string provide data id dict example code traceback recent file /users rowen uw lsst lsstsw stack darwinx86 pipe_tasks/12.0.rc1 gb785bf9 bin processccd.py line 25 processccdtask.parseandrun file /users rowen uw lsst lsstsw stack darwinx86 pipe_base/12.0.rc1 g832266b python lsst pipe base cmdlinetask.py line 450 parseandrun parsedcmd argumentparser.parse_args(config config args args log log override cls.applyoverride file /users rowen uw lsst lsstsw stack darwinx86 pipe_base/12.0.rc1 g832266b python lsst pipe base argumentparser.py line 479 parse_args self._processdataids(namespace file /users rowen uw lsst lsstsw stack darwinx86 pipe_base/12.0.rc1 g832266b python lsst pipe base argumentparser.py line 577 processdataids dataidcontainer.makedatareflist(namespace file /users rowen uw lsst lsstsw stack darwinx86 pipe_base/12.0.rc1 g832266b python lsst pipe base argumentparser.py line 126 makedatareflist dataref dr file /users rowen uw lsst lsstsw stack darwinx86 pipe_base/12.0.rc1 g832266b python lsst pipe base argumentparser.py line 935 dataexist return butler.datasetexists(datasettype datasettype dataid dataref.dataid file /users rowen uw lsst lsstsw stack darwinx86 daf_persistence/12.0.rc1 gc553c11 python lsst daf persistence butler.py line 288 datasetexist location self.repository.map(datasettype dataid file /users rowen uw lsst lsstsw stack darwinx86 daf_persistence/12.0.rc1 gc553c11 python lsst daf persistence repository.py line 392 map return self.doparents(repository.domap args kwargs file /users rowen uw lsst lsstsw stack darwinx86 daf_persistence/12.0.rc1 gc553c11 python lsst daf persistence repository.py line 325 doparent re func(parent args kwargs file /users rowen uw lsst lsstsw stack darwinx86 daf_persistence/12.0.rc1 gc553c11 python lsst daf persistence repository.py line 405 domap loc self._mapper.map(*args kwargs file /users rowen uw lsst lsstsw stack darwinx86 daf_persistence/12.0.rc1 gc553c11 python lsst daf persistence mapper.py line 169 map return func(self.validate(dataid write file /users rowen uw lsst lsstsw stack darwinx86 daf_butlerutils/12.0.rc1 python lsst daf butlerutil cameramapper.py line 284 mapclosure return mapping.map(mapper dataid write file /users rowen uw lsst lsstsw stack darwinx86 daf_butlerutils/12.0.rc1 python lsst daf butlerutil mapping.py line 123 map path mapper._mapactualtopath(self.template actualid file /users rowen uw lsst lsstsw stack darwinx86 daf_butlerutils/12.0.rc1 python lsst daf butlerutil cameramapper.py line 732 mapactualtopath return template self._transformid(actualid typeerror format number require nonetype code unclear string format datum make problem difficult diagnose correct suggest change line 732 cameramapper.py code return template self._transformid(actualid code like follow code try transformedid self._transformid(actualid return template transformedid exception raise runtimeerror(""faile format datum template transformedid code line traceback apply change code path mapper._mapactualtopath(self.template actualid file /users rowen uw lsst lsstsw stack darwinx86 daf_butlerutils/12.0.rc1 python lsst daf butlerutil cameramapper.py line 735 mapactualtopath raise runtimeerror(""faile format datum template transformedid runtimeerror fail format date)s/%(filter)s decam%(visit)07d.fits.fz[%(hdu)d datum date 2013 02 10 ccdnum 10 hdu visit 176837 filter format number require nonetype code bit wordy easy figure go wrong stumble problem twice week consider change fairly important time cause defective format string paf file time sure cause","Better error messages from the camera mapper when a template cannot be formatted The CameraMapper produces a very unhelpful traceback if it cannot format a template string with the provided data ID dict. For example: {code} Traceback (most recent call last): File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_tasks/12.0.rc1-3-gb785bf9/bin/processCcd.py"", line 25, in  ProcessCcdTask.parseAndRun() File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 479, in parse_args self._processDataIds(namespace) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 577, in _processDataIds dataIdContainer.makeDataRefList(namespace) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 126, in makeDataRefList dataRef=dr)] File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 935, in dataExists return butler.datasetExists(datasetType = datasetType, dataId = dataRef.dataId) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/butler.py"", line 288, in datasetExists locations = self.repository.map(datasetType, dataId) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 392, in map return self.doParents(Repository.doMap, *args, **kwargs) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 325, in doParents res = func(parent, *args, **kwargs) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 405, in doMap loc = self._mapper.map(*args, **kwargs) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/mapper.py"", line 169, in map return func(self.validate(dataId), write) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 284, in mapClosure return mapping.map(mapper, dataId, write) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/mapping.py"", line 123, in map path = mapper._mapActualToPath(self.template, actualId) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 732, in _mapActualToPath return template % self._transformId(actualId) TypeError: %d format: a number is required, not NoneType {code} It is unclear what string was being formatted with what data, making the problem difficult to diagnose and correct. I suggest changing line 732 of CameraMapper.py from: {code} return template % self._transformId(actualId) {code} to something like the following: {code} try: transformedId = self._transformId(actualId) return template % transformedId except Exception as e: raise RuntimeError(""Failed to format %r with data %r: %s"" % (template, transformedId, e)) {code} Here are the last few lines of the same traceback after applying this change: {code} path = mapper._mapActualToPath(self.template, actualId) File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 735, in _mapActualToPath raise RuntimeError(""Failed to format %r with data %r: %s"" % (template, transformedId, e)) RuntimeError: Failed to format '%(date)s/%(filter)s/decam%(visit)07d.fits.fz[%(hdu)d]' with data {'date': '2013-02-10', 'ccdnum': 10, 'hdu': None, 'visit': 176837, 'filter': 'z'}: %d format: a number is required, not NoneType {code} A bit wordy, but it is much easier to figure out what went wrong. I have stumbled across this problem twice in the last few weeks, so I consider this change fairly important. The first time it was caused by a defective format string in a paf file. This time I'm not yet sure what is causing it, but at least I have something to go on."
"Assist IN2P3 engineer in loading DC2013 data sample Bogdan Vulpescu, IN2P3 engineer, tried to load DC2013 data sample. Fabrice help was required to install Qserv in multi-nodes and understand data-loading system.    Some issues have been found and will be reported in future tickets:    - a script to publish loaded data (i.e. insert db name in qservw_worker.Dbs) would be useful  - mysql client might break proxy if option are not provided correctly (a bug report will be available soon)",3,DM-6497,datamanagement,assist in2p3 engineer loading dc2013 datum sample bogdan vulpescu in2p3 engineer try load dc2013 datum sample fabrice help require install qserv multi nodes understand data loading system issue find report future ticket script publish load datum i.e. insert db qservw_worker dbs useful mysql client break proxy option provide correctly bug report available soon,"Assist IN2P3 engineer in loading DC2013 data sample Bogdan Vulpescu, IN2P3 engineer, tried to load DC2013 data sample. Fabrice help was required to install Qserv in multi-nodes and understand data-loading system. Some issues have been found and will be reported in future tickets: - a script to publish loaded data (i.e. insert db name in qservw_worker.Dbs) would be useful - mysql client might break proxy if option are not provided correctly (a bug report will be available soon)"
"Assist IN2P3 student in using Openstack and following LSST coding standards [~oachbal] has written a code to automate Qserv cluster boot on Openstack cloud. Soma support was required to understand and solve cloud-init and openstack issues, Qserv container deployment and LSST coding standards.",6,DM-6498,datamanagement,assist in2p3 student openstack follow lsst code standard ~oachbal write code automate qserv cluster boot openstack cloud soma support require understand solve cloud init openstack issue qserv container deployment lsst code standard,"Assist IN2P3 student in using Openstack and following LSST coding standards [~oachbal] has written a code to automate Qserv cluster boot on Openstack cloud. Soma support was required to understand and solve cloud-init and openstack issues, Qserv container deployment and LSST coding standards."
"Regrid needed to for WebGrid When compute the points for the grid lines, there is no guarantee that the number of points will all be the same.  However, the points can be regrided to ensure all the lines have the same number of points.",6,DM-6501,datamanagement,regrid need webgrid compute point grid line guarantee number point point regride ensure line number point,"Regrid needed to for WebGrid When compute the points for the grid lines, there is no guarantee that the number of points will all be the same. However, the points can be regrided to ensure all the lines have the same number of points."
"setup test framework Need to decide what to check so we have a consistent testing, requirement is opensource  - language  - license  - maturity  - funding  - ease of install  -- dependencies  - OS requirements  - ease to create a workflow  - ability to execute on clusters/laptop  - test with simple LSST workflow  - willingness to meet and answer our questions  -- open bug reporting site  -- speed at which bugs are resolved  -- size of community, external collaborators  - how big a graph can it support?  - is shared filesystem required for data, or can it take care of data transfer  - does it support MPI other parallel code?  - smart wrt to data available on node (optional)  ",2,DM-6502,datamanagement,setup test framework need decide check consistent testing requirement opensource language license maturity funding ease install dependency os requirement ease create workflow ability execute cluster laptop test simple lsst workflow willingness meet answer question open bug reporting site speed bug resolve size community external collaborator big graph support share filesystem require datum care data transfer support mpi parallel code smart wrt datum available node optional,"setup test framework Need to decide what to check so we have a consistent testing, requirement is opensource - language - license - maturity - funding - ease of install -- dependencies - OS requirements - ease to create a workflow - ability to execute on clusters/laptop - test with simple LSST workflow - willingness to meet and answer our questions -- open bug reporting site -- speed at which bugs are resolved -- size of community, external collaborators - how big a graph can it support? - is shared filesystem required for data, or can it take care of data transfer - does it support MPI other parallel code? - smart wrt to data available on node (optional)"
pegasus Review Pegasus Workflow Management System (https://pegasus.isi.edu) against criteria defined in the epic.,6,DM-6503,datamanagement,pegasus review pegasus workflow management system https://pegasus.isi.edu criterion define epic,pegasus Review Pegasus Workflow Management System (https://pegasus.isi.edu) against criteria defined in the epic.
Swift Review [Swift|http://swift-lang.org/main/index.php] scripting language against criteria defined in the epic.,6,DM-6504,datamanagement,swift review swift|http://swift lang.org main index.php script language criterion define epic,Swift Review [Swift|http://swift-lang.org/main/index.php] scripting language against criteria defined in the epic.
"Verification Plan Systems Engineering Status Review This follows on from  [DM-5315] and covers collating comments from DMLT, submitting the status report and document to Systems Engineering and dealing with the comments.     ",4,DM-6510,datamanagement,verification plan systems engineering status review follow dm-5315 cover collating comment dmlt submit status report document systems engineering deal comment,"Verification Plan Systems Engineering Status Review This follows on from [DM-5315] and covers collating comments from DMLT, submitting the status report and document to Systems Engineering and dealing with the comments."
"Remove unsused ""version.h"" file and associated code This code seems obsolete and unused:    {code:bash}  qserv@clrinfopc04:~/src/qserv (tickets/DM-5967)$ grep -r ""version.h"" *  admin/tools/docker/git/src/qserv/site_scons/genversion.py:# genversion.py : declare a builder for global version headers.  admin/tools/docker/git/src/qserv/site_scons/genversion.py:    """"""Construct a version header from git-describe output and store  admin/tools/docker/git/src/qserv/core/modules/SConscript:versionFile = env.Command(['global/version.h'], None, genversion.buildVersionHeader)  core/modules/SConscript:versionFile = env.Command(['global/version.h'], None, genversion.buildVersionHeader)  {code}",3,DM-6513,datamanagement,remove unsuse version.h file associate code code obsolete unused code bash qserv@clrinfopc04:~/src qserv ticket dm-5967)$ grep version.h admin tool docker git src qserv site_scon genversion.py genversion.py declare builder global version header admin tool docker git src qserv site_scon genversion.py construct version header git describe output store admin tool docker git src qserv core module sconscript versionfile env command(['global version.h genversion.buildversionheader core module sconscript versionfile env command(['global version.h genversion.buildversionheader code,"Remove unsused ""version.h"" file and associated code This code seems obsolete and unused: {code:bash} qserv@clrinfopc04:~/src/qserv (tickets/DM-5967)$ grep -r ""version.h"" * admin/tools/docker/git/src/qserv/site_scons/genversion.py:# genversion.py : declare a builder for global version headers. admin/tools/docker/git/src/qserv/site_scons/genversion.py: """"""Construct a version header from git-describe output and store admin/tools/docker/git/src/qserv/core/modules/SConscript:versionFile = env.Command(['global/version.h'], None, genversion.buildVersionHeader) core/modules/SConscript:versionFile = env.Command(['global/version.h'], None, genversion.buildVersionHeader) {code}"
"Minor fixes to linearization DM-5462 added linearization to {{IsrTask}} but had a few loose ends which this ticket aims to correct:  - I intended to enable linearization by default, but somehow lost that change.  - I intended to update obs_test to use null linearization, but I forgot and the previous item meant I didn't catch the omission  - It turns out that the butler data proxy object will not work with functors (attempting to call the retrieved item results in an error, rather than resolving the proxy). This is easily worked around by using immediate=True when retrieving linearizers. This didn't show up until DM-6356 because obs_decam is the only camera that uses linearization lookup tables, and obs_subaru avoids the problem by not returning a proxy.  ",1,DM-6514,datamanagement,minor fix linearization dm-5462 add linearization isrtask loose end ticket aim correct intend enable linearization default lose change intend update obs_test use null linearization forget previous item mean catch omission turn butler datum proxy object work functor attempt retrieve item result error resolve proxy easily work immediate true retrieve linearizer dm-6356 obs_decam camera use linearization lookup table obs_subaru avoid problem return proxy,"Minor fixes to linearization DM-5462 added linearization to {{IsrTask}} but had a few loose ends which this ticket aims to correct: - I intended to enable linearization by default, but somehow lost that change. - I intended to update obs_test to use null linearization, but I forgot and the previous item meant I didn't catch the omission - It turns out that the butler data proxy object will not work with functors (attempting to call the retrieved item results in an error, rather than resolving the proxy). This is easily worked around by using immediate=True when retrieving linearizers. This didn't show up until DM-6356 because obs_decam is the only camera that uses linearization lookup tables, and obs_subaru avoids the problem by not returning a proxy."
"Fix scheduler delays caused by mlock call in memman. Locking tables in memory with mmap and mlock greatly increases scan query speeds but makes the worker scheduler unresponsive to interactive queries. This also tends to have only one scheduler (fast, slow, medium) running at a given time.",8,DM-6518,datamanagement,fix scheduler delay cause mlock memman lock table memory mmap mlock greatly increase scan query speed make worker scheduler unresponsive interactive query tend scheduler fast slow medium run give time,"Fix scheduler delays caused by mlock call in memman. Locking tables in memory with mmap and mlock greatly increases scan query speeds but makes the worker scheduler unresponsive to interactive queries. This also tends to have only one scheduler (fast, slow, medium) running at a given time."
Temp local background broken The temp local background feature has been broken and needs to be fixed.,1,DM-6519,datamanagement,temp local background break temp local background feature break need fix,Temp local background broken The temp local background feature has been broken and needs to be fixed.
"Enhance lsst.log by having a Log object and Python interface  Based on branch u/ktlim/getLogger in {{log}} and requests from DM-3532, implement a lsst::log Python interface through Log objects, and allow controllability of logger names and levels in Python.  ",7,DM-6521,datamanagement,enhance lsst.log have log object python interface base branch ktlim getlogger log request dm-3532 implement lsst::log python interface log object allow controllability logg name level python,"Enhance lsst.log by having a Log object and Python interface Based on branch u/ktlim/getLogger in {{log}} and requests from DM-3532, implement a lsst::log Python interface through Log objects, and allow controllability of logger names and levels in Python."
Capture ProjMgmt WG Long Term Planning conclusions in DMTN-020 The ProjMgmt WG is going to agree on a strategy for long term planning. Make sure it's captured in DMTN-020.,3,DM-6524,datamanagement,capture projmgmt wg long term planning conclusion dmtn-020 projmgmt wg go agree strategy long term planning sure capture dmtn-020,Capture ProjMgmt WG Long Term Planning conclusions in DMTN-020 The ProjMgmt WG is going to agree on a strategy for long term planning. Make sure it's captured in DMTN-020.
"Investigate single frame processing astrometry failures/poor solutions on some HSC chip/visits. The astrometric solution of some visit/ccd combinations for HSC data are failing or finding very poor solutions.  This typically occurs for the outermost (highly fringed) ccds (e.g. 100..103, 95).  I provide some sample output below.    {code:title=LSST bad fit: visit=19696 ccd=100}  processCcd.calibrate.astrometry.refObjLoader: Loaded 71 reference objects  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 71 stars  processCcd.calibrate.astrometry.matcher: Purged 4436 unusable sources, leaving 288 usable sources  processCcd.calibrate.astrometry.matcher: Matched 6 sources  processCcd.calibrate.astrometry.matcher WARNING: Number of matches is smaller than request  processCcd.calibrate.astrometry: Matched and fit WCS in 1 iterations; found 6 matches with scatter = 0.000 +- 0.000 arcsec  {code}    {code:title=HSC fit: visit=19696 ccd=100}  2016-06-10T17:13:54: processCcd.calibrate.astrometry: Found 80 catalog sources  2016-06-10T17:13:54: processCcd.calibrate.astrometry: Matching to 119/148 good input sources  2016-06-10T17:13:55: processCcd.calibrate.astrometry: Matched 20 sources  2016-06-10T17:13:55: processCcd.calibrate.astrometry WARNING: Number of matches is smaller than request  2016-06-10T17:13:55: processCcd.calibrate.astrometry: 20 astrometric matches for 100, 0_31  2016-06-10T17:13:55: processCcd.calibrate.astrometry: Refitting WCS  2016-06-10T17:13:55: processCcd.calibrate.astrometry: Astrometric scatter: 0.038076 arcsec (with non-linear terms, 20 matches, 0 rejected)  {code}    {code:title=LSST failed fit: visit=19696 ccd=103}  processCcd.calibrate.astrometry.refObjLoader: Loaded 68 reference objects  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 68 stars  processCcd.calibrate.astrometry.matcher: Purged 2206 unusable sources, leaving 225 usable sources  processCcd.calibrate.astrometry.matcher: Matched 4 sources  processCcd.calibrate.astrometry.matcher WARNING: Number of matches is smaller than request  processCcd FATAL: Failed on dataId={'taiObs': '2015-01-21', 'pointing': 1116, 'visit': 19696, 'dateObs': '2015-01-21', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 103, 'expTime': 300.0}:     File ""src/sip/CreateWcsWithSip.cc"", line 142, in lsst::meas::astrom::sip::CreateWcsWithSip<MatchT>::CreateWcsWithSip(const std::vector<_RealType>&, const lsst::afw::image::Wcs&, int, const lsst::afw::geom::Box2I&, int) [with MatchT = lsst::afw::table::Match<lsst::afw::table::SimpleRecord, lsst::afw::table::SourceRecord>]      Number of matches less than requested sip order {0}  lsst::pex::exceptions::LengthError: 'Number of matches less than requested sip order'  {code}    {code:title=HSC fit: visit=19696 ccd=103}  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Found 84 catalog sources  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Matching to 137/162 good input sources  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Matched 19 sources  2016-06-10T17:20:11: processCcd.calibrate.astrometry WARNING: Number of matches is smaller than request  2016-06-10T17:20:11: processCcd.calibrate.astrometry: 19 astrometric matches for 103, 1_31  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Refitting WCS  2016-06-10T17:20:11: processCcd.calibrate.astrometry: Astrometric scatter: 0.086131 arcsec (with non-linear terms, 18 matches, 1 rejected)  {code}    Other failed visit/ccd combos:  visit=19684 ccd=101  visit=30488 ccd=95: RuntimeError: Unable to match sources    This may simply be due to some threshold in the configs that is rejecting more stars on the LSST side, but this is not confirmed.  Please investigate the cause of these failures.",6,DM-6529,datamanagement,investigate single frame processing astrometry failure poor solution hsc chip visit astrometric solution visit ccd combination hsc datum fail find poor solution typically occur outermost highly fringed ccd e.g. 100 .. 103 95 provide sample output code title lsst bad fit visit=19696 ccd=100 processccd.calibrate.astrometry.refobjloader loaded 71 reference object processccd.calibrate.astrometry.matcher filterstar purge reference star leave 71 star processccd.calibrate.astrometry.matcher purge 4436 unusable source leave 288 usable source processccd.calibrate.astrometry.matcher match source processccd.calibrate.astrometry.matcher warning number match small request processccd.calibrate.astrometry matched fit wcs iteration find match scatter 0.000 0.000 arcsec code code title hsc fit visit=19696 ccd=100 2016 06 10t17:13:54 processccd.calibrate.astrometry find 80 catalog source 2016 06 10t17:13:54 processccd.calibrate.astrometry match 119/148 good input source 2016 06 10t17:13:55 processccd.calibrate.astrometry match 20 source 2016 06 10t17:13:55 processccd.calibrate.astrometry warning number match small request 2016 06 10t17:13:55 processccd.calibrate.astrometry 20 astrometric match 100 0_31 2016 06 10t17:13:55 processccd.calibrate.astrometry refit wcs 2016 06 10t17:13:55 processccd.calibrate.astrometry astrometric scatter 0.038076 arcsec non linear term 20 match reject code code title lsst fail fit visit=19696 ccd=103 processccd.calibrate.astrometry.refobjloader loaded 68 reference object processccd.calibrate.astrometry.matcher filterstar purge reference star leave 68 star processccd.calibrate.astrometry.matcher purge 2206 unusable source leave 225 usable source processccd.calibrate.astrometry.matcher match source processccd.calibrate.astrometry.matcher warning number match small request processccd fatal fail dataid={'taiobs 2015 01 21 point 1116 visit 19696 dateobs 2015 01 21 filter hsc field ssp_udeep_cosmo ccd 103 exptime 300.0 file src sip createwcswithsip.cc line 142 lsst::meas::astrom::sip::createwcswithsip::createwcswithsip(const std::vector<_realtype const lsst::afw::image::wcs int const lsst::afw::geom::box2i int matcht lsst::afw::table::match number match request sip order lsst::pex::exceptions::lengtherror number match request sip order code code title hsc fit visit=19696 ccd=103 2016 06 10t17:20:11 processccd.calibrate.astrometry find 84 catalog source 2016 06 10t17:20:11 processccd.calibrate.astrometry match 137/162 good input source 2016 06 10t17:20:11 processccd.calibrate.astrometry matched 19 source 2016 06 10t17:20:11 processccd.calibrate.astrometry warning number match small request 2016 06 10t17:20:11 processccd.calibrate.astrometry 19 astrometric match 103 1_31 2016 06 10t17:20:11 processccd.calibrate.astrometry refit wcs 2016 06 10t17:20:11 processccd.calibrate.astrometry astrometric scatter 0.086131 arcsec non linear term 18 match reject code fail visit ccd combo visit=19684 ccd=101 visit=30488 ccd=95 runtimeerror unable match source simply threshold config reject star lsst confirm investigate cause failure,"Investigate single frame processing astrometry failures/poor solutions on some HSC chip/visits. The astrometric solution of some visit/ccd combinations for HSC data are failing or finding very poor solutions. This typically occurs for the outermost (highly fringed) ccds (e.g. 100..103, 95). I provide some sample output below. {code:title=LSST bad fit: visit=19696 ccd=100} processCcd.calibrate.astrometry.refObjLoader: Loaded 71 reference objects processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 71 stars processCcd.calibrate.astrometry.matcher: Purged 4436 unusable sources, leaving 288 usable sources processCcd.calibrate.astrometry.matcher: Matched 6 sources processCcd.calibrate.astrometry.matcher WARNING: Number of matches is smaller than request processCcd.calibrate.astrometry: Matched and fit WCS in 1 iterations; found 6 matches with scatter = 0.000 +- 0.000 arcsec {code} {code:title=HSC fit: visit=19696 ccd=100} 2016-06-10T17:13:54: processCcd.calibrate.astrometry: Found 80 catalog sources 2016-06-10T17:13:54: processCcd.calibrate.astrometry: Matching to 119/148 good input sources 2016-06-10T17:13:55: processCcd.calibrate.astrometry: Matched 20 sources 2016-06-10T17:13:55: processCcd.calibrate.astrometry WARNING: Number of matches is smaller than request 2016-06-10T17:13:55: processCcd.calibrate.astrometry: 20 astrometric matches for 100, 0_31 2016-06-10T17:13:55: processCcd.calibrate.astrometry: Refitting WCS 2016-06-10T17:13:55: processCcd.calibrate.astrometry: Astrometric scatter: 0.038076 arcsec (with non-linear terms, 20 matches, 0 rejected) {code} {code:title=LSST failed fit: visit=19696 ccd=103} processCcd.calibrate.astrometry.refObjLoader: Loaded 68 reference objects processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 68 stars processCcd.calibrate.astrometry.matcher: Purged 2206 unusable sources, leaving 225 usable sources processCcd.calibrate.astrometry.matcher: Matched 4 sources processCcd.calibrate.astrometry.matcher WARNING: Number of matches is smaller than request processCcd FATAL: Failed on dataId={'taiObs': '2015-01-21', 'pointing': 1116, 'visit': 19696, 'dateObs': '2015-01-21', 'filter': 'HSC-I', 'field': 'SSP_UDEEP_COSMOS', 'ccd': 103, 'expTime': 300.0}: File ""src/sip/CreateWcsWithSip.cc"", line 142, in lsst::meas::astrom::sip::CreateWcsWithSip::CreateWcsWithSip(const std::vector<_RealType>&, const lsst::afw::image::Wcs&, int, const lsst::afw::geom::Box2I&, int) [with MatchT = lsst::afw::table::Match] Number of matches less than requested sip order {0} lsst::pex::exceptions::LengthError: 'Number of matches less than requested sip order' {code} {code:title=HSC fit: visit=19696 ccd=103} 2016-06-10T17:20:11: processCcd.calibrate.astrometry: Found 84 catalog sources 2016-06-10T17:20:11: processCcd.calibrate.astrometry: Matching to 137/162 good input sources 2016-06-10T17:20:11: processCcd.calibrate.astrometry: Matched 19 sources 2016-06-10T17:20:11: processCcd.calibrate.astrometry WARNING: Number of matches is smaller than request 2016-06-10T17:20:11: processCcd.calibrate.astrometry: 19 astrometric matches for 103, 1_31 2016-06-10T17:20:11: processCcd.calibrate.astrometry: Refitting WCS 2016-06-10T17:20:11: processCcd.calibrate.astrometry: Astrometric scatter: 0.086131 arcsec (with non-linear terms, 18 matches, 1 rejected) {code} Other failed visit/ccd combos: visit=19684 ccd=101 visit=30488 ccd=95: RuntimeError: Unable to match sources This may simply be due to some threshold in the configs that is rejecting more stars on the LSST side, but this is not confirmed. Please investigate the cause of these failures."
"LDM-151 adjustments Adding text to LDM-151 where appropriate, working around the structure defined by Jim et al.",2,DM-6533,datamanagement,ldm-151 adjustment add text ldm-151 appropriate work structure define jim et al,"LDM-151 adjustments Adding text to LDM-151 where appropriate, working around the structure defined by Jim et al."
"lsst-dev shared stack should provide release builds The shared stack on {{lsst-dev}} (etc) currently only provides tagged weekly builds of the LSST stack. Releases, RCs, etc are not included. Please update the build script so that they are.    NB simply including these builds is easy enough by changing the {{VERSION_GLOB}} regular expression in {{shared_stack.py}}. However, the {{current}} version is selected by a lexicographic sort of available versions. That works well enough for weekly builds ({{w_2016_XX}} is less than {{w_2016_XX+1}}), but fails with other tags. Better use a sort based on the date the tag was created on the HTTP server instead, perhaps.",1,DM-6541,datamanagement,lsst dev share stack provide release build share stack lsst dev etc currently provide tag weekly build lsst stack release rcs etc include update build script nb simply include build easy change version_glob regular expression shared_stack.py current version select lexicographic sort available version work weekly build w_2016_xx w_2016_xx+1 fail tag well use sort base date tag create http server instead,"lsst-dev shared stack should provide release builds The shared stack on {{lsst-dev}} (etc) currently only provides tagged weekly builds of the LSST stack. Releases, RCs, etc are not included. Please update the build script so that they are. NB simply including these builds is easy enough by changing the {{VERSION_GLOB}} regular expression in {{shared_stack.py}}. However, the {{current}} version is selected by a lexicographic sort of available versions. That works well enough for weekly builds ({{w_2016_XX}} is less than {{w_2016_XX+1}}), but fails with other tags. Better use a sort based on the date the tag was created on the HTTP server instead, perhaps."
"Prevent external viewer from popup blockers. Currently, when external viewer launched, it is blocked by pop-up blockers. Need to change polling logic to a pushed solution so the 'launch' action can happen immediately. ",3,DM-6542,datamanagement,prevent external viewer popup blocker currently external viewer launch block pop blocker need change polling logic push solution launch action happen immediately,"Prevent external viewer from popup blockers. Currently, when external viewer launched, it is blocked by pop-up blockers. Need to change polling logic to a pushed solution so the 'launch' action can happen immediately."
Release Note integration for 12_0 Stack Release Transcribe v12_0 release notes prepared by development teams on Confluence into the Pipelines documentation sphinx project.    Pipelines documentation is published with LSST the Docs to https://pipelines.lsst.io.,3,DM-6544,datamanagement,release note integration 12_0 stack release transcribe v12_0 release note prepare development team confluence pipelines documentation sphinx project pipeline documentation publish lsst docs https://pipelines.lsst.io,Release Note integration for 12_0 Stack Release Transcribe v12_0 release notes prepared by development teams on Confluence into the Pipelines documentation sphinx project. Pipelines documentation is published with LSST the Docs to https://pipelines.lsst.io.
Setup mononode test environment for initial learning about installations How to load data and perform queries.  Investigate DAX interface to qserv.,4,DM-6545,datamanagement,setup mononode test environment initial learning installation load datum perform query investigate dax interface qserv,Setup mononode test environment for initial learning about installations How to load data and perform queries. Investigate DAX interface to qserv.
"Add queryId to messages at start and end of user queries. The queryId, ""QI=xxx:"", needs to be added to log messages that are useful for analysis. of primary interest are messages that indicate that a query has begun or finished, such as ""Discarded UserQuerySelect"".",1,DM-6546,datamanagement,add queryid message start end user query queryid qi xxx need add log message useful analysis primary interest message indicate query begin finish discard userqueryselect,"Add queryId to messages at start and end of user queries. The queryId, ""QI=xxx:"", needs to be added to log messages that are useful for analysis. of primary interest are messages that indicate that a query has begun or finished, such as ""Discarded UserQuerySelect""."
Capture proposed epic review procedure in DMTN-020 See notes at https://confluence.lsstcorp.org/display/DM/ProjMgmt+Meeting+2016-06-14,1,DM-6547,datamanagement,capture propose epic review procedure dmtn-020 note https://confluence.lsstcorp.org/display/dm/projmgmt+meeting+2016-06-14,Capture proposed epic review procedure in DMTN-020 See notes at https://confluence.lsstcorp.org/display/DM/ProjMgmt+Meeting+2016-06-14
"Capture release policy in DMTN-020 Capture the policy for releases (all work to be done 2 weeks before end of cycle, release at the cycle changeover) in DMTN-020.",1,DM-6548,datamanagement,capture release policy dmtn-020 capture policy release work week end cycle release cycle changeover dmtn-020,"Capture release policy in DMTN-020 Capture the policy for releases (all work to be done 2 weeks before end of cycle, release at the cycle changeover) in DMTN-020."
Attend SBAG prep meeting at UW [~nlust] will travel to UW for preparatory discussions in advance of this month's SBAG meeting.,8,DM-6552,datamanagement,attend sbag prep meeting uw ~nlust travel uw preparatory discussion advance month sbag meeting,Attend SBAG prep meeting at UW [~nlust] will travel to UW for preparatory discussions in advance of this month's SBAG meeting.
Fix order of flags in Kron photometry The flags are not added to the flag handler in the correct order for Kron photometry.,1,DM-6561,datamanagement,fix order flag kron photometry flag add flag handler correct order kron photometry,Fix order of flags in Kron photometry The flags are not added to the flag handler in the correct order for Kron photometry.
"Clean-up rerun documentation Following DM-4443, there are a few ambiguities in the new {{--rerun}} documentation. Fix them.",1,DM-6563,datamanagement,clean rerun documentation follow dm-4443 ambiguity new --rerun documentation fix,"Clean-up rerun documentation Following DM-4443, there are a few ambiguities in the new {{--rerun}} documentation. Fix them."
Make updateSourceCoords and updateRefCentroids more visible Implement RFC-197 to make updateSourceCoords and updateRefCentroids more visible,1,DM-6566,datamanagement,updatesourcecoords updaterefcentroids visible implement rfc-197 updatesourcecoords updaterefcentroids visible,Make updateSourceCoords and updateRefCentroids more visible Implement RFC-197 to make updateSourceCoords and updateRefCentroids more visible
"Further prep for SBAG meeting, attend video telecon with Heidi et al. Read back ground materials on LSST moving object simulations. This will be used to prepare for both the SBAG meeting and to come up with questions that need clarification in the preparatory telecons.",5,DM-6568,datamanagement,prep sbag meeting attend video telecon heidi et al read ground material lsst move object simulation prepare sbag meeting come question need clarification preparatory telecon,"Further prep for SBAG meeting, attend video telecon with Heidi et al. Read back ground materials on LSST moving object simulations. This will be used to prepare for both the SBAG meeting and to come up with questions that need clarification in the preparatory telecons."
"Remove the extra init method from the SourceDetectionTask SourceDetectionTask defines both {{init(self, schema=None, **kwds)}} and {{\_\_init\_\_(self, schema=None, **kwds)}}. The first exists purely because of a Doxygen bug that makes {{\copydoc \_\_init\_\_}} fail. However,   {code}  copydoc \_\_init\_\_  {code}  works. Remove the non-dunder init method and update the documentation with  {code}  \copydoc \_\_init\_\_  {code}.",1,DM-6569,datamanagement,remove extra init method sourcedetectiontask sourcedetectiontask define init(self schema kwds \_\_init\_\_(self schema kwds exist purely doxygen bug make \copydoc \_\_init\_\ fail code copydoc \_\_init\_\ code work remove non dunder init method update documentation code \copydoc \_\_init\_\ code,"Remove the extra init method from the SourceDetectionTask SourceDetectionTask defines both {{init(self, schema=None, **kwds)}} and {{\_\_init\_\_(self, schema=None, **kwds)}}. The first exists purely because of a Doxygen bug that makes {{\copydoc \_\_init\_\_}} fail. However, {code} copydoc \_\_init\_\_ {code} works. Remove the non-dunder init method and update the documentation with {code} \copydoc \_\_init\_\_ {code}."
Refactor Known Issues and Metrics pages in Pipelines Docs Make Known Issues and Metric Report both top-level pages. Link to installation issues from installation page.    See https://pipelines.lsst.io/v/DM-6575/index.html,1,DM-6575,datamanagement,refactor known issues metric page pipelines docs known issues metric report level page link installation issue installation page https://pipelines.lsst.io/v/dm-6575/index.html,Refactor Known Issues and Metrics pages in Pipelines Docs Make Known Issues and Metric Report both top-level pages. Link to installation issues from installation page. See https://pipelines.lsst.io/v/DM-6575/index.html
"Convert jointcalTask unittest into a validation measure Now that jointcal has some basic unittests that check whether the relative and absolute astrometry are less than some value, we should convert those tests into validation measures a la [validate_drp|https://github.com/lsst/validate_drp]. This would help us track whether we are actually improving things as we tweak the algorithm and the mappings that we fit.",4,DM-6577,datamanagement,convert jointcaltask unittest validation measure jointcal basic unittest check relative absolute astrometry value convert test validation measure la validate_drp|https://github.com lsst validate_drp help track actually improve thing tweak algorithm mapping fit,"Convert jointcalTask unittest into a validation measure Now that jointcal has some basic unittests that check whether the relative and absolute astrometry are less than some value, we should convert those tests into validation measures a la [validate_drp|https://github.com/lsst/validate_drp]. This would help us track whether we are actually improving things as we tweak the algorithm and the mappings that we fit."
"Initial tests running HTCondor jobs utilizing Shifter We start with initial tests of Shifter, with the first goal to  submit PBS jobs on the Blue Waters test system utilizing Shifter that start HTCondor master/startd daemons on compute nodes.  These daemons will communicate to a remote HTCondor central manager (e.g., running on the Nebula OpenStack) and glide-in to join a working pool.  The setup will then be tested with simple payload jobs (these  submitted from a Nebula instance running the schedd)  that verify access to the LSST stack within the UDI (User Defined Image).",4,DM-6578,datamanagement,initial test run htcondor job utilize shifter start initial test shifter goal submit pbs job blue waters test system utilize shifter start htcondor master startd daemon compute node daemon communicate remote htcondor central manager e.g. run nebula openstack glide join work pool setup test simple payload job submit nebula instance run schedd verify access lsst stack udi user defined image,"Initial tests running HTCondor jobs utilizing Shifter We start with initial tests of Shifter, with the first goal to submit PBS jobs on the Blue Waters test system utilizing Shifter that start HTCondor master/startd daemons on compute nodes. These daemons will communicate to a remote HTCondor central manager (e.g., running on the Nebula OpenStack) and glide-in to join a working pool. The setup will then be tested with simple payload jobs (these submitted from a Nebula instance running the schedd) that verify access to the LSST stack within the UDI (User Defined Image)."
"Understand and ensure variance plane compliance with diffim decorrelation Understand how the variance plane should be adjusted in the decorrelation (ZOGY) correction, and ensure it is being done correctly.",8,DM-6580,datamanagement,understand ensure variance plane compliance diffim decorrelation understand variance plane adjust decorrelation zogy correction ensure correctly,"Understand and ensure variance plane compliance with diffim decorrelation Understand how the variance plane should be adjusted in the decorrelation (ZOGY) correction, and ensure it is being done correctly."
Decrease warning messages in dipoleFitTask The dipoleFitTask was spitting out too many warnings. Change many of those to debug statements and remove the `lmfit` UserWarnings.,1,DM-6581,datamanagement,decrease warn message dipolefittask dipolefittask spit warning change debug statement remove lmfit userwarnings,Decrease warning messages in dipoleFitTask The dipoleFitTask was spitting out too many warnings. Change many of those to debug statements and remove the `lmfit` UserWarnings.
"Design a metadata system for LSST code and documentation repositories (technote) This ticket involves the research and design of a metadata system for describing LSST code and documentation repositories. Such metadata would be leveraged by DocHub and LSST the Docs (see [SQR-011|https://sqr-011.lsst.io]) and would reside as a YAML/JSON file in a resource’s GitHub repository.    [JSON-LD|http://json-ld.org] is of particular interest. I’m also consulting with GitHub, ADS, Zenodo, and CfA Library on making a sustainable system.    *Note: this story should be moved to a DocHub epic.*",3,DM-6582,datamanagement,design metadata system lsst code documentation repository technote ticket involve research design metadata system describe lsst code documentation repository metadata leverage dochub lsst docs sqr-011|https://sqr-011.lsst.io reside yaml json file resource github repository json ld|http://json ld.org particular interest consult github ads zenodo cfa library make sustainable system note story move dochub epic,"Design a metadata system for LSST code and documentation repositories (technote) This ticket involves the research and design of a metadata system for describing LSST code and documentation repositories. Such metadata would be leveraged by DocHub and LSST the Docs (see [SQR-011|https://sqr-011.lsst.io]) and would reside as a YAML/JSON file in a resource s GitHub repository. [JSON-LD|http://json-ld.org] is of particular interest. I m also consulting with GitHub, ADS, Zenodo, and CfA Library on making a sustainable system. *Note: this story should be moved to a DocHub epic.*"
Adapt qa analysis script for LSST vs. HSC coadd processing comparison The analysis script was adapted for single visit processing comparisons in DM-4393 and DM-4730.  Do the same here for coadd processing comparisons.,4,DM-6588,datamanagement,adapt qa analysis script lsst vs. hsc coadd process comparison analysis script adapt single visit processing comparison dm-4393 dm-4730 coadd processing comparison,Adapt qa analysis script for LSST vs. HSC coadd processing comparison The analysis script was adapted for single visit processing comparisons in DM-4393 and DM-4730. Do the same here for coadd processing comparisons.
Implement exception translators in upstream pybind11 Pybind11 does not currently support translation of custom exceptions. This ticket tracks work done on upstream pybind11 (internal fork https://github.com/lsst-dm/pybind11-1) to implement this functionality. It should support functionality equivalent to (but not necessarily with the same API) as Boost Python exception translators (http://www.boost.org/doc/libs/1_61_0/libs/python/doc/html/reference/high_level_components/boost_python_exception_translato.html).,4,DM-6591,datamanagement,implement exception translator upstream pybind11 pybind11 currently support translation custom exception ticket track work upstream pybind11 internal fork https://github.com/lsst-dm/pybind11-1 implement functionality support functionality equivalent necessarily api boost python exception translator http://www.boost.org/doc/libs/1_61_0/libs/python/doc/html/reference/high_level_components/boost_python_exception_translato.html,Implement exception translators in upstream pybind11 Pybind11 does not currently support translation of custom exceptions. This ticket tracks work done on upstream pybind11 (internal fork https://github.com/lsst-dm/pybind11-1) to implement this functionality. It should support functionality equivalent to (but not necessarily with the same API) as Boost Python exception translators (http://www.boost.org/doc/libs/1_61_0/libs/python/doc/html/reference/high_level_components/boost_python_exception_translato.html).
"firefly api related issues due to irsa integration. * firefly_loader.js mistakenly uses relative path to load dependencies when it should resolve it via location of the loading script.  * TablePanel should render html content as html by default.  * paging bar style does not show correctly in irsa html  * row height does not resize to the icon size, the old api did. and the default row selectable is set to false in the old api.  * the help button needs to be added on top of the table panel.  * the expand button does not function as expected (open a full table panel).",6,DM-6593,datamanagement,firefly api relate issue irsa integration firefly_loader.js mistakenly use relative path load dependency resolve location loading script tablepanel render html content html default page bar style correctly irsa html row height resize icon size old api default row selectable set false old api help button need add table panel expand button function expect open table panel,"firefly api related issues due to irsa integration. * firefly_loader.js mistakenly uses relative path to load dependencies when it should resolve it via location of the loading script. * TablePanel should render html content as html by default. * paging bar style does not show correctly in irsa html * row height does not resize to the icon size, the old api did. and the default row selectable is set to false in the old api. * the help button needs to be added on top of the table panel. * the expand button does not function as expected (open a full table panel)."
"Write command-line driver tutorial for LSST@Europe2 meeting This will be done as DMTN-023 so the results are preserved for posterity.    This may be somewhat redundant with the work Mandeep Gill is doing in translating HSC docs, but I need it now; we can merge later.",1,DM-6596,datamanagement,write command line driver tutorial lsst@europe2 meet dmtn-023 result preserve posterity somewhat redundant work mandeep gill translate hsc doc need merge later,"Write command-line driver tutorial for LSST@Europe2 meeting This will be done as DMTN-023 so the results are preserved for posterity. This may be somewhat redundant with the work Mandeep Gill is doing in translating HSC docs, but I need it now; we can merge later."
Prepare presentation for SPIE Write the presentation for the SPIE conference. Date of presentation: 26th June.,2,DM-6598,datamanagement,prepare presentation spie write presentation spie conference date presentation 26th june,Prepare presentation for SPIE Write the presentation for the SPIE conference. Date of presentation: 26th June.
"Clean up naming of multiband tasks and scripts Several of the multiband processing tasks and files in pipe_tasks and pipe_drivers have inconsistent names:   - Some task names do not agree with the script names.   - Words like ""Coadd"" and ""Merged"" are not consistently used.     Actually making these changes is trivial, but the work also requires creating and shepherding an RFC.",1,DM-6600,datamanagement,clean name multiband task script multiband processing task file pipe_task pipe_driver inconsistent name task name agree script name word like coadd merged consistently actually make change trivial work require create shepherd rfc,"Clean up naming of multiband tasks and scripts Several of the multiband processing tasks and files in pipe_tasks and pipe_drivers have inconsistent names: - Some task names do not agree with the script names. - Words like ""Coadd"" and ""Merged"" are not consistently used. Actually making these changes is trivial, but the work also requires creating and shepherding an RFC."
"Port change to EXP-ID handling From [HSC-1409|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1409]:  {quote}  Due to an operational reason (to meet the requirement of Subaru FITS dictionary), the definition of EXP-ID is soon to be changed in the data acquisition side.  In the new definition, EXP-ID is set to 'HSCE%08d' where the letter 'E' is fixed as requested in the dictionary, and the number part corresponds to exactly the same number as our familiar 'visit'.  obs_subaru:ingest.py needs to be updated to include this rule.  The data taken with this change so far are:  HSCA07441200--HSCA07441757  HSCA90925200--HSCA90929557  {quote}    The change made as part of HSC-1409 introduces a new code path for the updated data, while old data continue to be supported with the old code path.",1,DM-6601,datamanagement,port change exp id handle hsc-1409|https://hsc jira.astro.princeton.edu jira browse hsc-1409 quote operational reason meet requirement subaru fits dictionary definition exp id soon change datum acquisition new definition exp id set hsce%08d letter fix request dictionary number correspond exactly number familiar visit obs_subaru ingest.py need update include rule datum take change far hsca07441200 -hsca07441757 hsca90925200 -hsca90929557 quote change hsc-1409 introduce new code path update datum old datum continue support old code path,"Port change to EXP-ID handling From [HSC-1409|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1409]: {quote} Due to an operational reason (to meet the requirement of Subaru FITS dictionary), the definition of EXP-ID is soon to be changed in the data acquisition side. In the new definition, EXP-ID is set to 'HSCE%08d' where the letter 'E' is fixed as requested in the dictionary, and the number part corresponds to exactly the same number as our familiar 'visit'. obs_subaru:ingest.py needs to be updated to include this rule. The data taken with this change so far are: HSCA07441200--HSCA07441757 HSCA90925200--HSCA90929557 {quote} The change made as part of HSC-1409 introduces a new code path for the updated data, while old data continue to be supported with the old code path."
Reporting improvements read and critique Jacek's  LPM document and the more manual oriented work from John Swinbank.  Phone con with Kevin w.r.t. reporting channel for equipment expenses in Jira (as opposed to the now clear separate distinct financial channel).  Worked out checklist and principals for revised WBS. ,6,DM-6603,datamanagement,report improvement read critique jacek lpm document manual orient work john swinbank phone con kevin w.r.t report channel equipment expense jira oppose clear separate distinct financial channel work checklist principal revise wbs,Reporting improvements read and critique Jacek's LPM document and the more manual oriented work from John Swinbank. Phone con with Kevin w.r.t. reporting channel for equipment expenses in Jira (as opposed to the now clear separate distinct financial channel). Worked out checklist and principals for revised WBS.
Finalize v12 Pipelines release documentation Add the release announcement and finalize other documentation details in pipelines.lsst.io for the v12 release.,1,DM-6608,datamanagement,finalize v12 pipelines release documentation add release announcement finalize documentation detail pipelines.lsst.io v12 release,Finalize v12 Pipelines release documentation Add the release announcement and finalize other documentation details in pipelines.lsst.io for the v12 release.
"Make HSC processing without bright object catalogs easier obs_subaru enables bright object masks by default, as that's desirable for HSC production runs.      However, when HSC data is processed without bright object masks available (as will happen in most GO observations and development use), multiBandDriver.py will fail because the BRIGHT_OBJECT mask plane is not present but the base_PixelFlags algorithm is configured to make use of it. This is confusing, and it also requires the definition of a configuration file to fix the problem because base_PixelFlags cannot be configured directly on the command-line.    Some possibilities for fixing this:   - Add the BRIGHT_OBJECT mask plane in AssembleCoadd if doMaskBrightObjects is True but the external catalog is not found.  This will make the PixelFlags operation a silent no-op.   - Allow configuration options to allow PixelFlags algorithm to silently skip some flags if the appropriate masks are not available.    I am sure there are other options as well.  ",2,DM-6612,datamanagement,hsc processing bright object catalog easy obs_subaru enable bright object mask default desirable hsc production run hsc datum process bright object mask available happen observation development use multibanddriver.py fail mask plane present base_pixelflag algorithm configure use confusing require definition configuration file fix problem base_pixelflags configure directly command line possibility fix add mask plane assemblecoadd domaskbrightobject true external catalog find pixelflags operation silent op allow configuration option allow pixelflags algorithm silently skip flag appropriate mask available sure option,"Make HSC processing without bright object catalogs easier obs_subaru enables bright object masks by default, as that's desirable for HSC production runs. However, when HSC data is processed without bright object masks available (as will happen in most GO observations and development use), multiBandDriver.py will fail because the BRIGHT_OBJECT mask plane is not present but the base_PixelFlags algorithm is configured to make use of it. This is confusing, and it also requires the definition of a configuration file to fix the problem because base_PixelFlags cannot be configured directly on the command-line. Some possibilities for fixing this: - Add the BRIGHT_OBJECT mask plane in AssembleCoadd if doMaskBrightObjects is True but the external catalog is not found. This will make the PixelFlags operation a silent no-op. - Allow configuration options to allow PixelFlags algorithm to silently skip some flags if the appropriate masks are not available. I am sure there are other options as well."
"Include Kron parameters in algorithm metadata The Kron code doesn't set the algorithm metadata.  E.g.  {code}  algMetadata.set(""ext_photometryKron_KronFlux_nRadiusForFlux"",                  config.plugins[""ext_photometryKron_KronFlux""].nRadiusForFlux)  {code}  ",1,DM-6614,datamanagement,"include kron parameter algorithm metadata kron code set algorithm metadata e.g. code algmetadata.set(""ext_photometrykron_kronflux_nradiusforflux config.plugins[""ext_photometrykron_kronflux""].nradiusforflux code","Include Kron parameters in algorithm metadata The Kron code doesn't set the algorithm metadata. E.g. {code} algMetadata.set(""ext_photometryKron_KronFlux_nRadiusForFlux"", config.plugins[""ext_photometryKron_KronFlux""].nRadiusForFlux) {code}"
"Cannot instantiate LoadAstrometryNetObjectsTask without Config object One should be able to create a LoadAstrometryNetObjectsTask without passing a Config object, if one only wants the default configuration. Currently it raises TypeError:    {code}  Traceback (most recent call last):    File ""testJointcal.py"", line 79, in setUp      refLoader = LoadAstrometryNetObjectsTask()  TypeError: __init__() takes at least 2 arguments (1 given)  {code}    If the config object really is a kwarg, it should default None and create a default config, so that one doesn't have to do, e.g.:    {code}  LoadAstrometryNetObjectsTask(LoadAstrometryNetObjectsConfig())  {code}",1,DM-6620,datamanagement,instantiate loadastrometrynetobjectstask config object able create loadastrometrynetobjectstask pass config object want default configuration currently raise typeerror code traceback recent file testjointcal.py line 79 setup refloader loadastrometrynetobjectstask typeerror init take argument give code config object kwarg default create default config e.g. code loadastrometrynetobjectstask(loadastrometrynetobjectsconfig code,"Cannot instantiate LoadAstrometryNetObjectsTask without Config object One should be able to create a LoadAstrometryNetObjectsTask without passing a Config object, if one only wants the default configuration. Currently it raises TypeError: {code} Traceback (most recent call last): File ""testJointcal.py"", line 79, in setUp refLoader = LoadAstrometryNetObjectsTask() TypeError: __init__() takes at least 2 arguments (1 given) {code} If the config object really is a kwarg, it should default None and create a default config, so that one doesn't have to do, e.g.: {code} LoadAstrometryNetObjectsTask(LoadAstrometryNetObjectsConfig()) {code}"
"cleanup non-survey-generic python in jointcal jointcal.py current does things like:    {code}  for dataRef in dataRefs:      if dataRef.dataId[""visit""] == int(visit) and dataRef.dataId[""ccd""] == int(ccd):          ...  {code}    This is not survey generic, and is probably not the best way to identify data blocks anyway. This, and other non-generic things in jointcal.py should be cleaned up so they work across surveys.",4,DM-6621,datamanagement,"cleanup non survey generic python jointcal jointcal.py current thing like code dataref dataref dataref.dataid[""visit int(visit dataref.dataid[""ccd int(ccd code survey generic probably good way identify data block non generic thing jointcal.py clean work survey","cleanup non-survey-generic python in jointcal jointcal.py current does things like: {code} for dataRef in dataRefs: if dataRef.dataId[""visit""] == int(visit) and dataRef.dataId[""ccd""] == int(ccd): ... {code} This is not survey generic, and is probably not the best way to identify data blocks anyway. This, and other non-generic things in jointcal.py should be cleaned up so they work across surveys."
"Fix base_* stuff in CcdImage.cc CcdImage.cc currently has hard-coded a bunch of {{getSchema().find(""base_blah"").key}} things. These should either be replaced with ""slot_*"", config.blahName, or dealt with at a higher level (e.g. not loading all those values directly inside of ccdImage::LoadCatalog).    Once this is done, we should delete the comments at the top of the file.",1,DM-6627,datamanagement,"fix base stuff ccdimage.cc ccdimage.cc currently hard code bunch getschema().find(""base_blah"").key thing replace slot config.blahname deal high level e.g. load value directly inside ccdimage::loadcatalog delete comment file","Fix base_* stuff in CcdImage.cc CcdImage.cc currently has hard-coded a bunch of {{getSchema().find(""base_blah"").key}} things. These should either be replaced with ""slot_*"", config.blahName, or dealt with at a higher level (e.g. not loading all those values directly inside of ccdImage::LoadCatalog). Once this is done, we should delete the comments at the top of the file."
Reimplement diffim decorrelation as task Reimplement the image decorrelation as a subtask rather than a direct call to a function.,6,DM-6628,datamanagement,reimplement diffim decorrelation task reimplement image decorrelation subtask direct function,Reimplement diffim decorrelation as task Reimplement the image decorrelation as a subtask rather than a direct call to a function.
Support ingesting reference catalogs from FITS files Support a means of ingesting index reference catalogs from FITS tables (e.g. SDSS catalogs).,2,DM-6630,datamanagement,support ingest reference catalog fits file support means ingest index reference catalog fits table e.g. sdss catalog,Support ingesting reference catalogs from FITS files Support a means of ingesting index reference catalogs from FITS tables (e.g. SDSS catalogs).
"Single-frame processing tasks are no longer usable without a Butler Adding a butler argument to the constructor signatures for {{CharacterizeImageTask}}, {{CalibrateTask}}, and {{ProcessCcdTask}} makes these tasks difficult to use without a butler.    The fix is to make the butler argument optional (with a default of None), while adding another argument that allows a fully-constructed reference object loader to be provided directly instead.    This is closely related to DM-6597, which has the opposite problem: pipe_drivers' {{SingleFrameDriverTask}} doesn't take a butler argument, but it needs to in order to provide one to {{ProcessCcdTask}}.    I have a fix for this just about ready, but I'd like to add some unit tests that verify we can run all of these tasks both from the command-line and directly before calling it complete.",3,DM-6631,datamanagement,single frame processing task long usable butler add butler argument constructor signature characterizeimagetask calibratetask processccdtask make task difficult use butler fix butler argument optional default add argument allow fully construct reference object loader provide directly instead closely relate dm-6597 opposite problem pipe_driver singleframedrivertask butler argument need order provide processccdtask fix ready like add unit test verify run task command line directly call complete,"Single-frame processing tasks are no longer usable without a Butler Adding a butler argument to the constructor signatures for {{CharacterizeImageTask}}, {{CalibrateTask}}, and {{ProcessCcdTask}} makes these tasks difficult to use without a butler. The fix is to make the butler argument optional (with a default of None), while adding another argument that allows a fully-constructed reference object loader to be provided directly instead. This is closely related to DM-6597, which has the opposite problem: pipe_drivers' {{SingleFrameDriverTask}} doesn't take a butler argument, but it needs to in order to provide one to {{ProcessCcdTask}}. I have a fix for this just about ready, but I'd like to add some unit tests that verify we can run all of these tasks both from the command-line and directly before calling it complete."
"Make match and flag propagation more reusable We have two bits of code for doing spatial matches and propagating flags:   - {{PropagateVistFlagsTask}}: propagates flags from individual visit catalogs to coadd catalogs, and depends on a butler to do so (reasonably; it includes the smarts to load the appropriate catalogs, so it has to do I/O).   - {{CalibrateTask.copyIcSourceFields}}: propagates fields from icSrc to src, but is only usable as part of {{CalibrateTask}}.    Both of these should delegate at least some of their work to new class (possibly a Task) that manages the Schemas, SchemaMappers, and cross-matching necessary to do this work.  This new class should be reusable without a butler and without constructing any higher-level tasks.",4,DM-6632,datamanagement,match flag propagation reusable bit code spatial match propagate flag propagatevistflagstask propagate flag individual visit catalog coadd catalog depend butler reasonably include smart load appropriate catalog calibratetask.copyicsourcefields propagate field icsrc src usable calibratetask delegate work new class possibly task manage schemas schemamappers cross matching necessary work new class reusable butler construct high level task,"Make match and flag propagation more reusable We have two bits of code for doing spatial matches and propagating flags: - {{PropagateVistFlagsTask}}: propagates flags from individual visit catalogs to coadd catalogs, and depends on a butler to do so (reasonably; it includes the smarts to load the appropriate catalogs, so it has to do I/O). - {{CalibrateTask.copyIcSourceFields}}: propagates fields from icSrc to src, but is only usable as part of {{CalibrateTask}}. Both of these should delegate at least some of their work to new class (possibly a Task) that manages the Schemas, SchemaMappers, and cross-matching necessary to do this work. This new class should be reusable without a butler and without constructing any higher-level tasks."
"HSC ISR configuration file is applied to ProcessCcdTask, not IsrTask {{obs_subaru/config/hsc/isr.py}} has its config options specified relative to {{ProcessCcdTask}}'s config hierarchy, not {{IsrTask}}'s.  This allows the ISR task to be retargeted in this file, but it will prevent {{IsrTask}} from being run as a {{CmdLineTask}} directly.    ISR Task retargeting should be moved to {{config/processCcd.py}}, allowing the {{config/isr.py}} level to be moved to the appropriate level.",1,DM-6633,datamanagement,hsc isr configuration file apply processccdtask isrtask obs_subaru config hsc isr.py config option specify relative processccdtask config hierarchy isrtask allow isr task retargete file prevent isrtask run cmdlinetask directly isr task retargete move config processccd.py allow config isr.py level move appropriate level,"HSC ISR configuration file is applied to ProcessCcdTask, not IsrTask {{obs_subaru/config/hsc/isr.py}} has its config options specified relative to {{ProcessCcdTask}}'s config hierarchy, not {{IsrTask}}'s. This allows the ISR task to be retargeted in this file, but it will prevent {{IsrTask}} from being run as a {{CmdLineTask}} directly. ISR Task retargeting should be moved to {{config/processCcd.py}}, allowing the {{config/isr.py}} level to be moved to the appropriate level."
Add JIRA-wrangling howto to DMTN-020 Expand https://dmtn-020.lsst.io/v/DM-6447/#jira-maintenance to describe best practices for T/CAMs working with JIRA. Include:    * Appropriate labels;  * Teams;  * ... other things?,1,DM-6634,datamanagement,add jira wrangle howto dmtn-020 expand https://dmtn-020.lsst.io/v/dm-6447/#jira-maintenance describe good practice cam work jira include appropriate label team thing,Add JIRA-wrangling howto to DMTN-020 Expand https://dmtn-020.lsst.io/v/DM-6447/#jira-maintenance to describe best practices for T/CAMs working with JIRA. Include: * Appropriate labels; * Teams; * ... other things?
"LTD Keeper: Auto slug for edition paths deals with underscores Had a bug where {{utils.auto_slugify_edition}} did not replace underscores with a dash, and therefore failed {{utils.validate_path_slug}}. This created a silent breaked where a branch like {{u/rowen/r12_patch1}} did not get an edition created for it.    This ticket adds this replacement code and adds a test for such a case.",1,DM-6638,datamanagement,ltd keeper auto slug edition path deal underscore bug utils.auto_slugify_edition replace underscore dash fail utils.validate_path_slug create silent break branch like rowen r12_patch1 edition create ticket add replacement code add test case,"LTD Keeper: Auto slug for edition paths deals with underscores Had a bug where {{utils.auto_slugify_edition}} did not replace underscores with a dash, and therefore failed {{utils.validate_path_slug}}. This created a silent breaked where a branch like {{u/rowen/r12_patch1}} did not get an edition created for it. This ticket adds this replacement code and adds a test for such a case."
"IsrTask is not a valid CmdLineTask IsrTask is a command-line task, but its run method does not take a dataRef (it instead has a {{runDataRef}} method.  This is inconsistent with other {{CmdLineTasks}} and more importantly breaks {{parseAndRun}}.    I'm committing a small workaround on DM-6631 to get {{parseAndRun}} working, but the ultimately method names should be made consistent across CmdLineTasks.  That will require an API change and hence an RFC.",1,DM-6640,datamanagement,isrtask valid cmdlinetask isrtask command line task run method dataref instead rundataref method inconsistent cmdlinetasks importantly break parseandrun commit small workaround dm-6631 parseandrun work ultimately method name consistent cmdlinetasks require api change rfc,"IsrTask is not a valid CmdLineTask IsrTask is a command-line task, but its run method does not take a dataRef (it instead has a {{runDataRef}} method. This is inconsistent with other {{CmdLineTasks}} and more importantly breaks {{parseAndRun}}. I'm committing a small workaround on DM-6631 to get {{parseAndRun}} working, but the ultimately method names should be made consistent across CmdLineTasks. That will require an API change and hence an RFC."
Make list of elements for consideration for planning packages Create a detailed checklist for developing the planning packages for the replan and WBS restructuring.,2,DM-6642,datamanagement,list element consideration planning package create detailed checklist develop planning package replan wbs restructuring,Make list of elements for consideration for planning packages Create a detailed checklist for developing the planning packages for the replan and WBS restructuring.
RADICAL-Pilot Review [RADICAL-Pilot|http://radicalpilot.readthedocs.io/en/latest/index.html] against criteria defined in the epic.,6,DM-6643,datamanagement,radical pilot review radical pilot|http://radicalpilot.readthedocs.io en late index.html criterion define epic,RADICAL-Pilot Review [RADICAL-Pilot|http://radicalpilot.readthedocs.io/en/latest/index.html] against criteria defined in the epic.
Makeflow Review Makeflow against criteria defined in the epic.   http://ccl.cse.nd.edu/software/makeflow/,6,DM-6644,datamanagement,makeflow review makeflow criterion define epic http://ccl.cse.nd.edu/software/makeflow/,Makeflow Review Makeflow against criteria defined in the epic. http://ccl.cse.nd.edu/software/makeflow/
pinball Review [pinball|https://github.com/pinterest/pinball] workflow management system.,6,DM-6645,datamanagement,pinball review pinball|https://github.com pinterest pinball workflow management system,pinball Review [pinball|https://github.com/pinterest/pinball] workflow management system.
CloudSlang Review CloudSlang against criteria defined in the epic.   http://cloudslang-docs.readthedocs.io/en/v0.9.60/index.html,6,DM-6646,datamanagement,cloudslang review cloudslang criterion define epic http://cloudslang-docs.readthedocs.io/en/v0.9.60/index.html,CloudSlang Review CloudSlang against criteria defined in the epic. http://cloudslang-docs.readthedocs.io/en/v0.9.60/index.html
Adapt qa analysis script to apply corrections measured by meas_mosaic DM-2674 involves getting HSC's {{meas_mosaic}} working with the LSST stack.  This issue consists of adapting the analysis.py script of DM-4393 & DM-4730 to (optionally) apply the astrometric and photometric solutions derived running {{meas_mosaic}} to the individual visits before comparison.  This is useful in general and is specifically useful in comparing the {{meas_mosaic}} results between the HSC and LSST stacks.,2,DM-6647,datamanagement,adapt qa analysis script apply correction measure meas_mosaic dm-2674 involve get hsc meas_mosaic work lsst stack issue consist adapt analysis.py script dm-4393 dm-4730 optionally apply astrometric photometric solution derive run meas_mosaic individual visit comparison useful general specifically useful compare meas_mosaic result hsc lsst stack,Adapt qa analysis script to apply corrections measured by meas_mosaic DM-2674 involves getting HSC's {{meas_mosaic}} working with the LSST stack. This issue consists of adapting the analysis.py script of DM-4393 & DM-4730 to (optionally) apply the astrometric and photometric solutions derived running {{meas_mosaic}} to the individual visits before comparison. This is useful in general and is specifically useful in comparing the {{meas_mosaic}} results between the HSC and LSST stacks.
"Management level review of two products of the Management process working group Reviewed https://github.com/lsst/LDM-PMT/blob/integration/index.rst and https://dmtn-020.lsst.io/v/DM-6447/    Made extensive markup of LDM-PMT,  delivered to Mario Juric.  Assessed DM-6447,  which show promise of an actual workable manual, though not complete.",1,DM-6650,datamanagement,management level review product management process work group review https://github.com/lsst/ldm-pmt/blob/integration/index.rst https://dmtn-020.lsst.io/v/dm-6447/ extensive markup ldm pmt deliver mario juric assessed dm-6447 promise actual workable manual complete,"Management level review of two products of the Management process working group Reviewed https://github.com/lsst/LDM-PMT/blob/integration/index.rst and https://dmtn-020.lsst.io/v/DM-6447/ Made extensive markup of LDM-PMT, delivered to Mario Juric. Assessed DM-6447, which show promise of an actual workable manual, though not complete."
"Move new reference loader so meas_astrom can use it and perform some cleanup The new reference object loader code lives in pipe_tasks, which means it cannot be directly used by code in meas_astrom. This will hamper separating astrometry.net out of meas_astrom, because unit tests need reference catalogs and meas_astrom cannot depend on pipe_tasks.    Also, I'd like to take a cleanup pass on the module names, so the new code is easier to find, and improve the unit tests.",2,DM-6651,datamanagement,new reference loader meas_astrom use perform cleanup new reference object loader code live pipe_task mean directly code meas_astrom hamper separate astrometry.net meas_astrom unit test need reference catalog meas_astrom depend pipe_task like cleanup pass module name new code easy find improve unit test,"Move new reference loader so meas_astrom can use it and perform some cleanup The new reference object loader code lives in pipe_tasks, which means it cannot be directly used by code in meas_astrom. This will hamper separating astrometry.net out of meas_astrom, because unit tests need reference catalogs and meas_astrom cannot depend on pipe_tasks. Also, I'd like to take a cleanup pass on the module names, so the new code is easier to find, and improve the unit tests."
"Remove database hack DM-5988 introduced a hack in reading the raw files: we use a database to cache metadata from the shutter files and update the camera files at read time.  The camera files have now been ""sanitised"" (updated with the appropriate metadata), and it's time to remove the hack.    [~mfisherlevine] writes:  {quote}  Data is on lsst-dev in:    /nfs/lsst2/photocalData/data/monocam/sanitised9/1m3/1m3    Raw calibs are in:    /nfs/lsst2/photocalData/data/monocam/sanitised9/1m3/calibs    Regarding what I want: everything to be the same, but with a normal ingest, i.e. no splicing, just taking everything that is needed from one set of files. Some points to note:    * should be able to ingest all the raws and calibs files, and register their OBJECT types to allow processing with these as ids (inc. pipe_drivers scripts)  * pipe_drivers master calib scripts should still run (and their outputs still be ingestable)  * processCcd should run  {quote}",2,DM-6652,datamanagement,remove database hack dm-5988 introduce hack read raw file use database cache metadata shutter file update camera file read time camera file sanitise update appropriate metadata time remove hack ~mfisherlevine write quote data lsst dev /nfs lsst2 photocaldata data monocam sanitised9/1m3/1m3 raw calib /nfs lsst2 photocaldata data monocam sanitised9/1m3 calibs want normal ingest i.e. splicing take need set file point note able ingest raw calibs file register object type allow processing ids inc pipe_driver script pipe_driver master calib script run output ingestable processccd run quote,"Remove database hack DM-5988 introduced a hack in reading the raw files: we use a database to cache metadata from the shutter files and update the camera files at read time. The camera files have now been ""sanitised"" (updated with the appropriate metadata), and it's time to remove the hack. [~mfisherlevine] writes: {quote} Data is on lsst-dev in: /nfs/lsst2/photocalData/data/monocam/sanitised9/1m3/1m3 Raw calibs are in: /nfs/lsst2/photocalData/data/monocam/sanitised9/1m3/calibs Regarding what I want: everything to be the same, but with a normal ingest, i.e. no splicing, just taking everything that is needed from one set of files. Some points to note: * should be able to ingest all the raws and calibs files, and register their OBJECT types to allow processing with these as ids (inc. pipe_drivers scripts) * pipe_drivers master calib scripts should still run (and their outputs still be ingestable) * processCcd should run {quote}"
"implement the active target When a dialog such as catalog search is displayed, it should be able to pick up the active target or the coordinates from a highlighted row in a table. Please, implement the mechanism that will automatically pick up those coordinates and pre-fill the search form for you.",6,DM-6653,datamanagement,implement active target dialog catalog search display able pick active target coordinate highlight row table implement mechanism automatically pick coordinate pre fill search form,"implement the active target When a dialog such as catalog search is displayed, it should be able to pick up the active target or the coordinates from a highlighted row in a table. Please, implement the mechanism that will automatically pick up those coordinates and pre-fill the search form for you."
"ffApi image related issues found by irsa integration *-for external image viewer, the default RangeValues causes problem, i.e. other defaults not set-. (FIXED)  *-global default does not always apply to external image viewer-(FIXED DM-7016)  The Gator implementation related to coverage map   (1) default symbol size, shape, color setting is different from that of original map   (2) cannot specify the shape, size, and color through API;   (3) cannot specify the shape, size, and color of a search center through API;   (4) -does not display any image and source when the table has only one ra,dec values, for example:  one table with one position value or one table with many records but has the same ra,dec values.- MOVED to [DM-7001]   (5) -the sources on coverage map are not clickable. However, on table and plot are clickable and work fine.- (FIXED)  ",6,DM-6656,datamanagement,image relate issue find irsa integration -for external image viewer default rangevalues cause problem i.e. default set- fixed -global default apply external image viewer-(fixed dm-7016 gator implementation relate coverage map default symbol size shape color setting different original map specify shape size color api specify shape size color search center api -doe display image source table ra dec value example table position value table record ra dec values.- moved dm-7001 -the source coverage map clickable table plot clickable work fine.- fixed,"ffApi image related issues found by irsa integration *-for external image viewer, the default RangeValues causes problem, i.e. other defaults not set-. (FIXED) *-global default does not always apply to external image viewer-(FIXED DM-7016) The Gator implementation related to coverage map (1) default symbol size, shape, color setting is different from that of original map (2) cannot specify the shape, size, and color through API; (3) cannot specify the shape, size, and color of a search center through API; (4) -does not display any image and source when the table has only one ra,dec values, for example: one table with one position value or one table with many records but has the same ra,dec values.- MOVED to [DM-7001] (5) -the sources on coverage map are not clickable. However, on table and plot are clickable and work fine.- (FIXED)"
"ffApi XYplot related issues found by irsa integration * default  symbol size, shape,and color setting is different from that of original version.  * no XY Plot Options pop-out windows  *  the plot displays non-ascii characters on the panel (for example: Â FitÂ Â )  * miss Filter Dialog on the plot panel comparing with the original version.  *  does not accept default column names for the plot.",6,DM-6657,datamanagement,ffapi xyplot relate issue find irsa integration default symbol size shape color setting different original version xy plot options pop window plot display non ascii character panel example fit miss filter dialog plot panel compare original version accept default column name plot,"ffApi XYplot related issues found by irsa integration * default symbol size, shape,and color setting is different from that of original version. * no XY Plot Options pop-out windows * the plot displays non-ascii characters on the panel (for example: Fit ) * miss Filter Dialog on the plot panel comparing with the original version. * does not accept default column names for the plot."
"CR finder does not care about XY0 of input image Port of [HSC-1391|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1391]:  {quote}  The current version of CR finder does not care about XY0 of the input image and when I try to run CR finder on warped (difference) image, PSF cannot be properly extracted.  {quote}  and:  {quote}  I have noticed that the center of warped image is a gap between CCDs and PSF estimation there will fail. So get PSF without specifying the position is good enough. PSF class will select the best position.  {quote}",1,DM-6660,datamanagement,cr finder care xy0 input image port hsc-1391|https://hsc jira.astro.princeton.edu jira browse hsc-1391 quote current version cr finder care xy0 input image try run cr finder warp difference image psf properly extract quote quote notice center warped image gap ccds psf estimation fail psf specify position good psf class select good position quote,"CR finder does not care about XY0 of input image Port of [HSC-1391|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1391]: {quote} The current version of CR finder does not care about XY0 of the input image and when I try to run CR finder on warped (difference) image, PSF cannot be properly extracted. {quote} and: {quote} I have noticed that the center of warped image is a gap between CCDs and PSF estimation there will fail. So get PSF without specifying the position is good enough. PSF class will select the best position. {quote}"
"ConfigDictField says ""Inequality in keys for..."" even if I give 2 same configurations From [HSC-1401|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1401]:  {quote}  config.py:  {code:python}  from lsst.meas.photocal.colorterms import ColortermGroupConfig    for key in ['i', 'i2', 'y', 'r', 'N1', 'N2', 'N3', 'z']:      root.calibrate.photocal.colorterms.library[key] = ColortermGroupConfig.fromValues({}){code}    This comamnd line  {code:bash}  rm -fr output ; for i in {1..2} ; do processCcd.py ./HSC --output output -C config.py  ; done  {code}  raises following error  {noformat}  2016-06-01T02:43:45: processCcd FATAL: Comparing configuration: Inequality in keys for calibrate.photocal.colorterms.library: ['z', 'i', 'i2', 'r', 'y', 'N1', 'N2', 'N3'] != ['N3', 'i', 'i2', 'r', 'y', 'N1', 'N2', 'z']  2016-06-01T02:43:45: processCcd FATAL: Failed in task initialization: Config does match existing config on disk for this task; tasks configurations must be consistent within the same output repo (override with --clobber-config)  {noformat}  {quote}",1,DM-6661,datamanagement,configdictfield say inequality key configuration hsc-1401|https://hsc jira.astro.princeton.edu jira browse hsc-1401 quote config.py code python lsst.meas.photocal.colorterm import colortermgroupconfig key i2 n1 n2 n3 root.calibrate.photocal.colorterms.library[key colortermgroupconfig.fromvalues({}){code comamnd line code bash rm -fr output .. processccd.py output -c config.py code raise follow error noformat 2016 06 01t02:43:45 processccd fatal compare configuration inequality key calibrate.photocal.colorterms.library i2 n1 n2 n3 n3 i2 n1 n2 2016 06 01t02:43:45 processccd fatal fail task initialization config match exist config disk task task configuration consistent output repo override --clobber config noformat quote,"ConfigDictField says ""Inequality in keys for..."" even if I give 2 same configurations From [HSC-1401|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1401]: {quote} config.py: {code:python} from lsst.meas.photocal.colorterms import ColortermGroupConfig for key in ['i', 'i2', 'y', 'r', 'N1', 'N2', 'N3', 'z']: root.calibrate.photocal.colorterms.library[key] = ColortermGroupConfig.fromValues({}){code} This comamnd line {code:bash} rm -fr output ; for i in {1..2} ; do processCcd.py ./HSC --output output -C config.py ; done {code} raises following error {noformat} 2016-06-01T02:43:45: processCcd FATAL: Comparing configuration: Inequality in keys for calibrate.photocal.colorterms.library: ['z', 'i', 'i2', 'r', 'y', 'N1', 'N2', 'N3'] != ['N3', 'i', 'i2', 'r', 'y', 'N1', 'N2', 'z'] 2016-06-01T02:43:45: processCcd FATAL: Failed in task initialization: Config does match existing config on disk for this task; tasks configurations must be consistent within the same output repo (override with --clobber-config) {noformat} {quote}"
"Investigate why afw.table.IdFactory doesn't allow reserved=0 Setting reserved=0 when constructing a source ID factory (as would be logical when there is no exposure ID to reserve bits for) strangely doesn't work; it seems to be necessary to reserve at least one bit.  This may be a signedness problem (we use signed 64-bit integers for IDs to appease FITS, which is unfortunate), but we should be careful just reducing the number of available bits, as this could break code that expect to read IDs already written to disk.    Note that any change to this code in afw.table may require changes to code in daf.butlerUtils.ExposureIdInfo as well.",2,DM-6664,datamanagement,investigate afw.table idfactory allow set construct source id factory logical exposure id reserve bit strangely work necessary reserve bit signedness problem use sign 64 bit integer id appease fits unfortunate careful reduce number available bit break code expect read id write disk note change code afw.table require change code daf.butlerutil exposureidinfo,"Investigate why afw.table.IdFactory doesn't allow reserved=0 Setting reserved=0 when constructing a source ID factory (as would be logical when there is no exposure ID to reserve bits for) strangely doesn't work; it seems to be necessary to reserve at least one bit. This may be a signedness problem (we use signed 64-bit integers for IDs to appease FITS, which is unfortunate), but we should be careful just reducing the number of available bits, as this could break code that expect to read IDs already written to disk. Note that any change to this code in afw.table may require changes to code in daf.butlerUtils.ExposureIdInfo as well."
"set up unit test for projection in Java While working on DM-6438 (set up unit test for projection in JavaScript), we realized we should have a parallel unit test system set up for Java code, to keep the two systems in sync. ",6,DM-6665,datamanagement,set unit test projection java work dm-6438 set unit test projection javascript realize parallel unit test system set java code system sync,"set up unit test for projection in Java While working on DM-6438 (set up unit test for projection in JavaScript), we realized we should have a parallel unit test system set up for Java code, to keep the two systems in sync."
"Data Backbone conops iteration 4: submit to TCT Submit the document for TCT change control. Process is TBD.     If it is not accepted by TCT, further work is not in the scope of this epic, and would need to be planned in the EV system.",1,DM-6667,datamanagement,data backbone conop iteration submit tct submit document tct change control process tbd accept tct work scope epic need plan ev system,"Data Backbone conops iteration 4: submit to TCT Submit the document for TCT change control. Process is TBD. If it is not accepted by TCT, further work is not in the scope of this epic, and would need to be planned in the EV system."
"Data Backbone conops: develop engineering considerations for BOE for work package Based on the data backbone services conops, develop a list of engineering considerations for making a BOE for the data backbone planning package.",3,DM-6668,datamanagement,data backbone conop develop engineering consideration boe work package base datum backbone service conop develop list engineering consideration make boe data backbone planning package,"Data Backbone conops: develop engineering considerations for BOE for work package Based on the data backbone services conops, develop a list of engineering considerations for making a BOE for the data backbone planning package."
Authentication & Authorization conops iteration 1: create raw draft (internal) Write a raw draft of the concept of operations for authentication and authorization services. In this iteration the document is developed in Google docs following the ConOps template.,5,DM-6669,datamanagement,authentication authorization conop iteration create raw draft internal write raw draft concept operation authentication authorization service iteration document develop google doc follow conops template,Authentication & Authorization conops iteration 1: create raw draft (internal) Write a raw draft of the concept of operations for authentication and authorization services. In this iteration the document is developed in Google docs following the ConOps template.
"Authentication & Authorization conops iteration 2: group review to produce first draft Review raw draft of concept of operations for the AA services to work through underdeveloped areas, clear up uncertainties, and make readable.",2,DM-6670,datamanagement,authentication authorization conop iteration group review produce draft review raw draft concept operation aa service work underdeveloped area clear uncertainty readable,"Authentication & Authorization conops iteration 2: group review to produce first draft Review raw draft of concept of operations for the AA services to work through underdeveloped areas, clear up uncertainties, and make readable."
"Authentication & Authorization conops iteration 3: larger review to produce second draft Review first draft of AA services conops within Data Processing Architecture working group, bringing in relevant experts.    Input from review is incorporated into a second draft.  ",6,DM-6671,datamanagement,authentication authorization conop iteration large review produce second draft review draft aa service conop data processing architecture working group bring relevant expert input review incorporate second draft,"Authentication & Authorization conops iteration 3: larger review to produce second draft Review first draft of AA services conops within Data Processing Architecture working group, bringing in relevant experts. Input from review is incorporated into a second draft."
"Authentication & Authorization conops formatting: convert second draft to reStructuredText When the AA services conops is in a solid state, convert the Google doc to reStructuredText following DM's documentation versioning process.",2,DM-6672,datamanagement,authentication authorization conop format convert second draft restructuredtext aa service conop solid state convert google doc restructuredtext follow dm documentation versioning process,"Authentication & Authorization conops formatting: convert second draft to reStructuredText When the AA services conops is in a solid state, convert the Google doc to reStructuredText following DM's documentation versioning process."
"Authentication & Authorization conops iteration 4: submit to Systems Engineering Submit the document for TCT change control. Process is TBD.    If it is not accepted by TCT, further work is not in the scope of this epic, and would need to be planned in the EV system.",1,DM-6673,datamanagement,authentication authorization conop iteration submit systems engineering submit document tct change control process tbd accept tct work scope epic need plan ev system,"Authentication & Authorization conops iteration 4: submit to Systems Engineering Submit the document for TCT change control. Process is TBD. If it is not accepted by TCT, further work is not in the scope of this epic, and would need to be planned in the EV system."
"Authentication & Authorization conops: develop engineering considerations for BOE for work package Based on the AA services conops, develop a list of engineering considerations for making a BOE for the AA planning package.  ",3,DM-6674,datamanagement,authentication authorization conop develop engineering consideration boe work package base aa service conop develop list engineering consideration make boe aa planning package,"Authentication & Authorization conops: develop engineering considerations for BOE for work package Based on the AA services conops, develop a list of engineering considerations for making a BOE for the AA planning package."
Level 3 Hosting conops iteration 1: create raw draft (internal) Write a raw draft of the concept of operations for Level 3 Hosting services. In this iteration the document is developed in Google docs following the ConOps template.,4,DM-6675,datamanagement,level hosting conop iteration create raw draft internal write raw draft concept operation level hosting service iteration document develop google doc follow conops template,Level 3 Hosting conops iteration 1: create raw draft (internal) Write a raw draft of the concept of operations for Level 3 Hosting services. In this iteration the document is developed in Google docs following the ConOps template.
"Level 3 Hosting conops iteration 2: group review to produce first draft Review raw draft of concept of operations for the L3 Hosting services to work through underdeveloped areas, clear up uncertainties, and make readable.",2,DM-6676,datamanagement,level hosting conop iteration group review produce draft review raw draft concept operation l3 host service work underdeveloped area clear uncertainty readable,"Level 3 Hosting conops iteration 2: group review to produce first draft Review raw draft of concept of operations for the L3 Hosting services to work through underdeveloped areas, clear up uncertainties, and make readable."
"Level 3 Hosting conops iteration 3: larger review to produce second draft Review first draft of Level 3 Hosting services conops within Data Processing Architecture working group, bringing in relevant experts.    Input from review is incorporated into a second draft.  ",6,DM-6677,datamanagement,level hosting conop iteration large review produce second draft review draft level hosting service conop data processing architecture working group bring relevant expert input review incorporate second draft,"Level 3 Hosting conops iteration 3: larger review to produce second draft Review first draft of Level 3 Hosting services conops within Data Processing Architecture working group, bringing in relevant experts. Input from review is incorporated into a second draft."
"Level 3 Hosting conops formatting: convert second draft to reStructuredText When the L3 Hosting services conops is in a solid state, convert the Google doc to reStructuredText following DM's documentation versioning process.",2,DM-6678,datamanagement,level hosting conop format convert second draft restructuredtext l3 hosting service conop solid state convert google doc restructuredtext follow dm documentation versioning process,"Level 3 Hosting conops formatting: convert second draft to reStructuredText When the L3 Hosting services conops is in a solid state, convert the Google doc to reStructuredText following DM's documentation versioning process."
"Level 3 Hosting conops iteration 4: submit to TCT Submit the document for TCT change control. Process is TBD.    If it is not accepted by TCT, further work is not in the scope of this epic, and would need to be planned in the EV system.",1,DM-6679,datamanagement,level hosting conop iteration submit tct submit document tct change control process tbd accept tct work scope epic need plan ev system,"Level 3 Hosting conops iteration 4: submit to TCT Submit the document for TCT change control. Process is TBD. If it is not accepted by TCT, further work is not in the scope of this epic, and would need to be planned in the EV system."
"Level 3 Hosting conops: develop engineering considerations for BOE for work package Based on the L3 Hosting services conops, develop a list of engineering considerations for making a BOE for the L3 Hosting planning package.",3,DM-6680,datamanagement,level hosting conop develop engineering consideration boe work package base l3 hosting service conop develop list engineering consideration make boe l3 host planning package,"Level 3 Hosting conops: develop engineering considerations for BOE for work package Based on the L3 Hosting services conops, develop a list of engineering considerations for making a BOE for the L3 Hosting planning package."
Batch Processing for commissioning conops iteration 1: create raw draft (internal) Write a raw draft of the concept of operations for batch processing services for the commissioning phase. In this iteration the document is developed in Google docs following the ConOps template.,4,DM-6681,datamanagement,batch processing commission conop iteration create raw draft internal write raw draft concept operation batch processing service commission phase iteration document develop google doc follow conops template,Batch Processing for commissioning conops iteration 1: create raw draft (internal) Write a raw draft of the concept of operations for batch processing services for the commissioning phase. In this iteration the document is developed in Google docs following the ConOps template.
"Batch Processing for commissioning conops iteration 2: group review to produce first draft Review raw draft of concept of operations for the batch processing for commissioning services to work through underdeveloped areas, clear up uncertainties, and make readable.",2,DM-6682,datamanagement,batch processing commission conop iteration group review produce draft review raw draft concept operation batch processing commission service work underdeveloped area clear uncertainty readable,"Batch Processing for commissioning conops iteration 2: group review to produce first draft Review raw draft of concept of operations for the batch processing for commissioning services to work through underdeveloped areas, clear up uncertainties, and make readable."
"Batch Processing for commissioning conops formatting: convert first draft to reStructuredText in a technical note When the batch production for commissioning services conops is in a solid state, convert the Google doc to a DM technical note in reStructuredText.",1,DM-6683,datamanagement,batch processing commission conop format convert draft restructuredtext technical note batch production commission service conop solid state convert google doc dm technical note restructuredtext,"Batch Processing for commissioning conops formatting: convert first draft to reStructuredText in a technical note When the batch production for commissioning services conops is in a solid state, convert the Google doc to a DM technical note in reStructuredText."
"Batch Processing for commissioning conops: develop engineering considerations for BOE for work package Based on the batch services for commissioning services conops, develop a list of engineering considerations for making a BOE for the batch services planning package.",3,DM-6684,datamanagement,batch processing commission conop develop engineering consideration boe work package base batch service commission service conop develop list engineering consideration make boe batch service planning package,"Batch Processing for commissioning conops: develop engineering considerations for BOE for work package Based on the batch services for commissioning services conops, develop a list of engineering considerations for making a BOE for the batch services planning package."
"Planning package for Management, Engineering and Integration with engineering judgement BOE based on RACI diagram Following list of elements for consideration (DM-6642), estimate planning packages for Management, Engineering and Integration WBS element.    BOE is derived from RACI document, which list roles and responsibilities of line management, reporting group, steering group, and area technical leads.",1,DM-6685,datamanagement,planning package management engineering integration engineering judgement boe base raci diagram follow list element consideration dm-6642 estimate planning package management engineering integration wbs element boe derive raci document list role responsibility line management report group steering group area technical lead,"Planning package for Management, Engineering and Integration with engineering judgement BOE based on RACI diagram Following list of elements for consideration (DM-6642), estimate planning packages for Management, Engineering and Integration WBS element. BOE is derived from RACI document, which list roles and responsibilities of line management, reporting group, steering group, and area technical leads."
"Planning package for L1 Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Level 1 Services WBS element.    BOE is derived from detailed plan for prompt processing and archiving services created in February and engineering judgement based on conops documents.",1,DM-6686,datamanagement,planning package l1 services engineering judgement boe follow list element consideration dm-6642 estimate planning package level services wbs element boe derive detailed plan prompt processing archiving service create february engineering judgement base conop document,"Planning package for L1 Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Level 1 Services WBS element. BOE is derived from detailed plan for prompt processing and archiving services created in February and engineering judgement based on conops documents."
"Planning package for Batch Production Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Batch Production Services WBS element.    BOE is derived from engineering judgement based on conops documents.  ",1,DM-6687,datamanagement,planning package batch production services engineering judgement boe follow list element consideration dm-6642 estimate planning package batch production services wbs element boe derive engineering judgement base conop document,"Planning package for Batch Production Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Batch Production Services WBS element. BOE is derived from engineering judgement based on conops documents."
"Planning package for Data Backbone Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Data Backbone Services WBS element.    BOE is derived from engineering judgement based on conops documents.  ",1,DM-6688,datamanagement,planning package data backbone services engineering judgement boe follow list element consideration dm-6642 estimate planning package data backbone services wbs element boe derive engineering judgement base conop document,"Planning package for Data Backbone Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Data Backbone Services WBS element. BOE is derived from engineering judgement based on conops documents."
"Planning package for Data Access Hosting Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Data Access Hosting Services WBS element.    BOE is derived from engineering judgement based on conops documents.",1,DM-6689,datamanagement,planning package data access hosting services engineering judgement boe follow list element consideration dm-6642 estimate planning package data access hosting services wbs element boe derive engineering judgement base conop document,"Planning package for Data Access Hosting Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Data Access Hosting Services WBS element. BOE is derived from engineering judgement based on conops documents."
"Planning package for Common Workflow/Middleware with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Common Workflow/Middleware WBS element.    BOE is derived from engineering judgement based on conops documents.",1,DM-6690,datamanagement,planning package common workflow middleware engineering judgement boe follow list element consideration dm-6642 estimate planning package common workflow middleware wbs element boe derive engineering judgement base conop document,"Planning package for Common Workflow/Middleware with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Common Workflow/Middleware WBS element. BOE is derived from engineering judgement based on conops documents."
"Planning package for Misc. Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Miscellaneous Services WBS element. An example is the Authentication and Authorization services.    BOE is derived from engineering judgement based on conops documents.",1,DM-6691,datamanagement,planning package misc service engineering judgement boe follow list element consideration dm-6642 estimate planning package miscellaneous services wbs element example authentication authorization service boe derive engineering judgement base conop document,"Planning package for Misc. Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Miscellaneous Services WBS element. An example is the Authentication and Authorization services. BOE is derived from engineering judgement based on conops documents."
"Planning package for Development Support Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Development Support Services WBS element.    BOE is derived from engineering judgement.",1,DM-6692,datamanagement,planning package development support services engineering judgement boe follow list element consideration dm-6642 estimate planning package development support services wbs element boe derive engineering judgement,"Planning package for Development Support Services with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Development Support Services WBS element. BOE is derived from engineering judgement."
"Planning package for ITC and Fabric Provisioning and Operation with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for ITC Fabric Provisioning and Operation WBS element.    BOE is derived from engineering judgement.  ",1,DM-6693,datamanagement,planning package itc fabric provisioning operation engineering judgement boe follow list element consideration dm-6642 estimate planning package itc fabric provisioning operation wbs element boe derive engineering judgement,"Planning package for ITC and Fabric Provisioning and Operation with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for ITC Fabric Provisioning and Operation WBS element. BOE is derived from engineering judgement."
"Planning package for Service Management with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Service Management WBS element.    BOE is derived from engineering judgement based on ITIL methodology.",1,DM-6694,datamanagement,planning package service management engineering judgement boe follow list element consideration dm-6642 estimate planning package service management wbs element boe derive engineering judgement base itil methodology,"Planning package for Service Management with engineering judgement BOE Following list of elements for consideration (DM-6642), estimate planning packages for Service Management WBS element. BOE is derived from engineering judgement based on ITIL methodology."
Submit change request Submit formal change request to restructure NCSA WBS in PMCS.,1,DM-6695,datamanagement,submit change request submit formal change request restructure ncsa wbs pmcs,Submit change request Submit formal change request to restructure NCSA WBS in PMCS.
Revise Level 1 ConOps     Revise the Level 1 conops to incorporate the minimal required functionality of Level 1 Services: minimal data archiving of camera data and minimal transport via data backbone to NCSA.,4,DM-6696,datamanagement,revise level conops revise level conop incorporate minimal require functionality level service minimal datum archiving camera datum minimal transport datum backbone ncsa,Revise Level 1 ConOps Revise the Level 1 conops to incorporate the minimal required functionality of Level 1 Services: minimal data archiving of camera data and minimal transport via data backbone to NCSA.
Discuss elements of RFC Discuss elements of RFC (technical details and scope).,6,DM-6700,datamanagement,discuss element rfc discuss element rfc technical detail scope,Discuss elements of RFC Discuss elements of RFC (technical details and scope).
Produce RFC Write up and submit RFC.,2,DM-6701,datamanagement,produce rfc write submit rfc,Produce RFC Write up and submit RFC.
Respond to RFC comments and update RFC as needed Respond to RFC comments and update RFC as needed.,2,DM-6702,datamanagement,respond rfc comment update rfc need respond rfc comment update rfc need,Respond to RFC comments and update RFC as needed Respond to RFC comments and update RFC as needed.
"Select workflow based on conops and review of workflow systems Based on use cases/requirements gathered in DM-6270 and evaluation reports completed in DM-6276, select workflow system.",2,DM-6705,datamanagement,select workflow base conop review workflow system base use case requirement gather dm-6270 evaluation report complete dm-6276 select workflow system,"Select workflow based on conops and review of workflow systems Based on use cases/requirements gathered in DM-6270 and evaluation reports completed in DM-6276, select workflow system."
"Discuss elements of RFC Discuss elements of workflow RFC (technical details, scope, requirements).",4,DM-6706,datamanagement,discuss element rfc discuss element workflow rfc technical detail scope requirement,"Discuss elements of RFC Discuss elements of workflow RFC (technical details, scope, requirements)."
"Pull down and install OCS SAL code in prep for ConOps development At the Camera Workshop in Mid-June, OCS Team members suggested that DM pull down their Service Abstraction Layer software and gain familiarity with it. The User manual is being studied before compiling the software and running it with DM software as a means of simulating planned Telescope & Site processes and how DM will interact with them.  Most of the work for this epic will be conducted in August. This story captures our prep work.",4,DM-6709,datamanagement,pull install ocs sal code prep conops development camera workshop mid june ocs team member suggest dm pull service abstraction layer software gain familiarity user manual study compile software run dm software means simulate plan telescope site process dm interact work epic conduct august story capture prep work,"Pull down and install OCS SAL code in prep for ConOps development At the Camera Workshop in Mid-June, OCS Team members suggested that DM pull down their Service Abstraction Layer software and gain familiarity with it. The User manual is being studied before compiling the software and running it with DM software as a means of simulating planned Telescope & Site processes and how DM will interact with them. Most of the work for this epic will be conducted in August. This story captures our prep work."
Monitoring plan for Startup procedure Identify startup processes to be monitored for health and to provide notification for startup failure.,1,DM-6710,datamanagement,monitoring plan startup procedure identify startup process monitor health provide notification startup failure,Monitoring plan for Startup procedure Identify startup processes to be monitored for health and to provide notification for startup failure.
"Message Dictionary additions Message types for system bookkeeping acknowledgements as well as report messages were added to the existing dictionary and the means for acting upon these message types are being added to component prototype code.  In addition, needed changes were made to the existing dictionary so all reporting entities write more complete details to their report message queues.",4,DM-6712,datamanagement,message dictionary addition message type system bookkeepe acknowledgement report message add exist dictionary mean act message type add component prototype code addition need change exist dictionary report entity write complete detail report message queue,"Message Dictionary additions Message types for system bookkeeping acknowledgements as well as report messages were added to the existing dictionary and the means for acting upon these message types are being added to component prototype code. In addition, needed changes were made to the existing dictionary so all reporting entities write more complete details to their report message queues."
Amendments to message interaction Proper acknowledgements began being added to the messaging system this month.,6,DM-6713,datamanagement,amendment message interaction proper acknowledgement begin add message system month,Amendments to message interaction Proper acknowledgements began being added to the messaging system this month.
Camera Workshop attendance Work on preliminary specific additions to the camera interaction ConOps took place this month during attendance at the Camera Workshop,6,DM-6714,datamanagement,camera workshop attendance work preliminary specific addition camera interaction conops take place month attendance camera workshop,Camera Workshop attendance Work on preliminary specific additions to the camera interaction ConOps took place this month during attendance at the Camera Workshop
"Use Shifter+HTCondor  in processing Stripe82 ref data at modest scale To test out processing at modest scales (~ 100 -- 1000 cores)  utilizing Shifter+HTCondor on machines like BW, organize processing (processCcd of obs_sdss) of stripe82 data similar to that used in the lsst_dm_stack_demo (run=4192 field=300).",6,DM-6715,datamanagement,use shifter+htcondor process stripe82 ref datum modest scale test processing modest scale 100 -- 1000 core utilize shifter+htcondor machine like bw organize processing processccd obs_sdss stripe82 datum similar lsst_dm_stack_demo run=4192 field=300,"Use Shifter+HTCondor in processing Stripe82 ref data at modest scale To test out processing at modest scales (~ 100 -- 1000 cores) utilizing Shifter+HTCondor on machines like BW, organize processing (processCcd of obs_sdss) of stripe82 data similar to that used in the lsst_dm_stack_demo (run=4192 field=300)."
"Add tests for order of flags to all measurment plugins In the meas_base framework, we independently define an enumeration of available flags [(e.g.)|https://github.com/lsst/meas_extensions_photometryKron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/include/lsst/meas/extensions/photometryKron.h#L82] and a set of table fields for storing flags [(e.g.)|https://github.com/lsst/meas_extensions_photometryKron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/src/KronPhotometry.cc#L422]. We implicitly assume that these are declared in the same order, but do not, in general, enforce this.    In DM-6561, these were found *not* to be in the same order in meas_extensions_photometryKron. Setting a flag based on a bad result would therefore set the wrong flag in the output table.    In the DM-6561 solution, we introduced a test for this which is specific to the photometryKron codebase. However, the basic structure of the test would be easily extended to cover all meas_base plugins to ensure this error can never occur. Do so.",3,DM-6723,datamanagement,add test order flag measurment plugin meas_base framework independently define enumeration available flag e.g.)|https://github.com lsst meas_extensions_photometrykron blob cba01575dab0cd609c7e2a3f3d08632b94f97f58 include lsst mea extension photometrykron.h#l82 set table field store flag e.g.)|https://github.com lsst meas_extensions_photometrykron blob cba01575dab0cd609c7e2a3f3d08632b94f97f58 src kronphotometry.cc#l422 implicitly assume declare order general enforce dm-6561 find order meas_extensions_photometrykron set flag base bad result set wrong flag output table dm-6561 solution introduce test specific photometrykron codebase basic structure test easily extend cover meas_base plugin ensure error occur,"Add tests for order of flags to all measurment plugins In the meas_base framework, we independently define an enumeration of available flags [(e.g.)|https://github.com/lsst/meas_extensions_photometryKron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/include/lsst/meas/extensions/photometryKron.h#L82] and a set of table fields for storing flags [(e.g.)|https://github.com/lsst/meas_extensions_photometryKron/blob/cba01575dab0cd609c7e2a3f3d08632b94f97f58/src/KronPhotometry.cc#L422]. We implicitly assume that these are declared in the same order, but do not, in general, enforce this. In DM-6561, these were found *not* to be in the same order in meas_extensions_photometryKron. Setting a flag based on a bad result would therefore set the wrong flag in the output table. In the DM-6561 solution, we introduced a test for this which is specific to the photometryKron codebase. However, the basic structure of the test would be easily extended to cover all meas_base plugins to ensure this error can never occur. Do so."
"Default chart and other optimizations These are the changes to support defalt chart and single chart type (as for IRSA release)  - Remove chart selection from chart area  - Use dropdown for chart selection (can be omitted if single chart type is used)  - Populate current values in chart options  - Support Clear and Reset in chart options  - For tables, connected to charts, if no default parameters are specified, default chart is an XY plot with CATALOG_COORD_COLS (used to produce an overlay) for catalogs or two first numeric columns for other tables.  - Label, matching column expression, and unit, matching table model, are default parameters for both app and api now.  ",6,DM-6726,datamanagement,default chart optimization change support defalt chart single chart type irsa release remove chart selection chart area use dropdown chart selection omit single chart type populate current value chart option support clear reset chart option table connect chart default parameter specify default chart xy plot catalog_coord_cols produce overlay catalog numeric column table label match column expression unit match table model default parameter app api,"Default chart and other optimizations These are the changes to support defalt chart and single chart type (as for IRSA release) - Remove chart selection from chart area - Use dropdown for chart selection (can be omitted if single chart type is used) - Populate current values in chart options - Support Clear and Reset in chart options - For tables, connected to charts, if no default parameters are specified, default chart is an XY plot with CATALOG_COORD_COLS (used to produce an overlay) for catalogs or two first numeric columns for other tables. - Label, matching column expression, and unit, matching table model, are default parameters for both app and api now."
"Camera workshop attendance Attend camera workshop, Meeting did not fully address the need.  Travel to SLAC.",8,DM-6728,datamanagement,camera workshop attendance attend camera workshop meeting fully address need travel slac,"Camera workshop attendance Attend camera workshop, Meeting did not fully address the need. Travel to SLAC."
Service Management for F16 June Dividing F16 Service Management  ~ monthly.,4,DM-6752,datamanagement,service management f16 june dividing f16 service management monthly,Service Management for F16 June Dividing F16 Service Management ~ monthly.
Service Management for F16 July  Dividing F16 Service Management  ~ monthly.  ,4,DM-6753,datamanagement,service management f16 july dividing f16 service management monthly,Service Management for F16 July Dividing F16 Service Management ~ monthly.
Service Management for F16 August  Dividing F16 Service Management  ~ monthly.,4,DM-6754,datamanagement,service management f16 august dividing f16 service management monthly,Service Management for F16 August Dividing F16 Service Management ~ monthly.
"remove SizeMagnitudeStarSelector The sizeMagnitudeStarSelector is still in meas_algorithms, but it is unused and likely no longer works. We should either remove it, or update it to be fully supported.    The same holds true for any other C++-based star selectors we still have lying around.",1,DM-6781,datamanagement,remove sizemagnitudestarselector sizemagnitudestarselector meas_algorithm unused likely long work remove update fully support hold true c++-based star selector lie,"remove SizeMagnitudeStarSelector The sizeMagnitudeStarSelector is still in meas_algorithms, but it is unused and likely no longer works. We should either remove it, or update it to be fully supported. The same holds true for any other C++-based star selectors we still have lying around."
Add support for deriving from Python exception types to pybind11 DM-6302 adds support for custom exception translators to pybind11. However exceptions mapped do not inherit from Python {{BaseException}} or higher. This prevents exceptions from being raised and caught with {{except Exception as e}} in Python. This behaviour also occurs with Boost Python and Swig (we hack around it with a pure Python wrapper).    This ticket aims to solve the problem by adding support for inheritance from Python exception types to pybind11.,4,DM-6783,datamanagement,add support derive python exception type pybind11 dm-6302 add support custom exception translator pybind11 exception map inherit python baseexception high prevent exception raise catch exception python behaviour occur boost python swig hack pure python wrapper ticket aim solve problem add support inheritance python exception type pybind11,Add support for deriving from Python exception types to pybind11 DM-6302 adds support for custom exception translators to pybind11. However exceptions mapped do not inherit from Python {{BaseException}} or higher. This prevents exceptions from being raised and caught with {{except Exception as e}} in Python. This behaviour also occurs with Boost Python and Swig (we hack around it with a pure Python wrapper). This ticket aims to solve the problem by adding support for inheritance from Python exception types to pybind11.
Port meas_extensions_convolved from HSC HSC has a new measurement extension: meas_extensions_convolved.  This performs aperture photometry with the PSF degraded to nominated seeings (similar to how galaxy photometry is commonly done these days).    Relevant HSC tickets are [HSC-1395|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1395] and [HSC-1408|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1408].,5,DM-6784,datamanagement,port meas_extensions_convolve hsc hsc new measurement extension meas_extensions_convolve perform aperture photometry psf degrade nominate seeing similar galaxy photometry commonly day relevant hsc ticket hsc-1395|https://hsc jira.astro.princeton.edu jira browse hsc-1395 hsc-1408|https://hsc jira.astro.princeton.edu jira browse hsc-1408,Port meas_extensions_convolved from HSC HSC has a new measurement extension: meas_extensions_convolved. This performs aperture photometry with the PSF degraded to nominated seeings (similar to how galaxy photometry is commonly done these days). Relevant HSC tickets are [HSC-1395|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1395] and [HSC-1408|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1408].
"Port parent/child measurement from HSC The deblender sometimes gets into trouble with cluster galaxies, and the deblended fluxes aren't accurate.  In that case it helps to have measurements on the image without any deblending having been performed.  This is a feature used in HSC's mid-2016 production run afterburner, ticket [HSC-1400|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1400].  This feature should be ported for use in LSST.",5,DM-6785,datamanagement,port parent child measurement hsc deblender get trouble cluster galaxy deblended flux accurate case help measurement image deblending having perform feature hsc mid-2016 production run afterburner ticket hsc-1400|https://hsc jira.astro.princeton.edu jira browse hsc-1400 feature port use lsst,"Port parent/child measurement from HSC The deblender sometimes gets into trouble with cluster galaxies, and the deblended fluxes aren't accurate. In that case it helps to have measurements on the image without any deblending having been performed. This is a feature used in HSC's mid-2016 production run afterburner, ticket [HSC-1400|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1400]. This feature should be ported for use in LSST."
"Document meas_extensions_ngmix meas_extensions_ngmix has no useful documentation, not even a {{doc}} directory. Add some.    This should include at least an overview of the package contents, a description of its capabilities, and instructions on enabling it within the meas_base framework. The package should have a README.",2,DM-6788,datamanagement,document meas_extensions_ngmix meas_extensions_ngmix useful documentation doc directory add include overview package content description capability instruction enable meas_base framework package readme,"Document meas_extensions_ngmix meas_extensions_ngmix has no useful documentation, not even a {{doc}} directory. Add some. This should include at least an overview of the package contents, a description of its capabilities, and instructions on enabling it within the meas_base framework. The package should have a README."
"Process HSC ""RC"" dataset through the LSST stack Process the ""RC"" dataset used to verify HSC data releases through the LSST stack using the configuration specified by DM-6815.",4,DM-6816,datamanagement,process hsc rc dataset lsst stack process rc dataset verify hsc datum release lsst stack configuration specify dm-6815,"Process HSC ""RC"" dataset through the LSST stack Process the ""RC"" dataset used to verify HSC data releases through the LSST stack using the configuration specified by DM-6815."
"Compare HSC and LSST processing of RC dataset Using the script enhanced in DM-6588, compare HSC and LSST (DM-6816) processing of the RC dataset.",8,DM-6817,datamanagement,compare hsc lsst processing rc dataset script enhance dm-6588 compare hsc lsst dm-6816 processing rc dataset,"Compare HSC and LSST processing of RC dataset Using the script enhanced in DM-6588, compare HSC and LSST (DM-6816) processing of the RC dataset."
Quality check LSST processing of RC dataset Perform a quality analysis on the LSST processing of the RC dataset (DM-6816) in the same way as would be performed before an HSC data release.,5,DM-6818,datamanagement,quality check lsst processing rc dataset perform quality analysis lsst processing rc dataset dm-6816 way perform hsc datum release,Quality check LSST processing of RC dataset Perform a quality analysis on the LSST processing of the RC dataset (DM-6816) in the same way as would be performed before an HSC data release.
Resolve CModel issues with aperture corrections While working on DM-4202 it became apparent that the aperture corrections calculated and applied by CModel were too large. This ticket is intended to trace down where the failure is occurring and correct it.,4,DM-6819,datamanagement,resolve cmodel issue aperture correction work dm-4202 apparent aperture correction calculate apply cmodel large ticket intend trace failure occur correct,Resolve CModel issues with aperture corrections While working on DM-4202 it became apparent that the aperture corrections calculated and applied by CModel were too large. This ticket is intended to trace down where the failure is occurring and correct it.
Add meas_extensions_ngmix to lsst_distrib Primarily so it can enjoy the benefits of regular CI runs.,2,DM-6822,datamanagement,add meas_extensions_ngmix lsst_distrib primarily enjoy benefit regular ci run,Add meas_extensions_ngmix to lsst_distrib Primarily so it can enjoy the benefits of regular CI runs.
"Use meas.algorithms.astrometrySourceSelector in measOptimisticB Now that there is a working astrometrySourceSelector (just merged in meas_algorithms from DM-5933), we should get matchOptimisticB working with it. This would entail replacing matchOptimisticB.SourceInfo with AstrometrySourceSelectorTask and tweaking the latter to do whatever matchOptimisticB needs, and removing SourceInfo.",2,DM-6824,datamanagement,use meas.algorithms.astrometrysourceselector measoptimisticb work astrometrysourceselector merge meas_algorithm dm-5933 matchoptimisticb work entail replace matchoptimisticb.sourceinfo astrometrysourceselectortask tweak matchoptimisticb need remove sourceinfo,"Use meas.algorithms.astrometrySourceSelector in measOptimisticB Now that there is a working astrometrySourceSelector (just merged in meas_algorithms from DM-5933), we should get matchOptimisticB working with it. This would entail replacing matchOptimisticB.SourceInfo with AstrometrySourceSelectorTask and tweaking the latter to do whatever matchOptimisticB needs, and removing SourceInfo."
"Deliver sections for  Operations Use Case Report For each diagram covering a key use case, provide a narrative interpretation of the key concepts being conveyed, including significant operational implications from the concepts being presented.    Fill in the table for the assigned use case areas.    Key use cases/concepts include: L1 production, L2 production, ITC incident response, ITC problem management.",6,DM-6828,datamanagement,deliver section operations use case report diagram cover key use case provide narrative interpretation key concept convey include significant operational implication concept present fill table assign use case area key use case concept include l1 production l2 production itc incident response itc problem management,"Deliver sections for Operations Use Case Report For each diagram covering a key use case, provide a narrative interpretation of the key concepts being conveyed, including significant operational implications from the concepts being presented. Fill in the table for the assigned use case areas. Key use cases/concepts include: L1 production, L2 production, ITC incident response, ITC problem management."
"Deliver sections for Concept of Operations Contribute to Concept of Operations sections about Chilean, NCSA, and CC-IN2P3 facilities. Describing the ""nuts and bolts"" basics and summarize each facility's role in the LSST operational system.",3,DM-6829,datamanagement,deliver section concept operations contribute concept operations section chilean ncsa cc in2p3 facility describe nut bolt basic summarize facility role lsst operational system,"Deliver sections for Concept of Operations Contribute to Concept of Operations sections about Chilean, NCSA, and CC-IN2P3 facilities. Describing the ""nuts and bolts"" basics and summarize each facility's role in the LSST operational system."
"Investigate effects of turning on the Brighter-Fatter correction for single-frame processing of HSC data In the process of comparing HSC vs. LSST stack single-frame processing runs, we have been running with the Brighter-Fatter correction (BFC) turned off.  The reason for this to begin with was that is was not yet available on the LSST stack when we started these comparisons.  We also want to isolate as many features as possible in order to confidently assess their individual effects  The functionality was ported on DM-4837 with a default of *doBrighterFatter=False*.  This issue is to continue the single-visit run comparisons (see e.g. DM-5301, DM-6490, DM-6491) with BFC turned on on both stacks.    In particular, we are finding that slight differences in the reference stars selected for a given CCD can result in significantly different psf models.  Also, it was noted in DM-4960 that LSST seems to select reference stars to a brighter cutoff than HSC.  If a given field has a larger fraction of bright stars considered in the psf modeling, it is conceivable that it will be more significantly influenced by the BF effect, thus causing the large CCD-to-CCD variations seen in, e.g. DM-6490 (https://jira.lsstcorp.org/secure/attachment/28213/compareVisit-v1322-diff_base_PsfFlux-skyZp.png).",6,DM-6830,datamanagement,investigate effect turn brighter fatter correction single frame processing hsc datum process compare hsc vs. lsst stack single frame processing run run brighter fatter correction bfc turn reason begin available lsst stack start comparison want isolate feature possible order confidently assess individual effect functionality port dm-4837 default dobrighterfatter false issue continue single visit run comparison e.g. dm-5301 dm-6490 dm-6491 bfc turn stack particular find slight difference reference star select give ccd result significantly different psf model note dm-4960 lsst select reference star bright cutoff hsc give field large fraction bright star consider psf modeling conceivable significantly influence bf effect cause large ccd ccd variation see e.g. dm-6490 https://jira.lsstcorp.org/secure/attachment/28213/comparevisit-v1322-diff_base_psfflux-skyzp.png,"Investigate effects of turning on the Brighter-Fatter correction for single-frame processing of HSC data In the process of comparing HSC vs. LSST stack single-frame processing runs, we have been running with the Brighter-Fatter correction (BFC) turned off. The reason for this to begin with was that is was not yet available on the LSST stack when we started these comparisons. We also want to isolate as many features as possible in order to confidently assess their individual effects The functionality was ported on DM-4837 with a default of *doBrighterFatter=False*. This issue is to continue the single-visit run comparisons (see e.g. DM-5301, DM-6490, DM-6491) with BFC turned on on both stacks. In particular, we are finding that slight differences in the reference stars selected for a given CCD can result in significantly different psf models. Also, it was noted in DM-4960 that LSST seems to select reference stars to a brighter cutoff than HSC. If a given field has a larger fraction of bright stars considered in the psf modeling, it is conceivable that it will be more significantly influenced by the BF effect, thus causing the large CCD-to-CCD variations seen in, e.g. DM-6490 (https://jira.lsstcorp.org/secure/attachment/28213/compareVisit-v1322-diff_base_PsfFlux-skyZp.png)."
Wrap base with pybind11 Split off from DM-6302.,2,DM-6831,datamanagement,wrap base pybind11 split dm-6302,Wrap base with pybind11 Split off from DM-6302.
Wrap utils with pybind11 Split off from DM-6302.,2,DM-6832,datamanagement,wrap util pybind11 split dm-6302,Wrap utils with pybind11 Split off from DM-6302.
add 'placeholder' attribute to the input element An attribute called placholder is available in html element <input> to give a hint to the user of what can be entered. The placeholder text must not contain carriage returns or line-feeds.      Add it as proptype to <inputfield> component.,1,DM-6833,datamanagement,add placeholder attribute input element attribute call placholder available html element hint user enter placeholder text contain carriage return line feed add proptype component,add 'placeholder' attribute to the input element An attribute called placholder is available in html element  to give a hint to the user of what can be entered. The placeholder text must not contain carriage returns or line-feeds. Add it as proptype to  component.
Write report on SPIE conference Write a report on my visit to the SPIE conference in Edinburgh.,3,DM-6834,datamanagement,write report spie conference write report visit spie conference edinburgh,Write report on SPIE conference Write a report on my visit to the SPIE conference in Edinburgh.
Learning about Openstack Sahand progress on learning about Openstack,4,DM-6835,datamanagement,learn openstack sahand progress learn openstack,Learning about Openstack Sahand progress on learning about Openstack
Create a python interface to access OpenStack Sahand progress on getting the interface to access the Openstack interface using Nova Client,3,DM-6836,datamanagement,create python interface access openstack sahand progress get interface access openstack interface nova client,Create a python interface to access OpenStack Sahand progress on getting the interface to access the Openstack interface using Nova Client
"Data Backbone Conops  iteration 1 prep:  Create a list of service endpoints Create a for list of service endpoints, with service considerations, and deliver to the Development file tree. Del with new ambiguities from the camera meeting at SLAC by listing the ""summit data services""  for both main camera and spectrograph as service endpoints,  since this may increase the functionality required, and it seems prudent to flow any of these requirement into further processes, since they seem likely.",2,DM-6837,datamanagement,data backbone conops iteration prep create list service endpoint create list service endpoint service consideration deliver development file tree del new ambiguity camera meeting slac list summit datum service main camera spectrograph service endpoint increase functionality require prudent flow requirement process likely,"Data Backbone Conops iteration 1 prep: Create a list of service endpoints Create a for list of service endpoints, with service considerations, and deliver to the Development file tree. Del with new ambiguities from the camera meeting at SLAC by listing the ""summit data services"" for both main camera and spectrograph as service endpoints, since this may increase the functionality required, and it seems prudent to flow any of these requirement into further processes, since they seem likely."
Learning about Spark Sahand progress on getting familiar with Spark and use of the interface to  create a small Spark cluster in OpenStack,4,DM-6838,datamanagement,learn spark sahand progress get familiar spark use interface create small spark cluster openstack,Learning about Spark Sahand progress on getting familiar with Spark and use of the interface to create a small Spark cluster in OpenStack
Learning about Docker Sahand progress on learning Docker containers and potential automatic deploy in OpenStack,3,DM-6839,datamanagement,learn docker sahand progress learn docker container potential automatic deploy openstack,Learning about Docker Sahand progress on learning Docker containers and potential automatic deploy in OpenStack
Set up and install Spark Sahand progress on getting Spark installed ,2,DM-6840,datamanagement,set install spark sahand progress get spark instal,Set up and install Spark Sahand progress on getting Spark installed
Learning about Kubernetes Sahand progress on learning about automatic deploy and scalability of Docker containers using Kubernetes ,2,DM-6841,datamanagement,learn kubernetes sahand progress learn automatic deploy scalability docker container kubernete,Learning about Kubernetes Sahand progress on learning about automatic deploy and scalability of Docker containers using Kubernetes
"Deal with emergent related requests  affecting operations planning in June There emergent request for comment emerged in June.     1) The interim project manager,  directed that the project begin an investigation into Amazon Wen Service due to contacts he developed at a Data base orient workshop he sponsors.  Formulating  a response required a review of the service offered by AWS, and inquiring about the validity of pursing an evaluation of just one vendor in a marketplace that has many vendors, and a deciding that an appropriate amount of work was to send additional NCSA staff to an AWS workshop to gain a similar appreciation of AWS as was gained at the database meeting at SLAC.  (Authority of interim project manager to insist on immediate action was also sorted out)    2) Request to understand computing capabilities at alternate site from the Deputy director.  Support for for alternate site capabilities are documented in the  the emerging L2 Batch concept of operations a copy of which was shared (though draft status noted)     3) Processed a summary of the Camera meeting which occurred at SLAC. Did not find  conclusions that related to a concept of operations.    IN particular we could not understand it there was a call for computing and a summit data service to support disconnected operations,  or if this was a mere optimization in the system to relocate the acquisition and forwarding infrstructure to the summit, with no other changes.",4,DM-6842,datamanagement,deal emergent relate request affect operation planning june emergent request comment emerge june interim project manager direct project begin investigation amazon wen service contact develop data base orient workshop sponsor formulate response require review service offer aws inquire validity purse evaluation vendor marketplace vendor deciding appropriate work send additional ncsa staff aws workshop gain similar appreciation aws gain database meeting slac authority interim project manager insist immediate action sort request understand compute capability alternate site deputy director support alternate site capability document emerge l2 batch concept operation copy share draft status note process summary camera meeting occur slac find conclusion relate concept operation particular understand computing summit datum service support disconnected operation mere optimization system relocate acquisition forward infrstructure summit change,"Deal with emergent related requests affecting operations planning in June There emergent request for comment emerged in June. 1) The interim project manager, directed that the project begin an investigation into Amazon Wen Service due to contacts he developed at a Data base orient workshop he sponsors. Formulating a response required a review of the service offered by AWS, and inquiring about the validity of pursing an evaluation of just one vendor in a marketplace that has many vendors, and a deciding that an appropriate amount of work was to send additional NCSA staff to an AWS workshop to gain a similar appreciation of AWS as was gained at the database meeting at SLAC. (Authority of interim project manager to insist on immediate action was also sorted out) 2) Request to understand computing capabilities at alternate site from the Deputy director. Support for for alternate site capabilities are documented in the the emerging L2 Batch concept of operations a copy of which was shared (though draft status noted) 3) Processed a summary of the Camera meeting which occurred at SLAC. Did not find conclusions that related to a concept of operations. IN particular we could not understand it there was a call for computing and a summit data service to support disconnected operations, or if this was a mere optimization in the system to relocate the acquisition and forwarding infrstructure to the summit, with no other changes."
Learning about Swift and HDFS Sahand progress on storage objects to be used in OpenStack,4,DM-6843,datamanagement,learn swift hdfs sahand progress storage object openstack,Learning about Swift and HDFS Sahand progress on storage objects to be used in OpenStack
Learning about Openstack and Jupyter Di progress on learning these web technologies,5,DM-6844,datamanagement,learn openstack jupyter di progress learn web technology,Learning about Openstack and Jupyter Di progress on learning these web technologies
Learning about SocketIO and HTML REST API Di progress on Communication technologies for the web,4,DM-6846,datamanagement,learn socketio html rest api di progress communication technology web,Learning about SocketIO and HTML REST API Di progress on Communication technologies for the web
Write wrapper API for JS9 and Jupyter Di progress in writing a wrapper to interact between JS9 within Jupyter,5,DM-6848,datamanagement,write wrapper api js9 jupyter di progress write wrapper interact js9 jupyter,Write wrapper API for JS9 and Jupyter Di progress in writing a wrapper to interact between JS9 within Jupyter
Setup multinode test environment for initial learning about installations  Setup up one master node and one worker node.,3,DM-6851,datamanagement,setup multinode test environment initial learning installation setup master node worker node,Setup multinode test environment for initial learning about installations Setup up one master node and one worker node.
"Finalize documentation and current issues of prototype After updating some latest changes, need to update documentation to explain the extend of this supertask and activator initial implementation.",4,DM-6854,datamanagement,finalize documentation current issue prototype update late change need update documentation explain extend supertask activator initial implementation,"Finalize documentation and current issues of prototype After updating some latest changes, need to update documentation to explain the extend of this supertask and activator initial implementation."
"Document that the catalog returned from star selectors is a view Star selectors return a catalog whose records are shallow copies of the input catalog records. Document the shallow copy aspect. This is important for two reasons:  - The user should know  - Implementers must be told this, because if the records are deep copies then the code that sets a flag for stars will not set a flag in the input catalog, which loses most of the point of setting that flag.",1,DM-6857,datamanagement,document catalog return star selector view star selector return catalog record shallow copy input catalog record document shallow copy aspect important reason user know implementer tell record deep copy code set flag star set flag input catalog lose point set flag,"Document that the catalog returned from star selectors is a view Star selectors return a catalog whose records are shallow copies of the input catalog records. Document the shallow copy aspect. This is important for two reasons: - The user should know - Implementers must be told this, because if the records are deep copies then the code that sets a flag for stars will not set a flag in the input catalog, which loses most of the point of setting that flag."
"Mapper tests require modification when new datasets are added [~price] [recommends|https://community.lsst.org/t/centrally-defined-butler-datasets/841] a new way to define datasets common to all cameras in daf_butlerUtils, but modifying these yaml files require explicit lists of datasets to be modified in tests/cameraMapper.py.    If these tests are still useful, they need to depend on a minimal set of dataset definitions instead of the real ones.",1,DM-6858,datamanagement,mapper test require modification new dataset add ~price recommends|https://community.lsst.org centrally define butler datasets/841 new way define dataset common camera daf_butlerutil modify yaml file require explicit list dataset modify test cameramapper.py test useful need depend minimal set dataset definition instead real one,"Mapper tests require modification when new datasets are added [~price] [recommends|https://community.lsst.org/t/centrally-defined-butler-datasets/841] a new way to define datasets common to all cameras in daf_butlerUtils, but modifying these yaml files require explicit lists of datasets to be modified in tests/cameraMapper.py. If these tests are still useful, they need to depend on a minimal set of dataset definitions instead of the real ones."
"Participation according to direction from interim project management Given directions from interim project management, participation consisted of direct conversations with Kevin and Jacek plus background work talking to staff related to assembling a plan.",5,DM-6859,datamanagement,participation accord direction interim project management give direction interim project management participation consist direct conversation kevin jacek plus background work talk staff relate assemble plan,"Participation according to direction from interim project management Given directions from interim project management, participation consisted of direct conversations with Kevin and Jacek plus background work talking to staff related to assembling a plan."
"Refine simple 1D DCR correction DM-5695 created a functional implementation of a simple DCR correction algorithm. While it appears to successfully create template images with airmass and DCR matched to science images, it is computationally inefficient and appears to introduce new artifacts to the template image. This ticket is to enhance the simple algorithm in several ways:  * Convert to sparse matrices where possible  * use variance weighting of the images  * propagate masked pixels correctly  * Refine the algorithm to mitigate the new artifacts",6,DM-6860,datamanagement,refine simple 1d dcr correction dm-5695 create functional implementation simple dcr correction algorithm appear successfully create template image airmass dcr match science image computationally inefficient appear introduce new artifact template image ticket enhance simple algorithm way convert sparse matrix possible use variance weighting image propagate mask pixel correctly refine algorithm mitigate new artifact,"Refine simple 1D DCR correction DM-5695 created a functional implementation of a simple DCR correction algorithm. While it appears to successfully create template images with airmass and DCR matched to science images, it is computationally inefficient and appears to introduce new artifacts to the template image. This ticket is to enhance the simple algorithm in several ways: * Convert to sparse matrices where possible * use variance weighting of the images * propagate masked pixels correctly * Refine the algorithm to mitigate the new artifacts"
Understand how to render conops documents in Sphinx Learn how to render conops documents in reStructuredText. Prototype conops template and for delivery into Technical Control Team Sphinx engineering environment.,1,DM-6861,datamanagement,understand render conop document sphinx learn render conop document restructuredtext prototype conop template delivery technical control team sphinx engineering environment,Understand how to render conops documents in Sphinx Learn how to render conops documents in reStructuredText. Prototype conops template and for delivery into Technical Control Team Sphinx engineering environment.
Raw draft of System Monitor and Comfort Display Produce raw draft of conops for review by steering committee. Includes operational components and connectivity for the system monitoring services that will monitor devices from the summit to NCSA.,3,DM-6862,datamanagement,raw draft system monitor comfort display produce raw draft conop review steering committee include operational component connectivity system monitoring service monitor device summit ncsa,Raw draft of System Monitor and Comfort Display Produce raw draft of conops for review by steering committee. Includes operational components and connectivity for the system monitoring services that will monitor devices from the summit to NCSA.
Appreciate amount of effort needed to run preliminary planning exercise Run planning process with local staff to appreciate amount of effort needed.,5,DM-6870,datamanagement,appreciate effort need run preliminary planning exercise run planning process local staff appreciate effort need,Appreciate amount of effort needed to run preliminary planning exercise Run planning process with local staff to appreciate amount of effort needed.
Review evaluation criteria with CC-IN2P3 Review evaluation criteria with Fabio during his visit from CC-IN2P3 to NCSA.   https://drive.google.com/open?id=1Xhj6kaFEnNhCyRPskB6BCXYxgfs_9-cl3BRX9wCh1sE,1,DM-6871,datamanagement,review evaluation criterion cc in2p3 review evaluation criterion fabio visit cc in2p3 ncsa https://drive.google.com/open?id=1xhj6kafennhcyrpskb6bcxyxgfs_9-cl3brx9wch1se,Review evaluation criteria with CC-IN2P3 Review evaluation criteria with Fabio during his visit from CC-IN2P3 to NCSA. https://drive.google.com/open?id=1Xhj6kaFEnNhCyRPskB6BCXYxgfs_9-cl3BRX9wCh1sE
Create evaluation plan from evaluation criteria Turn criteria into tabular comparison chart and respect test implementation constraints.,4,DM-6872,datamanagement,create evaluation plan evaluation criterion turn criterion tabular comparison chart respect test implementation constraint,Create evaluation plan from evaluation criteria Turn criteria into tabular comparison chart and respect test implementation constraints.
"Estimate amount of effort needed to run detailed planning exercise Run through process of detailing activities down to story size requested by the LSST EVM system.     Do detailed estimation of conops development and a sample of technical areas, and extrapolated based on number of epics, size of staff, and complexity of mission. Total = 100 hours for 3 months of activities for current staff size.",2,DM-6873,datamanagement,estimate effort need run detailed planning exercise run process detail activity story size request lsst evm system detailed estimation conop development sample technical area extrapolate base number epic size staff complexity mission total 100 hour month activity current staff size,"Estimate amount of effort needed to run detailed planning exercise Run through process of detailing activities down to story size requested by the LSST EVM system. Do detailed estimation of conops development and a sample of technical areas, and extrapolated based on number of epics, size of staff, and complexity of mission. Total = 100 hours for 3 months of activities for current staff size."
"Design framework for reporting and steering meetings Run the process with staff to assess and supervise technical status, the appropriateness of work compared to architectural vision, consistency with NCSA general acumen, and status vs. plan.",6,DM-6875,datamanagement,design framework report steering meeting run process staff assess supervise technical status appropriateness work compare architectural vision consistency ncsa general acuman status vs. plan,"Design framework for reporting and steering meetings Run the process with staff to assess and supervise technical status, the appropriateness of work compared to architectural vision, consistency with NCSA general acumen, and status vs. plan."
"TBD processes coordinated with impending hire Design and implement critical processes defined in the RACI document, coordinated with impending hire.",6,DM-6876,datamanagement,tbd process coordinate impending hire design implement critical process define raci document coordinate impending hire,"TBD processes coordinated with impending hire Design and implement critical processes defined in the RACI document, coordinated with impending hire."
"Address concerns with source side (Dave Mills) Work with Dave Mills and others to understand architecture and use of ""source"" EFD. The goal is to understand the amount of volume of data that would be in reformatted EFD that otherwise would not have been, should we proceed with the proposed change.",8,DM-6879,datamanagement,address concern source dave mills work dave mills understand architecture use source efd goal understand volume datum reformatte efd proceed propose change,"Address concerns with source side (Dave Mills) Work with Dave Mills and others to understand architecture and use of ""source"" EFD. The goal is to understand the amount of volume of data that would be in reformatted EFD that otherwise would not have been, should we proceed with the proposed change."
"Address concerns with target side (SLAC) Understand permissions and protections that would be in the reformatted EFD that otherwise would not have been, should we proceed with the proposed change.",8,DM-6880,datamanagement,address concern target slac understand permission protection reformatte efd proceed propose change,"Address concerns with target side (SLAC) Understand permissions and protections that would be in the reformatted EFD that otherwise would not have been, should we proceed with the proposed change."
Address internal concerns Understand whether the file annex should be kept in the same cluster as EFD as opposed to general files in the data backbone.,6,DM-6881,datamanagement,address internal concern understand file annex keep cluster efd oppose general file data backbone,Address internal concerns Understand whether the file annex should be kept in the same cluster as EFD as opposed to general files in the data backbone.
"Incorporate into ConOps and any draft design notes Incorporate concerns, solutions and agreements into ConOps and any draft design notes.",6,DM-6882,datamanagement,incorporate conops draft design note incorporate concern solution agreement conops draft design note,"Incorporate into ConOps and any draft design notes Incorporate concerns, solutions and agreements into ConOps and any draft design notes."
Address additional emergent concerns  Address TBD additional emergent concerns ,2,DM-6883,datamanagement,address additional emergent concern address tbd additional emergent concern,Address additional emergent concerns Address TBD additional emergent concerns
Rework MemMan to be inline with the qserv worker Scheduler. Split the memory mapping function from the memory locking function to allow the scheduler to initiate locking without blocking. Add additional memory tracking improvements in line with current thinking. Reduce lock contention. Add logging.,4,DM-6884,datamanagement,rework memman inline qserv worker scheduler split memory mapping function memory locking function allow scheduler initiate lock blocking add additional memory tracking improvement line current thinking reduce lock contention add logging,Rework MemMan to be inline with the qserv worker Scheduler. Split the memory mapping function from the memory locking function to allow the scheduler to initiate locking without blocking. Add additional memory tracking improvements in line with current thinking. Reduce lock contention. Add logging.
"forcedPhotCoadd.py fails on CFHT data due to a CModel bug Hello,    forcedPhotCoadd fails while running on CFHT data due to a CModel bug. Here is an example on the error message that we get:    {code}  python: src/CModel.cc:1368: void lsst::meas::modelfit::CModelAlgorithm::measure(lsst::afw::table::SourceRecord&, const lsst::afw::image::Exposure<float>&, const lsst::afw::table::SourceRecord&) const: Assertion `measRecord.getFootprint()->getArea()' failed.  Aborted  {code}    Adding the following lines in cmodel.py (in CModelForcedPlugin.measure, before the call to self.algorithm.measure) allows to go around the problem for the time being, which seems to arise for null value of the number of pixel in a given footprint:    {code}  if not measRecord.getFootprint().getArea():      raise ValueError(""measRecord.getFootprint().getArea(): 0. No pixel in this footprint."")  {code}",1,DM-6886,datamanagement,"forcedphotcoadd.py fail cfht datum cmodel bug hello forcedphotcoadd fail run cfht datum cmodel bug example error message code python src cmodel.cc:1368 void lsst::meas::modelfit::cmodelalgorithm::measure(lsst::afw::table::sourcerecord const lsst::afw::image::exposure const lsst::afw::table::sourcerecord const assertion measrecord.getfootprint()->getarea fail aborted code add follow line cmodel.py cmodelforcedplugin.measure self.algorithm.measure allow problem time arise null value number pixel give footprint code measrecord.getfootprint().getarea raise valueerror(""measrecord.getfootprint().getarea pixel footprint code","forcedPhotCoadd.py fails on CFHT data due to a CModel bug Hello, forcedPhotCoadd fails while running on CFHT data due to a CModel bug. Here is an example on the error message that we get: {code} python: src/CModel.cc:1368: void lsst::meas::modelfit::CModelAlgorithm::measure(lsst::afw::table::SourceRecord&, const lsst::afw::image::Exposure&, const lsst::afw::table::SourceRecord&) const: Assertion `measRecord.getFootprint()->getArea()' failed. Aborted {code} Adding the following lines in cmodel.py (in CModelForcedPlugin.measure, before the call to self.algorithm.measure) allows to go around the problem for the time being, which seems to arise for null value of the number of pixel in a given footprint: {code} if not measRecord.getFootprint().getArea(): raise ValueError(""measRecord.getFootprint().getArea(): 0. No pixel in this footprint."") {code}"
Access to system with LSST stack Secure access to machine(s) with the LSST stack. This includes installation on local desktop/laptop.,2,DM-6892,datamanagement,access system lsst stack secure access machine(s lsst stack include installation local desktop laptop,Access to system with LSST stack Secure access to machine(s) with the LSST stack. This includes installation on local desktop/laptop.
"Controlled Test of LMSimpleShape using high SNR objects Some issues came up during DM-6300 which indicated that a more controlled set of tests would be required than the random Great3Sims tests to understand the behavior of NGMIX LMSimpleShape.      LMSimpleShape appears to fail computing moments on low SNR objects.  It also shows pretty wide variation in shear bias which did not show up with CModel.    The needed tests with would include controlled profiles (Gauss, Dev, and Exp), controlled SNR, and controlled q, theta, and flux.  This should separate out the causes of failure and shear variation which we have seen.",6,DM-6893,datamanagement,control test lmsimpleshape high snr object issue come dm-6300 indicate control set test require random great3sims test understand behavior ngmix lmsimpleshape lmsimpleshape appear fail compute moment low snr object show pretty wide variation shear bias cmodel need test include control profile gauss dev exp control snr control theta flux separate cause failure shear variation see,"Controlled Test of LMSimpleShape using high SNR objects Some issues came up during DM-6300 which indicated that a more controlled set of tests would be required than the random Great3Sims tests to understand the behavior of NGMIX LMSimpleShape. LMSimpleShape appears to fail computing moments on low SNR objects. It also shows pretty wide variation in shear bias which did not show up with CModel. The needed tests with would include controlled profiles (Gauss, Dev, and Exp), controlled SNR, and controlled q, theta, and flux. This should separate out the causes of failure and shear variation which we have seen."
"Ensure DipoleFitTask uses correct PSF(s) in case when Decorrelation is turned on Diffim A&L decorrelation (DM-6241) modifies the diffim PSF, but leaves the ""pre-subtraction"" images used by DipoleFitTask as they were. Ensure that the correct PSFs are being used for dipole fitting when decorrelation is turned on (and actually, in all cases).",8,DM-6894,datamanagement,ensure dipolefittask use correct psf(s case decorrelation turn diffim a&l decorrelation dm-6241 modify diffim psf leave pre subtraction image dipolefittask ensure correct psf dipole fitting decorrelation turn actually case,"Ensure DipoleFitTask uses correct PSF(s) in case when Decorrelation is turned on Diffim A&L decorrelation (DM-6241) modifies the diffim PSF, but leaves the ""pre-subtraction"" images used by DipoleFitTask as they were. Ensure that the correct PSFs are being used for dipole fitting when decorrelation is turned on (and actually, in all cases)."
Get data stream from socket into a fits file Get data stream (module?) from Jim into a fits file than can be loaded subsequently to the butler.,2,DM-6897,datamanagement,datum stream socket fit file data stream module jim fit file load subsequently butler,Get data stream from socket into a fits file Get data stream (module?) from Jim into a fits file than can be loaded subsequently to the butler.
Load known image data format into the Butler Use some type of known data (image) to load and test into the buttler. Data types might include DECam MEF images of single-plain image files from simulations.,2,DM-6898,datamanagement,load known image datum format butler use type know datum image load test buttler datum type include decam mef image single plain image file simulation,Load known image data format into the Butler Use some type of known data (image) to load and test into the buttler. Data types might include DECam MEF images of single-plain image files from simulations.
Assemble data stream from socket to lsst-stack pipeline Connect all of the parts together.,4,DM-6899,datamanagement,assemble datum stream socket lsst stack pipeline connect part,Assemble data stream from socket to lsst-stack pipeline Connect all of the parts together.
"ci_hsc failure: insufficient PSF sources classified as stars Since [ci_hsc#396|https://ci.lsst.codes/job/ci_hsc/396/], the regular ci_hsc build has been failing with:  {code}  [2016-07-05T23:59:53.929169Z]  FATAL: At least 95% of sources used to build the PSF are classified as stars (49 > 50): FAIL  [2016-07-05T23:59:53.929201Z] Traceback (most recent call last):  [2016-07-05T23:59:53.929238Z]   File ""/home/build0/lsstsw/build/ci_hsc/bin/validate.py"", line 3, in <module>  [2016-07-05T23:59:53.929249Z]     main()  [2016-07-05T23:59:53.929317Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 53, in main  [2016-07-05T23:59:53.929334Z]     validator.run(dataId)  [2016-07-05T23:59:53.929375Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 163, in run  [2016-07-05T23:59:53.929394Z]     self.validateSources(dataId)  [2016-07-05T23:59:53.929436Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 201, in validateSources  [2016-07-05T23:59:53.929451Z]     0.95*psfStars.sum()  [2016-07-05T23:59:53.929510Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 91, in assertGreater  [2016-07-05T23:59:53.929547Z]     self.assertTrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2)  [2016-07-05T23:59:53.929587Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 82, in assertTrue  [2016-07-05T23:59:53.929614Z]     raise AssertionError(""Failed test: %s"" % description)  [2016-07-05T23:59:53.929660Z] AssertionError: Failed test: At least 95% of sources used to build the PSF are classified as stars (49 > 50)  {code}  This error appears to be related to DM-5877: when I reverted to versions of pipe_tasks, meas_base, meas_algorithms, ip_diffim, meas_extensions_photometryKron and obs_subaru predating that ticket landing, the error vanishes.    I note that [~nlust] reports that he does not see these failures on his (OS X) system, but I can reproduce them on Linux: we should investigate if that's just a coincidence, or if there is per-platform variation here.",1,DM-6900,datamanagement,"ci_hsc failure insufficient psf source classify star ci_hsc#396|https://ci.lsst.code job ci_hsc/396/ regular ci_hsc build fail code 2016 07 05t23:59:53.929169z fatal 95 source build psf classify star 49 50 fail 2016 07 05t23:59:53.929201z traceback recent 2016 07 05t23:59:53.929238z file /home build0 lsstsw build ci_hsc bin validate.py line 2016 07 05t23:59:53.929249z main 2016 07 05t23:59:53.929317z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 53 main 2016 07 05t23:59:53.929334z validator.run(dataid 2016 07 05t23:59:53.929375z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 163 run 2016 07 05t23:59:53.929394z self.validatesources(dataid 2016 07 05t23:59:53.929436z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 201 validatesource 2016 07 05t23:59:53.929451z 0.95*psfstars.sum 2016 07 05t23:59:53.929510z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 91 assertgreater 2016 07 05t23:59:53.929547z self.asserttrue(description num1 num2 num1 num2 2016 07 05t23:59:53.929587z file /home build0 lsstsw build ci_hsc python lsst ci hsc validate.py line 82 asserttrue 2016 07 05t23:59:53.929614z raise assertionerror(""failed test description 2016 07 05t23:59:53.929660z assertionerror fail test 95 source build psf classify star 49 50 code error appear relate dm-5877 revert version pipe_task meas_base meas_algorithms ip_diffim meas_extensions_photometrykron obs_subaru predate ticket landing error vanishe note ~nlust report failure os system reproduce linux investigate coincidence platform variation","ci_hsc failure: insufficient PSF sources classified as stars Since [ci_hsc#396|https://ci.lsst.codes/job/ci_hsc/396/], the regular ci_hsc build has been failing with: {code} [2016-07-05T23:59:53.929169Z] FATAL: At least 95% of sources used to build the PSF are classified as stars (49 > 50): FAIL [2016-07-05T23:59:53.929201Z] Traceback (most recent call last): [2016-07-05T23:59:53.929238Z] File ""/home/build0/lsstsw/build/ci_hsc/bin/validate.py"", line 3, in  [2016-07-05T23:59:53.929249Z] main() [2016-07-05T23:59:53.929317Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 53, in main [2016-07-05T23:59:53.929334Z] validator.run(dataId) [2016-07-05T23:59:53.929375Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 163, in run [2016-07-05T23:59:53.929394Z] self.validateSources(dataId) [2016-07-05T23:59:53.929436Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 201, in validateSources [2016-07-05T23:59:53.929451Z] 0.95*psfStars.sum() [2016-07-05T23:59:53.929510Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 91, in assertGreater [2016-07-05T23:59:53.929547Z] self.assertTrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2) [2016-07-05T23:59:53.929587Z] File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 82, in assertTrue [2016-07-05T23:59:53.929614Z] raise AssertionError(""Failed test: %s"" % description) [2016-07-05T23:59:53.929660Z] AssertionError: Failed test: At least 95% of sources used to build the PSF are classified as stars (49 > 50) {code} This error appears to be related to DM-5877: when I reverted to versions of pipe_tasks, meas_base, meas_algorithms, ip_diffim, meas_extensions_photometryKron and obs_subaru predating that ticket landing, the error vanishes. I note that [~nlust] reports that he does not see these failures on his (OS X) system, but I can reproduce them on Linux: we should investigate if that's just a coincidence, or if there is per-platform variation here."
"VO search doesn't trigger coverage image nor overlay While migrating the VO search panel, i found the follwing problem: once the table gets back, no image coverage or overlay is rendered.  One problem from IRSA simple cone search result is that the VO table doesn't contain the right UCDs expected. The second problem when the VO table does contain the proper UCDs is that the META_INFO is not set.   The edu.caltech.ipac.firefly.server.query.SearchManager#jsonTablePartRequest doesn't set the attributes from the DataGroup object into the TableMeta data as it is done previously in OPS by edu.caltech.ipac.firefly.server.query.SearchManager#getRawDataSet    Please, add the META_INFO object missing to the table and set the proper CATALOG_OVERLAY_TYPE, and CATALOG_COORD_COLS needed so the Coverage image and overlay can be rendered.",4,DM-6902,datamanagement,vo search trigger coverage image overlay migrate vo search panel find follwing problem table get image coverage overlay render problem irsa simple cone search result vo table contain right ucds expect second problem vo table contain proper ucds meta_info set edu.caltech.ipac.firefly.server.query searchmanager#jsontablepartrequ set attribute datagroup object tablemeta datum previously ops edu.caltech.ipac.firefly.server.query searchmanager#getrawdataset add meta_info object miss table set proper catalog_overlay_type catalog_coord_cols need coverage image overlay render,"VO search doesn't trigger coverage image nor overlay While migrating the VO search panel, i found the follwing problem: once the table gets back, no image coverage or overlay is rendered. One problem from IRSA simple cone search result is that the VO table doesn't contain the right UCDs expected. The second problem when the VO table does contain the proper UCDs is that the META_INFO is not set. The edu.caltech.ipac.firefly.server.query.SearchManager#jsonTablePartRequest doesn't set the attributes from the DataGroup object into the TableMeta data as it is done previously in OPS by edu.caltech.ipac.firefly.server.query.SearchManager#getRawDataSet Please, add the META_INFO object missing to the table and set the proper CATALOG_OVERLAY_TYPE, and CATALOG_COORD_COLS needed so the Coverage image and overlay can be rendered."
Add an option to label ccd serial number on the showVisitSkyMap.py plot  (actual assignee: Samuel Piehl)     Sometimes it is useful to know where the CCDs are on the plot. Add an option to label the CCD numbers. ,1,DM-6903,datamanagement,add option label ccd serial number showvisitskymap.py plot actual assignee samuel piehl useful know ccds plot add option label ccd number,Add an option to label ccd serial number on the showVisitSkyMap.py plot (actual assignee: Samuel Piehl) Sometimes it is useful to know where the CCDs are on the plot. Add an option to label the CCD numbers.
Create DCR visualization tools Several visualization tools will be very helpful to fully understand the effect of DCR correction algorithms and their failure modes.   * A function that generates difference images with the sources used for calibration and/or psf fitting marked.  * A visualization that indicates the spectral type of each source in an image. This could be a mask overlay where the color corresponds to the spectral type.  * A visualization of the coarse spectral resolution model built for DCR correction,5,DM-6904,datamanagement,create dcr visualization tool visualization tool helpful fully understand effect dcr correction algorithm failure mode function generate difference image source calibration and/or psf fitting mark visualization indicate spectral type source image mask overlay color correspond spectral type visualization coarse spectral resolution model build dcr correction,Create DCR visualization tools Several visualization tools will be very helpful to fully understand the effect of DCR correction algorithms and their failure modes. * A function that generates difference images with the sources used for calibration and/or psf fitting marked. * A visualization that indicates the spectral type of each source in an image. This could be a mask overlay where the color corresponds to the spectral type. * A visualization of the coarse spectral resolution model built for DCR correction
Locate the test dataset for PDAC Locate and evaluate a dataset of SDSS Stripe82 which is going to be used for testing the prototype DAC.,2,DM-6905,datamanagement,locate test dataset pdac locate evaluate dataset sdss stripe82 go test prototype dac,Locate the test dataset for PDAC Locate and evaluate a dataset of SDSS Stripe82 which is going to be used for testing the prototype DAC.
"XYPlot density plot with log scale - bin size is not reflected correctly XYPlot with the large number of points does not display correctly when log scale is selected. When log scale is selected, binning on the server should be using the logs, so that the bins are the same size on the log scale. ",4,DM-6907,datamanagement,xyplot density plot log scale bin size reflect correctly xyplot large number point display correctly log scale select log scale select bin server log bin size log scale,"XYPlot density plot with log scale - bin size is not reflected correctly XYPlot with the large number of points does not display correctly when log scale is selected. When log scale is selected, binning on the server should be using the logs, so that the bins are the same size on the log scale."
Filter editor on a chart toolbar Need to add filter editor to the chart toolbar. Filter editor should be without selectable rows.,4,DM-6908,datamanagement,filter editor chart toolbar need add filter editor chart toolbar filter editor selectable row,Filter editor on a chart toolbar Need to add filter editor to the chart toolbar. Filter editor should be without selectable rows.
"Filtering from expanded mode cancels expanded mode When a table is filtered from the expanded mode, the layout is changed back to unexpanded.    It looks like the issue is more general: table actions trigger layout changes, which are not always right. For example, TABLE_REMOVE action while in a dropdown makes the  dropdown to get closed. I've traced it to FireflyLayoutManager.js:layoutManager generator function.    Test sequence in firefly:   - When a table is loaded, open ""Charts"" dropdown, select Col link for X, then select Col link for Y. (At this point the previous table is removed).  - TABLE_REMOVE action on the second click triggers dropdown to go away.   ",2,DM-6909,datamanagement,filter expand mode cancels expand mode table filter expand mode layout change unexpanded look like issue general table action trigger layout change right example table_remove action dropdown make dropdown closed trace fireflylayoutmanager.js layoutmanager generator function test sequence firefly table load open charts dropdown select col link select col link y. point previous table remove table_remove action second click trigger dropdown away,"Filtering from expanded mode cancels expanded mode When a table is filtered from the expanded mode, the layout is changed back to unexpanded. It looks like the issue is more general: table actions trigger layout changes, which are not always right. For example, TABLE_REMOVE action while in a dropdown makes the dropdown to get closed. I've traced it to FireflyLayoutManager.js:layoutManager generator function. Test sequence in firefly: - When a table is loaded, open ""Charts"" dropdown, select Col link for X, then select Col link for Y. (At this point the previous table is removed). - TABLE_REMOVE action on the second click triggers dropdown to go away."
"git-lfs.lsst.codes certificate is expired Per reports on hipchat, the tls certifcate on git-lfs.lsst.codes was not upgraded to the new *.lsst.codes cert.    {code:java}  John Swinbank  9:52 AM  @josh @jmatt I'm seeing the following, which I think might be the same as @srp's error above. Any ideas?  Get https://git-lfs.lsst.codes/objects/24874b686b9479a823987dc2bd2700cad5b73e74a43108fb61b91d7f79f0cd99: x509: certificate has expired or is not yet valid  Followed by git lfs failing.  (I assumed it was user error on my part at first, but if so it's coincidence that Steve's git lfs fails at the same time.)  {code}    ",1,DM-6914,datamanagement,certificate expire report hipchat tls certifcate git-lfs.lsst.code upgrade new cert code java john swinbank 9:52 @josh @jmatt see following think @srp error idea https://git-lfs.lsst.codes/objects/24874b686b9479a823987dc2bd2700cad5b73e74a43108fb61b91d7f79f0cd99 x509 certificate expire valid follow git lfs fail assume user error coincidence steve git lfs fail time code,"git-lfs.lsst.codes certificate is expired Per reports on hipchat, the tls certifcate on git-lfs.lsst.codes was not upgraded to the new *.lsst.codes cert. {code:java} John Swinbank 9:52 AM @josh @jmatt I'm seeing the following, which I think might be the same as @srp's error above. Any ideas? Get https://git-lfs.lsst.codes/objects/24874b686b9479a823987dc2bd2700cad5b73e74a43108fb61b91d7f79f0cd99: x509: certificate has expired or is not yet valid Followed by git lfs failing. (I assumed it was user error on my part at first, but if so it's coincidence that Steve's git lfs fails at the same time.) {code}"
"jointcalRunner passing tract to jointcal, which had tract removed from run() When cleaning up jointcal for testing, I removed tract from jointcal.run(), but did not remove it from the return list of JointcalRunner.getTargetList(). Tract isn't actually used anywhere in jointcal.run(), so we should be able to just remove it from getTargetList's return.    Keeping those two in sync may be a bit tricky without a unittest that compares them.",4,DM-6915,datamanagement,pass tract jointcal tract remove run clean jointcal testing remove tract jointcal.run remove return list jointcalrunner.gettargetlist tract actually jointcal.run able remove gettargetlist return keep sync bit tricky unitt compare,"jointcalRunner passing tract to jointcal, which had tract removed from run() When cleaning up jointcal for testing, I removed tract from jointcal.run(), but did not remove it from the return list of JointcalRunner.getTargetList(). Tract isn't actually used anywhere in jointcal.run(), so we should be able to just remove it from getTargetList's return. Keeping those two in sync may be a bit tricky without a unittest that compares them."
"Documenteer seeds Git revision date and branch name if not present in metadata.yaml If {{last_revised}} and {{version}} are not present in metadata.yaml, then the Git commit date and branch name should be used while building metadata instead.    Also updates lsst-technote-bootstrap to take advantage of automated metadata for new projects.",1,DM-6916,datamanagement,documenteer seed git revision date branch present metadata.yaml last_revise version present metadata.yaml git commit date branch build metadata instead update lsst technote bootstrap advantage automate metadata new project,"Documenteer seeds Git revision date and branch name if not present in metadata.yaml If {{last_revised}} and {{version}} are not present in metadata.yaml, then the Git commit date and branch name should be used while building metadata instead. Also updates lsst-technote-bootstrap to take advantage of automated metadata for new projects."
"Write User Guide for new validate_drp metric/measurement API DM-6629 provided a new API for consistently specifying metrics, their specification, and reporting results of measurements.    This API can, and should, be used beyond validate_drp for any code that wants to submit metadata to SQUASH. This ticket will provide user documentation on the API base classes to help other developers write new metrics and measurements.",4,DM-6917,datamanagement,write user guide new validate_drp metric measurement api dm-6629 provide new api consistently specify metric specification report result measurement api validate_drp code want submit metadata squash ticket provide user documentation api base class help developer write new metric measurement,"Write User Guide for new validate_drp metric/measurement API DM-6629 provided a new API for consistently specifying metrics, their specification, and reporting results of measurements. This API can, and should, be used beyond validate_drp for any code that wants to submit metadata to SQUASH. This ticket will provide user documentation on the API base classes to help other developers write new metrics and measurements."
"Please rename ""afterburners"" In DM-4887 we introduced a new measurement post-processing system which we called ""afterburners"".    The term ""afterburner"" is overloaded and applied in multiple contexts. To save confusion, please rename this system to something less ambiguous. Best if we can do this soon, before this usage spreads.",1,DM-6919,datamanagement,rename afterburner dm-4887 introduce new measurement post processing system call afterburner term afterburner overload apply multiple context save confusion rename system ambiguous well soon usage spread,"Please rename ""afterburners"" In DM-4887 we introduced a new measurement post-processing system which we called ""afterburners"". The term ""afterburner"" is overloaded and applied in multiple contexts. To save confusion, please rename this system to something less ambiguous. Best if we can do this soon, before this usage spreads."
"Upgrade to new stack install procedure for containers LSST stack install has evolved: https://pipelines.lsst.io/install/newinstall.html#  Release container creation script needs to be update.  Latest Docker version will be tested, as [~bvan] reported cmd line options have changed.",2,DM-6922,datamanagement,upgrade new stack install procedure container lsst stack install evolve https://pipelines.lsst.io/install/newinstall.html release container creation script need update latest docker version test ~bvan report cmd line option change,"Upgrade to new stack install procedure for containers LSST stack install has evolved: https://pipelines.lsst.io/install/newinstall.html# Release container creation script needs to be update. Latest Docker version will be tested, as [~bvan] reported cmd line options have changed."
"Apply distortion when searching for astrometric reference objects While investigating DM-6529 I found that LSST generally finds fewer reference objects than HSC when doing astrometry.  For the CCDs on the edge of the focal plane the number of stars was typically very low causing frequent failures.  I found that in the HSC code, there is a distortion being applied that shifts the exposure bounding box when getting objects from the reference catalog.  This distortion is not being applied in the LSST code.",1,DM-6923,datamanagement,apply distortion search astrometric reference object investigate dm-6529 find lsst generally find few reference object hsc astrometry ccd edge focal plane number star typically low cause frequent failure find hsc code distortion apply shift exposure bound box get object reference catalog distortion apply lsst code,"Apply distortion when searching for astrometric reference objects While investigating DM-6529 I found that LSST generally finds fewer reference objects than HSC when doing astrometry. For the CCDs on the edge of the focal plane the number of stars was typically very low causing frequent failures. I found that in the HSC code, there is a distortion being applied that shifts the exposure bounding box when getting objects from the reference catalog. This distortion is not being applied in the LSST code."
Resurrect obs_file obs_file needs to be resurrected.  This is partially due to the reorganization of processCcd.  My take is to try to make the ingest script read the files and ingest them keyed on the filename.  Then the dataId will be just the filename.  Hopefully we can then mock all the other info needed for processing in a general way.  Calibration (astrometric and photometric will be off by default).  ,8,DM-6924,datamanagement,resurrect obs_file obs_file need resurrect partially reorganization processccd try ingest script read file ingest key filename dataid filename hopefully mock info need processing general way calibration astrometric photometric default,Resurrect obs_file obs_file needs to be resurrected. This is partially due to the reorganization of processCcd. My take is to try to make the ingest script read the files and ingest them keyed on the filename. Then the dataId will be just the filename. Hopefully we can then mock all the other info needed for processing in a general way. Calibration (astrometric and photometric will be off by default).
"star selector and PSF determiner are selecting stars that are not valid point sources When turning on CModel a more robust extendedness classifier relieved that many of the stars being used as PSF candidates were being classified as extended as shown in the attached plot. This plot was generated from the output of ci_hsc. Work should be done to determine why these stars are mistakenly being selected and fix the bad behavior. Additionally the [temporary work around in ci_hsc|https://github.com/lsst/ci_hsc/commit/6daf43ca41b6d192b6e1dbedb60cde0bec90b615], where the success criteria for validate sources in validate.py should be reverted from 85% to 95%.",4,DM-6925,datamanagement,star selector psf determiner select star valid point source turn cmodel robust extendedness classifier relieve star psf candidate classify extend show attached plot plot generate output ci_hsc work determine star mistakenly select fix bad behavior additionally temporary work ci_hsc|https://github.com lsst ci_hsc commit/6daf43ca41b6d192b6e1dbedb60cde0bec90b615 success criterion validate source validate.py revert 85 95,"star selector and PSF determiner are selecting stars that are not valid point sources When turning on CModel a more robust extendedness classifier relieved that many of the stars being used as PSF candidates were being classified as extended as shown in the attached plot. This plot was generated from the output of ci_hsc. Work should be done to determine why these stars are mistakenly being selected and fix the bad behavior. Additionally the [temporary work around in ci_hsc|https://github.com/lsst/ci_hsc/commit/6daf43ca41b6d192b6e1dbedb60cde0bec90b615], where the success criteria for validate sources in validate.py should be reverted from 85% to 95%."
"HSC backport: Include PSF moments in the output tables This is effectively a port of [HSC-110|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-110] though, due to the considerable differences in bookkeeping for the SdssShape code, this will be more of a reimplementation.  ",4,DM-6928,datamanagement,hsc backport include psf moment output table effectively port hsc-110|https://hsc jira.astro.princeton.edu jira browse hsc-110 considerable difference bookkeepe sdssshape code reimplementation,"HSC backport: Include PSF moments in the output tables This is effectively a port of [HSC-110|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-110] though, due to the considerable differences in bookkeeping for the SdssShape code, this will be more of a reimplementation."
access policy for PDAC This is a Prototype DAC (PDAC) and the access to it is limited. We need to draft a access policy. ,1,DM-6933,datamanagement,access policy pdac prototype dac pdac access limited need draft access policy,access policy for PDAC This is a Prototype DAC (PDAC) and the access to it is limited. We need to draft a access policy.
"Startup Scaffolding Machine Requirements - Base Plan Base machine needs for Startup, including those processes that must run together on the same physical host (such as databases resident in memory needed by other processes).",2,DM-6934,datamanagement,startup scaffolding machine requirements base plan base machine need startup include process run physical host database resident memory need process,"Startup Scaffolding Machine Requirements - Base Plan Base machine needs for Startup, including those processes that must run together on the same physical host (such as databases resident in memory needed by other processes)."
"Plan Base site machine startup pattern List all dependencies for specific processes that must be previously up an running. Establish the settings for 'time zero' on the startup timeline, such as purging queues, clearing specific data stores, arranging file system for startup, etc.",2,DM-6935,datamanagement,plan base site machine startup pattern list dependency specific process previously running establish setting time zero startup timeline purge queue clear specific datum store arrange file system startup etc,"Plan Base site machine startup pattern List all dependencies for specific processes that must be previously up an running. Establish the settings for 'time zero' on the startup timeline, such as purging queues, clearing specific data stores, arranging file system for startup, etc."
"Prep for and attend Kubernetes meeting Kubernetes is a machine provisioning application that has potential to assist LSST Startup scaffolding such as Just In Time personality assignment, persisting config settings, etc.",1,DM-6936,datamanagement,prep attend kubernetes meet kubernetes machine provisioning application potential assist lsst startup scaffold time personality assignment persist config setting etc,"Prep for and attend Kubernetes meeting Kubernetes is a machine provisioning application that has potential to assist LSST Startup scaffolding such as Just In Time personality assignment, persisting config settings, etc."
Download and install Kubernetes Implement a Kubernetes instance running on the Nebula cluster and begin configuration for testing Base site Startup behavior.,4,DM-6937,datamanagement,download install kubernetes implement kubernetes instance run nebula cluster begin configuration test base site startup behavior,Download and install Kubernetes Implement a Kubernetes instance running on the Nebula cluster and begin configuration for testing Base site Startup behavior.
"Evaluation of Kubernetes for Startup Scaffolding After Kubernetes is running as a simulated base site start up and provisioning tool, begin evaluation with fault injection such as the need for hot swap machine failover, sudden changes to Network topology and name server entries, etc.",3,DM-6938,datamanagement,evaluation kubernetes startup scaffolding kubernetes run simulate base site start provision tool begin evaluation fault injection need hot swap machine failover sudden change network topology server entry etc,"Evaluation of Kubernetes for Startup Scaffolding After Kubernetes is running as a simulated base site start up and provisioning tool, begin evaluation with fault injection such as the need for hot swap machine failover, sudden changes to Network topology and name server entries, etc."
"Set up proposed start up tools and procedure for NCSA L1 components If Kubernetes is the answer for startup provisioning which it is hoped to be, apply it to NCSA L1 machine startup requirements.",2,DM-6939,datamanagement,set propose start tool procedure ncsa l1 component kubernetes answer startup provisioning hope apply ncsa l1 machine startup requirement,"Set up proposed start up tools and procedure for NCSA L1 components If Kubernetes is the answer for startup provisioning which it is hoped to be, apply it to NCSA L1 machine startup requirements."
"Final Startup Scaffolding document This is expected to be a document specifying the final Startup scaffolding disposition. If specification is not final, it will assess which requirements for this epic were not reached.",2,DM-6940,datamanagement,final startup scaffolding document expect document specify final startup scaffold disposition specification final assess requirement epic reach,"Final Startup Scaffolding document This is expected to be a document specifying the final Startup scaffolding disposition. If specification is not final, it will assess which requirements for this epic were not reached."
"First round of updates to DRP LDM-151 sections from reviews Will address comments from [~swinbank], [~rhl], and probably [~ktl].",4,DM-6941,datamanagement,round update drp ldm-151 section review address comment ~swinbank ~rhl probably ~ktl,"First round of updates to DRP LDM-151 sections from reviews Will address comments from [~swinbank], [~rhl], and probably [~ktl]."
Explore and experiment the process of creating a Jupyter widget Study the Jupyter notebook and understand the concept of Jupyter widget. Try to make a simple Jupyter widget that works with Firefly visualization.,4,DM-6942,datamanagement,explore experiment process create jupyter widget study jupyter notebook understand concept jupyter widget try simple jupyter widget work firefly visualization,Explore and experiment the process of creating a Jupyter widget Study the Jupyter notebook and understand the concept of Jupyter widget. Try to make a simple Jupyter widget that works with Firefly visualization.
Integrate multiple-backgrounds concept into LDM-151 It's recently become apparent that we need to at least consider using different background estimation techniques for different kinds of measurements.  This is will require some thought to work into our current processing plans.,4,DM-6944,datamanagement,integrate multiple background concept ldm-151 recently apparent need consider different background estimation technique different kind measurement require thought work current processing plan,Integrate multiple-backgrounds concept into LDM-151 It's recently become apparent that we need to at least consider using different background estimation techniques for different kinds of measurements. This is will require some thought to work into our current processing plans.
"Add text to algorithmic components sections in LDM-151 While [~swinbank] has commented that the outlines are probably good enough for planning work (and I thnk that's broadly true), the lack of text in the algorithmic components section did occasionally lead to some misunderstandings in [~rhl]'s first review pass, so I think I should flesh that out with text sooner rather than later.    In this issue, I'll stick to sections that no one else has added text for, but eventually I'll also need to work with [~krughoff] and perhaps others to ensure that section has a consistent level of detail and focus.",8,DM-6945,datamanagement,add text algorithmic component section ldm-151 ~swinbank comment outline probably good plan work thnk broadly true lack text algorithmic component section occasionally lead misunderstanding ~rhl review pass think flesh text soon later issue stick section add text eventually need work ~krughoff ensure section consistent level detail focus,"Add text to algorithmic components sections in LDM-151 While [~swinbank] has commented that the outlines are probably good enough for planning work (and I thnk that's broadly true), the lack of text in the algorithmic components section did occasionally lead to some misunderstandings in [~rhl]'s first review pass, so I think I should flesh that out with text sooner rather than later. In this issue, I'll stick to sections that no one else has added text for, but eventually I'll also need to work with [~krughoff] and perhaps others to ensure that section has a consistent level of detail and focus."
"Firefly has problem to render in other browsers than Chrome Couple of problem using Firefly in  Safari:  * the components appears blank,    in Firefox:   * image and xyplot are not aligned (Gator).    The alignment can be reproduced in my Chrome and Safari.  Search parameters: ALLWISE source catalog, m81 100arcsec.   ",1,DM-6949,datamanagement,firefly problem render browser chrome couple problem firefly safari component appear blank firefox image xyplot align gator alignment reproduce chrome safari search parameter allwise source catalog m81 100arcsec,"Firefly has problem to render in other browsers than Chrome Couple of problem using Firefly in Safari: * the components appears blank, in Firefox: * image and xyplot are not aligned (Gator). The alignment can be reproduced in my Chrome and Safari. Search parameters: ALLWISE source catalog, m81 100arcsec."
"Table problems Table component has couple of problems:    #  Scrambled table values after column selection and saving the table, then reset mess up the table. Saving the table and reset makes the table comes back.  #  table display no longer redefines column names in the table based on the column label, e.g. ""Field Size"" instead of ""s_fov"".  #  Downloaded file is not a valid IPAC table.  #  Filtering table does not change image overlay or plot until table is saved. At that point, the filtering works, but the plot symbol changes (happens when result is decimated, datapoints > 5000?)  #  In Edit Table Options, it's unclear what the reset button resets to.  ",6,DM-6952,datamanagement,table problem table component couple problem scramble table value column selection save table reset mess table save table reset make table come table display long redefine column name table base column label e.g. field size instead s_fov downloaded file valid ipac table filtering table change image overlay plot table save point filtering work plot symbol change happen result decimate datapoint 5000 edit table options unclear reset button reset,"Table problems Table component has couple of problems: # Scrambled table values after column selection and saving the table, then reset mess up the table. Saving the table and reset makes the table comes back. # table display no longer redefines column names in the table based on the column label, e.g. ""Field Size"" instead of ""s_fov"". # Downloaded file is not a valid IPAC table. # Filtering table does not change image overlay or plot until table is saved. At that point, the filtering works, but the plot symbol changes (happens when result is decimated, datapoints > 5000?) # In Edit Table Options, it's unclear what the reset button resets to."
"XY plot problems found implement the items listed here:  * min/max options are now gone after migration, need to be added.  * Use the expression for X, Y column as the default label, otherwise the read out could be confusing.  * label changes for decimation: X-Bins and Y-Bins:  Number of X-Bins, Number of Y-Bins * shrink the size (to 1/5?)  of the blue dots for data representation.  I do like the circle when the point is highlighted.  * The units on the plot are indicated with a comma, e.g. “dec, deg”, should be ""dec (deg)"" as before   Need to confirm again ([~ejoliet])  * -Making a change to the plot (e.g. ra = ra * -1), then clicking on the gears to close makes the shading legend disappear. It also “quantizes” the plot- (not any more) * What happens now is the plot appears without legend after a search. Then making a change to the plot (e.g. ra = ra * -1 or filtering the table), then applying makes the legend appears/disappears. Expanding the table and collapsing it, make the legend disappears. *- Step to reproduce: Catalog search on 2MASS around m16 with 10' radius. * Greyscale introduced, where different colors represent different numbers of points. After filtering, the points change color to blue.  (This is because the it is not decimated any more) * -Clicking on plot gears makes plot unusably small.- (Not applicable any more since gear now brings up the options in popup) ",6,DM-6954,datamanagement,xy plot problem find implement item list min max option go migration need add use expression column default label read confusing label change decimation bins bins number bins number bins shrink size 1/5 blue dot datum representation like circle point highlight unit plot indicate comma e.g. dec deg dec deg need confirm ~ejoliet -make change plot e.g. ra ra -1 click gear close make shading legend disappear quantize plot- happen plot appear legend search make change plot e.g. ra ra filter table apply make legend appear disappear expand table collapse legend disappear step reproduce catalog search 2mass m16 10 radius greyscale introduce different color represent different number point filtering point change color blue decimate -clicke plot gear make plot unusably small.- applicable gear bring option popup,"XY plot problems found implement the items listed here: * min/max options are now gone after migration, need to be added. * Use the expression for X, Y column as the default label, otherwise the read out could be confusing. * label changes for decimation: X-Bins and Y-Bins: Number of X-Bins, Number of Y-Bins * shrink the size (to 1/5?) of the blue dots for data representation. I do like the circle when the point is highlighted. * The units on the plot are indicated with a comma, e.g. dec, deg , should be ""dec (deg)"" as before Need to confirm again ([~ejoliet]) * -Making a change to the plot (e.g. ra = ra * -1), then clicking on the gears to close makes the shading legend disappear. It also quantizes the plot- (not any more) * What happens now is the plot appears without legend after a search. Then making a change to the plot (e.g. ra = ra * -1 or filtering the table), then applying makes the legend appears/disappears. Expanding the table and collapsing it, make the legend disappears. *- Step to reproduce: Catalog search on 2MASS around m16 with 10' radius. * Greyscale introduced, where different colors represent different numbers of points. After filtering, the points change color to blue. (This is because the it is not decimated any more) * -Clicking on plot gears makes plot unusably small.- (Not applicable any more since gear now brings up the options in popup)"
"Message Dictionary Adjustment. Audit format of existing messaging and adjust according to 'wants' not task 'needs'...that is, msg body format that exists now is sufficient to fulfill tasks, but destination components must receive a broader description of overall system state. This will allow all components to log a more comprehensive snapshot of current state and is needed for troubleshooting. These additions to the message dictionary will be configurable like a logging priority levels function, and additions to message payload can be turned off for typical nightly operation.",4,DM-6955,datamanagement,message dictionary adjustment audit format exist messaging adjust accord want task needs' msg body format exist sufficient fulfill task destination component receive broad description overall system state allow component log comprehensive snapshot current state need troubleshooting addition message dictionary configurable like log priority level function addition message payload turn typical nightly operation,"Message Dictionary Adjustment. Audit format of existing messaging and adjust according to 'wants' not task 'needs'...that is, msg body format that exists now is sufficient to fulfill tasks, but destination components must receive a broader description of overall system state. This will allow all components to log a more comprehensive snapshot of current state and is needed for troubleshooting. These additions to the message dictionary will be configurable like a logging priority levels function, and additions to message payload can be turned off for typical nightly operation."
"'ACK' (Acknowledgement) message formats Enumerate ACK messages for all primary message types. Prototype both blocking and non-blocking acknowledgement aggregator that works via timeout, behavioral change, etc. This is related to DM-6411",4,DM-6956,datamanagement,ack acknowledgement message format enumerate ack message primary message type prototype block non blocking acknowledgement aggregator work timeout behavioral change etc relate dm-6411,"'ACK' (Acknowledgement) message formats Enumerate ACK messages for all primary message types. Prototype both blocking and non-blocking acknowledgement aggregator that works via timeout, behavioral change, etc. This is related to DM-6411"
Adding ACK messages to existing code framework New entries in message dictionary must be added and tested with the existing messaging code base.,4,DM-6957,datamanagement,add ack message exist code framework new entry message dictionary add test exist message code base,Adding ACK messages to existing code framework New entries in message dictionary must be added and tested with the existing messaging code base.
Documentation for new message types Add new ACK message types to existing Dictionary documentation.,2,DM-6958,datamanagement,documentation new message type add new ack message type exist dictionary documentation,Documentation for new message types Add new ACK message types to existing Dictionary documentation.
"Messages as objects Consider creating a dictionary of code objects to represent messages. Currently message bodys are built on the fly - evaluate pros and cons of switching to a message factory pattern. Message types are not a very extensive list, but switching to object implementation could increase maintainability of code.",2,DM-6959,datamanagement,message object consider create dictionary code object represent message currently message bodys build fly evaluate pro con switch message factory pattern message type extensive list switch object implementation increase maintainability code,"Messages as objects Consider creating a dictionary of code objects to represent messages. Currently message bodys are built on the fly - evaluate pros and cons of switching to a message factory pattern. Message types are not a very extensive list, but switching to object implementation could increase maintainability of code."
Overlay health check code Implement and overlay health check mechanism on existing messaging control code. Prototype and gather timing information to determine optimal frequency of checks and the location in the exposure cycle when these checks should occur.,8,DM-6960,datamanagement,overlay health check code implement overlay health check mechanism exist message control code prototype gather time information determine optimal frequency check location exposure cycle check occur,Overlay health check code Implement and overlay health check mechanism on existing messaging control code. Prototype and gather timing information to determine optimal frequency of checks and the location in the exposure cycle when these checks should occur.
Fault Injection in the form of unsuccessful health checks for components Build testing mechanism to inject faults into into health framework and stub code to address health check failures,4,DM-6961,datamanagement,fault injection form unsuccessful health check component build testing mechanism inject fault health framework stub code address health check failure,Fault Injection in the form of unsuccessful health checks for components Build testing mechanism to inject faults into into health framework and stub code to address health check failures
"Create policy for health check failure Document policy regarding action to take when various components are found to be unhealthy. This can vary depending when 1 component type (Forwarder) is offline versus 21 Forwarders offline. In addition, plans must be formulated for addressing the point in the exposure cycle when the health failure occurs.",4,DM-6962,datamanagement,create policy health check failure document policy action component find unhealthy vary depend component type forwarder offline versus 21 forwarders offline addition plan formulate address point exposure cycle health failure occur,"Create policy for health check failure Document policy regarding action to take when various components are found to be unhealthy. This can vary depending when 1 component type (Forwarder) is offline versus 21 Forwarders offline. In addition, plans must be formulated for addressing the point in the exposure cycle when the health failure occurs."
Implement health failure policy Formalize the prototypical implementation of health checks and associated policy into 'what to do' actions,4,DM-6963,datamanagement,implement health failure policy formalize prototypical implementation health check associate policy action,Implement health failure policy Formalize the prototypical implementation of health checks and associated policy into 'what to do' actions
"Make a proposal for API support for representation of relationships between table columns End users and the SUIT need to be able to determine a variety of relationships between columns in the tabular data products produced by LSST.  The particular example motivating this ticket is the need to answer the question ""where in the table is the uncertainty data for column 'x'?"".    The answer could be:  * ""there isn't any""  * ""a symmetric Gaussian uncertainty is in column 'sigma_x'""  * ""asymmetric Gaussian uncertainties are in columns 'sigmaplus_x' and 'sigmaminus_x'""  * ""'x' is correlated with 'y' and the covariance matrix is in 'covar_xx', 'covar_xy', and 'covar_yy'""     Ideally we would find a way for these relationships to be defined when the Apps code generates its afw.table outputs, discoverable through an API usable in the afw.table context, exportable to the database, and made available to end users and the SUIT.  It should be usable whether the data are delivered to end users as reconstituted afw.table objects or as tables in common Python formats (at least Astropy tables).    It should assist the SUIT in determining how to (automatically, though optionally) display uncertainty data when the primary data are requested.    This ticket expresses the idea that a solution that consists purely of a documented convention about prefixes to the string names of columns is inadequate.  We would like to avoid having to write code implementing that convention in, potentially, hundreds of places, and we would like to avoid requiring that end users know these conventions in order to see proper displays with error bars.  ",3,DM-6964,datamanagement,proposal api support representation relationship table column end user suit need able determine variety relationship column tabular data product produce lsst particular example motivate ticket need answer question table uncertainty datum column answer symmetric gaussian uncertainty column sigma_x asymmetric gaussian uncertainty column sigmaplus_x sigmaminus_x correlate covariance matrix covar_xx covar_xy covar_yy ideally find way relationship define apps code generate afw.table output discoverable api usable afw.table context exportable database available end user suit usable datum deliver end user reconstitute afw.table object table common python format astropy table assist suit determine automatically optionally display uncertainty datum primary datum request ticket express idea solution consist purely document convention prefix string name column inadequate like avoid have write code implement convention potentially hundred place like avoid require end user know convention order proper display error bar,"Make a proposal for API support for representation of relationships between table columns End users and the SUIT need to be able to determine a variety of relationships between columns in the tabular data products produced by LSST. The particular example motivating this ticket is the need to answer the question ""where in the table is the uncertainty data for column 'x'?"". The answer could be: * ""there isn't any"" * ""a symmetric Gaussian uncertainty is in column 'sigma_x'"" * ""asymmetric Gaussian uncertainties are in columns 'sigmaplus_x' and 'sigmaminus_x'"" * ""'x' is correlated with 'y' and the covariance matrix is in 'covar_xx', 'covar_xy', and 'covar_yy'"" Ideally we would find a way for these relationships to be defined when the Apps code generates its afw.table outputs, discoverable through an API usable in the afw.table context, exportable to the database, and made available to end users and the SUIT. It should be usable whether the data are delivered to end users as reconstituted afw.table objects or as tables in common Python formats (at least Astropy tables). It should assist the SUIT in determining how to (automatically, though optionally) display uncertainty data when the primary data are requested. This ticket expresses the idea that a solution that consists purely of a documented convention about prefixes to the string names of columns is inadequate. We would like to avoid having to write code implementing that convention in, potentially, hundreds of places, and we would like to avoid requiring that end users know these conventions in order to see proper displays with error bars."
Flesh out software primitives Jim has put together a fairly complete software primitives section.  This task is to read it over from the perspective of Alert Production and expand/refine where necessary.,4,DM-6966,datamanagement,flesh software primitive jim fairly complete software primitive section task read perspective alert production expand refine necessary,Flesh out software primitives Jim has put together a fairly complete software primitives section. This task is to read it over from the perspective of Alert Production and expand/refine where necessary.
"create a shared stack on NFS for use with  the current local condor pool It is well known that building, setting up a stack, and interactive devel work with those operations on NFS has performance issues.  Hence the official shared stack on lsstdev uses /ssd .    However,  a shared stack on NFS is useful and adequate for one important  use case --   users need a stack that can be used for small productions on the local condor pool currently available  on lsstdev.   For this use case multiple ""source""/""setups"" on a node/against the file system  can be avoidable by using a script to directly declare the environment.  run_orca /ctrl_orca supports this feature.       While GPFS is coming soon, there is expected to be a transition period of 2-3 months and so the NFS file system and a stack on it can serve users for an interim period.   If building a shared stack on NFS is not a heavy labor, we think it is worth the effort for this interim period, and as such make this request for a shared stack on NFS. ",1,DM-6968,datamanagement,"create share stack nfs use current local condor pool know building set stack interactive devel work operation nfs performance issue official share stack lsstdev use share stack nfs useful adequate important use case user need stack small production local condor pool currently available lsstdev use case multiple source""/""setups node file system avoidable script directly declare environment run_orca /ctrl_orca support feature gpfs come soon expect transition period month nfs file system stack serve user interim period build share stack nfs heavy labor think worth effort interim period request share stack nfs","create a shared stack on NFS for use with the current local condor pool It is well known that building, setting up a stack, and interactive devel work with those operations on NFS has performance issues. Hence the official shared stack on lsstdev uses /ssd . However, a shared stack on NFS is useful and adequate for one important use case -- users need a stack that can be used for small productions on the local condor pool currently available on lsstdev. For this use case multiple ""source""/""setups"" on a node/against the file system can be avoidable by using a script to directly declare the environment. run_orca /ctrl_orca supports this feature. While GPFS is coming soon, there is expected to be a transition period of 2-3 months and so the NFS file system and a stack on it can serve users for an interim period. If building a shared stack on NFS is not a heavy labor, we think it is worth the effort for this interim period, and as such make this request for a shared stack on NFS."
Fixes to LoadIndexedReferenceObjects Bug fixes for using the new {{LoadIndexedReferenceObjectTask}} and its associated components.,2,DM-6969,datamanagement,fix loadindexedreferenceobject bug fix new loadindexedreferenceobjecttask associate component,Fixes to LoadIndexedReferenceObjects Bug fixes for using the new {{LoadIndexedReferenceObjectTask}} and its associated components.
"Add tests for bindings of Eigen::Array and ndarray::EigenView [~pschella] has discovered that we don't have test coverage for converting less-common Eigen types to Python.  This is not urgent, but it should be fixed.",1,DM-6970,datamanagement,add test binding eigen::array ndarray::eigenview ~pschella discover test coverage convert common eigen type python urgent fix,"Add tests for bindings of Eigen::Array and ndarray::EigenView [~pschella] has discovered that we don't have test coverage for converting less-common Eigen types to Python. This is not urgent, but it should be fixed."
Qserv 2016_07 release - Update release notes  - Publish docs and bump version numbers,1,DM-6971,datamanagement,qserv 2016_07 release update release note publish doc bump version number,Qserv 2016_07 release - Update release notes - Publish docs and bump version numbers
Fix Qserv install doc and scripts for new newinstall.sh Update qserv install docs per new info at https://pipelines.lsst.io/install/newinstall.html,1,DM-6972,datamanagement,fix qserv install doc script new newinstall.sh update qserv install doc new info https://pipelines.lsst.io/install/newinstall.html,Fix Qserv install doc and scripts for new newinstall.sh Update qserv install docs per new info at https://pipelines.lsst.io/install/newinstall.html
"Fix metadata date problem in LDM-{463,152,135} These docs are currently borken in CI -- just need to have the dates reformatted in their metadata.yaml",1,DM-6973,datamanagement,"fix metadata date problem ldm-{463,152,135 doc currently borken ci need date reformatte metadata.yaml","Fix metadata date problem in LDM-{463,152,135} These docs are currently borken in CI -- just need to have the dates reformatted in their metadata.yaml"
"Type of IngestIndexedReferenceTask_config wrong in obs_ paf files In DM-6651 I moved the new HTM indexed reference catalog code from pipe_tasks to meas_algorithms, but didn't do a complete job. The type of IngestIndexedReferenceTask_config in obs_ paf files still must be updated.",1,DM-6974,datamanagement,type ingestindexedreferencetask_config wrong obs paf file dm-6651 move new htm indexed reference catalog code pipe_task meas_algorithm complete job type ingestindexedreferencetask_config obs paf file update,"Type of IngestIndexedReferenceTask_config wrong in obs_ paf files In DM-6651 I moved the new HTM indexed reference catalog code from pipe_tasks to meas_algorithms, but didn't do a complete job. The type of IngestIndexedReferenceTask_config in obs_ paf files still must be updated."
Document release milestone changes in DMTN-020 Please add a note to DMTN-020 describing the changes to release milestones discussed at the [DMLT meeting of 2016-07-18|https://confluence.lsstcorp.org/display/DM/DM+Leadership+Team+Meeting+2016-07-18].,1,DM-6975,datamanagement,document release milestone change dmtn-020 add note dmtn-020 describe change release milestone discuss dmlt meeting 2016 07 18|https://confluence.lsstcorp.org display dm dm+leadership+team+meeting+2016 07 18,Document release milestone changes in DMTN-020 Please add a note to DMTN-020 describing the changes to release milestones discussed at the [DMLT meeting of 2016-07-18|https://confluence.lsstcorp.org/display/DM/DM+Leadership+Team+Meeting+2016-07-18].
"watch for Highcharts update  There is an issue in the density plot for displaying the legends. Highcharts does not support the setting of the symbol size in the legends. So when the symbol size is too small or too large, the legends are not displayed.     We don't want to do too much workaround currently. This ticket is to watch for the Highcharts update. ",1,DM-6976,datamanagement,watch highcharts update issue density plot display legend highchart support setting symbol size legend symbol size small large legend display want workaround currently ticket watch highcharts update,"watch for Highcharts update There is an issue in the density plot for displaying the legends. Highcharts does not support the setting of the symbol size in the legends. So when the symbol size is too small or too large, the legends are not displayed. We don't want to do too much workaround currently. This ticket is to watch for the Highcharts update."
Update qserv for changes in Log interface DM-6521 improved Log class interface by replacing some static methods with non-static. Qserv is currently using couple of static methods which were retained in Log class for the duration of this migration. Once updated log package is released update qserv code to use new non-static methods and remove static methods from Log class after that.,2,DM-6978,datamanagement,update qserv change log interface dm-6521 improve log class interface replace static method non static qserv currently couple static method retain log class duration migration update log package release update qserv code use new non static method remove static method log class,Update qserv for changes in Log interface DM-6521 improved Log class interface by replacing some static methods with non-static. Qserv is currently using couple of static methods which were retained in Log class for the duration of this migration. Once updated log package is released update qserv code to use new non-static methods and remove static methods from Log class after that.
Markers don't show up in PNG download Markers don't show up in PNG download,4,DM-6980,datamanagement,marker png download markers png download,Markers don't show up in PNG download Markers don't show up in PNG download
"Fix oversampling settings in psfex The current settings in psfex will only turn on oversampling only if the seeing is < 0.5"", even if you have configured it do oversampling. This needs to be changed so that everything is determined by the config parameters.    We have also seen on HSC data that oversampling in general does not work well in psfex.  We need to change the current configuration which does 2x oversampling to just use the native pixel scale.",1,DM-6982,datamanagement,fix oversample setting psfex current setting psfex turn oversample seeing 0.5 configure oversample need change determine config parameter see hsc datum oversample general work psfex need change current configuration 2x oversample use native pixel scale,"Fix oversampling settings in psfex The current settings in psfex will only turn on oversampling only if the seeing is < 0.5"", even if you have configured it do oversampling. This needs to be changed so that everything is determined by the config parameters. We have also seen on HSC data that oversampling in general does not work well in psfex. We need to change the current configuration which does 2x oversampling to just use the native pixel scale."
"ci_hsc failure: AttributeError: 'Butler' object has no attribute 'repository' Following [~npease]'s [recent changes to the Butler|https://community.lsst.org/t/im-checking-in-butler-changes-related-to-rfc-184/959], ci_hsc is failing as follows:    {code}  [2016-07-20T07:57:31.954576Z] Traceback (most recent call last):  [2016-07-20T07:57:31.954643Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/bin/validate.py"", line 3, in <module>  [2016-07-20T07:57:31.954664Z]     main()  [2016-07-20T07:57:31.954732Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 53, in main  [2016-07-20T07:57:31.954756Z]     validator.run(dataId)  [2016-07-20T07:57:31.954825Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 155, in run  [2016-07-20T07:57:31.954851Z]     self.validateDataset(dataId, ds)  [2016-07-20T07:57:31.954923Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 117, in validateDataset  [2016-07-20T07:57:31.954956Z]     mappers = self.butler.repository.mappers()  [2016-07-20T07:57:31.954991Z] AttributeError: 'Butler' object has no attribute 'repository'  [2016-07-20T07:57:32.023212Z] scons: *** [.scons/ingestValidation-903342-100] Error 1  {code}    See e.g. https://ci.lsst.codes/job/stack-os-matrix/13274/compiler=gcc,label=centos-7,python=py2/console.    Please fix it. ",1,DM-6983,datamanagement,"ci_hsc failure attributeerror butler object attribute repository follow ~npease recent change butler|https://community.lsst.org im checking butler changes relate rfc-184/959 ci_hsc fail follow code 2016 07 20t07:57:31.954576z traceback recent 2016 07 20t07:57:31.954643z file /home jenkin slave workspace stack os matrix compiler gcc label centos-7 python py2 lsstsw build ci_hsc bin validate.py line 2016 07 20t07:57:31.954664z main 2016 07 20t07:57:31.954732z file /home jenkin slave workspace stack os matrix compiler gcc label centos-7 python py2 lsstsw build ci_hsc python lsst ci hsc validate.py line 53 main 2016 07 20t07:57:31.954756z validator.run(dataid 2016 07 20t07:57:31.954825z file /home jenkin slave workspace stack os matrix compiler gcc label centos-7 python py2 lsstsw build ci_hsc python lsst ci hsc validate.py line 155 run 2016 07 20t07:57:31.954851z self.validatedataset(dataid ds 2016 07 20t07:57:31.954923z file /home jenkin slave workspace stack os matrix compiler gcc label centos-7 python py2 lsstsw build ci_hsc python lsst ci hsc validate.py line 117 validatedataset 2016 07 20t07:57:31.954956z mapper self.butler.repository.mapper 2016 07 20t07:57:31.954991z attributeerror butler object attribute repository 2016 07 20t07:57:32.023212z scon .scons ingestvalidation-903342 100 error code e.g. https://ci.lsst.codes/job/stack-os-matrix/13274/compiler=gcc,label=centos-7,python=py2/console fix","ci_hsc failure: AttributeError: 'Butler' object has no attribute 'repository' Following [~npease]'s [recent changes to the Butler|https://community.lsst.org/t/im-checking-in-butler-changes-related-to-rfc-184/959], ci_hsc is failing as follows: {code} [2016-07-20T07:57:31.954576Z] Traceback (most recent call last): [2016-07-20T07:57:31.954643Z] File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/bin/validate.py"", line 3, in  [2016-07-20T07:57:31.954664Z] main() [2016-07-20T07:57:31.954732Z] File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 53, in main [2016-07-20T07:57:31.954756Z] validator.run(dataId) [2016-07-20T07:57:31.954825Z] File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 155, in run [2016-07-20T07:57:31.954851Z] self.validateDataset(dataId, ds) [2016-07-20T07:57:31.954923Z] File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 117, in validateDataset [2016-07-20T07:57:31.954956Z] mappers = self.butler.repository.mappers() [2016-07-20T07:57:31.954991Z] AttributeError: 'Butler' object has no attribute 'repository' [2016-07-20T07:57:32.023212Z] scons: *** [.scons/ingestValidation-903342-100] Error 1 {code} See e.g. https://ci.lsst.codes/job/stack-os-matrix/13274/compiler=gcc,label=centos-7,python=py2/console. Please fix it."
Suggest logging migration in daf_persistence and daf_butlerUtils Use lsst::log instead of pex::logging in daf_persistence and daf_butlerUtils,2,DM-6984,datamanagement,suggest log migration daf_persistence daf_butlerutil use lsst::log instead pex::logge daf_persistence daf_butlerutil,Suggest logging migration in daf_persistence and daf_butlerUtils Use lsst::log instead of pex::logging in daf_persistence and daf_butlerUtils
Suggest logging migration in afw Suggest a changeset with lsst::log instead of pex::logging in afw,5,DM-6985,datamanagement,suggest log migration afw suggest changeset lsst::log instead pex::logge afw,Suggest logging migration in afw Suggest a changeset with lsst::log instead of pex::logging in afw
Suggest logging migration in pipe_tasks and meas packages Suggest changesets using lsst::log instead of pex::logging,5,DM-6986,datamanagement,suggest log migration pipe_task mea package suggest changeset lsst::log instead pex::logge,Suggest logging migration in pipe_tasks and meas packages Suggest changesets using lsst::log instead of pex::logging
"Write up a description of Composite Datasets based on input from KT Write a description of composite datasets as I understand them based on the email KT sent on May 20 (attached), and on conversation I had with KT and Fritz on July 20.",2,DM-6987,datamanagement,write description composite datasets base input kt write description composite dataset understand base email kt send 20 attach conversation kt fritz july 20,"Write up a description of Composite Datasets based on input from KT Write a description of composite datasets as I understand them based on the email KT sent on May 20 (attached), and on conversation I had with KT and Fritz on July 20."
"Review Composite Dataset description document with stakeholders Review the composite dataset description document with [~jbosch] and [~Parejkoj], and any others who may be interested (e.g. post on community or do an RFD)",2,DM-6988,datamanagement,review composite dataset description document stakeholder review composite dataset description document ~jbosch ~parejkoj interested e.g. post community rfd,"Review Composite Dataset description document with stakeholders Review the composite dataset description document with [~jbosch] and [~Parejkoj], and any others who may be interested (e.g. post on community or do an RFD)"
"ctrl_events/tests/EventAppenderTest.py fails Jenkins run-rebuild ctrl_events/tests/EventAppenderTest.py started failing on Jenkins ""run-rebuild"" last night:   https://ci.lsst.codes/job/run-rebuild/354//console    All test cases in EventAppenderTest.py did run and pass, but it failed with a Segmentation fault in the end.     Jenkins ""run-rebuild"" uses a stack on NFS on lsst-dev (/nfs/home/lsstsw).  The same test passes on regular Jenkins (stack-os-matrix).      ",3,DM-6989,datamanagement,ctrl_event test eventappendertest.py fail jenkins run rebuild ctrl_event test eventappendertest.py start fail jenkins run rebuild night https://ci.lsst.codes/job/run-rebuild/354//console test case eventappendertest.py run pass fail segmentation fault end jenkins run rebuild use stack nfs lsst dev /nfs home lsstsw test pass regular jenkins stack os matrix,"ctrl_events/tests/EventAppenderTest.py fails Jenkins run-rebuild ctrl_events/tests/EventAppenderTest.py started failing on Jenkins ""run-rebuild"" last night: https://ci.lsst.codes/job/run-rebuild/354//console All test cases in EventAppenderTest.py did run and pass, but it failed with a Segmentation fault in the end. Jenkins ""run-rebuild"" uses a stack on NFS on lsst-dev (/nfs/home/lsstsw). The same test passes on regular Jenkins (stack-os-matrix)."
Improve testing in SQuaSH prototype This ticket captures some testing practices from https://ep2013.europython.eu/media/conference/slides/obey-the-testing-goat-rigorous-tdd-for-web-development-with-django-and-selenium.pdf  that we intend to use in the SQuaSH prototype.     - Use selenium to test user interactions  (functional tests)  - Use unittest module for unit tests   - Include some documentation about testing in sqr-009 ,5,DM-6990,datamanagement,improve testing squash prototype ticket capture testing practice https://ep2013.europython.eu/media/conference/slides/obey-the-testing-goat-rigorous-tdd-for-web-development-with-django-and-selenium.pdf intend use squash prototype use selenium test user interaction functional test use unittest module unit test include documentation testing sqr-009,Improve testing in SQuaSH prototype This ticket captures some testing practices from https://ep2013.europython.eu/media/conference/slides/obey-the-testing-goat-rigorous-tdd-for-web-development-with-django-and-selenium.pdf that we intend to use in the SQuaSH prototype. - Use selenium to test user interactions (functional tests) - Use unittest module for unit tests - Include some documentation about testing in sqr-009
"Add a script to summarize what visits are in what patches (Actual assignee: Samuel Piehl)     Have a script to show what visits are in what tracts/patches. This is especially useful for running coadd making and processing (e.g. makeCoaddTempExp, assembleCoadd) with runOrca and HTCondor, as the dataId of the jobs need to be specified. So this script's output will be in the format of a runOrca input file.     ",5,DM-6991,datamanagement,add script summarize visit patch actual assignee samuel piehl script visit tract patch especially useful run coadd make processing e.g. makecoaddtempexp assemblecoadd runorca htcondor dataid job need specify script output format runorca input file,"Add a script to summarize what visits are in what patches (Actual assignee: Samuel Piehl) Have a script to show what visits are in what tracts/patches. This is especially useful for running coadd making and processing (e.g. makeCoaddTempExp, assembleCoadd) with runOrca and HTCondor, as the dataId of the jobs need to be specified. So this script's output will be in the format of a runOrca input file."
"Extend SQuaSH dashboard to work with multiple datasets Currently SQuaSH dashboard works only with a fixed dataset. We want to ingest measurements of metrics computed by validate_drp for multiple test data e.g CFHT, DECam and HSC. In order to handle multiple datasets, we need a new model in SQuaSH  and extended the job API to ingest the measurements for different datasets. The user must be able to selected in the interface the dataset to be displayed.",3,DM-6992,datamanagement,extend squash dashboard work multiple dataset currently squash dashboard work fix dataset want ingest measurement metric compute validate_drp multiple test datum e.g cfht decam hsc order handle multiple dataset need new model squash extend job api ingest measurement different dataset user able select interface dataset display,"Extend SQuaSH dashboard to work with multiple datasets Currently SQuaSH dashboard works only with a fixed dataset. We want to ingest measurements of metrics computed by validate_drp for multiple test data e.g CFHT, DECam and HSC. In order to handle multiple datasets, we need a new model in SQuaSH and extended the job API to ingest the measurements for different datasets. The user must be able to selected in the interface the dataset to be displayed."
"Problems with MemoryTest ordering {{MemoryTestCase}} (or a derivative thereof) must be run as the last of all tests in a module in order to properly catch leaks.    [Our documentation|https://developer.lsst.io/coding/python_testing.html#memory-and-file-descriptor-leak-testing] implies, and [SQR-012 states|https://sqr-012.lsst.io/#memory-test], that this can be achieved by listing it as the last test case in the file.    This works for py.test, but not when using plain old unittest: the latter does not, so far as I can see, guarantee any sort of ordering as a matter of principle, and, in practice, it sorts things lexicographically (it uses whatever order it gets from running {{dir()}} on the test module, and I don't *think* that's guaranteed to be anything in particular).    For example, consider [{{testAstrometrySourceSelector.py}}|https://github.com/lsst/meas_algorithms/blob/master/tests/testAstrometrySourceSelector.py]. I made the following change to introduce a memory leak:    {code}  --- a/tests/testAstrometrySourceSelector.py  +++ b/tests/testAstrometrySourceSelector.py  @@ -70,8 +70,9 @@ class TestAstrometrySourceSelector(lsst.utils.tests.TestCase):           self.sourceSelector = sourceSelector.sourceSelectorRegistry['astrometry']()         def tearDown(self):  -        del self.src  -        del self.sourceSelector  +        pass  +        #del self.src  +        #del self.sourceSelector         def testSelectSources_good(self):           for i in range(5):  {code}    Py.test catches it:  {code}  $ py.test-2.7 testAstrometrySourceSelector.py  [...]  testAstrometrySourceSelector.py .........F  [...]  {code}    But simply running the test suite does not:  {code}  $ python testAstrometrySourceSelector.py  ..........  ----------------------------------------------------------------------  Ran 10 tests in 0.105s  {code}    Rename the test case:  {code}  @@ -144,7 +145,7 @@ def setup_module(module):       lsst.utils.tests.init()      -class MyMemoryTestCase(lsst.utils.tests.MemoryTestCase):  +class xMyMemoryTestCase(lsst.utils.tests.MemoryTestCase):       pass     if __name__ == ""__main__"":  {code}    And boom:  {code}  $ python testAstrometrySourceSelector.py  .........  54 Objects leaked:  {code}    Based on a very quick check, I think [sconsUtils runs tests by simply invoking {{python}}|https://github.com/lsst/sconsUtils/blob/f9763768d999cefa4c26b9f3418c28394dfb38df/python/lsst/sconsUtils/tests.py#L133], and I'm pretty sure that this is hard-wired into the muscle memory of many developers. In these cases, memory tests written following current guidelines won't be being properly executed.    ",3,DM-6998,datamanagement,"problem memorytest ordering memorytestcase derivative thereof run test module order properly catch leak documentation|https://developer.lsst.io code python_testing.html#memory file descriptor leak testing imply sqr-012 states|https://sqr-012.lsst.io/#memory test achieve list test case file work py.t plain old unitt far guarantee sort order matter principle practice sort thing lexicographically use order get run dir test module think guarantee particular example consider testastrometrysourceselector.py}}|https://github.com lsst meas_algorithms blob master test testastrometrysourceselector.py following change introduce memory leak code test testastrometrysourceselector.py test testastrometrysourceselector.py -70,8 +70,9 class testastrometrysourceselector(lsst.utils.test testcase self.sourceselector sourceselector.sourceselectorregistry['astrometry def teardown(self del self.src del self.sourceselector pass del self.src del self.sourceselector def testselectsources_good(self range(5 code py.t catch code py.test-2.7 testastrometrysourceselector.py ... testastrometrysourceselector.py ... code simply run test suite code python testastrometrysourceselector.py ran 10 test 0.105s code rename test case code +145,7 def setup_module(module lsst.utils.tests.init -clas mymemorytestcase(lsst.utils.tests memorytestcase class xmymemorytestcase(lsst.utils.tests memorytestcase pass main code boom code python testastrometrysourceselector.py 54 object leak code base quick check think sconsutil run test simply invoke python}}|https://github.com lsst sconsutils blob f9763768d999cefa4c26b9f3418c28394dfb38df python lsst sconsutils tests.py#l133 pretty sure hard wire muscle memory developer case memory test write follow current guideline will properly execute","Problems with MemoryTest ordering {{MemoryTestCase}} (or a derivative thereof) must be run as the last of all tests in a module in order to properly catch leaks. [Our documentation|https://developer.lsst.io/coding/python_testing.html#memory-and-file-descriptor-leak-testing] implies, and [SQR-012 states|https://sqr-012.lsst.io/#memory-test], that this can be achieved by listing it as the last test case in the file. This works for py.test, but not when using plain old unittest: the latter does not, so far as I can see, guarantee any sort of ordering as a matter of principle, and, in practice, it sorts things lexicographically (it uses whatever order it gets from running {{dir()}} on the test module, and I don't *think* that's guaranteed to be anything in particular). For example, consider [{{testAstrometrySourceSelector.py}}|https://github.com/lsst/meas_algorithms/blob/master/tests/testAstrometrySourceSelector.py]. I made the following change to introduce a memory leak: {code} --- a/tests/testAstrometrySourceSelector.py +++ b/tests/testAstrometrySourceSelector.py @@ -70,8 +70,9 @@ class TestAstrometrySourceSelector(lsst.utils.tests.TestCase): self.sourceSelector = sourceSelector.sourceSelectorRegistry['astrometry']() def tearDown(self): - del self.src - del self.sourceSelector + pass + #del self.src + #del self.sourceSelector def testSelectSources_good(self): for i in range(5): {code} Py.test catches it: {code} $ py.test-2.7 testAstrometrySourceSelector.py [...] testAstrometrySourceSelector.py .........F [...] {code} But simply running the test suite does not: {code} $ python testAstrometrySourceSelector.py .......... ---------------------------------------------------------------------- Ran 10 tests in 0.105s {code} Rename the test case: {code} @@ -144,7 +145,7 @@ def setup_module(module): lsst.utils.tests.init() -class MyMemoryTestCase(lsst.utils.tests.MemoryTestCase): +class xMyMemoryTestCase(lsst.utils.tests.MemoryTestCase): pass if __name__ == ""__main__"": {code} And boom: {code} $ python testAstrometrySourceSelector.py ......... 54 Objects leaked: {code} Based on a very quick check, I think [sconsUtils runs tests by simply invoking {{python}}|https://github.com/lsst/sconsUtils/blob/f9763768d999cefa4c26b9f3418c28394dfb38df/python/lsst/sconsUtils/tests.py#L133], and I'm pretty sure that this is hard-wired into the muscle memory of many developers. In these cases, memory tests written following current guidelines won't be being properly executed."
"Use lsst::log in pipe_base and pipe_tasks Per RFC-203, switch from using pex.logging to lsst.log in pipe_base and pipe_tasks (stage 2)",8,DM-6999,datamanagement,use lsst::log pipe_base pipe_task rfc-203 switch pex.logge lsst.log pipe_base pipe_task stage,"Use lsst::log in pipe_base and pipe_tasks Per RFC-203, switch from using pex.logging to lsst.log in pipe_base and pipe_tasks (stage 2)"
"Gator / Image Vis issue Issues with coverage:     * Toolbar icon not showing up  * If only one point that is no coverage image (or one table with many records but has the same ra,dec values)  * CAN’T REPEAT: In expanded mode, magnifier fails when image fills the visible space entirely (seems to affect 'Coverage' image only)  * Missing feature: before migration, in expanded mode, the toolbar had an 'added image' button which was bringing an image search panel to add images to the current view. => _move to:_ DM-7068  * CAN’T REPEAT: if marker/footprint overlay is clicked, that doesn't activate the image viewer and doesn't update the layer dialog either.  * in laptop screen size, the toolbar is not fully visible, scrolling from left to right only move the background but not the expanded panel.  * CAN”T REPEAT:in expand mode and zoom 'fill the visible space' clicked, the magnifier image doesn't show anything from the coverage image, can be reproduced in http://localhost:8080/firefly/demo/ffapi-highlevel-test.html (BTW, it happens in finderchart in OPS on any image in expanded mode ( ? ) )  * Readout is sometimes off the screen  * Expanded then return to normal: zoom is not adjusted correctly    If we find a way to repeat the items marked 'CAN'T REPEAT' they should go into another ticket.  Maybe in DM-7068 if it is still opened.  ",4,DM-7001,datamanagement,gator image vis issue issue coverage toolbar icon show point coverage image table record ra dec value repeat expand mode magnifier fail image fill visible space entirely affect coverage image miss feature migration expand mode toolbar add image button bring image search panel add image current view dm-7068 repeat marker footprint overlay click activate image viewer update layer dialog laptop screen size toolbar fully visible scroll left right background expand panel repeat expand mode zoom fill visible space click magnifier image coverage image reproduce http://localhost:8080 firefly demo ffapi highlevel test.html btw happen finderchart ops image expand mode readout screen expand return normal zoom adjust correctly find way repeat item mark can't repeat ticket maybe dm-7068 open,"Gator / Image Vis issue Issues with coverage: * Toolbar icon not showing up * If only one point that is no coverage image (or one table with many records but has the same ra,dec values) * CAN T REPEAT: In expanded mode, magnifier fails when image fills the visible space entirely (seems to affect 'Coverage' image only) * Missing feature: before migration, in expanded mode, the toolbar had an 'added image' button which was bringing an image search panel to add images to the current view. => _move to:_ DM-7068 * CAN T REPEAT: if marker/footprint overlay is clicked, that doesn't activate the image viewer and doesn't update the layer dialog either. * in laptop screen size, the toolbar is not fully visible, scrolling from left to right only move the background but not the expanded panel. * CAN T REPEAT:in expand mode and zoom 'fill the visible space' clicked, the magnifier image doesn't show anything from the coverage image, can be reproduced in http://localhost:8080/firefly/demo/ffapi-highlevel-test.html (BTW, it happens in finderchart in OPS on any image in expanded mode ( ? ) ) * Readout is sometimes off the screen * Expanded then return to normal: zoom is not adjusted correctly If we find a way to repeat the items marked 'CAN'T REPEAT' they should go into another ticket. Maybe in DM-7068 if it is still opened."
Match across filters -- Make color-color diagram Add the capability to match across filters.    1. Create color-color diagrams  2. Analyze performance metrics as a function of color.,4,DM-7003,datamanagement,match filter color color diagram add capability match filter create color color diagram analyze performance metric function color,Match across filters -- Make color-color diagram Add the capability to match across filters. 1. Create color-color diagrams 2. Analyze performance metrics as a function of color.
"Add ellipticity measurement to validate_drp Calculate the ellipticity, and the residual ellipticity (moments - PSF).    Add to calculated SRD statistics.    This will involve thinking about things on an image-by-image basis, which is the natural and largely SRD-specified way for considering ellipticity.",4,DM-7004,datamanagement,add ellipticity measurement validate_drp calculate ellipticity residual ellipticity moment psf add calculate srd statistic involve think thing image image basis natural largely srd specify way consider ellipticity,"Add ellipticity measurement to validate_drp Calculate the ellipticity, and the residual ellipticity (moments - PSF). Add to calculated SRD statistics. This will involve thinking about things on an image-by-image basis, which is the natural and largely SRD-specified way for considering ellipticity."
Show the list of packages that changed from build to build linked to the git url of the latest commit Motivated from the deviation seen from build 156 to 157 in  https://squash.lsst.codes/AM1 (caused by a commit in meas_algorithms package) we can show the list of packages that changed in the current build with respect to the previous build by comparing the git commit shas and return a list of tuples with the package name and git url.,2,DM-7005,datamanagement,list package change build build link git url late commit motivated deviation see build 156 157 https://squash.lsst.codes/am1 cause commit meas_algorithm package list package change current build respect previous build compare git commit sha return list tuple package git url,Show the list of packages that changed from build to build linked to the git url of the latest commit Motivated from the deviation seen from build 156 to 157 in https://squash.lsst.codes/AM1 (caused by a commit in meas_algorithms package) we can show the list of packages that changed in the current build with respect to the previous build by comparing the git commit shas and return a list of tuples with the package name and git url.
"Update squash to use bokeh 0.12.1 Bokeh 0.12 was just released and some issues are being fixed, before updating the bokeh version used in SQUASH we propose to wait for 0.12.1 release.  ",1,DM-7006,datamanagement,update squash use bokeh 0.12.1 bokeh 0.12 release issue fix update bokeh version squash propose wait 0.12.1 release,"Update squash to use bokeh 0.12.1 Bokeh 0.12 was just released and some issues are being fixed, before updating the bokeh version used in SQUASH we propose to wait for 0.12.1 release."
Investigate coverage of S13 databases found so far Look at databases located at *NCSA* so far to assess if they cover the full survey. The databases to be evaluated are mentioned in [DM-6905].    According to [S13 Testing Plan|https://dev.lsstcorp.org/trac/wiki/Summer2013/ConfigAndStackTestingPlans/Instructions] the S13 DRP dataset was split into two regions with an overalp used for cross-site verification:  * *NCSA*: -40< R.A. < +10  * *IN2P3*: +5 < R.A. < +55    Hence a goal of this task is to identify which previously located candidate databases and files correspond to either or both of these ranges.,4,DM-7007,datamanagement,investigate coverage s13 database find far look database locate ncsa far assess cover survey database evaluate mention dm-6905 accord s13 testing plan|https://dev.lsstcorp.org trac wiki summer2013 configandstacktestingplans instructions s13 drp dataset split region overalp cross site verification ncsa r.a. +10 in2p3 +5 r.a. +55 goal task identify previously locate candidate database file correspond range,Investigate coverage of S13 databases found so far Look at databases located at *NCSA* so far to assess if they cover the full survey. The databases to be evaluated are mentioned in [DM-6905]. According to [S13 Testing Plan|https://dev.lsstcorp.org/trac/wiki/Summer2013/ConfigAndStackTestingPlans/Instructions] the S13 DRP dataset was split into two regions with an overalp used for cross-site verification: * *NCSA*: -40< R.A. < +10 * *IN2P3*: +5 < R.A. < +55 Hence a goal of this task is to identify which previously located candidate databases and files correspond to either or both of these ranges.
Check boost.python building with Python 3 We may want to disable boost.python in the build. There are hints that there are problems with python3.5.,1,DM-7008,datamanagement,check boost.python building python want disable boost.python build hint problem python3.5,Check boost.python building with Python 3 We may want to disable boost.python in the build. There are hints that there are problems with python3.5.
"std::string construction from NULL pointer in ctrl_events I was browsing through ctrl_events package and found couple of instances in the headers where std::string instance is constructed from NULL pointer:  https://github.com/lsst/ctrl_events/blob/master/include/lsst/ctrl/events/Receiver.h#L87  https://github.com/lsst/ctrl_events/blob/master/include/lsst/ctrl/events/Transmitter.h#L81    I suspect that this code is never executed and those methods are overridden in subclasses because that construct will very likely crash when executed (std::string does not support construction from zero pointer, it will try to read from that pointer). Even if it's not executed it's better to change to return empty string or, if those two classes are never instantiated, make them abstract and make the methods pure virtual.  ",1,DM-7009,datamanagement,std::stre construction null pointer ctrl_event browse ctrl_event package find couple instance header std::stre instance construct null pointer https://github.com/lsst/ctrl_events/blob/master/include/lsst/ctrl/events/receiver.h#l87 https://github.com/lsst/ctrl_events/blob/master/include/lsst/ctrl/events/transmitter.h#l81 suspect code execute method overridden subclass construct likely crash execute std::string support construction zero pointer try read pointer execute well change return string class instantiate abstract method pure virtual,"std::string construction from NULL pointer in ctrl_events I was browsing through ctrl_events package and found couple of instances in the headers where std::string instance is constructed from NULL pointer: https://github.com/lsst/ctrl_events/blob/master/include/lsst/ctrl/events/Receiver.h#L87 https://github.com/lsst/ctrl_events/blob/master/include/lsst/ctrl/events/Transmitter.h#L81 I suspect that this code is never executed and those methods are overridden in subclasses because that construct will very likely crash when executed (std::string does not support construction from zero pointer, it will try to read from that pointer). Even if it's not executed it's better to change to return empty string or, if those two classes are never instantiated, make them abstract and make the methods pure virtual."
"Builds should be optimised by default By default, our builds are not optimised ({{-O0}}), which requires everyone who doesn't want to wait until the heat death of the universe to set {{SCONSFLAGS=""opt=3""}}, but other packages that are built with scons may not recognise this.  This default is also contrary to the standard practise for open-source software, which is that by default builds are optimised.  I will change the default optimisation level to {{opt=3}} from the current {{opt=0}}.  I will also add support for {{-Og}}.    This change was approved in RFC-202.",1,DM-7010,datamanagement,"build optimise default default build optimise -o0 require want wait heat death universe set sconsflags=""opt=3 package build scon recognise default contrary standard practise open source software default build optimise change default optimisation level opt=3 current opt=0 add support -og change approve rfc-202","Builds should be optimised by default By default, our builds are not optimised ({{-O0}}), which requires everyone who doesn't want to wait until the heat death of the universe to set {{SCONSFLAGS=""opt=3""}}, but other packages that are built with scons may not recognise this. This default is also contrary to the standard practise for open-source software, which is that by default builds are optimised. I will change the default optimisation level to {{opt=3}} from the current {{opt=0}}. I will also add support for {{-Og}}. This change was approved in RFC-202."
assign initial responsibilities in LDM-151 Assign first thoughts responsibilities to all software primitives and algorithmic components.  This is my take.  John will have his own take.,2,DM-7012,datamanagement,assign initial responsibility ldm-151 assign thought responsibility software primitive algorithmic component john,assign initial responsibilities in LDM-151 Assign first thoughts responsibilities to all software primitives and algorithmic components. This is my take. John will have his own take.
Memory cache leak in firefly server The visualization system is not update the memory accounting for the caching system.,2,DM-7014,datamanagement,memory cache leak firefly server visualization system update memory accounting cache system,Memory cache leak in firefly server The visualization system is not update the memory accounting for the caching system.
Analyze segmentation fault in EventAppenderTest Analyze the bug described in DM-6462,4,DM-7015,datamanagement,analyze segmentation fault eventappendertest analyze bug describe dm-6462,Analyze segmentation fault in EventAppenderTest Analyze the bug described in DM-6462
"Big image not showing working message when the load This is a problem with uploads, large image loads, and Atlas.   When a big image is loading the user does  not get feedback.  The problem is the the UI is not creating the ImageViewer soon enough.",2,DM-7016,datamanagement,big image show work message load problem upload large image load atlas big image load user feedback problem ui create imageviewer soon,"Big image not showing working message when the load This is a problem with uploads, large image loads, and Atlas. When a big image is loading the user does not get feedback. The problem is the the UI is not creating the ImageViewer soon enough."
Firefly JavaScript API documentation to support Camera team Convert API documentation and code examples to get Camera team started with the converted Firefly FITS viewer. ,6,DM-7017,datamanagement,firefly javascript api documentation support camera team convert api documentation code example camera team start converted firefly fit viewer,Firefly JavaScript API documentation to support Camera team Convert API documentation and code examples to get Camera team started with the converted Firefly FITS viewer.
"Firefly distribution build We need to support regular Firefly distribution builds (with bundled tomcat server),  similar to the builds we did in lsst firefly repository before the conversion.    This is to get Camera team started with new API.",2,DM-7018,datamanagement,firefly distribution build need support regular firefly distribution build bundle tomcat server similar build lsst firefly repository conversion camera team start new api,"Firefly distribution build We need to support regular Firefly distribution builds (with bundled tomcat server), similar to the builds we did in lsst firefly repository before the conversion. This is to get Camera team started with new API."
Setup standalone Firefly build using IPAC github Modify the existing Firefly-Standalone build in Jenkins to use IPAC's github.  Make sure github auto-releases still works.,3,DM-7019,datamanagement,setup standalone firefly build ipac github modify existing firefly standalone build jenkins use ipac github sure github auto release work,Setup standalone Firefly build using IPAC github Modify the existing Firefly-Standalone build in Jenkins to use IPAC's github. Make sure github auto-releases still works.
Update pex_exceptions to support Python 3 {{pex_exceptions}} needs to be updated to support Python 3.,1,DM-7021,datamanagement,update pex_exception support python pex_exceptions need update support python,Update pex_exceptions to support Python 3 {{pex_exceptions}} needs to be updated to support Python 3.
Package an experimental Firefly widget The aim is to package an experimental Jupyter widget with limited functionality so that it can be installed like other Jupyter widgets. Only a small set of Python and Javascript code will need to be packaged -- the widget will connect to a Firefly server. The [Jupyter widget cookiecutter|https://github.com/jupyter/widget-cookiecutter] provides a template.  ,4,DM-7022,datamanagement,package experimental firefly widget aim package experimental jupyter widget limited functionality instal like jupyter widget small set python javascript code need package widget connect firefly server jupyter widget cookiecutter|https://github.com jupyter widget cookiecutter provide template,Package an experimental Firefly widget The aim is to package an experimental Jupyter widget with limited functionality so that it can be installed like other Jupyter widgets. Only a small set of Python and Javascript code will need to be packaged -- the widget will connect to a Firefly server. The [Jupyter widget cookiecutter|https://github.com/jupyter/widget-cookiecutter] provides a template.
"Add more features to JS9 Wrapper  Di progress on adding extra features to JS9, including load and saving regions in the local notebook server, same with files. ",4,DM-7024,datamanagement,add feature js9 wrapper di progress add extra feature js9 include load save region local notebook server file,"Add more features to JS9 Wrapper Di progress on adding extra features to JS9, including load and saving regions in the local notebook server, same with files."
Investigate the option to use websockets used by jupyter to explore bi-directional communication  Di progress on understanding the possibility of using websocket locally to communicate with JS9 instances on local server. In this case we wouldn't need an external server and communication can be bi-directional. Now is only in one direction (mostly) when running Js9 locally ,3,DM-7025,datamanagement,investigate option use websocket jupyter explore bi directional communication di progress understand possibility websocket locally communicate js9 instance local server case need external server communication bi directional direction run js9 locally,Investigate the option to use websockets used by jupyter to explore bi-directional communication Di progress on understanding the possibility of using websocket locally to communicate with JS9 instances on local server. In this case we wouldn't need an external server and communication can be bi-directional. Now is only in one direction (mostly) when running Js9 locally
Setup up a cluster with kubernetes Sahand progress on installing and deploying a cluster automatically with Kubernetes. After this is completed we will use a user case example of running in cluster managed by Kubernetes/Spark,6,DM-7026,datamanagement,setup cluster kubernete sahand progress instal deploy cluster automatically kubernetes complete use user case example run cluster manage kubernetes spark,Setup up a cluster with kubernetes Sahand progress on installing and deploying a cluster automatically with Kubernetes. After this is completed we will use a user case example of running in cluster managed by Kubernetes/Spark
Port daf_base to Python 3 Changes necessary to get daf_base to work with Python 3.,1,DM-7028,datamanagement,port daf_base python change necessary daf_base work python,Port daf_base to Python 3 Changes necessary to get daf_base to work with Python 3.
"Image Bugs noticed in the API testing - Image is coming with one draw layer. (I can delete this draw layer and nothing changes on the image)    - When draw layer is deleted, and no more layers are present, the layers dialog should be closed. (It stays with nothing to display, you have to click x to close it.)    - After selecting an area in one viewer, I select an area in another viewer, then move the mouse to the first one:      147 ImageViewerLayout.jsx:314 Uncaught TypeError: Cannot read property 'x' of null at ImageViewerLayout.jsx:314   Nothing works after that. I have to reload.    - Selection appears with an offset, if the page, which contains the viewer is scrolled. (Load the attached script, press 'Start selection tracking' click to select a point, then scroll page a bit down, then click to select another point - it shows down from where it should be.)    - I have 2 image viewers in separate divs. Selected line in one, then selected line in the other. The line from the first one disappeared, but its label is still there. (See attached image.)    - Selection is working differently from distance. To select in another plot, I need to press selection again. I don't need to press ruler again to select new distance in another plot.    - It's possible to select distance tool and then area selection. First drag would define area selection, all the following line. A click would be defining a 0 length line, even if point selection is enabled. _move to_ DM-6473    - The payload.attValue of CHANGE_PLOT_ATTRIBUTE action is using WorldPt for area and point selections (when payload.attKey is 'SELECTION' or 'ACTIVE_POINT'), but ImagePt for line selection (when payload.attKey is 'ACTIVE_DISTANCE') How can I make them all use image coordinates?        ",6,DM-7029,datamanagement,image bugs notice api testing image come draw layer delete draw layer change image draw layer delete layer present layer dialog close stay display click close select area viewer select area viewer mouse 147 imageviewerlayout.jsx:314 uncaught typeerror read property null imageviewerlayout.jsx:314 work reload selection appear offset page contain viewer scroll load attached script press start selection tracking click select point scroll page bit click select point show image viewer separate divs select line select line line disappear label attach image selection work differently distance select plot need press selection need press ruler select new distance plot possible select distance tool area selection drag define area selection follow line click define length line point selection enable dm-6473 payload.attvalue change_plot_attribute action worldpt area point selection payload.attkey selection active_point imagept line selection payload.attkey active_distance use image coordinate,"Image Bugs noticed in the API testing - Image is coming with one draw layer. (I can delete this draw layer and nothing changes on the image) - When draw layer is deleted, and no more layers are present, the layers dialog should be closed. (It stays with nothing to display, you have to click x to close it.) - After selecting an area in one viewer, I select an area in another viewer, then move the mouse to the first one: 147 ImageViewerLayout.jsx:314 Uncaught TypeError: Cannot read property 'x' of null at ImageViewerLayout.jsx:314 Nothing works after that. I have to reload. - Selection appears with an offset, if the page, which contains the viewer is scrolled. (Load the attached script, press 'Start selection tracking' click to select a point, then scroll page a bit down, then click to select another point - it shows down from where it should be.) - I have 2 image viewers in separate divs. Selected line in one, then selected line in the other. The line from the first one disappeared, but its label is still there. (See attached image.) - Selection is working differently from distance. To select in another plot, I need to press selection again. I don't need to press ruler again to select new distance in another plot. - It's possible to select distance tool and then area selection. First drag would define area selection, all the following line. A click would be defining a 0 length line, even if point selection is enabled. _move to_ DM-6473 - The payload.attValue of CHANGE_PLOT_ATTRIBUTE action is using WorldPt for area and point selections (when payload.attKey is 'SELECTION' or 'ACTIVE_POINT'), but ImagePt for line selection (when payload.attKey is 'ACTIVE_DISTANCE') How can I make them all use image coordinates?"
Assign initial responsibilities in LDM-151 Assign first thoughts on responsibilities to all software primitives and algorithmic components. This is my take. Simon will have his own take.,2,DM-7031,datamanagement,assign initial responsibility ldm-151 assign thought responsibility software primitive algorithmic component simon,Assign initial responsibilities in LDM-151 Assign first thoughts on responsibilities to all software primitives and algorithmic components. This is my take. Simon will have his own take.
Estimate resource requirements for Software Primitives Meet with Jim & Simon. Discuss the Software Primitives section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate.,3,DM-7032,datamanagement,estimate resource requirement software primitives meet jim simon discuss software primitives section ldm-151 clarify ambiguity perform initial resource loading estimate,Estimate resource requirements for Software Primitives Meet with Jim & Simon. Discuss the Software Primitives section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate.
Estimate resource requirements for Software Primitives Meet with Simon & John. Discuss the Software Primitives section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate.,3,DM-7033,datamanagement,estimate resource requirement software primitives meet simon john discuss software primitives section ldm-151 clarify ambiguity perform initial resource loading estimate,Estimate resource requirements for Software Primitives Meet with Simon & John. Discuss the Software Primitives section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate.
Estimate resource requirements for Algorithmic Components Meet with Jim & Simon. Discuss the Algorithmic Components section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate,3,DM-7034,datamanagement,estimate resource requirement algorithmic components meet jim simon discuss algorithmic components section ldm-151 clarify ambiguity perform initial resource loading estimate,Estimate resource requirements for Algorithmic Components Meet with Jim & Simon. Discuss the Algorithmic Components section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate
Estimate resource requirements for Algorithmic Components Meet with Simon & John. Discuss the Algorithmic Components section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate,3,DM-7035,datamanagement,estimate resource requirement algorithmic components meet simon john discuss algorithmic components section ldm-151 clarify ambiguity perform initial resource loading estimate,Estimate resource requirements for Algorithmic Components Meet with Simon & John. Discuss the Algorithmic Components section of LDM-151: clarify any ambiguities and perform an initial resource loading estimate
Port pex_policy to Python 3 Changes needed to make pex_policy work on Python 3,1,DM-7036,datamanagement,port pex_policy python change need pex_policy work python,Port pex_policy to Python 3 Changes needed to make pex_policy work on Python 3
"Check endianness in ndarray/numpy conversions As reported on [community.lsst.org|https://community.lsst.org/t/how-to-run-the-dm-stack-on-simulated-fits-images/892/9], our {{ImageF(array)}} constructor will accept arrays with non-native endianness and interpret them as native.  This probably means the array converters in ndarray aren't including byte order when checking whether a passed array's dtype matches the expected C++ type.  ",2,DM-7037,datamanagement,check endianness ndarray numpy conversion report community.lsst.org|https://community.lsst.org run dm stack simulate fit images/892/9 imagef(array constructor accept array non native endianness interpret native probably mean array converter ndarray include byte order check pass array dtype match expect c++ type,"Check endianness in ndarray/numpy conversions As reported on [community.lsst.org|https://community.lsst.org/t/how-to-run-the-dm-stack-on-simulated-fits-images/892/9], our {{ImageF(array)}} constructor will accept arrays with non-native endianness and interpret them as native. This probably means the array converters in ndarray aren't including byte order when checking whether a passed array's dtype matches the expected C++ type."
"Setup JSDoc generation for the API portion of Firefly We need generate and publish JSDoc for Firefly JavaScript API, both high and low level.",4,DM-7038,datamanagement,setup jsdoc generation api portion firefly need generate publish jsdoc firefly javascript api high low level,"Setup JSDoc generation for the API portion of Firefly We need generate and publish JSDoc for Firefly JavaScript API, both high and low level."
"Familiarization with Footprint redesign Familiarize yourself with the RFC-37 driven Footprint redesign. Start thinking about ideas for how you could implement it and what the transition plan from the current Footprints might be.    A great outcome would be to propose a set of stories which would tackle the new Footprint development effort.    A good outcome would not be to have the stories ready to go, but to be well prepared for a discussion with [~jbosch] & [~swinbank] where we'll come up with some stories as a group.",5,DM-7039,datamanagement,familiarization footprint redesign familiarize rfc-37 drive footprint redesign start think idea implement transition plan current footprints great outcome propose set story tackle new footprint development effort good outcome story ready prepared discussion ~jbosch ~swinbank come story group,"Familiarization with Footprint redesign Familiarize yourself with the RFC-37 driven Footprint redesign. Start thinking about ideas for how you could implement it and what the transition plan from the current Footprints might be. A great outcome would be to propose a set of stories which would tackle the new Footprint development effort. A good outcome would not be to have the stories ready to go, but to be well prepared for a discussion with [~jbosch] & [~swinbank] where we'll come up with some stories as a group."
"Stars selected by starSelector change when number of cores varies Sogo Mineo writes in [HSC-1414|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1414]:  {quote}  See the following lines:    meas_algorithms/HSC-4.0.0/python/lsst/meas/algorithms/objectSizeStarSelector.py:466  in ObjectSizeStarSelector.selectStars():  {code}      if psfCandidate.getWidth() == 0:          psfCandidate.setBorderWidth(self._borderWidth)          psfCandidate.setWidth(self._kernelSize + 2*self._borderWidth)          psfCandidate.setHeight(self._kernelSize + 2*self._borderWidth)  {code}    In reduceFrames, these lines set the width of psfCandidate to be 21  for the first time the execution reaches there.    When the first CCD image has been processed, the worker process  continues to process another CCD image, and the execution reaches  here again.  This time, psfCandidate.getWidth() is 41, because  psfexPsfDeterminer has set it to be 41, and the value has been  retained because the width is a static member.  And so, for the second  CCD image, the width of psfCandidate is not 21 but 41.    Since psfCandidates are widened, stars positioned at edges of images  are rejected.  It results in a smaller number of PSF candidates than expected.    Only CCD images that are initially given to the worker processes  are processed with psfCandidate.getWidth() == 21. The other CCD images are  processed with psfCandidate.getWidth() == 41.  When the number of SMP cores changes, CCD images are processed with different  parameters.    The change in the number of PSF candidates results in different Psf, a different  result of image repair, and different catalogs.  {quote}    The line numbers are different on the LSST side because of refactoring (objectSizeStarSelector.py:466 has moved to starSelector.py:148), but the bug is still present.  The main problem appears to be that the {{PsfCandidate}} elements are {{static}}, are being set in both the star selector and the PSF determiner and one of those is conditional on what the value is.  I will investigate moving the {{static}} class variables to instance variables --- the desired size appears to vary by context, so it shouldn't be a class variable.",2,DM-7040,datamanagement,star select starselector change number core vary sogo mineo write hsc-1414|https://hsc jira.astro.princeton.edu jira browse hsc-1414 quote follow line meas_algorithm hsc-4.0.0 python lsst meas algorithm objectsizestarselector.py:466 objectsizestarselector.selectstar code psfcandidate.getwidth psfcandidate.setborderwidth(self._borderwidth psfcandidate.setwidth(self._kernelsize 2*self._borderwidth 2*self._borderwidth code reduceframe line set width psfcandidate 21 time execution reach ccd image process worker process continue process ccd image execution reach time psfcandidate.getwidth 41 psfexpsfdeterminer set 41 value retain width static member second ccd image width psfcandidate 21 41 psfcandidates widen star position edge image reject result small number psf candidate expect ccd image initially give worker process process psfcandidate.getwidth 21 ccd image process psfcandidate.getwidth 41 number smp core change ccd image process different parameter change number psf candidate result different psf different result image repair different catalog quote line number different lsst refactoring objectsizestarselector.py:466 move starselector.py:148 bug present main problem appear psfcandidate element static set star selector psf determiner conditional value investigate move static class variable instance variable desire size appear vary context class variable,"Stars selected by starSelector change when number of cores varies Sogo Mineo writes in [HSC-1414|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1414]: {quote} See the following lines: meas_algorithms/HSC-4.0.0/python/lsst/meas/algorithms/objectSizeStarSelector.py:466 in ObjectSizeStarSelector.selectStars(): {code} if psfCandidate.getWidth() == 0: psfCandidate.setBorderWidth(self._borderWidth) psfCandidate.setWidth(self._kernelSize + 2*self._borderWidth) psfCandidate.setHeight(self._kernelSize + 2*self._borderWidth) {code} In reduceFrames, these lines set the width of psfCandidate to be 21 for the first time the execution reaches there. When the first CCD image has been processed, the worker process continues to process another CCD image, and the execution reaches here again. This time, psfCandidate.getWidth() is 41, because psfexPsfDeterminer has set it to be 41, and the value has been retained because the width is a static member. And so, for the second CCD image, the width of psfCandidate is not 21 but 41. Since psfCandidates are widened, stars positioned at edges of images are rejected. It results in a smaller number of PSF candidates than expected. Only CCD images that are initially given to the worker processes are processed with psfCandidate.getWidth() == 21. The other CCD images are processed with psfCandidate.getWidth() == 41. When the number of SMP cores changes, CCD images are processed with different parameters. The change in the number of PSF candidates results in different Psf, a different result of image repair, and different catalogs. {quote} The line numbers are different on the LSST side because of refactoring (objectSizeStarSelector.py:466 has moved to starSelector.py:148), but the bug is still present. The main problem appears to be that the {{PsfCandidate}} elements are {{static}}, are being set in both the star selector and the PSF determiner and one of those is conditional on what the value is. I will investigate moving the {{static}} class variables to instance variables --- the desired size appears to vary by context, so it shouldn't be a class variable."
"Additional constraints on reference band selection for multiband Reference band selection currently depends on the configured band priority order, with exceptions made for sources with low signal-to-noise in the high priority bands.  [HSC-1411|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1411] points out that some additional qualifications, such as success for major measurements (e.g., CModel and Kron), would be helpful.",3,DM-7044,datamanagement,additional constraint reference band selection multiband reference band selection currently depend configure band priority order exception source low signal noise high priority band hsc-1411|https://hsc jira.astro.princeton.edu jira browse hsc-1411 point additional qualification success major measurement e.g. cmodel kron helpful,"Additional constraints on reference band selection for multiband Reference band selection currently depends on the configured band priority order, with exceptions made for sources with low signal-to-noise in the high priority bands. [HSC-1411|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1411] points out that some additional qualifications, such as success for major measurements (e.g., CModel and Kron), would be helpful."
"Prototype python cache for weak_ptr and weakref objects. There is a requirement for composite datasets that components of composites be cached and shareable, and we expect to use an object cache for this.   We have identified that we need to be able to cache C++ weak_ptr and python weakref in the same cache in an opaque way. This needs some prototype R&D.",2,DM-7046,datamanagement,prototype python cache weak_ptr weakref object requirement composite dataset component composite cache shareable expect use object cache identify need able cache c++ weak_ptr python weakref cache opaque way need prototype,"Prototype python cache for weak_ptr and weakref objects. There is a requirement for composite datasets that components of composites be cached and shareable, and we expect to use an object cache for this. We have identified that we need to be able to cache C++ weak_ptr and python weakref in the same cache in an opaque way. This needs some prototype R&D."
Port pex_config to Python 3 Work involved in ensuring that pex_config passes all tests on Python 3 and legacy Python.,1,DM-7047,datamanagement,port pex_config python work involve ensure pex_config pass test python legacy python,Port pex_config to Python 3 Work involved in ensuring that pex_config passes all tests on Python 3 and legacy Python.
validate_drp is failing because it's accessing butler internals that have changed need to change obs_decam's ingest task to use the newer class hierarchy to get the root of the butler's single repository. (longer term there should be a butler API for this or the task should get the value of root from somewhere else),1,DM-7048,datamanagement,validate_drp fail access butler internal change need change obs_decam ingest task use new class hierarchy root butler single repository long term butler api task value root,validate_drp is failing because it's accessing butler internals that have changed need to change obs_decam's ingest task to use the newer class hierarchy to get the root of the butler's single repository. (longer term there should be a butler API for this or the task should get the value of root from somewhere else)
"Move patch/tract and config mapping definitions to daf_butlerUtils Implement RFC-204 by adding new entries for all patch/tract and config mapping definitions to .yaml files in daf_butlerUtils, and removing any such entries that are identical to the common ones from .paf files in obs* packages.    I think the ""common"" entry can usually be defined by consensus between any two of obs_cfht, obs_decam, and obs_subaru (and frequently all three).  If there are any patch/tract or config datasets for which no two cameras agree, I think we should use obs_subaru's definitions (but I doubt there are any such cases).    Entries that are not identical to the common ones should not be removed on this issue (that should make this change entirely backwards compatible), but should be documented in new per-camera issues for later standardization.",4,DM-7049,datamanagement,patch tract config mapping definition daf_butlerutils implement rfc-204 add new entry patch tract config mapping definition .yaml file daf_butlerutil remove entry identical common one file obs package think common entry usually define consensus obs_cfht obs_decam obs_subaru frequently patch tract config dataset camera agree think use obs_subaru definition doubt case entry identical common one remove issue change entirely backwards compatible document new camera issue later standardization,"Move patch/tract and config mapping definitions to daf_butlerUtils Implement RFC-204 by adding new entries for all patch/tract and config mapping definitions to .yaml files in daf_butlerUtils, and removing any such entries that are identical to the common ones from .paf files in obs* packages. I think the ""common"" entry can usually be defined by consensus between any two of obs_cfht, obs_decam, and obs_subaru (and frequently all three). If there are any patch/tract or config datasets for which no two cameras agree, I think we should use obs_subaru's definitions (but I doubt there are any such cases). Entries that are not identical to the common ones should not be removed on this issue (that should make this change entirely backwards compatible), but should be documented in new per-camera issues for later standardization."
"LTD Keeper: Use Google Cloud Platform SQL Currently LTD Keeper uses a sqlite DB. This ticket will migrate that DB to Google’s Cloud Platform’s managed SQL. This solution provides automatic backups, and provides flexibility to run multiple ltd-keeper pods. Google’s SQL makes sense since LTD Keeper is run on Google Cloud Platform.",5,DM-7050,datamanagement,ltd keeper use google cloud platform sql currently ltd keeper use sqlite db ticket migrate db google cloud platform manage sql solution provide automatic backup provide flexibility run multiple ltd keeper pod google sql make sense ltd keeper run google cloud platform,"LTD Keeper: Use Google Cloud Platform SQL Currently LTD Keeper uses a sqlite DB. This ticket will migrate that DB to Google s Cloud Platform s managed SQL. This solution provides automatic backups, and provides flexibility to run multiple ltd-keeper pods. Google s SQL makes sense since LTD Keeper is run on Google Cloud Platform."
conda installation from the stack channel brings in astropy 1.2 If you do a conda install of lsst_sims from http://conda.lsst.codes/sims you get astropy-1.1.1.  If you do the same install from http://conda.lsst.codes/stack you get astropy-1.2.1.  This is a problem for the sims stack since the sncosmo package (on which we depend for our simulations of supernova light curves) is sensitive to which version of astropy you are running.    Was it intentional that the two channels deliver different versions of astropy?,2,DM-7051,datamanagement,conda installation stack channel bring astropy 1.2 conda install lsst_sim http://conda.lsst.codes/sim astropy-1.1.1 install http://conda.lsst.codes/stack astropy-1.2.1 problem sim stack sncosmo package depend simulation supernova light curve sensitive version astropy run intentional channel deliver different version astropy,conda installation from the stack channel brings in astropy 1.2 If you do a conda install of lsst_sims from http://conda.lsst.codes/sims you get astropy-1.1.1. If you do the same install from http://conda.lsst.codes/stack you get astropy-1.2.1. This is a problem for the sims stack since the sncosmo package (on which we depend for our simulations of supernova light curves) is sensitive to which version of astropy you are running. Was it intentional that the two channels deliver different versions of astropy?
Kick-off meeting [~nlust] & [~swinbank] will meet with the SUI/T team on 2016-07-26 and discuss how we can best engage with them.,1,DM-7054,datamanagement,kick meeting ~nlust ~swinbank meet sui team 2016 07 26 discuss well engage,Kick-off meeting [~nlust] & [~swinbank] will meet with the SUI/T team on 2016-07-26 and discuss how we can best engage with them.
fix miscellaneous table issues # disable sorting when content is html  # table options: auto-adjust all column width based on content  # table refractoring: exposing more actions to saga.  #* renamed a few actions to better reflect what it's doing.  #* added TABLE_FILTER  #* added document for sequence of actions where applicable.  # update build script to skip buildClient when possible.  # Catalog overlay should not use table id in drawing layer description. (See the attached screenshot.) It should be using MetaConst.CATALOG_OVERLAY_TYPE attribute value,4,DM-7055,datamanagement,fix miscellaneous table issue disable sorting content html table option auto adjust column width base content table refractore expose action saga rename action well reflect add add document sequence action applicable update build script skip buildclient possible catalog overlay use table draw layer description attached screenshot metaconst catalog_overlay_type attribute value,fix miscellaneous table issues # disable sorting when content is html # table options: auto-adjust all column width based on content # table refractoring: exposing more actions to saga. #* renamed a few actions to better reflect what it's doing. #* added TABLE_FILTER #* added document for sequence of actions where applicable. # update build script to skip buildClient when possible. # Catalog overlay should not use table id in drawing layer description. (See the attached screenshot.) It should be using MetaConst.CATALOG_OVERLAY_TYPE attribute value
"Wrap afw::table with pybind11 Following the same pattern as DM-6926, DM-6297, etc.",8,DM-7056,datamanagement,wrap afw::table pybind11 follow pattern dm-6926 dm-6297 etc,"Wrap afw::table with pybind11 Following the same pattern as DM-6926, DM-6297, etc."
"Plot sources/source density on WCS quiver plots We should show the individual sources (size/color scaled by S/N?) that went into the jointcal fit, and/or a density map of the sources (scaled by S/N?) under the quiver plots. This will help distinguish areas with poorly constrained fits or where the TAN-SIP function diverges, from those where the new WCS really is odd.",2,DM-7059,datamanagement,plot source source density wcs quiver plot individual source size color scale go jointcal fit and/or density map source scale quiver plot help distinguish area poorly constrain fit tan sip function diverge new wcs odd,"Plot sources/source density on WCS quiver plots We should show the individual sources (size/color scaled by S/N?) that went into the jointcal fit, and/or a density map of the sources (scaled by S/N?) under the quiver plots. This will help distinguish areas with poorly constrained fits or where the TAN-SIP function diverges, from those where the new WCS really is odd."
"Plot old/new jointcal WCS vs. tangent plane To better understand the jointcal WCS vs. the original single frame TAN-SIP, we need quiver and heatmap plots of each WCS (old and new) separately vs. a tangent_pixel or related ""non-distorted"" projection. This will let us compare the original single frame fit with jointcal's fit.    This probably could be done with CameraGeom, but would be easier with the upcoming new WCS/transform system, since it may involve pulling out a new Frame.",4,DM-7060,datamanagement,plot old new jointcal wcs vs. tangent plane well understand jointcal wcs vs. original single frame tan sip need quiver heatmap plot wcs old new separately vs. tangent_pixel related non distorted projection let compare original single frame fit jointcal fit probably camerageom easy upcoming new wcs transform system involve pull new frame,"Plot old/new jointcal WCS vs. tangent plane To better understand the jointcal WCS vs. the original single frame TAN-SIP, we need quiver and heatmap plots of each WCS (old and new) separately vs. a tangent_pixel or related ""non-distorted"" projection. This will let us compare the original single frame fit with jointcal's fit. This probably could be done with CameraGeom, but would be easier with the upcoming new WCS/transform system, since it may involve pulling out a new Frame."
"Plot ""real"" distortion by comparing with reference catalog To compare the old WCS and jointcal's fit with the ""real"" distortion, we can use the matched reference catalog to plot a quiver diagram or an interpolated heat map showing how far each star is from its reference star. We may have to think about how to select objects for this plot, since centroiding errors would make it not so useful.    This would probably be most useful for lsstSim, since that has an infinite-precision reference catalog.",4,DM-7061,datamanagement,plot real distortion compare reference catalog compare old wcs jointcal fit real distortion use match reference catalog plot quiver diagram interpolate heat map show far star reference star think select object plot centroide error useful probably useful lsstsim infinite precision reference catalog,"Plot ""real"" distortion by comparing with reference catalog To compare the old WCS and jointcal's fit with the ""real"" distortion, we can use the matched reference catalog to plot a quiver diagram or an interpolated heat map showing how far each star is from its reference star. We may have to think about how to select objects for this plot, since centroiding errors would make it not so useful. This would probably be most useful for lsstSim, since that has an infinite-precision reference catalog."
"Support work related to PDAC effort This issue captures emergent work to support for example DM-6905 , for which I spent some cycles locating datasets of the 2013  SDRP, staging some files off of BW tape through globus online and unpacking to /nfs/scratch,  etc.    This effort may not fit exactly as 'emergent middleware',  but it was roughly the best fit at this time. ",3,DM-7062,datamanagement,support work relate pdac effort issue capture emergent work support example dm-6905 spend cycle locate dataset 2013 sdrp stage file bw tape globus online unpack /nfs scratch etc effort fit exactly emergent middleware roughly good fit time,"Support work related to PDAC effort This issue captures emergent work to support for example DM-6905 , for which I spent some cycles locating datasets of the 2013 SDRP, staging some files off of BW tape through globus online and unpacking to /nfs/scratch, etc. This effort may not fit exactly as 'emergent middleware', but it was roughly the best fit at this time."
"support work for testing shared stack in NFS It was realized that the ""shared stack of lsstdev"" was not actually usable on the local condor pool due to /ssd usage.   In response to this,  an effort for a second shared stack on NFS  was initiated in DM-6968.  This issue captures the emergent work of pipeline testing to validate the new stack of that issue. ",2,DM-7063,datamanagement,support work test share stack nfs realize share stack lsstdev actually usable local condor pool /ssd usage response effort second share stack nfs initiate dm-6968 issue capture emergent work pipeline testing validate new stack issue,"support work for testing shared stack in NFS It was realized that the ""shared stack of lsstdev"" was not actually usable on the local condor pool due to /ssd usage. In response to this, an effort for a second shared stack on NFS was initiated in DM-6968. This issue captures the emergent work of pipeline testing to validate the new stack of that issue."
"Extend functionality of experimental Jupyter widgets for Firefly A package for experimental Jupyter widgets for Firefly is being developed in  https://github.com/Caltech-IPAC/firefly_widgets . Using the Firefly Javascript API for Images and Tables, add some further useful functionality for demonstration purposes.",4,DM-7065,datamanagement,extend functionality experimental jupyter widget firefly package experimental jupyter widget firefly develop https://github.com/caltech-ipac/firefly_widget firefly javascript api images tables add useful functionality demonstration purpose,"Extend functionality of experimental Jupyter widgets for Firefly A package for experimental Jupyter widgets for Firefly is being developed in https://github.com/Caltech-IPAC/firefly_widgets . Using the Firefly Javascript API for Images and Tables, add some further useful functionality for demonstration purposes."
Port pex_logging to Python 3 Work required to get pex_logging working on python 3. Will also include some package cleanups.,1,DM-7066,datamanagement,port pex_logge python work require pex_logging work python include package cleanup,Port pex_logging to Python 3 Work required to get pex_logging working on python 3. Will also include some package cleanups.
"Break joincal's link to upstream lsst_france repo lsst/jointcal is still linked to the upstream repo at lsst_france. I believe all the relevant changes have been ported. It's time to break that upstream link, so that pull requests can be made in a more obvious fashion.",1,DM-7067,datamanagement,break joincal link upstream lsst_france repo lsst jointcal link upstream repo lsst_france believe relevant change port time break upstream link pull request obvious fashion,"Break joincal's link to upstream lsst_france repo lsst/jointcal is still linked to the upstream repo at lsst_france. I believe all the relevant changes have been ported. It's time to break that upstream link, so that pull requests can be made in a more obvious fashion."
"Firefly API bugs 2 *Issues*    Gator:  * Missing feature: before migration, in expanded mode, the toolbar had an 'added image' button which was bringing an image search panel to add images to the current view.    * The Gator Multi-object search seems having problem with the coverage image.    Atlas:    * if marker/footprint overlay is clicked, that doesn't activate the image viewer and doesn't update the layer dialog either. Large drawing layers block viewer from becoming active, WFIRST footprint or WFC3/IR cause the problem.  *  -in expand mode and zoom 'fill the visible space' clicked, the magnifier image doesn't show anything from the coverage- - not a bug, magnifier is disable when zoom level is above 8x     API:  *  In API, it is not possible to drag dialogs (ex. Drawing Layers).    All Firefly (found by Tatiana):    * The highlight should of catalog or coverage overlay should just change if you are close to the point. For now I am setting it to 20 pixels  * -Catalog overlay should not use table id in drawing layer description. (See the attached screenshot.) It should be using MetaConst.CATALOG_OVERLAY_TYPE attribute value.- _Moved to_ DM-7055    * After using distance tool in one plot, then the other, clicking again in the first plot does not make it active. (You have to click on the border of the plot to make it active, clicking inside does not help.)    This seems to cause strange behavior, when selections do not work as expected. For example: select ruler, select some lines alternating first and second plot. Unselect ruler, select area icon. Selecting in the first plot will still show distance. I had to delete distance tool drawing layer or click on the border for things to start showing area selection.    In general, there is some confusion with active plot, when I have two viewers. Should a plot become active as soon as mouse enters is? Otherwise in readout, compass thumbnail will still show active plot, while readout thumbnail could use another plot.  _note from trey: this is the same problem as the marker/footprint in atlas described aboved_",4,DM-7068,datamanagement,firefly api bug issues gator miss feature migration expand mode toolbar add image button bring image search panel add image current view gator multi object search have problem coverage image atlas marker footprint overlay click activate image viewer update layer dialog large drawing layer block viewer active wfirst footprint wfc3 ir cause problem expand mode zoom fill visible space click magnifier image coverage- bug magnifier disable zoom level 8x api api possible drag dialog ex drawing layers firefly find tatiana highlight catalog coverage overlay change close point set 20 pixel -catalog overlay use table draw layer description attached screenshot metaconst catalog_overlay_type attribute value.- move dm-7055 distance tool plot click plot active click border plot active click inside help cause strange behavior selection work expect example select ruler select line alternate second plot unselect ruler select area icon select plot distance delete distance tool draw layer click border thing start show area selection general confusion active plot viewer plot active soon mouse enter readout compass thumbnail active plot readout thumbnail use plot note trey problem marker footprint atlas describe above,"Firefly API bugs 2 *Issues* Gator: * Missing feature: before migration, in expanded mode, the toolbar had an 'added image' button which was bringing an image search panel to add images to the current view. * The Gator Multi-object search seems having problem with the coverage image. Atlas: * if marker/footprint overlay is clicked, that doesn't activate the image viewer and doesn't update the layer dialog either. Large drawing layers block viewer from becoming active, WFIRST footprint or WFC3/IR cause the problem. * -in expand mode and zoom 'fill the visible space' clicked, the magnifier image doesn't show anything from the coverage- - not a bug, magnifier is disable when zoom level is above 8x API: * In API, it is not possible to drag dialogs (ex. Drawing Layers). All Firefly (found by Tatiana): * The highlight should of catalog or coverage overlay should just change if you are close to the point. For now I am setting it to 20 pixels * -Catalog overlay should not use table id in drawing layer description. (See the attached screenshot.) It should be using MetaConst.CATALOG_OVERLAY_TYPE attribute value.- _Moved to_ DM-7055 * After using distance tool in one plot, then the other, clicking again in the first plot does not make it active. (You have to click on the border of the plot to make it active, clicking inside does not help.) This seems to cause strange behavior, when selections do not work as expected. For example: select ruler, select some lines alternating first and second plot. Unselect ruler, select area icon. Selecting in the first plot will still show distance. I had to delete distance tool drawing layer or click on the border for things to start showing area selection. In general, there is some confusion with active plot, when I have two viewers. Should a plot become active as soon as mouse enters is? Otherwise in readout, compass thumbnail will still show active plot, while readout thumbnail could use another plot. _note from trey: this is the same problem as the marker/footprint in atlas described aboved_"
Port daf_persistence to Python 3 Work relating to getting daf_persistence to run on python 3. Includes some code modernization.,1,DM-7069,datamanagement,port daf_persistence python work relate get daf_persistence run python include code modernization,Port daf_persistence to Python 3 Work relating to getting daf_persistence to run on python 3. Includes some code modernization.
Move consts from top of Associations.cc into JointcalConfig There are three values at the top of Associations.cc under a TODO comment that should be lifted up into JointcalConfig so they can be configured at runtime. It would be good to try to add tests to check different values for them (and possibly just remove usnoMatchCut).,1,DM-7070,datamanagement,const associations.cc jointcalconfig value associations.cc todo comment lift jointcalconfig configure runtime good try add test check different value possibly remove usnomatchcut,Move consts from top of Associations.cc into JointcalConfig There are three values at the top of Associations.cc under a TODO comment that should be lifted up into JointcalConfig so they can be configured at runtime. It would be good to try to add tests to check different values for them (and possibly just remove usnoMatchCut).
"Fix Django admin interface  Django admin interface is useful to edit db entries in SQUASH if needed, e.g decam measurements that were incidentally pushed to the dashboard during X16.    A bug was found using the admin interface in development mode, due to a bad field returned by the Jobs model.    This ticket is to capture the fix for this bug, this new git ref will be deployed for better control of data in SQUASH database.    ",1,DM-7071,datamanagement,fix django admin interface django admin interface useful edit db entry squash need e.g decam measurement incidentally push dashboard x16 bug find admin interface development mode bad field return jobs model ticket capture fix bug new git ref deploy well control datum squash database,"Fix Django admin interface Django admin interface is useful to edit db entries in SQUASH if needed, e.g decam measurements that were incidentally pushed to the dashboard during X16. A bug was found using the admin interface in development mode, due to a bad field returned by the Jobs model. This ticket is to capture the fix for this bug, this new git ref will be deployed for better control of data in SQUASH database."
"visit DRP team, June 2016 Travel to Princeton June 13-17 and meet with the DRP team; work and learn about L2 processing; discuss the workflow requirements and use cases. ",8,DM-7072,datamanagement,visit drp team june 2016 travel princeton june 13 17 meet drp team work learn l2 processing discuss workflow requirement use case,"visit DRP team, June 2016 Travel to Princeton June 13-17 and meet with the DRP team; work and learn about L2 processing; discuss the workflow requirements and use cases."
Install ESXi on lsst-dm-mac.lsst.org Install and configure ESXi on the Mac Pro server.,7,DM-7073,datamanagement,install esxi lsst-dm-mac.lsst.org install configure esxi mac pro server,Install ESXi on lsst-dm-mac.lsst.org Install and configure ESXi on the Mac Pro server.
"Install Mac OS X Mountain Lion on ESXi Install, configure and snapshot Mac OS X Mountain Lion. Unfortunately this is required to install any other Mac OS VM on ESXi.",1,DM-7074,datamanagement,install mac os mountain lion esxi install configure snapshot mac os mountain lion unfortunately require install mac os vm esxi,"Install Mac OS X Mountain Lion on ESXi Install, configure and snapshot Mac OS X Mountain Lion. Unfortunately this is required to install any other Mac OS VM on ESXi."
"Install Mac OS X Yosemite on ESXi Install, configure and snapshot Mac OS X Yosemite. This requires configuring the vmx file then installing Mac OS X Mountain Lion and upgrading.",2,DM-7075,datamanagement,install mac os yosemite esxi install configure snapshot mac os yosemite require configure vmx file instal mac os mountain lion upgrading,"Install Mac OS X Yosemite on ESXi Install, configure and snapshot Mac OS X Yosemite. This requires configuring the vmx file then installing Mac OS X Mountain Lion and upgrading."
"Install Mac OS X El Capitan on ESXi Install, configure and snapshot Mac OS X El Capitan. This requires configuring the vmx file then installing Mac OS X Mountain Lion and upgrading.",2,DM-7076,datamanagement,install mac os el capitan esxi install configure snapshot mac os el capitan require configure vmx file instal mac os mountain lion upgrading,"Install Mac OS X El Capitan on ESXi Install, configure and snapshot Mac OS X El Capitan. This requires configuring the vmx file then installing Mac OS X Mountain Lion and upgrading."
"Install MacOS Sierra on ESXi Install, configure and snapshot MacOS Sierra. This requires configuring the vmx file then installing Mac OS X Mountain Lion and upgrading.",1,DM-7077,datamanagement,install macos sierra esxi install configure snapshot macos sierra require configure vmx file instal mac os mountain lion upgrading,"Install MacOS Sierra on ESXi Install, configure and snapshot MacOS Sierra. This requires configuring the vmx file then installing Mac OS X Mountain Lion and upgrading."
"Firewall and SSH configuration on ESXi Figure out and configure the firewall, ssh server and ssh client for ESXi.    This isn't especially well documented since it's part of VMWare vSphere.    This part specifically was time consuming since most users by vSphere.",2,DM-7078,datamanagement,firewall ssh configuration esxi figure configure firewall ssh server ssh client esxi especially document vmware vsphere specifically time consume user vsphere,"Firewall and SSH configuration on ESXi Figure out and configure the firewall, ssh server and ssh client for ESXi. This isn't especially well documented since it's part of VMWare vSphere. This part specifically was time consuming since most users by vSphere."
Upgrade panopticon to 5.0.0-alpha4 This upgrade requires moving from Topbeat to Metricbeat which requires some minor rework and upgrading the entire system at once.,2,DM-7079,datamanagement,upgrade panopticon 5.0.0 alpha4 upgrade require move topbeat metricbeat require minor rework upgrade entire system,Upgrade panopticon to 5.0.0-alpha4 This upgrade requires moving from Topbeat to Metricbeat which requires some minor rework and upgrading the entire system at once.
"Doxygen isn't updating The current build of our Doxygen documentation, as displayed at https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/index.html, is labelled ""Generated on Mon Jun 27 2016 03:52:22 for LSSTApplications"". At time of writing, that's more than a month ago. Important additions to the documentations made during the last month are missing.  ",1,DM-7080,datamanagement,doxygen update current build doxygen documentation display https://lsst-web.ncsa.illinois.edu/doxygen/x_masterdoxydoc/index.html label generate mon jun 27 2016 03:52:22 lsstapplications time writing month ago important addition documentation month miss,"Doxygen isn't updating The current build of our Doxygen documentation, as displayed at https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/index.html, is labelled ""Generated on Mon Jun 27 2016 03:52:22 for LSSTApplications"". At time of writing, that's more than a month ago. Important additions to the documentations made during the last month are missing."
Airflow Review [Airflow|https://github.com/apache/incubator-airflow] workflow management system against criteria defined in the epic.,6,DM-7081,datamanagement,airflow review airflow|https://github.com apache incubator airflow workflow management system criterion define epic,Airflow Review [Airflow|https://github.com/apache/incubator-airflow] workflow management system against criteria defined in the epic.
deploy django admin interface fix Test and deploy django admin fix from DM-7071.,1,DM-7082,datamanagement,deploy django admin interface fix test deploy django admin fix dm-7071,deploy django admin interface fix Test and deploy django admin fix from DM-7071.
"Install MySQL and PostgreSQL servers on ccqserv124 Time to run my incomplete L! prototype on real hardware, for that I need MySQL and PostgreSQL servers on a dedicated machine in in2p3 cluster (ccqserv124). Probably start with installing what comes with the system repos before trying latest and greatest stuff.    Both servers need to be configured to allow me create databases/tables, and I only need to enable connections from localhost.  ",1,DM-7083,datamanagement,install mysql postgresql server ccqserv124 time run incomplete prototype real hardware need mysql postgresql server dedicated machine in2p3 cluster ccqserv124 probably start instal come system repos try late great stuff server need configure allow create database table need enable connection localhost,"Install MySQL and PostgreSQL servers on ccqserv124 Time to run my incomplete L! prototype on real hardware, for that I need MySQL and PostgreSQL servers on a dedicated machine in in2p3 cluster (ccqserv124). Probably start with installing what comes with the system repos before trying latest and greatest stuff. Both servers need to be configured to allow me create databases/tables, and I only need to enable connections from localhost."
Astropy views not available on Catalog subclasses Somehow the {{asAstropy}} isn't being inherited by {{BaseCatalog}} subclasses in Python; it's probably getting messed up by the fact that {{Catalog}} is a template and this is added at the Swig level.,1,DM-7084,datamanagement,astropy view available catalog subclass asastropy inherit basecatalog subclasse python probably getting mess fact catalog template add swig level,Astropy views not available on Catalog subclasses Somehow the {{asAstropy}} isn't being inherited by {{BaseCatalog}} subclasses in Python; it's probably getting messed up by the fact that {{Catalog}} is a template and this is added at the Swig level.
Image select panel not yet working correctly with coverage The image select panel needs to be able to modify the coverage image.,4,DM-7088,datamanagement,image select panel work correctly coverage image select panel need able modify coverage image,Image select panel not yet working correctly with coverage The image select panel needs to be able to modify the coverage image.
"IrsaViewer catalog panel, labels and input fields moved as you type Catalog search panel in IrsaViewer, the target panel label, feedback, and input box are jumping as input is being typed.  Their position should be fixed.",2,DM-7090,datamanagement,irsaviewer catalog panel label input field move type catalog search panel irsaviewer target panel label feedback input box jump input type position fix,"IrsaViewer catalog panel, labels and input fields moved as you type Catalog search panel in IrsaViewer, the target panel label, feedback, and input box are jumping as input is being typed. Their position should be fixed."
"Develop Sphinx configuration for Pipelines Documentation, including MVP HTML/CSS Template This ticket will kick-off a pilot implementation of pipelines documentation in Sphinx. Specific goals are    1. Develop template for sphinx-ready doc/ directories in packages (based on SQR-006)  2. Setup a MVP sphinx template that works well with numpydoc and astropy automodsumm. Simply porting astropy’s sphinx template would be pragmatic.  3. documenteer-driven configuration for sphinx.    These will be MVPs, and iterated upon in later tickets that implement sphinx API docs for stack packages.",1,DM-7094,datamanagement,develop sphinx configuration pipelines documentation include mvp html css template ticket kick pilot implementation pipeline documentation sphinx specific goal develop template sphinx ready doc/ directory package base sqr-006 setup mvp sphinx template work numpydoc astropy automodsumm simply port astropy sphinx template pragmatic documenteer drive configuration sphinx mvp iterate later ticket implement sphinx api doc stack package,"Develop Sphinx configuration for Pipelines Documentation, including MVP HTML/CSS Template This ticket will kick-off a pilot implementation of pipelines documentation in Sphinx. Specific goals are 1. Develop template for sphinx-ready doc/ directories in packages (based on SQR-006) 2. Setup a MVP sphinx template that works well with numpydoc and astropy automodsumm. Simply porting astropy s sphinx template would be pragmatic. 3. documenteer-driven configuration for sphinx. These will be MVPs, and iterated upon in later tickets that implement sphinx API docs for stack packages."
"Run DAX containers at NCSA This is an initial step to manually launch the containerized DAX services on the new PDAC cluster.  This is meant to expose container configuration, account setup, privilege, logging, debugging, etc. issues.",4,DM-7103,datamanagement,run dax container ncsa initial step manually launch containerized dax service new pdac cluster mean expose container configuration account setup privilege log debugging etc issue,"Run DAX containers at NCSA This is an initial step to manually launch the containerized DAX services on the new PDAC cluster. This is meant to expose container configuration, account setup, privilege, logging, debugging, etc. issues."
"support PDAC Qserv deploy Support John in adapting scripts and methodology as necessary to support qserv deploy on the PDAC cluster at NCSA, as is currently done at IN2P3.  ",8,DM-7104,datamanagement,support pdac qserv deploy support john adapt script methodology necessary support qserv deploy pdac cluster ncsa currently in2p3,"support PDAC Qserv deploy Support John in adapting scripts and methodology as necessary to support qserv deploy on the PDAC cluster at NCSA, as is currently done at IN2P3."
"Deliver revised slides for Joint Status Review Deliver a modified version of the slides from the July 2016 Joint Directors (sic) Review, plus service any other requests from Project Management.",6,DM-7107,datamanagement,deliver revise slide joint status review deliver modify version slide july 2016 joint directors sic review plus service request project management,"Deliver revised slides for Joint Status Review Deliver a modified version of the slides from the July 2016 Joint Directors (sic) Review, plus service any other requests from Project Management."
"Provide updated F16 DRP plan for PMCS ingest Only the first three months of F16 were concretely resource loaded and ingested into PMCS at the start of the cycle. A provisional plan was loaded for the remaining three months. Check, refine and update than plan as necessary.",4,DM-7108,datamanagement,provide update f16 drp plan pmcs ingest month f16 concretely resource load ingest pmcs start cycle provisional plan load remain month check refine update plan necessary,"Provide updated F16 DRP plan for PMCS ingest Only the first three months of F16 were concretely resource loaded and ingested into PMCS at the start of the cycle. A provisional plan was loaded for the remaining three months. Check, refine and update than plan as necessary."
