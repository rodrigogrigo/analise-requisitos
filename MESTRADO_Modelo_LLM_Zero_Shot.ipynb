{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUSyX33m2YMEwdecFQob16",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigogrigo/analise-requisitos/blob/main/MESTRADO_Modelo_LLM_Zero_Shot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIi_BZc3KWH8",
        "outputId": "fcad868a-a6f3-4bb6-aa85-34c6465ac2cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            " Carregando modelo: gemma_2b\n",
            "==((====))==  Unsloth 2025.5.6: Fast Gemma patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "‚úÖ Resultados exportados: /content/drive/MyDrive/projetoMestrado/storypoints_inferidos_gemma_2b.csv\n",
            "üìä MAE para o modelo gemma_2b: 2.87\n",
            "\n",
            " Carregando modelo: llama3_8b\n",
            "==((====))==  Unsloth 2025.5.6: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "‚úÖ Resultados exportados: /content/drive/MyDrive/projetoMestrado/storypoints_inferidos_llama3_8b.csv\n",
            "üìä MAE para o modelo llama3_8b: 2.86\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U unsloth\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Lista de modelos a testar: (apelido, nome do modelo Unsloth)\n",
        "modelos_para_testar = [\n",
        "    (\"gemma_2b\", \"unsloth/gemma-2b-it-bnb-4bit\"),\n",
        "    (\"llama3_8b\", \"unsloth/llama-3-8b-Instruct-bnb-4bit\")\n",
        "]\n",
        "\n",
        "# Prompt com instru√ß√£o clara, exemplos e separador ###\n",
        "system_role = (\n",
        "    \"Voc√™ √© um engenheiro de software s√™nior respons√°vel por estimar o esfor√ßo necess√°rio para concluir tarefas, \"\n",
        "    \"com base em suas descri√ß√µes. Utilize a m√©trica de story points, que representa o esfor√ßo relativo.\\n\\n\"\n",
        "    \"Os valores v√°lidos s√£o baseados na sequ√™ncia de Fibonacci: 1, 2, 3, 5 e 8.\\n\"\n",
        "    \"- 1: tarefa muito simples\\n\"\n",
        "    \"- 2 ou 3: tarefa simples ou moderada\\n\"\n",
        "    \"- 5: tarefa complexa\\n\"\n",
        "    \"- 8: tarefa de alto esfor√ßo\\n\\n\"\n",
        "    \"IMPORTANTE:\\n\"\n",
        "    \"- Responda com apenas um n√∫mero: 1, 2, 3, 5 ou 8\\n\"\n",
        "    \"- N√£o adicione frases ou s√≠mbolos\\n\"\n",
        "    \"- A resposta deve vir ap√≥s o marcador ###\\n\\n\"\n",
        "    \"Exemplos:\\n\"\n",
        "    \"Descri√ß√£o da tarefa:\\nAdicionar autentica√ß√£o com OAuth2.\\n###\\n5\\n\\n\"\n",
        "    \"Descri√ß√£o da tarefa:\\nCorrigir erro de digita√ß√£o em mensagem de boas-vindas.\\n###\\n1\\n\\n\"\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"Descri√ß√£o da tarefa:\n",
        "{}\n",
        "\n",
        "###\"\"\"\n",
        "\n",
        "# Caminho para o dataset unificado\n",
        "csv_path = \"/content/drive/MyDrive/projetoMestrado/datasets_all_processados/moodle_unificado_processed.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Itera sobre os modelos e executa a infer√™ncia para cada um\n",
        "for apelido_modelo, nome_modelo_unsloth in modelos_para_testar:\n",
        "    print(f\"\\n Carregando modelo: {apelido_modelo}\")\n",
        "\n",
        "    # Carregar modelo\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=nome_modelo_unsloth,\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "        dtype=None,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    resultados = []\n",
        "\n",
        "    # Loop de infer√™ncia\n",
        "    for _, row in df.iterrows():\n",
        "        descricao = row[\"description\"]\n",
        "        issue_id = row[\"issuekey\"]\n",
        "        storypoint_real = row[\"storypoint\"]\n",
        "\n",
        "        prompt = prompt_template.format(descricao)\n",
        "        full_prompt = f\"{system_role}\\n{prompt}\"\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            full_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048,\n",
        "            padding=True\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs.get(\"attention_mask\", None),\n",
        "            max_new_tokens=3,\n",
        "            temperature=0.1,\n",
        "        )\n",
        "\n",
        "\n",
        "        result_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        raw_output = result_text.split(\"###\")[-1].strip()\n",
        "\n",
        "        resultados.append({\n",
        "            \"dataset\": \"moodle\",\n",
        "            \"id\": issue_id,\n",
        "            \"descricao\": descricao,\n",
        "            \"storypoint_previsto\": raw_output,\n",
        "            \"storypoint_real\": storypoint_real\n",
        "        })\n",
        "\n",
        "    df_result = pd.DataFrame(resultados)\n",
        "\n",
        "    # Tentar converter as colunas para int (somente v√°lidos)\n",
        "    df_result[\"storypoint_previsto_int\"] = pd.to_numeric(df_result[\"storypoint_previsto\"], errors=\"coerce\")\n",
        "    df_result[\"storypoint_real_int\"] = pd.to_numeric(df_result[\"storypoint_real\"], errors=\"coerce\")\n",
        "\n",
        "    # Remover valores inv√°lidos (NaN)\n",
        "    df_validos = df_result.dropna(subset=[\"storypoint_previsto_int\", \"storypoint_real_int\"])\n",
        "\n",
        "    # Calcular MAE\n",
        "    mae = (df_validos[\"storypoint_previsto_int\"] - df_validos[\"storypoint_real_int\"]).abs().mean()\n",
        "\n",
        "    # Exportar resultados\n",
        "    output_path = f\"/content/drive/MyDrive/projetoMestrado/storypoints_inferidos_{apelido_modelo}.csv\"\n",
        "    df_result.to_csv(output_path, index=False)\n",
        "    print(f\"‚úÖ Resultados exportados: {output_path}\")\n",
        "    print(f\"üìä MAE para o modelo {apelido_modelo}: {mae:.2f}\")\n",
        "\n",
        "    # üîÅ Limpar mem√≥ria GPU antes do pr√≥ximo modelo\n",
        "    import gc\n",
        "    del model\n",
        "    del tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2RXvJyFGTHdZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}